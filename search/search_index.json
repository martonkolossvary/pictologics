{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Pictologics","text":"<p>Pictologics is a pure python, IBSI 1 and 2 compliant library for radiomic feature extraction from medical images.</p> <p>See also the NOTICE file for attribution and third-party library information.</p>"},{"location":"#why-pictologics","title":"Why Pictologics?","text":"<ul> <li>\ud83d\ude80 High Performance: Uses <code>numba</code> for Just In Time (JIT) compilation, achieving significant speedups over other libraries (speedups between 15-300x compared to pyradiomics, see Benchmarks page for details).</li> <li>\u2705 IBSI Compliant: Implements standard algorithms verified against the IBSI digital and CT phantoms, and clinical datasets:<ul> <li>IBSI 1: Feature extraction (compliance report)</li> <li>IBSI 2: Image filters (filter compliance)</li> </ul> </li> <li>\ud83d\udd27 Versatile: Provides utilities for DICOM parsing and common scientific image processing tasks. Natively supports common image formats (NIfTI, DICOM, DICOM-SEG, DICOM-SR).</li> <li>\u2728 User-Friendly: Pure Python implementation with a simple installation process and user-friendly pipeline module supporting easy feature extraction and analysis, ensuring a smooth experience from setup to analysis.</li> <li>\ud83d\udee0\ufe0f Actively Maintained: Continuously maintained and developed with the intention to provide robust latent radiomic features that can reliably describe morphological characteristics of diseases on radiological images.</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Loaders: Support for NIfTI and DICOM image, segmentation (DICOM-SEG), and report (DICOM-SR) formats.</li> <li>Preprocessing: Resampling, resegmentation, outlier filtering, discretisation and others.</li> <li>Features:<ul> <li>Morphology: Volume, Surface Area, Compactness, etc.</li> <li>Intensity: Mean, Median, Skewness, Kurtosis, etc.</li> <li>Texture: GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM.</li> </ul> </li> <li>Filters: IBSI 2-compliant convolutional filters including Mean, LoG, Laws, Gabor, Wavelets, Simoncelli and others.</li> <li>Configuration Management: Export/import pipeline configurations in YAML/JSON. Share reproducible workflows, version control settings, and ensure consistent multi-site processing.</li> <li>Utilities: Built-in DICOM database parsing, organization and viewing tools.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Install: Follow the Installation guide.</li> <li>Learn: Check the Feature Calculations guide.</li> <li>Reference: Explore the API Documentation.</li> </ol>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#031-2026-01-31","title":"0.3.1 - 2026-01-31","text":""},{"location":"CHANGELOG/#added","title":"Added","text":"<ul> <li>Pipeline Configuration Serialization: Full YAML/JSON export/import for <code>RadiomicsPipeline</code> configurations:<ul> <li><code>save_configs()</code> / <code>load_configs()</code>: File-based configuration persistence</li> <li><code>to_yaml()</code> / <code>from_yaml()</code>: String-based YAML serialization</li> <li><code>to_json()</code> / <code>from_json()</code>: String-based JSON serialization</li> <li><code>to_dict()</code> / <code>from_dict()</code>: Dictionary conversion for programmatic use</li> </ul> </li> <li>Configuration Management Methods:<ul> <li><code>add_config()</code>: Register custom configurations</li> <li><code>get_config()</code>: Retrieve configuration by name (deep copy)</li> <li><code>remove_config()</code>: Delete configurations</li> <li><code>list_configs()</code>: List all registered configuration names</li> <li><code>merge_configs()</code>: Combine configurations from multiple pipelines</li> </ul> </li> <li>Template System: YAML-based configuration templates in <code>pictologics/templates/</code>:<ul> <li>Standard configurations now loaded from <code>standard_configs.yaml</code></li> <li>Template loading API: <code>list_template_files()</code>, <code>load_template_file()</code>, <code>get_standard_templates()</code>, <code>get_all_templates()</code>, <code>get_template_metadata()</code></li> </ul> </li> <li>Schema Versioning: Configuration files include <code>schema_version</code> for forward compatibility and automatic migration</li> <li>Configuration Validation: Opt-in validation via <code>validate=True</code> parameter logs warnings for unknown steps/parameters</li> <li>Documentation: New \"Predefined Configurations\" user guide page with comprehensive examples including end-to-end multi-site study workflow</li> </ul>"},{"location":"CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Standard configurations (<code>standard_fbn_*</code>, <code>standard_fbs_*</code>) now loaded from YAML templates instead of hardcoded dictionaries</li> <li>Updated pipeline.md documentation with condensed configuration section and cross-references</li> </ul>"},{"location":"CHANGELOG/#dependencies","title":"Dependencies","text":"<ul> <li>Added <code>pyyaml&gt;=6.0</code> as core dependency for YAML serialization</li> </ul>"},{"location":"CHANGELOG/#030-2026-01-25","title":"0.3.0 - 2026-01-25","text":""},{"location":"CHANGELOG/#added_1","title":"Added","text":"<ul> <li>IBSI 2 Convolutional Filters: Complete filter module (<code>pictologics/filters/</code>) with:<ul> <li>Mean filter (3D)</li> <li>Laplacian of Gaussian (LoG)</li> <li>Laws texture energy filters (3D rotation-invariant)</li> <li>Gabor filters (2D per-slice)</li> <li>Wavelet decomposition (Haar, Daubechies, Coiflet, Symlet families)</li> <li>Simoncelli steerable pyramid</li> </ul> </li> <li>IBSI 2 Phase 1 Compliance: Filter response map validation against digital phantoms</li> <li>IBSI 2 Phase 2 Compliance: Feature extraction from filtered images validated</li> <li>IBSI 2 Phase 3 Compliance: Multi-modality reproducibility validation across 51 patients \u00d7 3 modalities compared to 9 team submissions</li> <li>Filter Pipeline Integration: New <code>filter</code> step in <code>RadiomicsPipeline</code> for seamless filtered feature extraction</li> <li>Mask Binarization Pipeline Step: New <code>binarize_mask</code> preprocessing step with configurable <code>threshold</code>, <code>mask_values</code> (int/list/range tuple), and <code>apply_to</code> targeting.</li> </ul>"},{"location":"CHANGELOG/#changed_1","title":"Changed","text":"<ul> <li>Updated <code>mkdocs.yml</code> with IBSI 2 Phase 1, 2, 3 navigation</li> <li>Expanded pipeline documentation with filter usage examples and binarization</li> </ul>"},{"location":"CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>IBSI 1 Compliance (Morphology): Achieved passing values for Compactness 2 (<code>BQWJ</code>) and Asphericity (<code>25C7</code>) in Configs C/D/E and texture matrices in config D by binarizing masks before resampling.</li> </ul>"},{"location":"CHANGELOG/#020-2026-01-06","title":"0.2.0 - 2026-01-06","text":""},{"location":"CHANGELOG/#added_2","title":"Added","text":"<ul> <li>DICOM Database Utility: <code>DicomDatabase</code> class for parsing complex DICOM folder hierarchies with Patient \u2192 Study \u2192 Series \u2192 Instance traversal, multi-phase detection, and DataFrame/JSON/CSV exports</li> <li>DICOM SEG Loader: <code>load_seg()</code> for loading DICOM Segmentation objects with multi-segment handling, geometry alignment, and seamless auto-detection in <code>load_image()</code></li> <li>DICOM SR Parser: <code>SRDocument</code> class for parsing Structured Reports with measurement extraction, CSV/JSON export, and batch processing via <code>SRDocument.from_folders()</code></li> <li>DICOM Multi-Phase Support: <code>load_image()</code> now supports multi-phase DICOM series with <code>dataset_index</code>, plus <code>get_dicom_phases()</code> for phase discovery</li> <li>Visualization Utility: <code>visualize_slices()</code> for interactive viewing and <code>save_slices()</code> for batch export with window/level normalization and colormap options</li> <li>Cropped Image Repositioning: <code>load_image()</code> and <code>load_and_merge_images()</code> support repositioning cropped masks into reference volume coordinate space</li> <li>Intensity Rescaling: <code>apply_rescale</code> parameter in <code>load_image</code> and related functions to toggle DICOM rescale slope/intercept application (default: True)</li> <li>Sentinel Value Handling: Documentation and examples for handling sentinel values (e.g. -2048 in Siemens DICOMs) using the <code>resegment</code> preprocessing step</li> <li>Dependencies: Added <code>highdicom</code>, <code>matplotlib</code>, <code>pillow</code>; updated <code>pandas&gt;=2.0.0</code></li> </ul>"},{"location":"CHANGELOG/#optimized","title":"Optimized","text":"<ul> <li>Morphology Speedup: Implemented bounding box cropping for morphology features (mesh/moments), significantly accelerating extraction for sparse ROIs in large volumes</li> <li>Texture Speedup: Added slice-level skipping to texture calculation to ignore empty z-slices, vastly improving performance for disjoint ROIs (e.g. multiple tumors)</li> </ul>"},{"location":"CHANGELOG/#fixed_1","title":"Fixed","text":"<ul> <li>DICOM file loading improvements: proper Z-spacing, 3D SEG handling, direction matrix extraction</li> </ul>"},{"location":"CHANGELOG/#changed_2","title":"Changed","text":"<ul> <li><code>DicomDatabase</code> uses shared <code>split_dicom_phases()</code> for consistent multi-phase detection</li> <li>Comprehensive documentation updates for all new utilities</li> </ul>"},{"location":"CHANGELOG/#010-2025-12-28","title":"0.1.0 - 2025-12-28","text":"<p>Initial commit</p>"},{"location":"LICENSE/","title":"License","text":"<p>Apache License                            Version 2.0, January 2004                         http://www.apache.org/licenses/</p> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright 2026 M\u00e1rton Kolossv\u00e1ry</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"NOTICE/","title":"Notice","text":"<p>Pictologics Copyright 2026 M\u00e1rton Kolossv\u00e1ry</p> <p>This product includes software developed by M\u00e1rton Kolossv\u00e1ry.</p> <p>This product contains code derived from the Image Biomarker Standardisation Initiative (IBSI) reference implementations and guidelines. https://theibsi.github.io/</p> <p>This product uses the following third-party libraries:</p> <ul> <li>NumPy (BSD 3-Clause) - https://numpy.org/</li> <li>SciPy (BSD 3-Clause) - https://scipy.org/</li> <li>Numba (BSD 2-Clause) - https://numba.pydata.org/</li> <li>NiBabel (MIT) - https://nipy.org/nibabel/</li> <li>PyDICOM (MIT) - https://pydicom.github.io/</li> <li>highdicom (MIT) - https://github.com/ImagingDataCommons/highdicom</li> <li>pandas (BSD 3-Clause) - https://pandas.pydata.org/</li> <li>PyMCubes (BSD 3-Clause) - https://github.com/pmneila/PyMCubes</li> <li>tqdm (MIT/MPL 2.0) - https://tqdm.github.io/</li> <li>Matplotlib (PSF-based) - https://matplotlib.org/</li> <li>Pillow (MIT-CMU) - https://github.com/python-pillow/Pillow</li> <li>PyWavelets (MIT) - https://pywavelets.readthedocs.io/</li> <li>PyYAML (MIT) - https://github.com/yaml/pyyaml</li> </ul>"},{"location":"benchmarks/","title":"Benchmarks","text":""},{"location":"benchmarks/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"benchmarks/#benchmark-configuration","title":"Benchmark Configuration","text":"<p>Comparisons between Pictologics and PyRadiomics (single-thread parity). </p> <p>Test Data Generation:</p> <ul> <li>Texture: 3D correlated noise generated using Gaussian smoothing.</li> <li>Mask: Blob-like structures generated via thresholded smooth noise with random holes.</li> <li>Voxel Distribution: Mean=486.04, Std=90.24, Min=0.00, Max=1000.00.</li> </ul>"},{"location":"benchmarks/#hardware-used-for-calculations","title":"Hardware Used for Calculations","text":"<ul> <li>Hardware: Apple M4 Pro, 14 cores, 48 GB</li> <li>OS: macOS 26.2 (arm64)</li> <li>Python: 3.12.10</li> <li>Core deps: pictologics 0.3.1, numpy 2.2.6, scipy 1.17.0, numba 0.62.1, pandas 2.3.3, matplotlib 3.10.7</li> <li>PyRadiomics stack (parity runs): pyradiomics 3.1.1.dev111+g8ed579383, SimpleITK 2.5.3</li> <li>BLAS/LAPACK: Apple Accelerate (from <code>numpy.show_config()</code>)</li> </ul> <p>Note: the benchmark script explicitly calls <code>warmup_jit()</code> before timing to avoid including Numba compilation overhead in the measured runtimes. All calculations are repeated 5 times and the average runtime is reported.</p>"},{"location":"benchmarks/#intensity","title":"Intensity","text":"Execution Time (Log-Log) Speedup <p>Pictologics-only intensity families (IVH + spatial/local intensity):</p> Size Discretization Pictologics-only Time Pictologics-only Mem 25 FBS 10.0 0.0308 s 0.84 MB 25 FBS 25.0 0.0299 s 0.81 MB 25 FBS 50.0 0.0291 s 0.81 MB 25 FBN 16 0.0285 s 0.81 MB 25 FBN 32 0.0293 s 0.81 MB 25 FBN 64 0.0299 s 0.81 MB 50 FBS 10.0 1.2846 s 6.33 MB 50 FBS 25.0 1.2816 s 6.33 MB 50 FBS 50.0 1.2814 s 6.33 MB 50 FBN 16 1.2738 s 6.33 MB 50 FBN 32 1.2653 s 6.33 MB 50 FBN 64 1.2743 s 6.33 MB 75 FBS 10.0 Not calculated Not calculated 75 FBS 25.0 Not calculated Not calculated 75 FBS 50.0 Not calculated Not calculated 75 FBN 16 Not calculated Not calculated 75 FBN 32 Not calculated Not calculated 75 FBN 64 Not calculated Not calculated 100 FBS 10.0 Not calculated Not calculated 100 FBS 25.0 Not calculated Not calculated 100 FBS 50.0 Not calculated Not calculated 100 FBN 16 Not calculated Not calculated 100 FBN 32 Not calculated Not calculated 100 FBN 64 Not calculated Not calculated"},{"location":"benchmarks/#morphology","title":"Morphology","text":"Execution Time (Log-Log) Speedup <p>Pictologics-only morphology families (intensity-weighted morphology):</p> Size Discretization Pictologics-only Time Pictologics-only Mem 25 FBS 10.0 0.0041 s 1.17 MB 25 FBS 25.0 0.0040 s 1.17 MB 25 FBS 50.0 0.0041 s 1.17 MB 25 FBN 16 0.0041 s 1.17 MB 25 FBN 32 0.0041 s 1.17 MB 25 FBN 64 0.0043 s 1.17 MB 50 FBS 10.0 0.0109 s 5.43 MB 50 FBS 25.0 0.0106 s 5.43 MB 50 FBS 50.0 0.0110 s 5.43 MB 50 FBN 16 0.0108 s 5.43 MB 50 FBN 32 0.0110 s 5.43 MB 50 FBN 64 0.0108 s 5.43 MB 75 FBS 10.0 0.0171 s 8.84 MB 75 FBS 25.0 0.0173 s 8.84 MB 75 FBS 50.0 0.0172 s 8.84 MB 75 FBN 16 0.0174 s 8.84 MB 75 FBN 32 0.0173 s 8.84 MB 75 FBN 64 0.0172 s 8.84 MB 100 FBS 10.0 0.0330 s 20.46 MB 100 FBS 25.0 0.0336 s 20.46 MB 100 FBS 50.0 0.0337 s 20.46 MB 100 FBN 16 0.0333 s 20.46 MB 100 FBN 32 0.0336 s 20.46 MB 100 FBN 64 0.0338 s 20.46 MB"},{"location":"benchmarks/#texture","title":"Texture","text":"Execution Time (Log-Log) Speedup <p>Pictologics-only texture families (GLDZM):</p> Size Discretization Pictologics-only Time Pictologics-only Mem 25 FBS 10.0 0.0002 s 0.15 MB 25 FBS 25.0 0.0002 s 0.13 MB 25 FBS 50.0 0.0002 s 0.13 MB 25 FBN 16 0.0002 s 0.13 MB 25 FBN 32 0.0002 s 0.13 MB 25 FBN 64 0.0002 s 0.14 MB 50 FBS 10.0 0.0003 s 0.16 MB 50 FBS 25.0 0.0002 s 0.14 MB 50 FBS 50.0 0.0003 s 0.13 MB 50 FBN 16 0.0002 s 0.13 MB 50 FBN 32 0.0002 s 0.14 MB 50 FBN 64 0.0002 s 0.15 MB 75 FBS 10.0 0.0003 s 0.21 MB 75 FBS 25.0 0.0003 s 0.16 MB 75 FBS 50.0 0.0003 s 0.14 MB 75 FBN 16 0.0003 s 0.14 MB 75 FBN 32 0.0003 s 0.15 MB 75 FBN 64 0.0003 s 0.18 MB 100 FBS 10.0 0.0004 s 0.20 MB 100 FBS 25.0 0.0004 s 0.16 MB 100 FBS 50.0 0.0003 s 0.14 MB 100 FBN 16 0.0003 s 0.14 MB 100 FBN 32 0.0005 s 0.15 MB 100 FBN 64 0.0004 s 0.17 MB"},{"location":"benchmarks/#filters","title":"Filters","text":"Execution Time (Log-Log) Speedup <p>Pictologics-only filters (Gabor, Laws, Simoncelli, Riesz, Mean):</p> Size Discretization Pictologics-only Time Pictologics-only Mem 25 FBS 10.0 0.0088 s 1.70 MB 25 FBS 25.0 0.0092 s 1.70 MB 25 FBS 50.0 0.0086 s 1.70 MB 25 FBN 16 0.0094 s 1.70 MB 25 FBN 32 0.0091 s 1.70 MB 25 FBN 64 0.0088 s 1.70 MB 50 FBS 10.0 0.0285 s 13.50 MB 50 FBS 25.0 0.0273 s 13.50 MB 50 FBS 50.0 0.0272 s 13.50 MB 50 FBN 16 0.0280 s 13.50 MB 50 FBN 32 0.0285 s 13.50 MB 50 FBN 64 0.0277 s 13.50 MB 75 FBS 10.0 0.0555 s 45.50 MB 75 FBS 25.0 0.0652 s 45.52 MB 75 FBS 50.0 0.0562 s 45.50 MB 75 FBN 16 0.0557 s 45.50 MB 75 FBN 32 0.0563 s 45.50 MB 75 FBN 64 0.0561 s 45.50 MB 100 FBS 10.0 0.1023 s 107.80 MB 100 FBS 25.0 0.1029 s 107.80 MB 100 FBS 50.0 0.1026 s 107.80 MB 100 FBN 16 0.1041 s 107.80 MB 100 FBN 32 0.1053 s 107.80 MB 100 FBN 64 0.1061 s 107.80 MB"},{"location":"benchmarks/#detailed-parity-results","title":"Detailed Parity Results","text":"Family Size Discretization Pictologics Time PyRadiomics Time Speedup Pictologics Mem PyRadiomics Mem Filters 25 FBN 16 0.0011 s 0.0045 s 4.12x 0.48 MB 0.94 MB Filters 25 FBN 32 0.0011 s 0.0044 s 3.98x 0.48 MB 0.94 MB Filters 25 FBN 64 0.0010 s 0.0048 s 4.73x 0.48 MB 0.94 MB Filters 25 FBS 10.0 0.0011 s 0.0045 s 4.20x 0.48 MB 0.94 MB Filters 25 FBS 25.0 0.0011 s 0.0045 s 4.22x 0.48 MB 0.94 MB Filters 25 FBS 50.0 0.0010 s 0.0046 s 4.70x 0.48 MB 0.94 MB Filters 50 FBN 16 0.0058 s 0.0099 s 1.69x 3.82 MB 6.68 MB Filters 50 FBN 32 0.0056 s 0.0096 s 1.72x 3.82 MB 6.68 MB Filters 50 FBN 64 0.0055 s 0.0098 s 1.77x 3.82 MB 6.68 MB Filters 50 FBS 10.0 0.0056 s 0.0097 s 1.72x 3.82 MB 6.68 MB Filters 50 FBS 25.0 0.0055 s 0.0096 s 1.75x 3.82 MB 6.68 MB Filters 50 FBS 50.0 0.0056 s 0.0097 s 1.73x 3.82 MB 6.68 MB Filters 75 FBN 16 0.0184 s 0.0273 s 1.49x 12.88 MB 23.38 MB Filters 75 FBN 32 0.0182 s 0.0255 s 1.40x 12.88 MB 23.38 MB Filters 75 FBN 64 0.0186 s 0.0257 s 1.38x 12.88 MB 23.38 MB Filters 75 FBS 10.0 0.0184 s 0.0255 s 1.39x 12.88 MB 23.38 MB Filters 75 FBS 25.0 0.0185 s 0.0261 s 1.41x 12.88 MB 23.38 MB Filters 75 FBS 50.0 0.0183 s 0.0258 s 1.41x 12.88 MB 23.38 MB Filters 100 FBN 16 0.0427 s 0.0521 s 1.22x 30.52 MB 53.41 MB Filters 100 FBN 32 0.0428 s 0.0522 s 1.22x 30.52 MB 53.41 MB Filters 100 FBN 64 0.0429 s 0.0519 s 1.21x 30.52 MB 53.41 MB Filters 100 FBS 10.0 0.0427 s 0.0518 s 1.21x 30.52 MB 53.41 MB Filters 100 FBS 25.0 0.0429 s 0.0526 s 1.22x 30.52 MB 53.41 MB Filters 100 FBS 50.0 0.0430 s 0.0520 s 1.21x 30.52 MB 53.41 MB Intensity 25 FBN 16 0.0008 s 0.0128 s 16.54x 0.24 MB 0.71 MB Intensity 25 FBN 32 0.0008 s 0.0132 s 16.49x 0.24 MB 0.71 MB Intensity 25 FBN 64 0.0009 s 0.0142 s 16.46x 0.24 MB 0.71 MB Intensity 25 FBS 10.0 0.0020 s 0.0139 s 6.93x 0.25 MB 0.75 MB Intensity 25 FBS 25.0 0.0008 s 0.0132 s 15.98x 0.24 MB 0.71 MB Intensity 25 FBS 50.0 0.0008 s 0.0128 s 16.29x 0.24 MB 0.71 MB Intensity 50 FBN 16 0.0029 s 0.0653 s 22.61x 1.40 MB 4.61 MB Intensity 50 FBN 32 0.0029 s 0.0634 s 21.77x 1.40 MB 4.61 MB Intensity 50 FBN 64 0.0029 s 0.0647 s 22.13x 1.40 MB 4.61 MB Intensity 50 FBS 10.0 0.0031 s 0.0673 s 21.81x 1.40 MB 4.61 MB Intensity 50 FBS 25.0 0.0029 s 0.0627 s 21.91x 1.40 MB 4.61 MB Intensity 50 FBS 50.0 0.0028 s 0.0629 s 22.65x 1.40 MB 4.61 MB Intensity 75 FBN 16 0.0106 s 0.2478 s 23.32x 5.81 MB 17.95 MB Intensity 75 FBN 32 0.0116 s 0.2518 s 21.68x 5.81 MB 17.95 MB Intensity 75 FBN 64 0.0111 s 0.2549 s 22.91x 5.81 MB 17.95 MB Intensity 75 FBS 10.0 0.0115 s 0.2546 s 22.09x 5.81 MB 17.95 MB Intensity 75 FBS 25.0 0.0110 s 0.2505 s 22.72x 5.81 MB 17.95 MB Intensity 75 FBS 50.0 0.0108 s 0.2501 s 23.17x 5.81 MB 17.95 MB Intensity 100 FBN 16 0.0205 s 0.5010 s 24.41x 12.16 MB 39.01 MB Intensity 100 FBN 32 0.0215 s 0.5092 s 23.72x 12.16 MB 39.01 MB Intensity 100 FBN 64 0.0224 s 0.5277 s 23.54x 12.16 MB 39.01 MB Intensity 100 FBS 10.0 0.0233 s 0.5191 s 22.26x 12.16 MB 39.01 MB Intensity 100 FBS 25.0 0.0214 s 0.5186 s 24.18x 12.16 MB 39.01 MB Intensity 100 FBS 50.0 0.0210 s 0.5089 s 24.28x 12.16 MB 39.01 MB Morphology 25 FBN 16 0.0041 s 0.0552 s 13.34x 1.17 MB 1.18 MB Morphology 25 FBN 32 0.0039 s 0.0555 s 14.29x 1.17 MB 1.18 MB Morphology 25 FBN 64 0.0041 s 0.0600 s 14.51x 1.17 MB 1.18 MB Morphology 25 FBS 10.0 0.0059 s 0.0578 s 9.80x 1.17 MB 1.18 MB Morphology 25 FBS 25.0 0.0038 s 0.0552 s 14.39x 1.17 MB 1.18 MB Morphology 25 FBS 50.0 0.0040 s 0.0536 s 13.43x 1.17 MB 1.18 MB Morphology 50 FBN 16 0.0104 s 0.9549 s 91.63x 5.43 MB 8.68 MB Morphology 50 FBN 32 0.0107 s 0.9442 s 88.64x 5.43 MB 8.68 MB Morphology 50 FBN 64 0.0108 s 0.9481 s 87.89x 5.43 MB 8.68 MB Morphology 50 FBS 10.0 0.0109 s 0.9493 s 87.17x 5.43 MB 8.68 MB Morphology 50 FBS 25.0 0.0105 s 0.9370 s 89.51x 5.43 MB 8.68 MB Morphology 50 FBS 50.0 0.0108 s 0.9404 s 86.74x 5.43 MB 8.68 MB Morphology 75 FBN 16 0.0170 s 1.6906 s 99.40x 8.84 MB 36.60 MB Morphology 75 FBN 32 0.0180 s 1.6791 s 93.43x 8.84 MB 36.60 MB Morphology 75 FBN 64 0.0169 s 1.6808 s 99.51x 8.84 MB 36.60 MB Morphology 75 FBS 10.0 0.0171 s 1.7180 s 100.54x 8.84 MB 36.60 MB Morphology 75 FBS 25.0 0.0170 s 1.7095 s 100.64x 8.84 MB 36.60 MB Morphology 75 FBS 50.0 0.0172 s 1.6944 s 98.26x 8.84 MB 36.60 MB Morphology 100 FBN 16 0.0334 s 8.3787 s 250.84x 20.46 MB 77.49 MB Morphology 100 FBN 32 0.0332 s 8.3449 s 251.43x 20.46 MB 77.49 MB Morphology 100 FBN 64 0.0338 s 8.3437 s 247.12x 20.46 MB 77.49 MB Morphology 100 FBS 10.0 0.0333 s 8.5437 s 256.69x 20.46 MB 77.49 MB Morphology 100 FBS 25.0 0.0331 s 8.4334 s 254.90x 20.46 MB 77.49 MB Morphology 100 FBS 50.0 0.0333 s 8.4154 s 252.61x 20.46 MB 77.49 MB Texture 25 FBN 16 0.0050 s 0.0489 s 9.83x 2.13 MB 0.69 MB Texture 25 FBN 32 0.0054 s 0.0523 s 9.63x 2.07 MB 0.75 MB Texture 25 FBN 64 0.0056 s 0.0672 s 11.95x 4.53 MB 1.95 MB Texture 25 FBS 10.0 0.0072 s 0.0957 s 13.29x 10.10 MB 3.77 MB Texture 25 FBS 25.0 0.0052 s 0.0537 s 10.26x 2.16 MB 0.91 MB Texture 25 FBS 50.0 0.0051 s 0.0488 s 9.59x 2.13 MB 0.71 MB Texture 50 FBN 16 0.0223 s 0.2953 s 13.25x 19.85 MB 6.29 MB Texture 50 FBN 32 0.0236 s 0.2995 s 12.67x 19.95 MB 6.29 MB Texture 50 FBN 64 0.0177 s 0.3159 s 17.85x 9.83 MB 4.82 MB Texture 50 FBS 10.0 0.0168 s 0.3482 s 20.76x 11.96 MB 5.41 MB Texture 50 FBS 25.0 0.0222 s 0.3027 s 13.63x 19.30 MB 6.10 MB Texture 50 FBS 50.0 0.0256 s 0.2978 s 11.65x 21.08 MB 6.60 MB Texture 75 FBN 16 0.0772 s 1.1898 s 15.40x 84.43 MB 25.62 MB Texture 75 FBN 32 0.0762 s 1.1950 s 15.69x 86.59 MB 26.23 MB Texture 75 FBN 64 0.0712 s 1.2108 s 17.01x 63.72 MB 20.14 MB Texture 75 FBS 10.0 0.0483 s 1.2486 s 25.83x 13.36 MB 17.08 MB Texture 75 FBS 25.0 0.0703 s 1.2092 s 17.21x 89.44 MB 25.80 MB Texture 75 FBS 50.0 0.0741 s 1.2021 s 16.23x 88.88 MB 25.65 MB Texture 100 FBN 16 0.1634 s 2.4462 s 14.97x 213.37 MB 64.12 MB Texture 100 FBN 32 0.1812 s 2.4564 s 13.56x 224.11 MB 66.17 MB Texture 100 FBN 64 0.1989 s 2.4925 s 12.53x 204.71 MB 60.72 MB Texture 100 FBS 10.0 0.0992 s 2.5416 s 25.61x 22.17 MB 37.15 MB Texture 100 FBS 25.0 0.1862 s 2.4889 s 13.36x 230.74 MB 68.10 MB Texture 100 FBS 50.0 0.1802 s 2.4781 s 13.75x 229.77 MB 68.48 MB"},{"location":"ibsi1_compliance/","title":"Comprehensive IBSI Benchmark Results","text":""},{"location":"ibsi1_compliance/#how-to-run-the-benchmarks","title":"How to Run the Benchmarks","text":"<p>To reproduce these results or run the IBSI compliance benchmarks on your own system:</p>"},{"location":"ibsi1_compliance/#1-download-the-data","title":"1. Download the Data","text":"<p>The IBSI reference datasets (Digital Phantom and Lung Cancer CT) are available on the IBSI GitHub repository.</p> <ul> <li>Digital Phantom: Download <code>phantom.nii.gz</code> and <code>mask.nii.gz</code> from the <code>digital_phantom</code> folder.</li> <li>Lung Cancer CT: Download the <code>PAT1</code> NIfTI files. Note that IBSI recommends converting these to at least 32-bit floating point and rounding to the nearest integer before processing.</li> </ul> <p>Place these files in <code>dev/IBSI1/data/</code> or provide their paths to the script.</p>"},{"location":"ibsi1_compliance/#2-run-configurations-programmatically-using-radiomicspipeline","title":"2. Run Configurations Programmatically using <code>RadiomicsPipeline</code>","text":"<p>You can run any of the IBSI configurations programmatically using the <code>RadiomicsPipeline</code> class.</p> <pre><code>from pictologics.pipeline import RadiomicsPipeline\nfrom pictologics.loader import load_image\n\n# 1. Initialize Pipeline\npipeline = RadiomicsPipeline()\n\n# --- DEFINE CONFIGURATIONS ---\n\n# A. Digital Phantom Config (FBS 1.0, no resampling)\nconfig_digital_phantom = [\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": 1.0}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"], \"include_spatial_intensity\": True, \"include_local_intensity\": True}}\n]\n\n# B. Config C (2mm isotropic, resegment [-1000, 400], FBS 25 HU)\nconfig_c = [\n    {\"step\": \"binarize_mask\", \"params\": {\"threshold\": 0.5}},\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (2.0, 2.0, 2.0), \"interpolation\": \"linear\", \"round_intensities\": True}},\n    {\"step\": \"keep_largest_component\", \"params\": {\"apply_to\": \"morph\"}},\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": 25.0, \"min_val\": -1000}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"], \"include_spatial_intensity\": True, \"include_local_intensity\": True, \"ivh_discretisation\": {\"method\": \"FBS\", \"bin_width\": 2.5, \"min_val\": -1000}, \"ivh_params\": {\"target_range_max\": 400}}}\n]\n\n# C. Config D (2mm isotropic, 3-sigma outlier, FBN 32, Continuous IVH)\nconfig_d = [\n    {\"step\": \"binarize_mask\", \"params\": {\"threshold\": 0.5}},\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (2.0, 2.0, 2.0), \"interpolation\": \"linear\", \"round_intensities\": True}},\n    {\"step\": \"keep_largest_component\", \"params\": {\"apply_to\": \"morph\"}},\n    {\"step\": \"filter_outliers\", \"params\": {\"sigma\": 3.0}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"], \"include_spatial_intensity\": True, \"include_local_intensity\": True, \"ivh_use_continuous\": True}}\n]\n\n# D. Config E (Cubic resamp, 3-sigma, round last, FBN 32 for tex, FBN 1000 for IVH)\nconfig_e = [\n    {\"step\": \"binarize_mask\", \"params\": {\"threshold\": 0.5}},\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (2.0, 2.0, 2.0), \"interpolation\": \"cubic\", \"round_intensities\": False}},\n    {\"step\": \"keep_largest_component\", \"params\": {\"apply_to\": \"morph\"}},\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n    {\"step\": \"filter_outliers\", \"params\": {\"sigma\": 3.0}},\n    {\"step\": \"round_intensities\", \"params\": {}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"], \"include_spatial_intensity\": True, \"include_local_intensity\": True, \"ivh_discretisation\": {\"method\": \"FBN\", \"n_bins\": 1000}, \"ivh_params\": {\"bin_width\": 1.0, \"min_val\": 0.5, \"max_val\": 1000.5}}}\n]\n\n# --- RUN PIPELINE ---\n# Choose which config to run:\npipeline.add_config(\"ibsi_config_c\", config_c)\n\n# 3. Load image and mask\n# For Config C/D/E (Lung Cancer CT)\nimage = load_image(\"path/to/CT_image.nii.gz\")\nmask = load_image(\"path/to/CT_mask.nii.gz\")\n\n# 4. Run extraction\nresults = pipeline.run(image, \"ibsi_config_c\", mask=mask)\n\n# 5. Access results\nprint(results[\"ibsi_config_c\"])\n</code></pre> <p>Pictologics considers all data as 3D volumes (even 2D slices are converted to 3D volumes) and does not support 2D radiomic extraction mechanisms. Therefore, benchmark comparisons to configuration A and B are not possible.</p>"},{"location":"ibsi1_compliance/#digital-phantom","title":"Digital Phantom","text":""},{"location":"ibsi1_compliance/#morphology","title":"Morphology","text":"Feature Code Calc Ref Tol Status Volume RNU0 556 556 4 \u2705 PASS Volume voxel counting YEKZ 592 592 4 \u2705 PASS Surface area C0JK 388 388 3 \u2705 PASS Surface to volume ratio 2PR5 0.698 0.698 0.004 \u2705 PASS Compactness 1 SKGS 0.0411 0.0411 0.0003 \u2705 PASS Compactness 2 BQWJ 0.599 0.599 0.004 \u2705 PASS Spherical disproportion KRCK 1.19 1.19 0.01 \u2705 PASS Sphericity QCFX 0.843 0.843 0.005 \u2705 PASS Asphericity 25C7 0.186 0.186 0.001 \u2705 PASS Center of mass shift KLMA 0.672 0.672 0.004 \u2705 PASS Maximum 3D diameter L0JK 13.1 13.1 0.1 \u2705 PASS Major axis length TDIC 11.4 11.4 0.1 \u2705 PASS Minor axis length P9VJ 9.31 9.31 0.06 \u2705 PASS Least axis length 7J51 8.54 8.54 0.05 \u2705 PASS Elongation Q3CK 0.816 0.816 0.005 \u2705 PASS Flatness N17B 0.749 0.749 0.005 \u2705 PASS Volume density (AABB) PBX1 0.869 0.869 0.005 \u2705 PASS Area density (AABB) R59B 0.866 0.866 0.005 \u2705 PASS Volume density (OMBB) ZH1A 0.458 \u2014 \u2014 \u2757 REF. Area density (OMBB) IQYR 0.5673 \u2014 \u2014 \u2757 REF. Volume density (AEE) 6BDE 1.17 1.17 0.01 \u2705 PASS Area density (AEE) RDD2 1.36 1.36 0.01 \u2705 PASS Volume density (MVEE) SWZ1 0.513 \u2014 \u2014 \u2757 REF. Area density (MVEE) BRI8 0.7909 \u2014 \u2014 \u2757 REF. Volume density (convex hull) R3ER 0.961 0.961 0.006 \u2705 PASS Area density (convex hull) 7T7F 1.03 1.03 0.01 \u2705 PASS"},{"location":"ibsi1_compliance/#intensity","title":"Intensity","text":"Feature Code Calc Ref Tol Status Integrated intensity 99N0 1195 1.2e3 10 \u2705 PASS Moran's I index N365 0.0397 0.0397 0.0003 \u2705 PASS Geary's C measure NPT7 0.974 0.974 0.006 \u2705 PASS Local intensity peak VJGA 2.6 2.6 \u2014 \u2705 PASS Global intensity peak 0F91 3.1 3.1 \u2014 \u2705 PASS Mean intensity Q4LE 2.15 2.15 \u2014 \u2705 PASS Intensity variance ECT3 3.05 3.05 \u2014 \u2705 PASS Intensity skewness KE2A 1.08 1.08 \u2014 \u2705 PASS Intensity kurtosis IPH6 -0.355 -0.355 \u2014 \u2705 PASS Median intensity Y12H 1 1 \u2014 \u2705 PASS Minimum intensity 1GSF 1 1 \u2014 \u2705 PASS 10th intensity percentile QG58 1 1 \u2014 \u2705 PASS 90th intensity percentile 8DWT 4 4 \u2014 \u2705 PASS Maximum intensity 84IY 6 6 \u2014 \u2705 PASS Intensity interquartile range SALO 3 3 \u2014 \u2705 PASS Intensity range 2OJQ 5 5 \u2014 \u2705 PASS Intensity Mean absolute deviation 4FUA 1.55 1.55 \u2014 \u2705 PASS Intensity Robust mean absolute deviation 1128 1.11 1.11 \u2014 \u2705 PASS Intensity Median absolute deviation N72L 1.15 1.15 \u2014 \u2705 PASS Intensity Coefficient of variation 7TET 0.812 0.812 \u2014 \u2705 PASS Intensity Quartile coefficient of dispersion 9S40 0.6 0.6 \u2014 \u2705 PASS Intensity energy N8CA 567 567 \u2014 \u2705 PASS Root mean square intensity 5ZWQ 2.77 2.77 \u2014 \u2705 PASS"},{"location":"ibsi1_compliance/#intensity-histogram","title":"Intensity Histogram","text":"Feature Code Calc Ref Tol Status Mean discretised intensity X6K6 2.15 2.15 \u2014 \u2705 PASS Discretised intensity variance CH89 3.05 3.05 \u2014 \u2705 PASS Discretised intensity skewness 88K1 1.08 1.08 \u2014 \u2705 PASS Discretised intensity kurtosis C3I7 -0.355 -0.355 \u2014 \u2705 PASS Median discretised intensity WIFQ 1 1 \u2014 \u2705 PASS Minimum discretised intensity 1PR8 1 1 \u2014 \u2705 PASS 10th discretised intensity percentile 1PR 1 1 \u2014 \u2705 PASS 90th discretised intensity percentile GPMT 4 4 \u2014 \u2705 PASS Maximum discretised intensity 3NCY 6 6 \u2014 \u2705 PASS Intensity histogram mode AMMC 1 1 \u2014 \u2705 PASS Discretised intensity interquartile range WR0O 3 3 \u2014 \u2705 PASS Discretised intensity range 5Z3W 5 5 \u2014 \u2705 PASS Intensity histogram mean absolute deviation D2ZX 1.55 1.55 \u2014 \u2705 PASS Intensity histogram robust mean absolute deviation WRZB 1.11 1.11 \u2014 \u2705 PASS Intensity histogram median absolute deviation 4RNL 1.15 1.15 \u2014 \u2705 PASS Intensity histogram coefficient of variation CWYJ 0.812 0.812 \u2014 \u2705 PASS Intensity histogram quartile coefficient of dispersion SLWD 0.6 0.6 \u2014 \u2705 PASS Discretised intensity entropy TLU2 1.27 1.27 \u2014 \u2705 PASS Discretised intensity uniformity BJ5W 0.512 0.512 \u2014 \u2705 PASS Maximum histogram gradient 12CE 8 8 \u2014 \u2705 PASS Maximum histogram gradient intensity 8E6O 3 3 \u2014 \u2705 PASS Minimum histogram gradient VQB3 -50 -50 \u2014 \u2705 PASS Minimum histogram gradient intensity RHQZ 1 1 \u2014 \u2705 PASS"},{"location":"ibsi1_compliance/#intensity-volume-histogram","title":"Intensity-Volume Histogram","text":"Feature Code Calc Ref Tol Status Volume at intensity fraction 0.10 BC2M_10 0.324 0.324 \u2014 \u2705 PASS Volume at intensity fraction 0.90 BC2M_90 0.0946 0.0946 \u2014 \u2705 PASS Intensity at volume fraction 0.10 GBPN_10 5 5 \u2014 \u2705 PASS Intensity at volume fraction 0.90 GBPN_90 2 2 \u2014 \u2705 PASS Volume fraction difference between intensity 0.10 and 0.90 fractions DDTU 0.23 0.23 \u2014 \u2705 PASS Intensity fraction difference between volume 0.10 and 0.90 fractions CNV2 3 3 \u2014 \u2705 PASS Area under the IVH curve 9CMM 2.047 \u2014 \u2014 \u2757 REF."},{"location":"ibsi1_compliance/#glcm","title":"GLCM","text":"Feature Code Calc Ref Tol Status Joint maximum GYBY 0.509 0.509 \u2014 \u2705 PASS Joint average 60VM 2.15 2.15 \u2014 \u2705 PASS Joint variance UR99 3.13 3.13 \u2014 \u2705 PASS Joint entropy TU9B 2.57 2.57 \u2014 \u2705 PASS Difference average TF7R 1.38 1.38 \u2014 \u2705 PASS Difference variance D3YU 3.21 3.21 \u2014 \u2705 PASS Difference entropy NTRS 1.64 1.64 \u2014 \u2705 PASS Sum average ZGXS 4.3 4.3 \u2014 \u2705 PASS Sum variance OEEB 7.41 7.41 \u2014 \u2705 PASS Sum entropy P6QZ 2.11 2.11 \u2014 \u2705 PASS Angular second moment 8ZQL 0.291 0.291 \u2014 \u2705 PASS Contrast ACUI 5.12 5.12 \u2014 \u2705 PASS Dissimilarity 8S9J 1.38 1.38 \u2014 \u2705 PASS Inverse difference IB1Z 0.688 0.688 \u2014 \u2705 PASS Normalised inverse difference NDRX 0.856 0.856 \u2014 \u2705 PASS Inverse difference moment WF0Z 0.631 0.631 \u2014 \u2705 PASS Normalised inverse difference moment 1QCO 0.902 0.902 \u2014 \u2705 PASS Inverse variance E8JP 0.0574 0.0574 \u2014 \u2705 PASS Correlation NI2N 0.183 0.183 \u2014 \u2705 PASS Autocorrelation QWB0 5.19 5.19 \u2014 \u2705 PASS Cluster tendency DG8W 7.41 7.41 \u2014 \u2705 PASS Cluster shade 7NFM 17.4 17.4 \u2014 \u2705 PASS Cluster prominence AE86 147 147 \u2014 \u2705 PASS Information correlation 1 R8DG -0.0288 -0.0288 \u2014 \u2705 PASS Information correlation 2 JN9H 0.269 0.269 \u2014 \u2705 PASS"},{"location":"ibsi1_compliance/#glrlm","title":"GLRLM","text":"Feature Code Calc Ref Tol Status Short runs emphasis 22OV 0.729 0.729 \u2014 \u2705 PASS Long runs emphasis W4KF 2.76 2.76 \u2014 \u2705 PASS Low grey level run emphasis V3SW 0.607 0.607 \u2014 \u2705 PASS High grey level run emphasis G3QZ 9.64 9.64 \u2014 \u2705 PASS Short run low grey level emphasis HTZT 0.372 0.372 \u2014 \u2705 PASS Short run high grey level emphasis GD3A 8.67 8.67 \u2014 \u2705 PASS Long run low grey level emphasis IVPO 2.16 2.16 \u2014 \u2705 PASS Long run high grey level emphasis 3KUM 15.6 15.6 \u2014 \u2705 PASS Grey level non-uniformity R5YN 281 281 \u2014 \u2705 PASS Normalised grey level non-uniformity OVBL 0.43 0.43 \u2014 \u2705 PASS Run length non-uniformity W92Y 328 328 \u2014 \u2705 PASS Normalised run length non-uniformity IC23 0.501 0.501 \u2014 \u2705 PASS Run percentage 9ZK5 0.68 0.68 \u2014 \u2705 PASS Grey level variance 8CE5 3.48 3.48 \u2014 \u2705 PASS Run length variance SXLW 0.598 0.598 \u2014 \u2705 PASS Run entropy HJ9O 2.62 2.62 \u2014 \u2705 PASS"},{"location":"ibsi1_compliance/#glszm","title":"GLSZM","text":"Feature Code Calc Ref Tol Status Small zone emphasis P001 0.255 0.255 \u2014 \u2705 PASS Large zone emphasis 48P8 550 550 \u2014 \u2705 PASS Low grey level zone emphasis XMSY 0.253 0.253 \u2014 \u2705 PASS High grey level zone emphasis 5GN9 15.6 15.6 \u2014 \u2705 PASS Small zone low grey level emphasis 5RAI 0.0256 0.0256 \u2014 \u2705 PASS Small zone high grey level emphasis HW1V 2.76 2.76 \u2014 \u2705 PASS Large zone low grey level emphasis YH51 503 503 \u2014 \u2705 PASS Large zone high grey level emphasis J17V 1495 1.49e3 \u2014 \u2705 PASS Grey level non-uniformity JNSA 1.4 1.4 \u2014 \u2705 PASS Normalised grey level non-uniformity Y1RO 0.28 0.28 \u2014 \u2705 PASS Zone size non-uniformity 4JP3 1 1 \u2014 \u2705 PASS Normalised zone size non-uniformity VB3A 0.2 0.2 \u2014 \u2705 PASS Zone percentage P30P 0.0676 0.0676 \u2014 \u2705 PASS Grey level variance BYLV 2.64 2.64 \u2014 \u2705 PASS Zone size variance 3NSA 331 331 \u2014 \u2705 PASS Zone size entropy GU8N 2.32 2.32 \u2014 \u2705 PASS"},{"location":"ibsi1_compliance/#gldzm","title":"GLDZM","text":"Feature Code Calc Ref Tol Status Small distance emphasis 0GBI 1 1 \u2014 \u2705 PASS Large distance emphasis MB4I 1 1 \u2014 \u2705 PASS Low grey level zone emphasis S1RA 0.253 0.253 \u2014 \u2705 PASS High grey level zone emphasis K26C 15.6 15.6 \u2014 \u2705 PASS Small distance low grey level emphasis RUVG 0.253 0.253 \u2014 \u2705 PASS Small distance high grey level emphasis DKNJ 15.6 15.6 \u2014 \u2705 PASS Large distance low grey level emphasis A7WM 0.253 0.253 \u2014 \u2705 PASS Large distance high grey level emphasis KLTH 15.6 15.6 \u2014 \u2705 PASS Grey level non-uniformity VFT7 1.4 1.4 \u2014 \u2705 PASS Normalised grey level non-uniformity 7HP3 0.28 0.28 \u2014 \u2705 PASS Zone distance non-uniformity V294 5 5 \u2014 \u2705 PASS Normalised zone distance non-uniformity IATH 1 1 \u2014 \u2705 PASS Zone percentage VIWW 0.0676 0.0676 \u2014 \u2705 PASS Grey level variance QK93 2.64 2.64 \u2014 \u2705 PASS Zone distance variance 7WT1 0 0 \u2014 \u2705 PASS Zone distance entropy GBDU 1.92 1.92 \u2014 \u2705 PASS"},{"location":"ibsi1_compliance/#ngtdm","title":"NGTDM","text":"Feature Code Calc Ref Tol Status Coarseness QCDE 0.0296 0.0296 \u2014 \u2705 PASS Contrast 65HE 0.584 0.584 \u2014 \u2705 PASS Busyness NQ30 6.54 6.54 \u2014 \u2705 PASS Complexity HDEZ 13.5 13.5 \u2014 \u2705 PASS Strength 1X9X 0.763 0.763 \u2014 \u2705 PASS"},{"location":"ibsi1_compliance/#ngldm","title":"NGLDM","text":"Feature Code Calc Ref Tol Status Low dependence emphasis SODN 0.045 0.045 \u2014 \u2705 PASS High dependence emphasis IMOQ 109 109 \u2014 \u2705 PASS Low grey level count emphasis TL9H 0.693 0.693 \u2014 \u2705 PASS High grey level count emphasis OAE7 7.66 7.66 \u2014 \u2705 PASS Low dependence low grey level emphasis EQ3F 0.00963 0.00963 \u2014 \u2705 PASS Low dependence high grey level emphasis JA6D 0.736 0.736 \u2014 \u2705 PASS High dependence low grey level emphasis NBZI 102 102 \u2014 \u2705 PASS High dependence high grey level emphasis 9QMG 235 235 \u2014 \u2705 PASS Grey level non-uniformity FP8K 37.9 37.9 \u2014 \u2705 PASS Normalised grey level non-uniformity 5SPA 0.512 0.512 \u2014 \u2705 PASS Dependence count non-uniformity Z87G 4.86 4.86 \u2014 \u2705 PASS Normalised dependence count non-uniformity OKJI 0.0657 0.0657 \u2014 \u2705 PASS Dependence count percentage 6XV8 1 1 \u2014 \u2705 PASS Grey level variance 1PFV 3.05 3.05 \u2014 \u2705 PASS Dependence count variance DNX2 22.1 22.1 \u2014 \u2705 PASS Dependence count entropy FCBV 4.4 4.4 \u2014 \u2705 PASS Dependence count energy CAS9 0.0533 0.0533 \u2014 \u2705 PASS"},{"location":"ibsi1_compliance/#config-c","title":"Config C","text":""},{"location":"ibsi1_compliance/#morphology_1","title":"Morphology","text":"Feature Code Calc Ref Tol Status Volume RNU0 3.675e+05 3.67e5 6e3 \u2705 PASS Volume voxel counting YEKZ 3.679e+05 3.68e5 6e3 \u2705 PASS Surface area C0JK 3.431e+04 3.43e4 400 \u2705 PASS Surface to volume ratio 2PR5 0.0934 0.0934 0.0007 \u2705 PASS Compactness 1 SKGS 0.03263 \u2014 \u2014 \u2757 REF. Compactness 2 BQWJ 0.378 0.378 0.004 \u2705 PASS Spherical disproportion KRCK 1.38 1.38 0.01 \u2705 PASS Sphericity QCFX 0.723 0.723 0.003 \u2705 PASS Asphericity 25C7 0.383 0.383 0.004 \u2705 PASS Center of mass shift KLMA 45.6 45.6 2.8 \u2705 PASS Maximum 3D diameter L0JK 125 125 1 \u2705 PASS Major axis length TDIC 93.3 93.3 0.5 \u2705 PASS Minor axis length P9VJ 82 82 0.5 \u2705 PASS Least axis length 7J51 70.9 70.9 0.4 \u2705 PASS Elongation Q3CK 0.879 0.879 0.001 \u2705 PASS Flatness N17B 0.76 0.76 0.001 \u2705 PASS Volume density (AABB) PBX1 0.478 0.478 0.003 \u2705 PASS Area density (AABB) R59B 0.678 0.678 0.003 \u2705 PASS Volume density (OMBB) ZH1A 0.3408 \u2014 \u2014 \u2757 REF. Area density (OMBB) IQYR 0.5389 \u2014 \u2014 \u2757 REF. Volume density (AEE) 6BDE 1.29 1.29 0.01 \u2705 PASS Area density (AEE) RDD2 1.62 1.62 0.01 \u2705 PASS Volume density (MVEE) SWZ1 0.496 \u2014 \u2014 \u2757 REF. Area density (MVEE) BRI8 0.8805 \u2014 \u2014 \u2757 REF. Volume density (convex hull) R3ER 0.834 0.834 0.002 \u2705 PASS Area density (convex hull) 7T7F 1.13 1.13 0.01 \u2705 PASS"},{"location":"ibsi1_compliance/#intensity_1","title":"Intensity","text":"Feature Code Calc Ref Tol Status Integrated intensity 99N0 -1.8e+07 -1.8e7 1.4e6 \u2705 PASS Moran's I index N365 0.0824 0.0824 0.0003 \u2705 PASS Geary's C measure NPT7 0.846 0.846 0.001 \u2705 PASS Local intensity peak VJGA 169 169 10 \u2705 PASS Global intensity peak 0F91 180 180 5 \u2705 PASS Mean intensity Q4LE -49 -49 2.9 \u2705 PASS Intensity variance ECT3 5.064e+04 5.06e4 1.4e3 \u2705 PASS Intensity skewness KE2A -2.14 -2.14 0.05 \u2705 PASS Intensity kurtosis IPH6 3.53 3.53 0.23 \u2705 PASS Median intensity Y12H 40 40 0.4 \u2705 PASS Minimum intensity 1GSF -939 -939 4 \u2705 PASS 10th intensity percentile QG58 -424 -424 14 \u2705 PASS 90th intensity percentile 8DWT 86 86 0.1 \u2705 PASS Maximum intensity 84IY 393 393 10 \u2705 PASS Intensity interquartile range SALO 67 67 4.9 \u2705 PASS Intensity range 2OJQ 1332 1.33e3 20 \u2705 PASS Intensity Mean absolute deviation 4FUA 158 158 4 \u2705 PASS Intensity Robust mean absolute deviation 1128 66.8 66.8 3.5 \u2705 PASS Intensity Median absolute deviation N72L 119 119 4 \u2705 PASS Intensity Coefficient of variation 7TET -4.59 -4.59 0.29 \u2705 PASS Intensity Quartile coefficient of dispersion 9S40 1.03 1.03 0.4 \u2705 PASS Intensity energy N8CA 2.439e+09 2.44e9 1.2e8 \u2705 PASS Root mean square intensity 5ZWQ 230 230 4 \u2705 PASS"},{"location":"ibsi1_compliance/#intensity-histogram_1","title":"Intensity Histogram","text":"Feature Code Calc Ref Tol Status Mean discretised intensity X6K6 38.6 38.6 0.2 \u2705 PASS Discretised intensity variance CH89 81.1 81.1 2.1 \u2705 PASS Discretised intensity skewness 88K1 -2.14 -2.14 0.05 \u2705 PASS Discretised intensity kurtosis C3I7 3.52 3.52 0.23 \u2705 PASS Median discretised intensity WIFQ 42 42 \u2014 \u2705 PASS Minimum discretised intensity 1PR8 3 3 0.16 \u2705 PASS 10th discretised intensity percentile 1PR 24 24 0.7 \u2705 PASS 90th discretised intensity percentile GPMT 44 44 \u2014 \u2705 PASS Maximum discretised intensity 3NCY 56 56 0.5 \u2705 PASS Intensity histogram mode AMMC 43 43 0.1 \u2705 PASS Discretised intensity interquartile range WR0O 3 3 0.21 \u2705 PASS Discretised intensity range 5Z3W 53 53 0.6 \u2705 PASS Intensity histogram mean absolute deviation D2ZX 6.32 6.32 0.15 \u2705 PASS Intensity histogram robust mean absolute deviation WRZB 2.59 2.59 0.14 \u2705 PASS Intensity histogram median absolute deviation 4RNL 4.75 4.75 0.12 \u2705 PASS Intensity histogram coefficient of variation CWYJ 0.234 0.234 0.005 \u2705 PASS Intensity histogram quartile coefficient of dispersion SLWD 0.0361 0.0361 0.0027 \u2705 PASS Discretised intensity entropy TLU2 3.73 3.73 0.04 \u2705 PASS Discretised intensity uniformity BJ5W 0.14 0.14 0.003 \u2705 PASS Maximum histogram gradient 12CE 4746 4.75e3 30 \u2705 PASS Maximum histogram gradient intensity 8E6O 41 41 \u2014 \u2705 PASS Minimum histogram gradient VQB3 -4677 -4.68e3 50 \u2705 PASS Minimum histogram gradient intensity RHQZ 44 44 \u2014 \u2705 PASS"},{"location":"ibsi1_compliance/#intensity-volume-histogram_1","title":"Intensity-Volume Histogram","text":"Feature Code Calc Ref Tol Status Volume at intensity fraction 0.10 BC2M_10 0.998 0.998 0.001 \u2705 PASS Volume at intensity fraction 0.90 BC2M_90 0.000152 0.000152 2e-5 \u2705 PASS Intensity at volume fraction 0.10 GBPN_10 88.8 88.8 0.2 \u2705 PASS Intensity at volume fraction 0.90 GBPN_90 -421 -421 14 \u2705 PASS Volume fraction difference between intensity 0.10 and 0.90 fractions DDTU 0.997 0.997 0.001 \u2705 PASS Intensity fraction difference between volume 0.10 and 0.90 fractions CNV2 510 510 14 \u2705 PASS Area under the IVH curve 9CMM 891.3 \u2014 \u2014 \u2757 REF."},{"location":"ibsi1_compliance/#glcm_1","title":"GLCM","text":"Feature Code Calc Ref Tol Status Joint maximum GYBY 0.111 0.111 0.002 \u2705 PASS Joint average 60VM 39 39 0.2 \u2705 PASS Joint variance UR99 73.8 73.8 2 \u2705 PASS Joint entropy TU9B 6.42 6.42 0.06 \u2705 PASS Difference average TF7R 2.16 2.16 0.05 \u2705 PASS Difference variance D3YU 14.4 14.4 0.5 \u2705 PASS Difference entropy NTRS 2.64 2.64 0.03 \u2705 PASS Sum average ZGXS 78 78 0.3 \u2705 PASS Sum variance OEEB 276 276 8 \u2705 PASS Sum entropy P6QZ 4.56 4.56 0.04 \u2705 PASS Angular second moment 8ZQL 0.0447 0.0447 0.001 \u2705 PASS Contrast ACUI 19.1 19.1 0.7 \u2705 PASS Dissimilarity 8S9J 2.16 2.16 0.05 \u2705 PASS Inverse difference IB1Z 0.583 0.583 0.004 \u2705 PASS Normalised inverse difference NDRX 0.965 0.966 0.001 \u2705 PASS Inverse difference moment WF0Z 0.548 0.548 0.004 \u2705 PASS Normalised inverse difference moment 1QCO 0.994 0.994 0.001 \u2705 PASS Inverse variance E8JP 0.39 0.39 0.003 \u2705 PASS Correlation NI2N 0.871 0.871 0.001 \u2705 PASS Autocorrelation QWB0 1583 1.58e3 10 \u2705 PASS Cluster tendency DG8W 276 276 8 \u2705 PASS Cluster shade 7NFM -1.063e+04 -1.06e4 300 \u2705 PASS Cluster prominence AE86 5.696e+05 5.7e5 1.1e4 \u2705 PASS Information correlation 1 R8DG -0.228 -0.228 0.001 \u2705 PASS Information correlation 2 JN9H 0.899 0.899 0.001 \u2705 PASS"},{"location":"ibsi1_compliance/#glrlm_1","title":"GLRLM","text":"Feature Code Calc Ref Tol Status Short runs emphasis 22OV 0.787 0.787 0.003 \u2705 PASS Long runs emphasis W4KF 3.28 3.28 0.04 \u2705 PASS Low grey level run emphasis V3SW 0.00155 0.00155 5e-5 \u2705 PASS High grey level run emphasis G3QZ 1473 1.47e3 10 \u2705 PASS Short run low grey level emphasis HTZT 0.00136 0.00136 5e-5 \u2705 PASS Short run high grey level emphasis GD3A 1100 1.1e3 10 \u2705 PASS Long run low grey level emphasis IVPO 0.00314 0.00314 4e-5 \u2705 PASS Long run high grey level emphasis 3KUM 5525 5.53e3 80 \u2705 PASS Grey level non-uniformity R5YN 4.13e+04 4.13e4 100 \u2705 PASS Normalised grey level non-uniformity OVBL 0.102 0.102 0.003 \u2705 PASS Run length non-uniformity W92Y 2.336e+05 2.34e5 6e3 \u2705 PASS Normalised run length non-uniformity IC23 0.575 0.575 0.004 \u2705 PASS Run percentage 9ZK5 0.679 0.679 0.003 \u2705 PASS Grey level variance 8CE5 101 101 3 \u2705 PASS Run length variance SXLW 1.11 1.11 0.02 \u2705 PASS Run entropy HJ9O 5.35 5.35 0.03 \u2705 PASS"},{"location":"ibsi1_compliance/#glszm_1","title":"GLSZM","text":"Feature Code Calc Ref Tol Status Small zone emphasis P001 0.695 0.695 0.001 \u2705 PASS Large zone emphasis 48P8 3.893e+04 3.89e4 900 \u2705 PASS Low grey level zone emphasis XMSY 0.00235 0.00235 6e-5 \u2705 PASS High grey level zone emphasis 5GN9 971 971 7 \u2705 PASS Small zone low grey level emphasis 5RAI 0.0016 0.0016 4e-5 \u2705 PASS Small zone high grey level emphasis HW1V 657 657 4 \u2705 PASS Large zone low grey level emphasis YH51 21.6 21.6 0.5 \u2705 PASS Large zone high grey level emphasis J17V 7.071e+07 7.07e7 1.5e6 \u2705 PASS Grey level non-uniformity JNSA 195 195 6 \u2705 PASS Normalised grey level non-uniformity Y1RO 0.0286 0.0286 0.0003 \u2705 PASS Zone size non-uniformity 4JP3 3043 3.04e3 100 \u2705 PASS Normalised zone size non-uniformity VB3A 0.447 0.447 0.001 \u2705 PASS Zone percentage P30P 0.148 0.148 0.003 \u2705 PASS Grey level variance BYLV 106 106 1 \u2705 PASS Zone size variance 3NSA 3.888e+04 3.89e4 900 \u2705 PASS Zone size entropy GU8N 7 7 0.01 \u2705 PASS"},{"location":"ibsi1_compliance/#gldzm_1","title":"GLDZM","text":"Feature Code Calc Ref Tol Status Small distance emphasis 0GBI 0.531 0.531 0.006 \u2705 PASS Large distance emphasis MB4I 11 11 0.3 \u2705 PASS Low grey level zone emphasis S1RA 0.00235 0.00235 6e-5 \u2705 PASS High grey level zone emphasis K26C 971 971 7 \u2705 PASS Small distance low grey level emphasis RUVG 0.00149 0.00149 4e-5 \u2705 PASS Small distance high grey level emphasis DKNJ 476 476 11 \u2705 PASS Large distance low grey level emphasis A7WM 0.0154 0.0154 0.0005 \u2705 PASS Large distance high grey level emphasis KLTH 1.336e+04 1.34e4 200 \u2705 PASS Grey level non-uniformity VFT7 195 195 6 \u2705 PASS Normalised grey level non-uniformity 7HP3 0.0286 0.0286 0.0003 \u2705 PASS Zone distance non-uniformity V294 1866 1.87e3 40 \u2705 PASS Normalised zone distance non-uniformity IATH 0.274 0.274 0.005 \u2705 PASS Zone percentage VIWW 0.148 0.148 0.003 \u2705 PASS Grey level variance QK93 106 106 1 \u2705 PASS Zone distance variance 7WT1 4.6 4.6 0.06 \u2705 PASS Zone distance entropy GBDU 7.56 7.56 0.03 \u2705 PASS"},{"location":"ibsi1_compliance/#ngtdm_1","title":"NGTDM","text":"Feature Code Calc Ref Tol Status Coarseness QCDE 0.000216 0.000216 4e-6 \u2705 PASS Contrast 65HE 0.0873 0.0873 0.0019 \u2705 PASS Busyness NQ30 1.39 1.39 0.01 \u2705 PASS Complexity HDEZ 1809 1.81e3 60 \u2705 PASS Strength 1X9X 0.651 0.651 0.015 \u2705 PASS"},{"location":"ibsi1_compliance/#ngldm_1","title":"NGLDM","text":"Feature Code Calc Ref Tol Status Low dependence emphasis SODN 0.137 0.137 0.003 \u2705 PASS High dependence emphasis IMOQ 126 126 2 \u2705 PASS Low grey level count emphasis TL9H 0.0013 0.0013 4e-5 \u2705 PASS High grey level count emphasis OAE7 1568 1.57e3 10 \u2705 PASS Low dependence low grey level emphasis EQ3F 0.000306 0.000306 1.2e-5 \u2705 PASS Low dependence high grey level emphasis JA6D 141 141 2 \u2705 PASS High dependence low grey level emphasis NBZI 0.0828 0.0828 0.0003 \u2705 PASS High dependence high grey level emphasis 9QMG 2.267e+05 2.27e5 3e3 \u2705 PASS Grey level non-uniformity FP8K 6417 6.42e3 10 \u2705 PASS Normalised grey level non-uniformity 5SPA 0.14 0.14 0.003 \u2705 PASS Dependence count non-uniformity Z87G 2447 2.45e3 60 \u2705 PASS Normalised dependence count non-uniformity OKJI 0.0532 0.0532 0.0005 \u2705 PASS Dependence count percentage 6XV8 1 1 \u2014 \u2705 PASS Grey level variance 1PFV 81.1 81.1 2.1 \u2705 PASS Dependence count variance DNX2 39.2 39.2 0.1 \u2705 PASS Dependence count entropy FCBV 7.54 7.54 0.03 \u2705 PASS Dependence count energy CAS9 0.00789 0.00789 0.00011 \u2705 PASS"},{"location":"ibsi1_compliance/#config-d","title":"Config D","text":""},{"location":"ibsi1_compliance/#morphology_2","title":"Morphology","text":"Feature Code Calc Ref Tol Status Volume RNU0 3.675e+05 3.67e5 6e3 \u2705 PASS Volume voxel counting YEKZ 3.679e+05 3.68e5 6e3 \u2705 PASS Surface area C0JK 3.431e+04 3.43e4 400 \u2705 PASS Surface to volume ratio 2PR5 0.0934 0.0934 0.0007 \u2705 PASS Compactness 1 SKGS 0.0326 0.0326 0.0002 \u2705 PASS Compactness 2 BQWJ 0.378 0.378 0.004 \u2705 PASS Spherical disproportion KRCK 1.38 1.38 0.01 \u2705 PASS Sphericity QCFX 0.723 0.723 0.003 \u2705 PASS Asphericity 25C7 0.383 0.383 0.004 \u2705 PASS Center of mass shift KLMA 64.9 64.9 2.8 \u2705 PASS Maximum 3D diameter L0JK 125 125 1 \u2705 PASS Major axis length TDIC 93.3 93.3 0.5 \u2705 PASS Minor axis length P9VJ 82 82 0.5 \u2705 PASS Least axis length 7J51 70.9 70.9 0.4 \u2705 PASS Elongation Q3CK 0.879 0.879 0.001 \u2705 PASS Flatness N17B 0.76 0.76 0.001 \u2705 PASS Volume density (AABB) PBX1 0.478 0.478 0.003 \u2705 PASS Area density (AABB) R59B 0.678 0.678 0.003 \u2705 PASS Volume density (OMBB) ZH1A 0.3408 \u2014 \u2014 \u2757 REF. Area density (OMBB) IQYR 0.5389 \u2014 \u2014 \u2757 REF. Volume density (AEE) 6BDE 1.29 1.29 0.01 \u2705 PASS Area density (AEE) RDD2 1.62 1.62 0.01 \u2705 PASS Volume density (MVEE) SWZ1 0.496 \u2014 \u2014 \u2757 REF. Area density (MVEE) BRI8 0.8805 \u2014 \u2014 \u2757 REF. Volume density (convex hull) R3ER 0.834 0.834 0.002 \u2705 PASS Area density (convex hull) 7T7F 1.13 1.13 0.01 \u2705 PASS"},{"location":"ibsi1_compliance/#intensity_2","title":"Intensity","text":"Feature Code Calc Ref Tol Status Integrated intensity 99N0 -8.642e+06 -8.64e6 1.56e6 \u2705 PASS Moran's I index N365 0.0622 0.0622 0.0013 \u2705 PASS Geary's C measure NPT7 0.851 0.851 0.001 \u2705 PASS Local intensity peak VJGA 201 201 10 \u2705 PASS Global intensity peak 0F91 201 201 5 \u2705 PASS Mean intensity Q4LE -23.5 -23.5 3.9 \u2705 PASS Intensity variance ECT3 3.279e+04 3.28e4 2.1e3 \u2705 PASS Intensity skewness KE2A -2.28 -2.28 0.06 \u2705 PASS Intensity kurtosis IPH6 4.35 4.35 0.32 \u2705 PASS Median intensity Y12H 42 42 0.4 \u2705 PASS Minimum intensity 1GSF -724 -724 12 \u2705 PASS 10th intensity percentile QG58 -304 -304 20 \u2705 PASS 90th intensity percentile 8DWT 86 86 0.1 \u2705 PASS Maximum intensity 84IY 521 521 22 \u2705 PASS Intensity interquartile range SALO 57 57 4.1 \u2705 PASS Intensity range 2OJQ 1245 1.24e3 40 \u2705 PASS Intensity Mean absolute deviation 4FUA 123 123 6 \u2705 PASS Intensity Robust mean absolute deviation 1128 46.8 46.8 3.6 \u2705 PASS Intensity Median absolute deviation N72L 94.7 94.7 3.8 \u2705 PASS Intensity Coefficient of variation 7TET -7.7 -7.7 1.01 \u2705 PASS Intensity Quartile coefficient of dispersion 9S40 0.74 0.74 0.011 \u2705 PASS Intensity energy N8CA 1.482e+09 1.48e9 1.4e8 \u2705 PASS Root mean square intensity 5ZWQ 183 183 7 \u2705 PASS"},{"location":"ibsi1_compliance/#intensity-histogram_2","title":"Intensity Histogram","text":"Feature Code Calc Ref Tol Status Mean discretised intensity X6K6 18.5 18.5 0.5 \u2705 PASS Discretised intensity variance CH89 21.7 21.7 0.4 \u2705 PASS Discretised intensity skewness 88K1 -2.27 -2.27 0.06 \u2705 PASS Discretised intensity kurtosis C3I7 4.31 4.31 0.32 \u2705 PASS Median discretised intensity WIFQ 20 20 0.5 \u2705 PASS Minimum discretised intensity 1PR8 1 1 \u2014 \u2705 PASS 10th discretised intensity percentile 1PR 11 11 0.7 \u2705 PASS 90th discretised intensity percentile GPMT 21 21 0.5 \u2705 PASS Maximum discretised intensity 3NCY 32 32 \u2014 \u2705 PASS Intensity histogram mode AMMC 20 20 0.4 \u2705 PASS Discretised intensity interquartile range WR0O 2 2 0.06 \u2705 PASS Discretised intensity range 5Z3W 31 31 \u2014 \u2705 PASS Intensity histogram mean absolute deviation D2ZX 3.15 3.15 0.05 \u2705 PASS Intensity histogram robust mean absolute deviation WRZB 1.33 1.33 0.06 \u2705 PASS Intensity histogram median absolute deviation 4RNL 2.41 2.41 0.04 \u2705 PASS Intensity histogram coefficient of variation CWYJ 0.252 0.252 0.006 \u2705 PASS Intensity histogram quartile coefficient of dispersion SLWD 0.05 0.05 0.0021 \u2705 PASS Discretised intensity entropy TLU2 2.94 2.94 0.01 \u2705 PASS Discretised intensity uniformity BJ5W 0.229 0.229 0.003 \u2705 PASS Maximum histogram gradient 12CE 7263 7.26e3 200 \u2705 PASS Maximum histogram gradient intensity 8E6O 19 19 0.4 \u2705 PASS Minimum histogram gradient VQB3 -6674 -6.67e3 230 \u2705 PASS Minimum histogram gradient intensity RHQZ 22 22 0.4 \u2705 PASS"},{"location":"ibsi1_compliance/#intensity-volume-histogram_2","title":"Intensity-Volume Histogram","text":"Feature Code Calc Ref Tol Status Volume at intensity fraction 0.10 BC2M_10 0.972 0.972 0.003 \u2705 PASS Volume at intensity fraction 0.90 BC2M_90 0.00009 9e-5 0.000415 \u2705 PASS Intensity at volume fraction 0.10 GBPN_10 87 87 0.1 \u2705 PASS Intensity at volume fraction 0.90 GBPN_90 -303 -303 20 \u2705 PASS Volume fraction difference between intensity 0.10 and 0.90 fractions DDTU 0.971 0.971 0.001 \u2705 PASS Intensity fraction difference between volume 0.10 and 0.90 fractions CNV2 390 390 20 \u2705 PASS Area under the IVH curve 9CMM 701 \u2014 \u2014 \u2757 REF."},{"location":"ibsi1_compliance/#glcm_2","title":"GLCM","text":"Feature Code Calc Ref Tol Status Joint maximum GYBY 0.232 0.232 0.007 \u2705 PASS Joint average 60VM 18.9 18.9 0.5 \u2705 PASS Joint variance UR99 17.6 17.6 0.4 \u2705 PASS Joint entropy TU9B 4.96 4.96 0.03 \u2705 PASS Difference average TF7R 1.29 1.29 0.01 \u2705 PASS Difference variance D3YU 5.38 5.38 0.11 \u2705 PASS Difference entropy NTRS 2.14 2.14 0.01 \u2705 PASS Sum average ZGXS 37.7 37.7 0.8 \u2705 PASS Sum variance OEEB 63.5 63.5 1.3 \u2705 PASS Sum entropy P6QZ 3.68 3.68 0.02 \u2705 PASS Angular second moment 8ZQL 0.109 0.109 0.003 \u2705 PASS Contrast ACUI 7.05 7.05 0.13 \u2705 PASS Dissimilarity 8S9J 1.29 1.29 0.01 \u2705 PASS Inverse difference IB1Z 0.682 0.682 0.003 \u2705 PASS Normalised inverse difference NDRX 0.965 0.965 0.001 \u2705 PASS Inverse difference moment WF0Z 0.657 0.657 0.003 \u2705 PASS Normalised inverse difference moment 1QCO 0.994 0.994 0.001 \u2705 PASS Inverse variance E8JP 0.34 0.34 0.005 \u2705 PASS Correlation NI2N 0.8 0.8 0.005 \u2705 PASS Autocorrelation QWB0 370 370 16 \u2705 PASS Cluster tendency DG8W 63.5 63.5 1.3 \u2705 PASS Cluster shade 7NFM -1275 -1.28e3 40 \u2705 PASS Cluster prominence AE86 3.574e+04 3.57e4 1.5e3 \u2705 PASS Information correlation 1 R8DG -0.225 -0.225 0.003 \u2705 PASS Information correlation 2 JN9H 0.846 0.846 0.003 \u2705 PASS"},{"location":"ibsi1_compliance/#glrlm_2","title":"GLRLM","text":"Feature Code Calc Ref Tol Status Short runs emphasis 22OV 0.736 0.736 0.001 \u2705 PASS Long runs emphasis W4KF 6.56 6.56 0.18 \u2705 PASS Low grey level run emphasis V3SW 0.0257 0.0257 0.0012 \u2705 PASS High grey level run emphasis G3QZ 326 326 17 \u2705 PASS Short run low grey level emphasis HTZT 0.0232 0.0232 0.001 \u2705 PASS Short run high grey level emphasis GD3A 219 219 13 \u2705 PASS Long run low grey level emphasis IVPO 0.0478 0.0478 0.0031 \u2705 PASS Long run high grey level emphasis 3KUM 2626 2.63e3 30 \u2705 PASS Grey level non-uniformity R5YN 4.277e+04 4.28e4 200 \u2705 PASS Normalised grey level non-uniformity OVBL 0.134 0.134 0.002 \u2705 PASS Run length non-uniformity W92Y 1.604e+05 1.6e5 3e3 \u2705 PASS Normalised run length non-uniformity IC23 0.501 0.501 0.001 \u2705 PASS Run percentage 9ZK5 0.554 0.554 0.005 \u2705 PASS Grey level variance 8CE5 31.4 31.4 0.4 \u2705 PASS Run length variance SXLW 3.29 3.29 0.13 \u2705 PASS Run entropy HJ9O 5.08 5.08 0.02 \u2705 PASS"},{"location":"ibsi1_compliance/#glszm_2","title":"GLSZM","text":"Feature Code Calc Ref Tol Status Small zone emphasis P001 0.637 0.637 0.005 \u2705 PASS Large zone emphasis 48P8 9.908e+04 9.91e4 2.8e3 \u2705 PASS Low grey level zone emphasis XMSY 0.0409 0.0409 0.0005 \u2705 PASS High grey level zone emphasis 5GN9 188 188 10 \u2705 PASS Small zone low grey level emphasis 5RAI 0.0248 0.0248 0.0004 \u2705 PASS Small zone high grey level emphasis HW1V 117 117 7 \u2705 PASS Large zone low grey level emphasis YH51 241 241 14 \u2705 PASS Large zone high grey level emphasis J17V 4.14e+07 4.14e7 3e5 \u2705 PASS Grey level non-uniformity JNSA 212 212 6 \u2705 PASS Normalised grey level non-uniformity Y1RO 0.0491 0.0491 0.0008 \u2705 PASS Zone size non-uniformity 4JP3 1629 1.63e3 10 \u2705 PASS Normalised zone size non-uniformity VB3A 0.377 0.377 0.006 \u2705 PASS Zone percentage P30P 0.0972 0.0972 0.0007 \u2705 PASS Grey level variance BYLV 32.7 32.7 1.6 \u2705 PASS Zone size variance 3NSA 9.897e+04 9.9e4 2.8e3 \u2705 PASS Zone size entropy GU8N 6.52 6.52 0.01 \u2705 PASS"},{"location":"ibsi1_compliance/#gldzm_2","title":"GLDZM","text":"Feature Code Calc Ref Tol Status Small distance emphasis 0GBI 0.579 0.579 0.004 \u2705 PASS Large distance emphasis MB4I 10.3 10.3 0.1 \u2705 PASS Low grey level zone emphasis S1RA 0.0409 0.0409 0.0005 \u2705 PASS High grey level zone emphasis K26C 188 188 10 \u2705 PASS Small distance low grey level emphasis RUVG 0.0302 0.0302 0.0006 \u2705 PASS Small distance high grey level emphasis DKNJ 99.3 99.3 5.1 \u2705 PASS Large distance low grey level emphasis A7WM 0.183 0.183 0.004 \u2705 PASS Large distance high grey level emphasis KLTH 2619 2.62e3 110 \u2705 PASS Grey level non-uniformity VFT7 212 212 6 \u2705 PASS Normalised grey level non-uniformity 7HP3 0.0491 0.0491 0.0008 \u2705 PASS Zone distance non-uniformity V294 1369 1.37e3 20 \u2705 PASS Normalised zone distance non-uniformity IATH 0.317 0.317 0.004 \u2705 PASS Zone percentage VIWW 0.0972 0.0972 0.0007 \u2705 PASS Grey level variance QK93 32.7 32.7 1.6 \u2705 PASS Zone distance variance 7WT1 4.61 4.61 0.04 \u2705 PASS Zone distance entropy GBDU 6.61 6.61 0.03 \u2705 PASS"},{"location":"ibsi1_compliance/#ngtdm_2","title":"NGTDM","text":"Feature Code Calc Ref Tol Status Coarseness QCDE 0.000208 0.000208 4e-6 \u2705 PASS Contrast 65HE 0.046 0.046 0.0005 \u2705 PASS Busyness NQ30 5.14 5.14 0.14 \u2705 PASS Complexity HDEZ 400 400 5 \u2705 PASS Strength 1X9X 0.162 0.162 0.008 \u2705 PASS"},{"location":"ibsi1_compliance/#ngldm_2","title":"NGLDM","text":"Feature Code Calc Ref Tol Status Low dependence emphasis SODN 0.0912 0.0912 0.0007 \u2705 PASS High dependence emphasis IMOQ 223 223 5 \u2705 PASS Low grey level count emphasis TL9H 0.0168 0.0168 0.0009 \u2705 PASS High grey level count emphasis OAE7 364 364 16 \u2705 PASS Low dependence low grey level emphasis EQ3F 0.00357 0.00357 4e-5 \u2705 PASS Low dependence high grey level emphasis JA6D 18.9 18.9 1.1 \u2705 PASS High dependence low grey level emphasis NBZI 0.798 0.798 0.072 \u2705 PASS High dependence high grey level emphasis 9QMG 9.276e+04 9.28e4 1.3e3 \u2705 PASS Grey level non-uniformity FP8K 1.017e+04 1.02e4 300 \u2705 PASS Normalised grey level non-uniformity 5SPA 0.229 0.229 0.003 \u2705 PASS Dependence count non-uniformity Z87G 1837 1.84e3 30 \u2705 PASS Normalised dependence count non-uniformity OKJI 0.0413 0.0413 0.0003 \u2705 PASS Dependence count percentage 6XV8 1 1 \u2014 \u2705 PASS Grey level variance 1PFV 21.7 21.7 0.4 \u2705 PASS Dependence count variance DNX2 63.9 63.9 1.3 \u2705 PASS Dependence count entropy FCBV 6.98 6.98 0.01 \u2705 PASS Dependence count energy CAS9 0.0113 0.0113 0.0002 \u2705 PASS"},{"location":"ibsi1_compliance/#config-e","title":"Config E","text":""},{"location":"ibsi1_compliance/#morphology_3","title":"Morphology","text":"Feature Code Calc Ref Tol Status Volume RNU0 3.675e+05 3.67e5 6e3 \u2705 PASS Volume voxel counting YEKZ 3.679e+05 3.68e5 6e3 \u2705 PASS Surface area C0JK 3.431e+04 3.43e4 400 \u2705 PASS Surface to volume ratio 2PR5 0.0934 0.0934 0.0007 \u2705 PASS Compactness 1 SKGS 0.0326 0.0326 0.0002 \u2705 PASS Compactness 2 BQWJ 0.378 0.378 0.004 \u2705 PASS Spherical disproportion KRCK 1.38 1.38 0.01 \u2705 PASS Sphericity QCFX 0.723 0.723 0.003 \u2705 PASS Asphericity 25C7 0.383 0.383 0.004 \u2705 PASS Center of mass shift KLMA 68.5 68.5 2.1 \u2705 PASS Maximum 3D diameter L0JK 125 125 1 \u2705 PASS Major axis length TDIC 93.3 93.3 0.5 \u2705 PASS Minor axis length P9VJ 82 82 0.5 \u2705 PASS Least axis length 7J51 70.9 70.9 0.4 \u2705 PASS Elongation Q3CK 0.879 0.879 0.001 \u2705 PASS Flatness N17B 0.76 0.76 0.001 \u2705 PASS Volume density (AABB) PBX1 0.478 0.478 0.003 \u2705 PASS Area density (AABB) R59B 0.678 0.678 0.003 \u2705 PASS Volume density (OMBB) ZH1A 0.3408 \u2014 \u2014 \u2757 REF. Area density (OMBB) IQYR 0.5389 \u2014 \u2014 \u2757 REF. Volume density (AEE) 6BDE 1.29 1.29 0.01 \u2705 PASS Area density (AEE) RDD2 1.62 1.62 0.01 \u2705 PASS Volume density (MVEE) SWZ1 0.496 \u2014 \u2014 \u2757 REF. Area density (MVEE) BRI8 0.8805 \u2014 \u2014 \u2757 REF. Volume density (convex hull) R3ER 0.834 0.834 0.002 \u2705 PASS Area density (convex hull) 7T7F 1.13 1.13 0.01 \u2705 PASS"},{"location":"ibsi1_compliance/#intensity_3","title":"Intensity","text":"Feature Code Calc Ref Tol Status Integrated intensity 99N0 -8.332e+06 -8.31e6 1.6e6 \u2705 PASS Moran's I index N365 0.0596 0.0596 0.0014 \u2705 PASS Geary's C measure NPT7 0.853 0.853 0.001 \u2705 PASS Local intensity peak VJGA 181 181 13 \u2705 PASS Global intensity peak 0F91 181 181 5 \u2705 PASS Mean intensity Q4LE -22.7 -22.6 4.1 \u2705 PASS Intensity variance ECT3 3.513e+04 3.51e4 2.2e3 \u2705 PASS Intensity skewness KE2A -2.3 -2.3 0.07 \u2705 PASS Intensity kurtosis IPH6 4.44 4.44 0.33 \u2705 PASS Median intensity Y12H 43 43 0.5 \u2705 PASS Minimum intensity 1GSF -744 -743 13 \u2705 PASS 10th intensity percentile QG58 -311 -310 21 \u2705 PASS 90th intensity percentile 8DWT 93 93 0.2 \u2705 PASS Maximum intensity 84IY 345 345 9 \u2705 PASS Intensity interquartile range SALO 62 62 3.5 \u2705 PASS Intensity range 2OJQ 1089 1.09e3 30 \u2705 PASS Intensity Mean absolute deviation 4FUA 125 125 6 \u2705 PASS Intensity Robust mean absolute deviation 1128 46.5 46.5 3.7 \u2705 PASS Intensity Median absolute deviation N72L 97.9 97.9 3.9 \u2705 PASS Intensity Coefficient of variation 7TET -8.27 -8.28 0.95 \u2705 PASS Intensity Quartile coefficient of dispersion 9S40 0.795 0.795 0.337 \u2705 PASS Intensity energy N8CA 1.586e+09 1.58e9 1.4e8 \u2705 PASS Root mean square intensity 5ZWQ 189 189 7 \u2705 PASS"},{"location":"ibsi1_compliance/#intensity-histogram_3","title":"Intensity Histogram","text":"Feature Code Calc Ref Tol Status Mean discretised intensity X6K6 21.7 21.7 0.3 \u2705 PASS Discretised intensity variance CH89 30.5 30.4 0.8 \u2705 PASS Discretised intensity skewness 88K1 -2.29 -2.29 0.07 \u2705 PASS Discretised intensity kurtosis C3I7 4.4 4.4 0.33 \u2705 PASS Median discretised intensity WIFQ 24 24 0.2 \u2705 PASS Minimum discretised intensity 1PR8 1 1 \u2014 \u2705 PASS 10th discretised intensity percentile 1PR 13 13 0.7 \u2705 PASS 90th discretised intensity percentile GPMT 25 25 0.2 \u2705 PASS Maximum discretised intensity 3NCY 32 32 \u2014 \u2705 PASS Intensity histogram mode AMMC 24 24 0.1 \u2705 PASS Discretised intensity interquartile range WR0O 1 1 0.06 \u2705 PASS Discretised intensity range 5Z3W 31 31 \u2014 \u2705 PASS Intensity histogram mean absolute deviation D2ZX 3.69 3.69 0.1 \u2705 PASS Intensity histogram robust mean absolute deviation WRZB 1.46 1.46 0.09 \u2705 PASS Intensity histogram median absolute deviation 4RNL 2.89 2.89 0.07 \u2705 PASS Intensity histogram coefficient of variation CWYJ 0.254 0.254 0.006 \u2705 PASS Intensity histogram quartile coefficient of dispersion SLWD 0.0213 0.0213 0.0015 \u2705 PASS Discretised intensity entropy TLU2 3.22 3.22 0.02 \u2705 PASS Discretised intensity uniformity BJ5W 0.184 0.184 0.001 \u2705 PASS Maximum histogram gradient 12CE 6010 6.01e3 130 \u2705 PASS Maximum histogram gradient intensity 8E6O 23 23 0.2 \u2705 PASS Minimum histogram gradient VQB3 -6110 -6.11e3 180 \u2705 PASS Minimum histogram gradient intensity RHQZ 25 25 0.2 \u2705 PASS"},{"location":"ibsi1_compliance/#intensity-volume-histogram_3","title":"Intensity-Volume Histogram","text":"Feature Code Calc Ref Tol Status Volume at intensity fraction 0.10 BC2M_10 0.975 0.975 0.002 \u2705 PASS Volume at intensity fraction 0.90 BC2M_90 0.000157 0.000157 0.000248 \u2705 PASS Intensity at volume fraction 0.10 GBPN_10 770 770 5 \u2705 PASS Intensity at volume fraction 0.90 GBPN_90 399 399 17 \u2705 PASS Volume fraction difference between intensity 0.10 and 0.90 fractions DDTU 0.975 0.974 0.001 \u2705 PASS Intensity fraction difference between volume 0.10 and 0.90 fractions CNV2 371 371 13 \u2705 PASS Area under the IVH curve 9CMM 662.4 \u2014 \u2014 \u2757 REF."},{"location":"ibsi1_compliance/#glcm_3","title":"GLCM","text":"Feature Code Calc Ref Tol Status Joint maximum GYBY 0.153 0.153 0.003 \u2705 PASS Joint average 60VM 22.1 22.1 0.3 \u2705 PASS Joint variance UR99 24.5 24.4 0.9 \u2705 PASS Joint entropy TU9B 5.61 5.61 0.03 \u2705 PASS Difference average TF7R 1.7 1.7 0.01 \u2705 PASS Difference variance D3YU 8.24 8.23 0.06 \u2705 PASS Difference entropy NTRS 2.4 2.4 0.01 \u2705 PASS Sum average ZGXS 44.3 44.3 0.4 \u2705 PASS Sum variance OEEB 86.8 86.7 3.3 \u2705 PASS Sum entropy P6QZ 3.97 3.97 0.02 \u2705 PASS Angular second moment 8ZQL 0.0635 0.0635 0.0009 \u2705 PASS Contrast ACUI 11.1 11.1 0.1 \u2705 PASS Dissimilarity 8S9J 1.7 1.7 0.01 \u2705 PASS Inverse difference IB1Z 0.608 0.608 0.001 \u2705 PASS Normalised inverse difference NDRX 0.955 0.955 0.001 \u2705 PASS Inverse difference moment WF0Z 0.577 0.577 0.001 \u2705 PASS Normalised inverse difference moment 1QCO 0.99 0.99 0.001 \u2705 PASS Inverse variance E8JP 0.41 0.41 0.004 \u2705 PASS Correlation NI2N 0.773 0.773 0.006 \u2705 PASS Autocorrelation QWB0 509 509 8 \u2705 PASS Cluster tendency DG8W 86.8 86.7 3.3 \u2705 PASS Cluster shade 7NFM -2080 -2.08e3 70 \u2705 PASS Cluster prominence AE86 6.919e+04 6.9e4 2.1e3 \u2705 PASS Information correlation 1 R8DG -0.175 -0.175 0.003 \u2705 PASS Information correlation 2 JN9H 0.813 0.813 0.004 \u2705 PASS"},{"location":"ibsi1_compliance/#glrlm_3","title":"GLRLM","text":"Feature Code Calc Ref Tol Status Short runs emphasis 22OV 0.777 0.777 0.001 \u2705 PASS Long runs emphasis W4KF 3.52 3.52 0.07 \u2705 PASS Low grey level run emphasis V3SW 0.0204 0.0204 0.0008 \u2705 PASS High grey level run emphasis G3QZ 471 471 9 \u2705 PASS Short run low grey level emphasis HTZT 0.0187 0.0186 0.0007 \u2705 PASS Short run high grey level emphasis GD3A 347 347 7 \u2705 PASS Long run low grey level emphasis IVPO 0.0313 0.0311 0.0016 \u2705 PASS Long run high grey level emphasis 3KUM 1889 1.89e3 20 \u2705 PASS Grey level non-uniformity R5YN 5.195e+04 5.19e4 200 \u2705 PASS Normalised grey level non-uniformity OVBL 0.135 0.135 0.003 \u2705 PASS Run length non-uniformity W92Y 2.151e+05 2.15e5 4e3 \u2705 PASS Normalised run length non-uniformity IC23 0.56 0.56 0.001 \u2705 PASS Run percentage 9ZK5 0.664 0.664 0.003 \u2705 PASS Grey level variance 8CE5 39.7 39.7 0.9 \u2705 PASS Run length variance SXLW 1.25 1.25 0.05 \u2705 PASS Run entropy HJ9O 4.87 4.87 0.03 \u2705 PASS"},{"location":"ibsi1_compliance/#glszm_3","title":"GLSZM","text":"Feature Code Calc Ref Tol Status Small zone emphasis P001 0.676 0.676 0.003 \u2705 PASS Large zone emphasis 48P8 5.856e+04 5.86e4 800 \u2705 PASS Low grey level zone emphasis XMSY 0.034 0.034 0.0004 \u2705 PASS High grey level zone emphasis 5GN9 286 286 6 \u2705 PASS Small zone low grey level emphasis 5RAI 0.0223 0.0224 0.0004 \u2705 PASS Small zone high grey level emphasis HW1V 186 186 4 \u2705 PASS Large zone low grey level emphasis YH51 105 105 4 \u2705 PASS Large zone high grey level emphasis J17V 3.356e+07 3.36e7 3e5 \u2705 PASS Grey level non-uniformity JNSA 231 231 6 \u2705 PASS Normalised grey level non-uniformity Y1RO 0.0414 0.0414 0.0003 \u2705 PASS Zone size non-uniformity 4JP3 2367 2.37e3 40 \u2705 PASS Normalised zone size non-uniformity VB3A 0.424 0.424 0.004 \u2705 PASS Zone percentage P30P 0.126 0.126 0.001 \u2705 PASS Grey level variance BYLV 50.8 50.8 0.9 \u2705 PASS Zone size variance 3NSA 5.85e+04 5.85e4 800 \u2705 PASS Zone size entropy GU8N 6.57 6.57 0.01 \u2705 PASS"},{"location":"ibsi1_compliance/#gldzm_3","title":"GLDZM","text":"Feature Code Calc Ref Tol Status Small distance emphasis 0GBI 0.527 0.527 0.004 \u2705 PASS Large distance emphasis MB4I 12.6 12.6 0.1 \u2705 PASS Low grey level zone emphasis S1RA 0.034 0.034 0.0004 \u2705 PASS High grey level zone emphasis K26C 286 286 6 \u2705 PASS Small distance low grey level emphasis RUVG 0.0229 0.0228 0.0003 \u2705 PASS Small distance high grey level emphasis DKNJ 136 136 4 \u2705 PASS Large distance low grey level emphasis A7WM 0.178 0.179 0.004 \u2705 PASS Large distance high grey level emphasis KLTH 4854 4.85e3 60 \u2705 PASS Grey level non-uniformity VFT7 231 231 6 \u2705 PASS Normalised grey level non-uniformity 7HP3 0.0414 0.0414 0.0003 \u2705 PASS Zone distance non-uniformity V294 1503 1.5e3 30 \u2705 PASS Normalised zone distance non-uniformity IATH 0.269 0.269 0.003 \u2705 PASS Zone percentage VIWW 0.126 0.126 0.001 \u2705 PASS Grey level variance QK93 50.8 50.8 0.9 \u2705 PASS Zone distance variance 7WT1 5.56 5.56 0.05 \u2705 PASS Zone distance entropy GBDU 7.06 7.06 0.01 \u2705 PASS"},{"location":"ibsi1_compliance/#ngtdm_3","title":"NGTDM","text":"Feature Code Calc Ref Tol Status Coarseness QCDE 0.000188 0.000188 4e-6 \u2705 PASS Contrast 65HE 0.0753 0.0752 0.0019 \u2705 PASS Busyness NQ30 4.65 4.65 0.1 \u2705 PASS Complexity HDEZ 574 574 1 \u2705 PASS Strength 1X9X 0.167 0.167 0.006 \u2705 PASS"},{"location":"ibsi1_compliance/#ngldm_3","title":"NGLDM","text":"Feature Code Calc Ref Tol Status Low dependence emphasis SODN 0.118 0.118 0.001 \u2705 PASS High dependence emphasis IMOQ 134 134 3 \u2705 PASS Low grey level count emphasis TL9H 0.0155 0.0154 0.0007 \u2705 PASS High grey level count emphasis OAE7 501 502 8 \u2705 PASS Low dependence low grey level emphasis EQ3F 0.00387 0.00388 4e-5 \u2705 PASS Low dependence high grey level emphasis JA6D 36.7 36.7 0.5 \u2705 PASS High dependence low grey level emphasis NBZI 0.462 0.457 0.031 \u2705 PASS High dependence high grey level emphasis 9QMG 7.6e+04 7.6e4 600 \u2705 PASS Grey level non-uniformity FP8K 8168 8.17e3 130 \u2705 PASS Normalised grey level non-uniformity 5SPA 0.184 0.184 0.001 \u2705 PASS Dependence count non-uniformity Z87G 2247 2.25e3 30 \u2705 PASS Normalised dependence count non-uniformity OKJI 0.0505 0.0505 0.0003 \u2705 PASS Dependence count percentage 6XV8 1 1 \u2014 \u2705 PASS Grey level variance 1PFV 30.5 30.4 0.8 \u2705 PASS Dependence count variance DNX2 39.4 39.4 1 \u2705 PASS Dependence count entropy FCBV 7.06 7.06 0.02 \u2705 PASS Dependence count energy CAS9 0.0106 0.0106 0.0001 \u2705 PASS"},{"location":"ibsi2_compliance/","title":"IBSI 2 Compliance: Convolutional Filters","text":""},{"location":"ibsi2_compliance/#overview","title":"Overview","text":"<p>The Image Biomarker Standardisation Initiative Chapter 2 (IBSI 2) focuses on standardizing convolutional image filters for radiomics. This page documents Pictologics' compliance with IBSI 2 Phase 1: technical validation using digital phantoms.</p> <p>Important</p> <p>Pictologics implements 3D filters and 3D radiomic features only.</p> <p>The library is designed specifically for volumetric medical imaging analysis (CT, MRI, PET). 2D slice-by-slice processing is not supported as it loses critical spatial information needed for accurate radiomic feature extraction.</p>"},{"location":"ibsi2_compliance/#how-to-run-the-benchmarks","title":"How to Run the Benchmarks","text":""},{"location":"ibsi2_compliance/#1-download-the-data","title":"1. Download the Data","text":"<p>The IBSI 2 reference datasets (digital phantoms) are available on the IBSI GitHub repository.</p> <ul> <li>Digital Phantoms: Download the phantom NIfTI files (e.g., <code>checkerboard.nii.gz</code>, <code>impulse_response.nii.gz</code>) from the <code>ibsi_2_validation</code> folder.</li> </ul> <p>Place these files in a local directory (e.g., <code>data/ibsi2/</code>) to run the benchmarks.</p>"},{"location":"ibsi2_compliance/#2-run-configurations-programmatically-using-radiomicspipeline","title":"2. Run Configurations Programmatically using <code>RadiomicsPipeline</code>","text":"<p>You can run IBSI 2 filter configurations programmatically using the <code>RadiomicsPipeline</code> class.</p> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\n\n# Define an IBSI 2 Gabor filter configuration\n# (Gabor filter, orthogonal rotation invariant, 3D)\ngabor_config = [\n    # 1. IBSI 2 Preprocessing\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0), \"interpolation\": \"cubic\"}},\n    {\"step\": \"round_intensities\", \"params\": {}},\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n\n    # 2. Apply Gabor Filter (Phase 1 Validation)\n    {\"step\": \"filter\", \"params\": {\n        \"type\": \"gabor\",\n        \"sigma_mm\": 5.0,\n        \"lambda_mm\": 4.0, \n        \"gamma\": 0.5,\n        \"rotation_invariant\": True,\n        \"pooling\": \"average\"\n    }},\n\n    # 3. Extract Intensity Features from the response map\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\"]}}\n]\n\npipeline.add_config(\"ibsi2_gabor_demo\", gabor_config)\n\n# Run on an image\nresults = pipeline.run(\"path/to/phantom.nii.gz\", config_names=[\"ibsi2_gabor_demo\"])\nprint(results[\"ibsi2_gabor_demo\"])\n</code></pre> <p>Note</p> <p>The example above shows a Gabor filter configuration. This is just one example. You can configure any IBSI 2 compliant filter (Mean, LoG, Laws, Wavelet, etc.) similarly. For full specifications of filter parameters, please refer to the Image Filtering guide and the IBSI 2 Reference Manual.</p>"},{"location":"ibsi2_compliance/#phase-1-results","title":"Phase 1 Results","text":""},{"location":"ibsi2_compliance/#filter-performance-overview","title":"Filter Performance Overview","text":"Test Filter Phantom Error % Time Memory Status 1.a.1 Mean checkerboard 0.00% 2ms 1.0MB \u2705 PASS 1.a.2 Mean checkerboard 0.00% 2ms 1.0MB \u2705 PASS 1.a.3 Mean checkerboard 0.00% 2ms 1.0MB \u2705 PASS 1.a.4 Mean checkerboard 0.00% 2ms 1.0MB \u2705 PASS 1.b.1 Mean (2D) impulse_response - - - \u23ed SKIP 2.a LoG impulse_response 0.00% 10ms 3.0MB \u2705 PASS 2.b LoG checkerboard 0.03% 12ms 3.0MB \u2705 PASS 2.c LoG (2D) checkerboard - - - \u23ed SKIP 3.a.1 Laws impulse_response 0.00% 3ms 2.0MB \u2705 PASS 3.a.2 Laws impulse_response 0.00% 58ms 4.0MB \u2705 PASS 3.a.3 Laws impulse_response 0.00% 55ms 4.0MB \u2705 PASS 3.b.1 Laws checkerboard 0.00% 2ms 2.0MB \u2705 PASS 3.b.2 Laws checkerboard 0.00% 53ms 4.0MB \u2705 PASS 3.b.3 Laws checkerboard 0.00% 55ms 4.0MB \u2705 PASS 3.c.1 Laws (2D) checkerboard - - - \u23ed SKIP 3.c.2 Laws (2D) checkerboard - - - \u23ed SKIP 3.c.3 Laws (2D) checkerboard - - - \u23ed SKIP 4.a.1 Gabor impulse_response 0.27% 26ms 15.4MB \u2705 PASS 4.a.2 Gabor impulse_response 0.14% 391ms 20.8MB \u2705 PASS 4.b.1 Gabor sphere 0.01% 31ms 34.6MB \u2705 PASS 4.b.2 Gabor sphere 0.09% 907ms 50.4MB \u2705 PASS 5.a.1 Daubechies 2 impulse_response 0.00% 3ms 3.0MB \u2705 PASS 5.a.2 Daubechies 2 impulse_response 0.00% 72ms 6.0MB \u2705 PASS 6.a.1 Coiflet 1 sphere 0.00% 3ms 3.0MB \u2705 PASS 6.a.2 Coiflet 1 sphere 0.00% 80ms 6.0MB \u2705 PASS 7.a.1 Haar checkerboard 0.00% 124ms 6.0MB \u2705 PASS 7.a.2 Haar checkerboard 0.00% 135ms 6.0MB \u2705 PASS 8.a.1 Simoncelli checkerboard 0.38% 11ms 24.3MB \u2705 PASS 8.a.2 Simoncelli checkerboard 0.00% 11ms 24.3MB \u2705 PASS 8.a.3 Simoncelli checkerboard 0.00% 11ms 24.3MB \u2705 PASS 9.a Riesz-LoG impulse_response 0.05% 14ms 14.4MB \u2705 PASS 9.b.1 Riesz-LoG sphere 0.64% 14ms 14.4MB \u2705 PASS 9.b.2 Riesz-LoG (aligned) sphere - - - \u2757 REF. 10.a Riesz-Simoncelli impulse_response - - - \u2757 REF. 10.b.1 Riesz-Simoncelli pattern_1 0.79% 15ms 24.3MB \u2705 PASS 10.b.2 Riesz-Simoncelli (aligned) pattern_1 - - - \u2757 REF."},{"location":"ibsi2_compliance/#tolerance-criteria","title":"Tolerance Criteria","text":"<p>All tests use the IBSI 2 standard tolerance: <pre><code>max_difference \u2264 0.01 \u00d7 (reference_max - reference_min)\n</code></pre></p>"},{"location":"ibsi2_compliance/#known-deviations","title":"Known Deviations","text":""},{"location":"ibsi2_compliance/#2d-filters-not-implemented-5-tests-skipped","title":"2D Filters Not Implemented (5 Tests Skipped)","text":"<p>Note</p> <p>Design Decision: 3D Volumetric Processing Only</p> <p>Pictologics implements only 3D convolutional filters and only 3D radiomic features only. This is a deliberate design choice for clinical radiomics workflows with volumetric medical imaging data (CT, MRI, PET scans).</p> <p>The following tests are intentionally skipped because they require 2D filter implementations: Test 1.b.1 (Mean Filter 2D), Test 2.c (LoG 2D), and Tests 3.c.1-3.c.3 (Laws 2D).</p>"},{"location":"ibsi2_compliance/#structure-tensor-alignment-reference-missing","title":"Structure Tensor Alignment (Reference Missing)","text":"<p>The following tests are currently not implemented because they require structure tensor alignment and the IBSI 2 reference dataset does not contain the corresponding validity response maps:</p> <ul> <li>Tests 9.b.2, 10.b.2: Riesz Filter Alignment (ValidCRM missing)</li> <li>Test 10.a: Riesz-Simoncelli Alignment (ValidCRM missing)</li> </ul> <p>Warning</p> <p>Reference Data Unavailable</p> <p>The official IBSI 2 reference dataset does not contain the reference validity maps for these tests (<code>9_b_2-ValidCRM.nii</code>, <code>10_a-ValidCRM.nii</code>, <code>10_b_2-ValidCRM.nii</code>). Therefore, these tests cannot be validated and are marked as \u2757 REF. (Reference Missing).</p>"},{"location":"ibsi2_compliance/#summary","title":"Summary","text":"<p>In total, 28 tests passed validating all core 3D filter functionality. 5 tests were skipped as they relate to 2D filters which are not applicable to this 3D-focused library. 3 tests are marked as missing reference (structure tensor alignment) because the validation data is not provided by IBSI.</p>"},{"location":"ibsi2_phase2_compliance/","title":"IBSI 2 Phase 2 Compliance: Radiomic Features","text":""},{"location":"ibsi2_phase2_compliance/#overview","title":"Overview","text":"<p>IBSI 2 Phase 2 focuses on evaluating the reproducibility of radiomic features extracted from filtered images. In this phase, specific filters (Mean, LoG, Laws, Gabor, Wavelets) are applied to a digital phantom (Lung Cancer CT), and intensity features are extracted from the resulting response maps.</p> <p>This page documents Pictologics' compliance with IBSI 2 Phase 2 (Configuration B), which covers 3D volumetric processing.</p>"},{"location":"ibsi2_phase2_compliance/#how-to-run-the-benchmarks","title":"How to Run the Benchmarks","text":""},{"location":"ibsi2_phase2_compliance/#1-download-the-data","title":"1. Download the Data","text":"<p>The IBSI 2 reference datasets are available on the IBSI GitHub repository.</p> <ul> <li>CT Phantom: Download the <code>PAT1_CT.nii.gz</code> and <code>PAT1_GTV.nii.gz</code> (or similarly named <code>image.nii.gz</code> / <code>mask.nii.gz</code>) from the <code>ibsi_2_phase_2</code> (or <code>ibsi_2_validation</code>) folder.</li> </ul> <p>Place these files in a local directory (e.g., <code>data/ibsi2/data/ct_phantom/</code>) to run the benchmarks.</p>"},{"location":"ibsi2_phase2_compliance/#2-run-configurations-programmatically-using-radiomicspipeline","title":"2. Run Configurations Programmatically using <code>RadiomicsPipeline</code>","text":"<p>You can verify any IBSI 2 configuration programmatically using the <code>RadiomicsPipeline</code>.</p> <pre><code>from pictologics import RadiomicsPipeline\n\n# Path to your downloaded phantom\nimage_path = \"data/ibsi2/data/ct_phantom/image.nii.gz\"\nmask_path = \"data/ibsi2/data/ct_phantom/mask.nii.gz\"\n\npipeline = RadiomicsPipeline()\n\n# Define common IBSI 2 Preprocessing (Config B)\npreprocess_steps = [\n    # 1. Resample to 1x1x1 mm using Tricubic Spline interpolation\n    {\"step\": \"resample\", \"params\": {\n        \"new_spacing\": (1.0, 1.0, 1.0), \n        \"interpolation\": \"cubic\", \n        \"mask_interpolation\": \"linear\", \n        \"mask_threshold\": 0.5\n    }},\n    # 2. Round intensities to nearest integer\n    {\"step\": \"round_intensities\", \"params\": {}},\n    # 3. Resegment range [-1000, 400] HU\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n]\n\n# Define a filter configuration (e.g., Test 3.B: LoG)\nlog_config = preprocess_steps + [\n    {\"step\": \"filter\", \"params\": {\n        \"type\": \"log\",\n        \"sigma_mm\": 1.5,\n        \"truncate\": 4.0\n    }},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\"]}}\n]\n\npipeline.add_config(\"ibsi2_test_3b\", log_config)\n\n# Run pipeline\nresults = pipeline.run(image_path, mask_path, config_names=[\"ibsi2_test_3b\"])\nprint(results[\"ibsi2_test_3b\"])\n</code></pre>"},{"location":"ibsi2_phase2_compliance/#phase-2-results","title":"Phase 2 Results","text":"<p>Summary: 9/9 Tests Passed (3D Configuration). Total Features: 161/161 passed.</p>"},{"location":"ibsi2_phase2_compliance/#test-1b-none","title":"Test 1.B: None","text":"<p>Configuration: Baseline (no filter)</p> Feature Code Calc Ref Tol Status Mean stat_mean -46.4 -46.4 5.9 \u2705 PASS Variance stat_var 5.26e+04 5.26e+04 2.8e+03 \u2705 PASS Skewness stat_skew -2.18 -2.18 0.09 \u2705 PASS (Excess) kurtosis stat_kurt 3.71 3.71 0.47 \u2705 PASS Median stat_median 41 41 0.7 \u2705 PASS Minimum stat_min -997 -997 3 \u2705 PASS 10th percentile stat_p10 -427 -427 29 \u2705 PASS 90th percentile stat_p90 92 92 0.1 \u2705 PASS Maximum stat_max 377 377 15 \u2705 PASS Interquartile range stat_iqr 67 67 9.1 \u2705 PASS Range stat_range 1.37e+03 1.37e+03 20 \u2705 PASS Mean absolute deviation stat_mad 159 159 7 \u2705 PASS Robust mean absolute deviation stat_rmad 63.6 63.6 7.3 \u2705 PASS Median absolute deviation stat_medad 121 121 6 \u2705 PASS Coefficient of variation stat_cov -4.94 -4.94 0.64 \u2705 PASS Quartile coefficient of dispersion stat_qcod 0.944 0.944 0.925 \u2705 PASS Energy stat_energy 1.96e+10 1.96e+10 1.9e+09 \u2705 PASS Root mean square stat_rms 234 234 7 \u2705 PASS"},{"location":"ibsi2_phase2_compliance/#test-2b-mean","title":"Test 2.B: Mean","text":"<p>Configuration: 3D, support=5</p> Feature Code Calc Ref Tol Status Mean stat_mean -49.9 -49.9 5.7 \u2705 PASS Variance stat_var 4.44e+04 4.44e+04 2.3e+03 \u2705 PASS Skewness stat_skew -2.13 -2.13 0.09 \u2705 PASS (Excess) kurtosis stat_kurt 3.59 3.59 0.46 \u2705 PASS Median stat_median 37.3 37.3 0.6 \u2705 PASS Minimum stat_min -906 -906 5 \u2705 PASS 10th percentile stat_p10 -389 -389 25 \u2705 PASS 90th percentile stat_p90 77.2 77.2 0.1 \u2705 PASS Maximum stat_max 316 316 7 \u2705 PASS Interquartile range stat_iqr 92.6 92.6 13.5 \u2705 PASS Range stat_range 1.22e+03 1.22e+03 10 \u2705 PASS Mean absolute deviation stat_mad 149 149 6 \u2705 PASS Robust mean absolute deviation stat_rmad 68.1 68.1 6.9 \u2705 PASS Median absolute deviation stat_medad 114 114 5 \u2705 PASS Coefficient of variation stat_cov -4.22 -4.22 0.47 \u2705 PASS Quartile coefficient of dispersion stat_qcod 2.97 2.97 0.58 \u2705 PASS Energy stat_energy 1.68e+10 1.68e+10 1.6e+09 \u2705 PASS Root mean square stat_rms 217 217 7 \u2705 PASS"},{"location":"ibsi2_phase2_compliance/#test-3b-log","title":"Test 3.B: LoG","text":"<p>Configuration: \u03c3=1.5mm, truncate=4\u03c3</p> Feature Code Calc Ref Tol Status Mean stat_mean -2.92 -2.94 0.2 \u2705 PASS Variance stat_var 720 720 33 \u2705 PASS Skewness stat_skew 0.428 0.428 0.009 \u2705 PASS (Excess) kurtosis stat_kurt 6.13 6.13 0.27 \u2705 PASS Median stat_median -0.927 -0.919 0.024 \u2705 PASS Minimum stat_min -173 -173 5 \u2705 PASS 10th percentile stat_p10 -32.1 -32.2 0.5 \u2705 PASS 90th percentile stat_p90 17.5 17.4 1.9 \u2705 PASS Maximum stat_max 204 204 1 \u2705 PASS Interquartile range stat_iqr 11.4 11.4 0.3 \u2705 PASS Range stat_range 377 377 5 \u2705 PASS Mean absolute deviation stat_mad 15.5 15.5 0.4 \u2705 PASS Robust mean absolute deviation stat_rmad 6.36 6.37 0.19 \u2705 PASS Median absolute deviation stat_medad 15.3 15.3 0.4 \u2705 PASS Coefficient of variation stat_cov -9.18 -9.12 1.63 \u2705 PASS Quartile coefficient of dispersion stat_qcod -2.34 -2.34 0.07 \u2705 PASS Energy stat_energy 2.61e+08 2.61e+08 1.9e+07 \u2705 PASS Root mean square stat_rms 27 27 0.6 \u2705 PASS"},{"location":"ibsi2_phase2_compliance/#test-4b-laws","title":"Test 4.B: Laws","text":"<p>Configuration: L5E5E5, rot-inv max, energy \u03b4=7</p> Feature Code Calc Ref Tol Status Mean stat_mean 142 142 3 \u2705 PASS Variance stat_var 1.11e+04 1.11e+04 300 \u2705 PASS Skewness stat_skew 0.645 0.645 0.028 \u2705 PASS (Excess) kurtosis stat_kurt -0.711 -0.711 0.044 \u2705 PASS Median stat_median 113 113 4 \u2705 PASS Minimum stat_min 28.5 28.5 0.1 \u2705 PASS 10th percentile stat_p10 35.6 35.6 0.1 \u2705 PASS 90th percentile stat_p90 293 293 4 \u2705 PASS Maximum stat_max 525 525 1 \u2705 PASS Interquartile range stat_iqr 188 188 4 \u2705 PASS Range stat_range 496 496 1 \u2705 PASS Mean absolute deviation stat_mad 92.4 92.4 1.4 \u2705 PASS Robust mean absolute deviation stat_rmad 75.9 75.9 1.4 \u2705 PASS Median absolute deviation stat_medad 90.8 90.8 1.6 \u2705 PASS Coefficient of variation stat_cov 0.743 0.743 0.005 \u2705 PASS Quartile coefficient of dispersion stat_qcod 0.699 0.699 0.003 \u2705 PASS Energy stat_energy 1.12e+10 1.12e+10 7e+08 \u2705 PASS Root mean square stat_rms 177 177 3 \u2705 PASS"},{"location":"ibsi2_phase2_compliance/#test-5b-gabor","title":"Test 5.B: Gabor","text":"<p>Configuration: \u03c3=5mm, \u03bb=2mm, \u03b3=1.5, rot-inv avg</p> Feature Code Calc Ref Tol Status Mean stat_mean 40.2 40.2 0.2 \u2705 PASS Variance stat_var 231 231 2 \u2705 PASS Skewness stat_skew 1.57 1.57 0.03 \u2705 PASS (Excess) kurtosis stat_kurt 4.34 4.34 0.2 \u2705 PASS Median stat_median 37.2 37.2 0.1 \u2705 PASS Minimum stat_min 9.53 9.53 0.11 \u2705 PASS 10th percentile stat_p10 24.6 24.6 0.1 \u2705 PASS 90th percentile stat_p90 59.3 59.3 0.3 \u2705 PASS Maximum stat_max 175 175 3 \u2705 PASS Interquartile range stat_iqr 17.4 17.4 0.1 \u2705 PASS Range stat_range 165 165 3 \u2705 PASS Mean absolute deviation stat_mad 11.3 11.3 0.1 \u2705 PASS Robust mean absolute deviation stat_rmad 7.31 7.31 0.06 \u2705 PASS Median absolute deviation stat_medad 11 11 0.1 \u2705 PASS Coefficient of variation stat_cov 0.377 0.377 0.004 \u2705 PASS Quartile coefficient of dispersion stat_qcod 0.226 0.226 0.002 \u2705 PASS Energy stat_energy 6.62e+08 6.62e+08 9e+06 \u2705 PASS Root mean square stat_rms 43 43 0.2 \u2705 PASS"},{"location":"ibsi2_phase2_compliance/#test-6b-daubechies-3","title":"Test 6.B: Daubechies 3","text":"<p>Configuration: LLH level 1, rot-inv avg</p> Feature Code Calc Ref Tol Status Mean stat_mean -0.182 -0.182 0.024 \u2705 PASS Variance stat_var 250 250 9 \u2705 PASS Skewness stat_skew 0.157 0.157 0.018 \u2705 PASS (Excess) kurtosis stat_kurt 8.98 8.98 0.35 \u2705 PASS Median stat_median 0.0576 0.0575 0.0046 \u2705 PASS Minimum stat_min -148 -148 1 \u2705 PASS 10th percentile stat_p10 -13.8 -13.8 0.5 \u2705 PASS 90th percentile stat_p90 12.1 12.1 0.4 \u2705 PASS Maximum stat_max 155 155 1 \u2705 PASS Interquartile range stat_iqr 9.35 9.35 0.15 \u2705 PASS Range stat_range 303 303 2 \u2705 PASS Mean absolute deviation stat_mad 9.26 9.26 0.22 \u2705 PASS Robust mean absolute deviation stat_rmad 4.21 4.21 0.09 \u2705 PASS Median absolute deviation stat_medad 9.25 9.25 0.22 \u2705 PASS Coefficient of variation stat_cov -86.9 -86.9 32.6 \u2705 PASS Quartile coefficient of dispersion stat_qcod -162 -162 27 \u2705 PASS Energy stat_energy 8.96e+07 8.96e+07 5.3e+06 \u2705 PASS Root mean square stat_rms 15.8 15.8 0.3 \u2705 PASS"},{"location":"ibsi2_phase2_compliance/#test-7b-daubechies-3","title":"Test 7.B: Daubechies 3","text":"<p>Configuration: HHH level 2, rot-inv avg</p> Feature Code Calc Ref Tol Status Mean stat_mean -0.0406 -0.0406 0.0051 \u2705 PASS Variance stat_var 422 422 11 \u2705 PASS Skewness stat_skew -0.0112 -0.0112 0.0027 \u2705 PASS (Excess) kurtosis stat_kurt 5.45 5.45 0.09 \u2705 PASS Median stat_median -0.0164 -0.0164 0.0013 \u2705 PASS Minimum stat_min -203 -203 3 \u2705 PASS 10th percentile stat_p10 -20.6 -20.6 0.4 \u2705 PASS 90th percentile stat_p90 20.4 20.4 0.4 \u2705 PASS Maximum stat_max 201 201 4 \u2705 PASS Interquartile range stat_iqr 16.3 16.3 0.2 \u2705 PASS Range stat_range 404 404 7 \u2705 PASS Mean absolute deviation stat_mad 13.4 13.4 0.2 \u2705 PASS Robust mean absolute deviation stat_rmad 7.2 7.2 0.1 \u2705 PASS Median absolute deviation stat_medad 13.4 13.4 0.2 \u2705 PASS Coefficient of variation stat_cov -506 -506 149 \u2705 PASS Quartile coefficient of dispersion stat_qcod -684 -684 130 \u2705 PASS Energy stat_energy 1.51e+08 1.51e+08 7e+06 \u2705 PASS Root mean square stat_rms 20.6 20.6 0.3 \u2705 PASS"},{"location":"ibsi2_phase2_compliance/#test-8b-simoncelli","title":"Test 8.B: Simoncelli","text":"<p>Configuration: B map level 1</p> Feature Code Calc Ref Tol Status Mean stat_mean 0.32 0.32 0.059 \u2705 PASS Variance stat_var 1.81e+03 1.81e+03 70 \u2705 PASS Skewness stat_skew -0.0719 -0.0719 0.0163 \u2705 PASS (Excess) kurtosis stat_kurt 7.64 7.64 0.33 \u2705 PASS Median stat_median -0.00194 -0.00947 0.0107 \u2705 PASS Minimum stat_min -411 -411 5 \u2705 PASS 10th percentile stat_p10 -36.6 -36.5 1.3 \u2705 PASS 90th percentile stat_p90 38.1 38.1 1.3 \u2705 PASS Maximum stat_max 374 374 3 \u2705 PASS Interquartile range stat_iqr 25.5 25.5 0.4 \u2705 PASS Range stat_range 785 785 6 \u2705 PASS Mean absolute deviation stat_mad 25.3 25.3 0.6 \u2705 PASS Robust mean absolute deviation stat_rmad 11.7 11.7 0.3 \u2705 PASS Median absolute deviation stat_medad 25.3 25.3 0.6 \u2705 PASS Coefficient of variation stat_cov 133 134 27 \u2705 PASS Energy stat_energy 6.48e+08 6.48e+08 3.9e+07 \u2705 PASS Root mean square stat_rms 42.5 42.5 0.9 \u2705 PASS"},{"location":"ibsi2_phase2_compliance/#test-9b-simoncelli","title":"Test 9.B: Simoncelli","text":"<p>Configuration: B map level 2</p> Feature Code Calc Ref Tol Status Mean stat_mean 2.68 2.68 0.22 \u2705 PASS Variance stat_var 5.49e+03 5.49e+03 220 \u2705 PASS Skewness stat_skew -0.0858 -0.0858 0.0107 \u2705 PASS (Excess) kurtosis stat_kurt 5.58 5.58 0.18 \u2705 PASS Median stat_median 0.233 0.233 0.046 \u2705 PASS Minimum stat_min -605 -605 2 \u2705 PASS 10th percentile stat_p10 -65.9 -65.9 2.2 \u2705 PASS 90th percentile stat_p90 82.9 82.8 1.8 \u2705 PASS Maximum stat_max 471 471 13 \u2705 PASS Interquartile range stat_iqr 41.4 41 1 \u2705 PASS Range stat_range 1.08e+03 1.08e+03 20 \u2705 PASS Mean absolute deviation stat_mad 45.1 45.1 1.1 \u2705 PASS Robust mean absolute deviation stat_rmad 21 21 0.5 \u2705 PASS Median absolute deviation stat_medad 45 45 1.1 \u2705 PASS Coefficient of variation stat_cov 27.7 27.7 20.4 \u2705 PASS Quartile coefficient of dispersion stat_qcod 47.4 47.4 20.7 \u2705 PASS Energy stat_energy 1.97e+09 1.97e+09 1.4e+08 \u2705 PASS Root mean square stat_rms 74.1 74.1 1.6 \u2705 PASS"},{"location":"ibsi2_phase2_compliance/#known-deviations","title":"Known Deviations","text":""},{"location":"ibsi2_phase2_compliance/#2d-configuration-config-a-not-implemented","title":"2D Configuration (Config A) Not Implemented","text":"<p>The following tests check filter performance in 2D mode (Config A). These are skipped because Pictologics is purely a 3D radiomics library.</p> <ul> <li>Tests 1.A - 9.A: 2D versions of the above tests.</li> </ul>"},{"location":"ibsi2_phase3_compliance/","title":"IBSI 2 Phase 3 Compliance: Reproducibility","text":""},{"location":"ibsi2_phase3_compliance/#overview","title":"Overview","text":"<p>IBSI 2 Phase 3 focuses on reproducibility across different software implementations. Unlike Phases 1 and 2, there are no consensus reference values. Instead, participant results are compared against each other to measure overlap.</p> <p>This page documents Pictologics' agreement with 9 other teams on a multimodal (CT, MRI, PET) soft-tissue sarcoma dataset of 51 patients.</p>"},{"location":"ibsi2_phase3_compliance/#how-to-run-the-benchmarks","title":"How to Run the Benchmarks","text":""},{"location":"ibsi2_phase3_compliance/#1-download-the-data","title":"1. Download the Data","text":"<ul> <li>IBSI 2 Phase 3 Data: Please refer to the IBSI GitHub repository for all data download instructions.</li> </ul> <p>Organize the data as follows: - <code>data/ibsi2/data/validation/ct/*.nii.gz</code> - <code>data/ibsi2/data/validation/pet/*.nii.gz</code> - <code>data/ibsi2/data/validation/mri/*.nii.gz</code> - <code>data/ibsi2/data/validation/masks/*.nii.gz</code></p>"},{"location":"ibsi2_phase3_compliance/#2-run-validation-programmatically","title":"2. Run Validation Programmatically","text":"<pre><code>from pictologics import RadiomicsPipeline\n\n# Example: Run Mean filter (ID 2) on a specific patient/modality\nimage_path = \"data/ibsi2/data/validation/ct/STS_001_image.nii.gz\"\nmask_path = \"data/ibsi2/data/validation/masks/STS_001_CT_mask.nii.gz\"\n\npipeline = RadiomicsPipeline()\n\n# Define IBSI 2 Phase 3 CT Preprocessing\npreprocess_steps = [\n    {\"step\": \"resample\", \"params\": {\n        \"new_spacing\": (1.0, 1.0, 1.0),\n        \"interpolation\": \"cubic\",\n        \"mask_interpolation\": \"linear\",\n        \"mask_threshold\": 0.5\n    }},\n    {\"step\": \"round_intensities\", \"params\": {}},\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -200, \"range_max\": 200}}\n]\n\n# Add Mean Filter (ID 2) and Feature Extraction\nconfig = preprocess_steps + [\n    {\"step\": \"filter\", \"params\": {\"type\": \"mean\", \"support\": 3}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\"]}}\n]\n\npipeline.add_config(\"phase3_demo\", config)\nresults = pipeline.run(image_path, mask_path, config_names=[\"phase3_demo\"])\nprint(results[\"phase3_demo\"])\n</code></pre> <p>Execution Time</p> <p>Processing 153 scans (51 patients \u00d7 3 modalities) with multiple filters takes significant time (minutes to hours depending on CPU). The full validation suite is designed to run in parallel on multiple cores.</p>"},{"location":"ibsi2_phase3_compliance/#phase-3-results","title":"Phase 3 Results","text":"<p>Summary: Processed 153 patients, compared against 9 teams.</p>"},{"location":"ibsi2_phase3_compliance/#tolerance-breakdown-of-feature-range","title":"Tolerance Breakdown (% of Feature Range)","text":"<p>Matches are counted if the error is within a percentage of the feature range (max-min across all team values).</p> Team Total Features Within 1% Within 5% Within 10% Status CERR 16524 16408 (99.3%) 16492 (99.8%) 16503 (99.9%) \u2705 95%+ Cardiff University 24786 24532 (99.0%) 24696 (99.6%) 24746 (99.8%) \u2705 95%+ King's College London 23868 23744 (99.5%) 23847 (99.9%) 23858 (100.0%) \u2705 95%+ NCT Dresden 24786 24674 (99.5%) 24740 (99.8%) 24769 (99.9%) \u2705 95%+ Qurit SERA 24751 23074 (93.2%) 24130 (97.5%) 24393 (98.6%) \u2705 95%+ UCSF 24786 15323 (61.8%) 19001 (76.7%) 20886 (84.3%) \ud83d\udfe2 80-95% USZ 16524 16499 (99.8%) 16516 (100.0%) 16520 (100.0%) \u2705 95%+ UdeS 24786 22266 (89.8%) 23388 (94.4%) 24172 (97.5%) \u2705 95%+ Veneto Institute of Oncology 24786 22571 (91.1%) 24016 (96.9%) 24468 (98.7%) \u2705 95%+"},{"location":"ibsi2_phase3_compliance/#agreement-by-configuration","title":"Agreement by Configuration","text":"<p>The heatmap below shows the percentage of features matching within 10% of feature range for each configuration/team combination.</p> <p></p>"},{"location":"ibsi2_phase3_compliance/#per-patient-agreement","title":"Per-Patient Agreement","text":"<p>The heatmap below shows per-patient agreement with each team across all filter configurations.</p> <p></p>"},{"location":"ibsi2_phase3_compliance/#mismatch-details","title":"Mismatch Details","text":"<p>Mismatches shown below are cases where the error exceeds 10% of the team's feature range.</p>"},{"location":"ibsi2_phase3_compliance/#configuration-legend","title":"Configuration Legend","text":"Short Name Full Description None No filter (baseline) Mean Mean filter (3\u00d73\u00d73 kernel) LoG Laplacian of Gaussian (\u03c3=3mm) Laws Laws S5E5L5 texture filter Gabor Gabor filter (2D, \u03b8=-5\u03c0/8) Coif3 LHH L1 Coiflet 3 wavelet, LHH decomposition, level 1 Coif3 HHH L2 Coiflet 3 wavelet, HHH decomposition, level 2 Simon. L1 Simoncelli steerable pyramid, level 1 Simon. L2 Simoncelli steerable pyramid, level 2"},{"location":"ibsi2_phase3_compliance/#cerr","title":"CERR","text":"Patient Feature Configuration Pictologics Value Team Value Error STS_001_CT stat_cov Coif3 LHH L1 -1.627e+04 1.448e+04 3.075e+04 STS_004_CT stat_cov Coif3 LHH L1 3.534e+04 -2212 3.755e+04 STS_007_MRI stat_qcod Coif3 LHH L1 -1695 7.673e+05 7.69e+05 STS_012_CT stat_cov Coif3 LHH L1 -3.282e+05 6075 3.343e+05 STS_017_CT stat_cov Coif3 LHH L1 -9388 1.614e+04 2.553e+04 STS_021_MRI stat_cov Coif3 LHH L1 5.033e+04 7908 4.242e+04 STS_021_CT stat_cov Coif3 LHH L1 -4.246e+04 1.327e+04 5.573e+04 STS_026_MRI stat_cov Coif3 LHH L1 -1895 -6.471e+04 6.281e+04 STS_026_CT stat_cov Coif3 LHH L1 -9.46e+04 -8537 8.606e+04 STS_028_CT stat_cov Coif3 LHH L1 2.898e+04 -9.878e+04 1.278e+05 Show 11 more mismatches... PatientFeatureConfigurationPictologics ValueTeam ValueError STS_020_CTstat_covCoif3 LHH L11.103e+041.014e+059.038e+04 STS_030_MRIstat_covCoif3 LHH L11.755e+051.132e+056.23e+04 STS_033_CTstat_covCoif3 LHH L1-18745.532e+045.719e+04 STS_034_MRIstat_minMean194.1131.462.69 STS_038_CTstat_covCoif3 LHH L11.289e+04-1.31e+051.439e+05 STS_037_MRIstat_covCoif3 LHH L1-3536-3.211e+042.857e+04 STS_043_MRIstat_covCoif3 LHH L1-1.382e+05-28211.354e+05 STS_044_MRIstat_minMean216145.370.74 STS_047_MRIstat_covCoif3 LHH L12.372e+04-20642.579e+04 STS_047_CTstat_covCoif3 LHH L1-3.454e+04-28183.173e+04 STS_048_CTstat_covCoif3 LHH L1-33653.858e+044.194e+04"},{"location":"ibsi2_phase3_compliance/#cardiff-university","title":"Cardiff University","text":"Patient Feature Configuration Pictologics Value Team Value Error STS_012_CT stat_cov Coif3 LHH L1 -3.282e+05 -2.762e+05 5.201e+04 STS_009_MRI stat_skew Coif3 HHH L2 5.766 9.081 3.316 STS_009_MRI stat_kurt Coif3 HHH L2 61.96 219.7 157.7 STS_009_CT stat_skew Coif3 HHH L2 0.9676 3.349 2.381 STS_009_CT stat_max Coif3 HHH L2 262 557.6 295.6 STS_009_CT stat_range Coif3 HHH L2 297.7 605.8 308.2 STS_022_MRI stat_skew Coif3 HHH L2 6.505 9.197 2.691 STS_022_MRI stat_skew Simon. L1 -1.804 2.364 4.169 STS_022_MRI stat_kurt Coif3 HHH L2 68.97 178.8 109.8 STS_022_MRI stat_min Simon. L1 -200.5 -74.12 126.4 Show 30 more mismatches... PatientFeatureConfigurationPictologics ValueTeam ValueError STS_022_CTstat_skewCoif3 HHH L20.6573.0472.39 STS_022_CTstat_skewSimon. L24.0170.061063.956 STS_022_CTstat_kurtSimon. L257.92.28955.61 STS_022_CTstat_minSimon. L1-221.9-108.4113.5 STS_022_CTstat_rangeSimon. L1529.4223.3306.2 STS_026_PETstat_skewCoif3 HHH L20.62742.6151.988 STS_021_CTstat_qcodSimon. L11.846e+05-5.431e+042.389e+05 STS_026_CTstat_skewSimon. L12.421-0.0030992.424 STS_026_CTstat_skewSimon. L24.5690.79063.779 STS_026_CTstat_kurtSimon. L248.536.14242.39 STS_026_CTstat_covCoif3 LHH L1-9.46e+04-1.14e+048.32e+04 STS_020_CTstat_skewCoif3 HHH L23.3616.3973.035 STS_020_CTstat_kurtCoif3 HHH L228.58164135.5 STS_020_CTstat_maxCoif3 HHH L2487.516411153 STS_020_CTstat_rangeCoif3 HHH L2524.216781153 STS_034_MRIstat_skewSimon. L11.504-0.62782.131 STS_034_MRIstat_qcodCoif3 LHH L1-441568021.122e+04 STS_034_CTstat_skewCoif3 HHH L21.5233.712.187 STS_034_CTstat_skewSimon. L12.079-0.12242.201 STS_038_MRIstat_skewCoif3 HHH L25.45711.886.422 STS_038_MRIstat_kurtCoif3 HHH L265.92405.6339.7 STS_038_CTstat_skewSimon. L11.972-0.067512.039 STS_038_CTstat_qcodSimon. L11.217e+05-227.31.219e+05 STS_043_MRIstat_covCoif3 LHH L1-1.382e+053.861e+041.769e+05 STS_045_MRIstat_skewCoif3 HHH L22.9695.6562.688 STS_045_MRIstat_kurtCoif3 HHH L217.1378.0260.88 STS_044_CTstat_skewCoif3 HHH L22.8155.5692.753 STS_044_CTstat_kurtCoif3 HHH L229.3995.6966.3 STS_044_CTstat_qcodSimon. L2-325.271357460 STS_050_CTstat_qcodCoif3 LHH L13.174e+04-1.726e+044.9e+04"},{"location":"ibsi2_phase3_compliance/#kings-college-london","title":"King's College London","text":"Patient Feature Configuration Pictologics Value Team Value Error STS_011_CT stat_cov Coif3 LHH L1 6854 -1.003e+05 1.072e+05 STS_015_CT stat_cov Coif3 LHH L1 1.458e+04 1.602e+05 1.456e+05 STS_016_CT stat_cov Coif3 LHH L1 -2.528e+04 -7.642e+04 5.114e+04 STS_021_CT stat_qcod Simon. L1 1.846e+05 -1.688e+04 2.015e+05 STS_026_CT stat_cov Coif3 LHH L1 -9.46e+04 1.903e+04 1.136e+05 STS_031_MRI stat_qcod Simon. L1 1896 3.754e+05 3.735e+05 STS_038_MRI stat_kurt Coif3 HHH L2 65.92 32.21 33.71 STS_036_MRI stat_cov Simon. L1 -1.632e+04 1.75e+05 1.913e+05 STS_038_CT stat_qcod Simon. L1 1.217e+05 2942 1.187e+05 STS_043_MRI stat_cov Coif3 LHH L1 -1.382e+05 3.368e+04 1.719e+05"},{"location":"ibsi2_phase3_compliance/#nct-dresden","title":"NCT Dresden","text":"Patient Feature Configuration Pictologics Value Team Value Error STS_009_CT stat_skew Simon. L1 0.9229 -0.8417 1.765 STS_022_MRI stat_skew Simon. L1 -1.804 2.289 4.093 STS_022_MRI stat_kurt Simon. L1 55.15 20.64 34.51 STS_022_MRI stat_min Simon. L1 -200.5 -72.72 127.8 STS_022_CT stat_skew Simon. L2 4.017 2.07 1.946 STS_022_CT stat_kurt Simon. L2 57.9 36.77 21.13 STS_021_CT stat_qcod Simon. L1 1.846e+05 8.06e+04 1.04e+05 STS_026_CT stat_skew Simon. L1 2.421 -0.3173 2.738 STS_026_CT stat_kurt Simon. L1 24.72 1.51 23.21 STS_034_MRI stat_skew Simon. L1 1.504 -0.5846 2.088 Show 7 more mismatches... PatientFeatureConfigurationPictologics ValueTeam ValueError STS_034_CTstat_skewSimon. L12.079-0.28562.365 STS_034_CTstat_kurtSimon. L118.161.30716.85 STS_036_MRIstat_covSimon. L1-1.632e+048.126e+058.289e+05 STS_038_CTstat_skewSimon. L11.972-0.068272.04 STS_038_CTstat_kurtSimon. L133.342.44530.9 STS_038_CTstat_qcodSimon. L11.217e+05-2251.219e+05 STS_043_MRIstat_covCoif3 LHH L1-1.382e+053.861e+041.769e+05"},{"location":"ibsi2_phase3_compliance/#qurit-sera","title":"Qurit SERA","text":"Patient Feature Configuration Pictologics Value Team Value Error STS_002_CT stat_mean Gabor 194.5 119.6 74.92 STS_002_CT stat_var Gabor 1.105e+04 2371 8684 STS_002_CT stat_p90 Gabor 341.7 184 157.7 STS_002_CT stat_max Gabor 614.8 200 414.8 STS_002_CT stat_iqr Gabor 149.3 78 71.3 STS_002_CT stat_range Gabor 614.2 199 415.2 STS_002_CT stat_mad Gabor 85.46 41.15 44.31 STS_002_CT stat_rmad Gabor 61.5 31.78 29.72 STS_002_CT stat_medad Gabor 84.75 41.06 43.69 STS_002_CT stat_rms Gabor 221.1 129.2 91.98 Show 348 more mismatches... PatientFeatureConfigurationPictologics ValueTeam ValueError STS_003_CTstat_meanGabor260.1126.1134 STS_003_CTstat_varGabor1.872e+0423501.637e+04 STS_003_CTstat_medianGabor244.6131113.6 STS_003_CTstat_p90Gabor448.6188260.6 STS_003_CTstat_maxGabor1009200808.8 STS_003_CTstat_iqrGabor192.177115.1 STS_003_CTstat_rangeGabor1009200808.7 STS_003_CTstat_madGabor110.440.969.55 STS_003_CTstat_rmadGabor79.0631.5747.48 STS_003_CTstat_medadGabor109.840.7169.08 STS_003_CTstat_rmsGabor293.9135.1158.8 STS_004_MRIstat_minNone39.51-85.25124.8 STS_004_MRIstat_minMean72.42-54.78127.2 STS_001_CTstat_varGabor1.245e+0425459906 STS_001_CTstat_p90Gabor326.4177149.4 STS_001_CTstat_maxGabor863.4200663.4 STS_001_CTstat_iqrGabor149.18267.1 STS_001_CTstat_rangeGabor863.3200663.3 STS_001_CTstat_madGabor88.4742.9445.53 STS_001_CTstat_rmadGabor62.0733.3328.74 STS_001_CTstat_medadGabor86.5842.9343.66 STS_001_CTstat_rmsGabor203.7117.586.16 STS_004_CTstat_varGabor900623836624 STS_004_CTstat_p90Gabor297.8181116.8 STS_004_CTstat_maxGabor677.2200477.2 STS_004_CTstat_iqrGabor126.77848.7 STS_004_CTstat_rangeGabor676.9200476.9 STS_004_CTstat_madGabor75.4241.2634.16 STS_004_CTstat_rmadGabor52.7831.7721.01 STS_004_CTstat_medadGabor74.4341.2533.18 STS_004_CTstat_covCoif3 LHH L13.534e+0453373e+04 STS_006_CTstat_meanGabor267.8126.9140.9 STS_006_CTstat_varGabor1.999e+0423171.767e+04 STS_006_CTstat_medianGabor251.2133118.2 STS_006_CTstat_p90Gabor460.2187273.2 STS_006_CTstat_maxGabor932.4200732.4 STS_006_CTstat_iqrGabor195.977118.9 STS_006_CTstat_rangeGabor932200732 STS_006_CTstat_madGabor113.740.5873.11 STS_006_CTstat_rmadGabor80.9531.0649.89 STS_006_CTstat_medadGabor112.940.3572.59 STS_006_CTstat_rmsGabor302.9135.7167.1 STS_005_CTstat_meanGabor280.9127.3153.6 STS_005_CTstat_varGabor2.541e+0422982.311e+04 STS_005_CTstat_medianGabor255.7133122.7 STS_005_CTstat_minSimon. L2-313-200113 STS_005_CTstat_p90Gabor496.6188308.6 STS_005_CTstat_maxGabor12932001093 STS_005_CTstat_iqrGabor208.176132.1 STS_005_CTstat_rangeGabor12921991093 STS_005_CTstat_madGabor125.240.3884.82 STS_005_CTstat_rmadGabor86.6631.0655.6 STS_005_CTstat_medadGabor123.640.1583.43 STS_005_CTstat_rmsGabor323136.1186.9 STS_008_CTstat_minSimon. L2-313.1-200113.1 STS_008_CTstat_maxGabor1037200837.2 STS_008_CTstat_rangeGabor1037200837 STS_007_CTstat_meanGabor231.3124107.3 STS_007_CTstat_varGabor1.657e+0423481.423e+04 STS_007_CTstat_medianGabor212.512983.5 STS_007_CTstat_p90Gabor405.9186219.9 STS_007_CTstat_maxGabor1002200802.3 STS_007_CTstat_iqrGabor172.27795.2 STS_007_CTstat_rangeGabor1002199802.8 STS_007_CTstat_madGabor102.140.9461.15 STS_007_CTstat_rmadGabor71.431.6339.77 STS_007_CTstat_medadGabor10140.7960.21 STS_007_CTstat_rmsGabor264.7133.2131.5 STS_012_CTstat_covCoif3 LHH L1-3.282e+0526283.309e+05 STS_009_MRIstat_skewCoif3 HHH L25.76511.986.217 STS_009_MRIstat_kurtCoif3 HHH L261.96281.3219.4 STS_009_CTstat_skewMean4.4521.013.443 STS_009_CTstat_minSimon. L1-286.6-20086.6 STS_009_CTstat_minSimon. L2-428-200228 STS_009_CTstat_maxGabor15092001309 STS_009_CTstat_rangeGabor15092001309 STS_009_CTstat_rangeSimon. L2755.3386369.3 STS_019_CTstat_skewGabor6.9182.6014.317 STS_019_CTstat_maxGabor1172200972 STS_019_CTstat_rangeGabor1172200971.9 STS_018_CTstat_varGabor826120476214 STS_018_CTstat_skewGabor5.6610.45385.207 STS_018_CTstat_maxGabor17702001570 STS_018_CTstat_rangeGabor17702001570 STS_016_CTstat_maxGabor593.9200393.9 STS_016_CTstat_rangeGabor593.9200393.9 STS_016_CTstat_covCoif3 LHH L1-2.528e+04-16432.363e+04 STS_013_CTstat_skewGabor7.970.59957.371 STS_013_CTstat_skewSimon. L2-3.493-0.32133.171 STS_013_CTstat_kurtGabor107.7-0.2043107.9 STS_013_CTstat_minSimon. L1-304.5-200104.5 STS_013_CTstat_minSimon. L2-465.1-200265.1 STS_013_CTstat_maxLaws540.1200340.1 STS_013_CTstat_maxGabor25932002393 STS_013_CTstat_rangeLaws526.5186340.5 STS_013_CTstat_rangeGabor25932002393 STS_013_CTstat_rangeSimon. L2684.3397287.3 STS_017_CTstat_maxGabor809.4200609.4 STS_017_CTstat_rangeGabor809.3200609.3 STS_022_MRIstat_skewCoif3 HHH L26.50515.078.562 STS_022_MRIstat_skewSimon. L1-1.8042.8254.629 STS_022_MRIstat_kurtCoif3 HHH L268.97411.2342.2 STS_022_MRIstat_minSimon. L1-200.5-78.62121.9 STS_022_CTstat_meanGabor307.1128.1179 STS_022_CTstat_varGabor2.893e+0423212.661e+04 STS_022_CTstat_skewSimon. L24.017-0.364.377 STS_022_CTstat_medianGabor282.8134148.8 STS_022_CTstat_minSimon. L1-221.9-107114.9 STS_022_CTstat_minSimon. L2-223.5-13093.5 STS_022_CTstat_p90Gabor537.9188349.9 STS_022_CTstat_maxGabor13132001113 STS_022_CTstat_iqrGabor226.676150.6 STS_022_CTstat_rangeGabor13132001113 STS_022_CTstat_rangeSimon. L1529.4230299.4 STS_022_CTstat_madGabor134.740.6294.09 STS_022_CTstat_rmadGabor94.0631.1762.89 STS_022_CTstat_medadGabor133.340.3792.96 STS_022_CTstat_energyGabor1.007e+114.488e+099.621e+10 STS_022_CTstat_rmsGabor351.1136.8214.2 STS_023_CTstat_varGabor989224727420 STS_023_CTstat_minSimon. L2-346.5-200146.5 STS_023_CTstat_p90Gabor299.2180119.2 STS_023_CTstat_maxGabor1068200867.9 STS_023_CTstat_iqrGabor133.68053.6 STS_023_CTstat_rangeGabor1068200867.6 STS_023_CTstat_madGabor78.6442.2136.44 STS_023_CTstat_rmadGabor55.2732.8122.45 STS_023_CTstat_medadGabor77.642.235.4 STS_021_CTstat_maxGabor615.8200415.8 STS_021_CTstat_rangeGabor615.8200415.8 STS_021_CTstat_madGabor61.6840.8120.87 STS_021_CTstat_medadGabor61.1340.820.32 STS_026_CTstat_meanGabor219.7124.595.16 STS_026_CTstat_varGabor1.384e+0423451.15e+04 STS_026_CTstat_skewSimon. L24.569-0.19624.766 STS_026_CTstat_p90Gabor380.3186194.3 STS_026_CTstat_maxGabor903.2200703.2 STS_026_CTstat_iqrGabor159.77683.7 STS_026_CTstat_rangeGabor902.7200702.7 STS_026_CTstat_madGabor93.8840.8253.06 STS_026_CTstat_rmadGabor66.1531.4234.73 STS_026_CTstat_medadGabor93.1140.6552.45 STS_026_CTstat_covCoif3 LHH L1-9.46e+04-2.449e+047.011e+04 STS_026_CTstat_rmsGabor249.2133.6115.6 STS_028_CTstat_skewCoif3 HHH L23.9147.3193.404 STS_028_CTstat_covCoif3 LHH L12.898e+04-5622.955e+04 STS_029_CTstat_meanGabor217.4121.895.55 STS_029_CTstat_varGabor1.63e+0423681.393e+04 STS_029_CTstat_p90Gabor387.7185202.7 STS_029_CTstat_maxGabor1036200836.5 STS_029_CTstat_iqrGabor162.17884.1 STS_029_CTstat_rangeGabor1036200836 STS_029_CTstat_madGabor99.2641.1458.12 STS_029_CTstat_rmadGabor67.7531.8335.92 STS_029_CTstat_medadGabor97.7841.0456.74 STS_029_CTstat_rmsGabor252.1131.2120.9 STS_020_CTstat_meanGabor246.3121.3124.9 STS_020_CTstat_varGabor2.131e+0424461.886e+04 STS_020_CTstat_medianGabor222.812597.8 STS_020_CTstat_minSimon. L1-376.9-200176.9 STS_020_CTstat_minSimon. L2-470.2-200270.2 STS_020_CTstat_p90Gabor443.1186257.1 STS_020_CTstat_maxGabor14382001238 STS_020_CTstat_maxCoif3 HHH L2487.5200287.5 STS_020_CTstat_iqrGabor193.280113.2 STS_020_CTstat_rangeGabor14382001238 STS_020_CTstat_rangeCoif3 HHH L2524.2210314.2 STS_020_CTstat_rangeSimon. L1686.1357329.1 STS_020_CTstat_rangeSimon. L2781387394 STS_020_CTstat_madGabor115.141.9873.09 STS_020_CTstat_rmadGabor80.2232.6447.59 STS_020_CTstat_medadGabor113.641.8771.69 STS_020_CTstat_energyGabor1.901e+111.735e+101.728e+11 STS_020_CTstat_rmsGabor286.3131155.3 STS_027_CTstat_meanGabor239.4116.3123.1 STS_027_CTstat_varGabor2.724e+0424272.482e+04 STS_027_CTstat_skewSimon. L2-4.122-1.2452.877 STS_027_CTstat_medianGabor200.811882.8 STS_027_CTstat_minSimon. L1-300-200100 STS_027_CTstat_minSimon. L2-400.4-200200.4 STS_027_CTstat_p90Gabor465183282 STS_027_CTstat_maxLaws483.7200283.7 STS_027_CTstat_maxGabor21492001949 STS_027_CTstat_iqrGabor203.579124.5 STS_027_CTstat_rangeLaws458.1174284.1 STS_027_CTstat_rangeGabor21492001949 STS_027_CTstat_madGabor126.841.885.02 STS_027_CTstat_rmadGabor85.8132.4553.36 STS_027_CTstat_medadGabor12341.7881.23 STS_027_CTstat_rmsGabor290.8126.3164.5 STS_031_CTstat_meanGabor268.2124.7143.5 STS_031_CTstat_varGabor2.451e+0423522.216e+04 STS_031_CTstat_medianGabor241.7130111.7 STS_031_CTstat_p90Gabor483.3187296.3 STS_031_CTstat_maxGabor1071200870.7 STS_031_CTstat_iqrGabor207.978129.9 STS_031_CTstat_rangeGabor1070199871.2 STS_031_CTstat_madGabor124.140.9383.14 STS_031_CTstat_rmadGabor86.5631.5455.02 STS_031_CTstat_medadGabor122.340.7681.54 STS_031_CTstat_rmsGabor310.6133.8176.8 STS_033_CTstat_minMean-307.9-200107.9 STS_033_CTstat_maxGabor497.8200297.8 STS_033_CTstat_rangeGabor497.7200297.7 STS_034_MRIstat_skewCoif3 HHH L23.9317.6243.693 STS_035_CTstat_varLaws1.275e+0423611.039e+04 STS_035_CTstat_varGabor2.263e+0414042.123e+04 STS_035_CTstat_varSimon. L21.69e+0439481.295e+04 STS_035_CTstat_skewGabor5.8291.4424.387 STS_035_CTstat_minSimon. L1-386.4-200186.4 STS_035_CTstat_minSimon. L2-434-200234 STS_035_CTstat_p10Simon. L2-274.9-47227.9 STS_035_CTstat_p90Laws305.9153152.9 STS_035_CTstat_maxLaws511.8200311.8 STS_035_CTstat_maxGabor18172001617 STS_035_CTstat_iqrLaws161.756105.7 STS_035_CTstat_rangeLaws489.5178311.5 STS_035_CTstat_rangeGabor18172001617 STS_035_CTstat_madLaws94.7139.3155.4 STS_035_CTstat_madGabor71.0328.0842.95 STS_035_CTstat_madSimon. L298.6747.8350.85 STS_035_CTstat_rmadLaws70.5226.6343.89 STS_035_CTstat_rmadSimon. L259.6432.0627.59 STS_035_CTstat_medadLaws85.6733.9551.72 STS_035_CTstat_medadGabor58.1427.0331.1 STS_035_CTstat_medadSimon. L288.847.741.1 STS_035_CTstat_rmsLaws166.282.0184.23 STS_035_CTstat_rmsGabor173.465.47108 STS_034_CTstat_meanGabor218.1102.2115.9 STS_034_CTstat_varGabor2.749e+0427552.474e+04 STS_034_CTstat_medianGabor177.59978.5 STS_034_CTstat_minSimon. L1-173.2-10073.2 STS_034_CTstat_p90Gabor445.9177268.9 STS_034_CTstat_maxGabor13752001175 STS_034_CTstat_iqrGabor213.788125.7 STS_034_CTstat_rangeGabor13752001175 STS_034_CTstat_madGabor129.345.0284.33 STS_034_CTstat_rmadGabor89.1335.4153.72 STS_034_CTstat_medadGabor125.444.9780.41 STS_034_CTstat_rmsGabor274114.9159.1 STS_038_MRIstat_skewCoif3 HHH L25.45721.8716.42 STS_038_MRIstat_kurtCoif3 HHH L265.92832.2766.3 STS_036_MRIstat_covSimon. L1-1.632e+04-5.911e+044.279e+04 STS_036_CTstat_minMean-325.9-200125.9 STS_036_CTstat_maxGabor707.8200507.8 STS_036_CTstat_rangeGabor707.7200507.7 STS_036_CTstat_madGabor64.8241.6723.14 STS_036_CTstat_medadGabor63.5541.621.95 STS_038_CTstat_minSimon. L2-322.7-200122.7 STS_038_CTstat_maxGabor687.4200487.4 STS_038_CTstat_rangeGabor687.2200487.2 STS_038_CTstat_rangeSimon. L2640.3341299.3 STS_038_CTstat_madGabor64.540.8523.65 STS_038_CTstat_medadGabor63.8440.8522.99 STS_041_CTstat_varGabor939323677026 STS_041_CTstat_p90Gabor296.6179117.6 STS_041_CTstat_maxGabor773200573 STS_041_CTstat_iqrGabor123.17845.1 STS_041_CTstat_rangeGabor772.4199573.4 STS_041_CTstat_madGabor75.941.0734.83 STS_041_CTstat_rmadGabor51.9831.6520.33 STS_041_CTstat_medadGabor74.4841.0733.41 STS_037_CTstat_varGabor1.184e+0424679374 STS_037_CTstat_minSimon. L2-291.9-19794.9 STS_037_CTstat_p90Gabor327.5181146.5 STS_037_CTstat_maxGabor14272001227 STS_037_CTstat_iqrGabor144.18163.1 STS_037_CTstat_rangeGabor14262001226 STS_037_CTstat_madGabor85.9842.1643.82 STS_037_CTstat_rmadGabor59.9332.8727.05 STS_037_CTstat_medadGabor84.5942.1642.43 STS_037_CTstat_rmsGabor208.9123.185.82 STS_039_CTstat_maxGabor888.3200688.3 STS_039_CTstat_rangeGabor888.3200688.3 STS_039_CTstat_madGabor69.7840.9528.83 STS_039_CTstat_medadGabor67.0540.7226.33 STS_040_CTstat_varGabor1.226e+0424009863 STS_040_CTstat_p90Gabor333.9181152.9 STS_040_CTstat_maxGabor846200646 STS_040_CTstat_iqrGabor145.17867.1 STS_040_CTstat_rangeGabor845.9200645.9 STS_040_CTstat_madGabor87.2941.4445.85 STS_040_CTstat_rmadGabor60.4932.0828.42 STS_040_CTstat_medadGabor85.6841.4444.24 STS_040_CTstat_rmsGabor213124.188.9 STS_043_MRIstat_covCoif3 LHH L1-1.382e+053.861e+041.769e+05 STS_043_CTstat_maxGabor846.9200646.9 STS_043_CTstat_rangeGabor846.7200646.7 STS_043_CTstat_madGabor60.7740.9519.83 STS_044_MRIstat_minMean216145.370.74 STS_045_MRIstat_skewCoif3 HHH L22.9698.4025.434 STS_045_MRIstat_kurtCoif3 HHH L217.13139.2122.1 STS_044_CTstat_varGabor1.032e+0424187901 STS_044_CTstat_p90Gabor309.2181128.2 STS_044_CTstat_maxGabor830.6200630.6 STS_044_CTstat_iqrGabor131.67952.6 STS_044_CTstat_rangeGabor830.5200630.5 STS_044_CTstat_madGabor79.5841.6537.93 STS_044_CTstat_rmadGabor54.8732.2422.63 STS_044_CTstat_medadGabor78.441.6436.76 STS_044_CTstat_rmsGabor199.8124.875 STS_047_CTstat_skewGabor9.2341.2387.997 STS_047_CTstat_kurtGabor1332.619130.4 STS_047_CTstat_minSimon. L2-337-200137 STS_047_CTstat_maxGabor15062001306 STS_047_CTstat_rangeGabor15062001306 STS_047_CTstat_covCoif3 LHH L1-3.454e+04-24903.205e+04 STS_045_CTstat_varGabor990624237483 STS_045_CTstat_minSimon. L2-318.8-200118.8 STS_045_CTstat_p90Gabor287175112 STS_045_CTstat_maxGabor12512001051 STS_045_CTstat_iqrGabor122.37844.3 STS_045_CTstat_rangeGabor12502001050 STS_045_CTstat_madGabor76.6241.6334.98 STS_045_CTstat_rmadGabor51.6532.2419.41 STS_045_CTstat_medadGabor74.6541.6133.04 STS_048_CTstat_maxGabor629.7200429.7 STS_048_CTstat_rangeGabor629.5200429.5 STS_048_CTstat_madGabor68.9340.8928.04 STS_048_CTstat_rmadGabor48.3131.4616.86 STS_048_CTstat_medadGabor68.340.8727.44 STS_046_CTstat_minSimon. L2-360.5-200160.5 STS_046_CTstat_maxGabor1073200872.6 STS_046_CTstat_rangeGabor1073200872.6 STS_050_CTstat_meanGabor203.5121.581.95 STS_050_CTstat_varGabor1.317e+0423491.082e+04 STS_050_CTstat_minSimon. L2-317.8-200117.8 STS_050_CTstat_p90Gabor356.5185171.5 STS_050_CTstat_maxGabor838.3200638.3 STS_050_CTstat_iqrGabor151.27774.2 STS_050_CTstat_rangeGabor838200638 STS_050_CTstat_madGabor90.2440.9149.33 STS_050_CTstat_rmadGabor62.6131.6430.97 STS_050_CTstat_medadGabor89.1740.8248.35 STS_050_CTstat_rmsGabor233.6130.8102.8 STS_049_CTstat_skewMean-0.2322-3.2443.012 STS_049_CTstat_skewGabor10.381.5458.834 STS_049_CTstat_skewSimon. L1-4.806-0.79764.009 STS_049_CTstat_kurtGabor156.63.937152.7 STS_049_CTstat_minMean-282.8-20082.8 STS_049_CTstat_minSimon. L1-335.5-200135.5 STS_049_CTstat_minSimon. L2-443.2-200243.2 STS_049_CTstat_maxLaws547200347 STS_049_CTstat_maxGabor24802002280 STS_049_CTstat_rangeMean704.9400304.9 STS_049_CTstat_rangeLaws537.8191346.8 STS_049_CTstat_rangeGabor24802002280 STS_049_CTstat_rangeSimon. L2689.8400289.8"},{"location":"ibsi2_phase3_compliance/#ucsf","title":"UCSF","text":"Patient Feature Configuration Pictologics Value Team Value Error STS_002_PET stat_cov Coif3 LHH L1 -137 0 137 STS_002_PET stat_cov Simon. L1 9 0 9 STS_002_PET stat_cov Simon. L2 4 0 4 STS_002_PET stat_qcod LoG -2 0 2 STS_002_PET stat_qcod Gabor 1 0 1 STS_002_PET stat_qcod Coif3 LHH L1 -47 0 47 STS_002_PET stat_qcod Simon. L1 6 0 6 STS_002_PET stat_qcod Simon. L2 2 0 2 STS_003_PET stat_cov Coif3 LHH L1 854 0 854 STS_003_PET stat_cov Simon. L1 9 0 9 Show 3890 more mismatches... PatientFeatureConfigurationPictologics ValueTeam ValueError STS_003_PETstat_covSimon. L2303 STS_003_PETstat_qcodLoG-101 STS_003_PETstat_qcodCoif3 LHH L1-4500450 STS_003_PETstat_qcodSimon. L1808 STS_003_PETstat_qcodSimon. L2202 STS_001_PETstat_covLoG-505 STS_001_PETstat_covCoif3 LHH L1204302043 STS_001_PETstat_covSimon. L142042 STS_001_PETstat_covSimon. L2707 STS_001_PETstat_qcodLoG-505 STS_001_PETstat_qcodCoif3 LHH L1-2180218 STS_001_PETstat_qcodSimon. L1-32032 STS_001_PETstat_qcodSimon. L2606 STS_004_PETstat_skewGabor202 STS_004_PETstat_covLoG-505 STS_004_PETstat_covCoif3 LHH L1461704617 STS_004_PETstat_covSimon. L11130113 STS_004_PETstat_covSimon. L2505 STS_004_PETstat_qcodLoG-404 STS_004_PETstat_qcodCoif3 LHH L1-4610461 STS_004_PETstat_qcodSimon. L1-21021 STS_004_PETstat_qcodSimon. L2303 STS_002_MRIstat_meanGabor74074 STS_002_MRIstat_varGabor914809148 STS_002_MRIstat_skewGabor404 STS_002_MRIstat_skewCoif3 HHH L2202 STS_002_MRIstat_kurtGabor19019 STS_002_MRIstat_minSimon. L1-1620162 STS_002_MRIstat_minSimon. L2-2490249 STS_002_MRIstat_p10Simon. L2-58058 STS_002_MRIstat_p90Gabor1610161 STS_002_MRIstat_maxLaws2150215 STS_002_MRIstat_maxGabor104201042 STS_002_MRIstat_maxSimon. L11760176 STS_002_MRIstat_maxSimon. L21980198 STS_002_MRIstat_iqrLaws48048 STS_002_MRIstat_iqrGabor60060 STS_002_MRIstat_iqrSimon. L242042 STS_002_MRIstat_rangeLaws2050205 STS_002_MRIstat_rangeGabor104201042 STS_002_MRIstat_rangeSimon. L13380338 STS_002_MRIstat_rangeSimon. L24470447 STS_002_MRIstat_madLaws27027 STS_002_MRIstat_madGabor58058 STS_002_MRIstat_madSimon. L230030 STS_002_MRIstat_rmadLaws19019 STS_002_MRIstat_rmadGabor28028 STS_002_MRIstat_rmadSimon. L218018 STS_002_MRIstat_medadLaws26026 STS_002_MRIstat_medadGabor50050 STS_002_MRIstat_medadSimon. L230030 STS_002_MRIstat_covCoif3 LHH L14580458 STS_002_MRIstat_covSimon. L1-48048 STS_002_MRIstat_covSimon. L2-707 STS_002_MRIstat_qcodLoG101 STS_002_MRIstat_qcodLaws101 STS_002_MRIstat_qcodGabor101 STS_002_MRIstat_qcodCoif3 LHH L1-6230623 STS_002_MRIstat_qcodCoif3 HHH L2101 STS_002_MRIstat_qcodSimon. L1-92092 STS_002_MRIstat_qcodSimon. L2-808 STS_002_MRIstat_rmsGabor1210121 STS_003_MRIstat_skewGabor202 STS_003_MRIstat_skewCoif3 HHH L2202 STS_003_MRIstat_kurtGabor909 STS_003_MRIstat_kurtCoif3 HHH L212012 STS_003_MRIstat_minSimon. L1-1370137 STS_003_MRIstat_minSimon. L2-2460246 STS_003_MRIstat_p10Simon. L2-54054 STS_003_MRIstat_maxLaws1680168 STS_003_MRIstat_maxGabor1400140 STS_003_MRIstat_maxSimon. L11640164 STS_003_MRIstat_maxSimon. L22510251 STS_003_MRIstat_iqrLaws44044 STS_003_MRIstat_iqrSimon. L242042 STS_003_MRIstat_rangeLaws1590159 STS_003_MRIstat_rangeGabor1400140 STS_003_MRIstat_rangeSimon. L13010301 STS_003_MRIstat_rangeSimon. L24970497 STS_003_MRIstat_madLaws24024 STS_003_MRIstat_madSimon. L229029 STS_003_MRIstat_rmadLaws18018 STS_003_MRIstat_rmadSimon. L218018 STS_003_MRIstat_medadLaws23023 STS_003_MRIstat_medadSimon. L228028 STS_003_MRIstat_covCoif3 LHH L1-1540154 STS_003_MRIstat_covSimon. L1-57057 STS_003_MRIstat_covSimon. L2-707 STS_003_MRIstat_qcodLoG101 STS_003_MRIstat_qcodLaws101 STS_003_MRIstat_qcodGabor101 STS_003_MRIstat_qcodCoif3 LHH L13470347 STS_003_MRIstat_qcodCoif3 HHH L2101 STS_003_MRIstat_qcodSimon. L1-54054 STS_003_MRIstat_qcodSimon. L2-808 STS_002_CTstat_meanLaws96096 STS_002_CTstat_meanGabor1950195 STS_002_CTstat_varGabor1.105e+0401.105e+04 STS_002_CTstat_medianLaws95095 STS_002_CTstat_medianGabor1800180 STS_002_CTstat_minLaws77077 STS_002_CTstat_minSimon. L1-1170117 STS_002_CTstat_minSimon. L2-93093 STS_002_CTstat_p10Laws86086 STS_002_CTstat_p10Gabor68068 STS_002_CTstat_p90Laws1060106 STS_002_CTstat_p90Gabor3420342 STS_002_CTstat_maxLaws1430143 STS_002_CTstat_maxGabor6150615 STS_002_CTstat_iqrGabor1490149 STS_002_CTstat_rangeGabor6140614 STS_002_CTstat_rangeCoif3 HHH L21360136 STS_002_CTstat_rangeSimon. L12320232 STS_002_CTstat_rangeSimon. L21880188 STS_002_CTstat_madGabor85085 STS_002_CTstat_madSimon. L122022 STS_002_CTstat_rmadGabor61061 STS_002_CTstat_medadGabor85085 STS_002_CTstat_medadSimon. L122022 STS_002_CTstat_covCoif3 LHH L1169101691 STS_002_CTstat_covSimon. L11130113 STS_002_CTstat_covSimon. L213013 STS_002_CTstat_qcodLoG-101 STS_002_CTstat_qcodCoif3 LHH L11340134 STS_002_CTstat_qcodSimon. L154054 STS_002_CTstat_qcodSimon. L2909 STS_002_CTstat_rmsLaws96096 STS_002_CTstat_rmsGabor2210221 STS_005_PETstat_covLoG-303 STS_005_PETstat_covCoif3 LHH L1244002440 STS_005_PETstat_covSimon. L11690169 STS_005_PETstat_covSimon. L211011 STS_005_PETstat_qcodLoG-303 STS_005_PETstat_qcodCoif3 LHH L1-3240324 STS_005_PETstat_qcodSimon. L1-39039 STS_005_PETstat_qcodSimon. L215015 STS_003_CTstat_meanLaws1190119 STS_003_CTstat_meanGabor2600260 STS_003_CTstat_varGabor1.872e+0401.872e+04 STS_003_CTstat_medianLaws1190119 STS_003_CTstat_medianGabor2450245 STS_003_CTstat_minLaws84084 STS_003_CTstat_minSimon. L1-1310131 STS_003_CTstat_minSimon. L2-85085 STS_003_CTstat_p10Laws1080108 STS_003_CTstat_p10Gabor93093 STS_003_CTstat_p90Laws1310131 STS_003_CTstat_p90Gabor4490449 STS_003_CTstat_maxLaws1710171 STS_003_CTstat_maxGabor100901009 STS_003_CTstat_maxCoif3 HHH L22180218 STS_003_CTstat_maxSimon. L11290129 STS_003_CTstat_iqrGabor1920192 STS_003_CTstat_rangeGabor100901009 STS_003_CTstat_rangeCoif3 HHH L22660266 STS_003_CTstat_rangeSimon. L12590259 STS_003_CTstat_rangeSimon. L21850185 STS_003_CTstat_madGabor1100110 STS_003_CTstat_madCoif3 HHH L222022 STS_003_CTstat_madSimon. L124024 STS_003_CTstat_rmadGabor79079 STS_003_CTstat_medadGabor1100110 STS_003_CTstat_medadCoif3 HHH L222022 STS_003_CTstat_medadSimon. L124024 STS_003_CTstat_covCoif3 LHH L1-6590659 STS_003_CTstat_covSimon. L1-1340134 STS_003_CTstat_covSimon. L244044 STS_003_CTstat_qcodLoG-101 STS_003_CTstat_qcodCoif3 LHH L1-385103851 STS_003_CTstat_qcodSimon. L1-1090109 STS_003_CTstat_qcodSimon. L233033 STS_003_CTstat_rmsLaws1200120 STS_003_CTstat_rmsGabor2940294 STS_006_PETstat_covLoG-404 STS_006_PETstat_covCoif3 LHH L1-8240824 STS_006_PETstat_covSimon. L135035 STS_006_PETstat_covSimon. L2606 STS_006_PETstat_qcodLoG-303 STS_006_PETstat_qcodGabor101 STS_006_PETstat_qcodCoif3 LHH L1-31031 STS_006_PETstat_qcodSimon. L11400140 STS_006_PETstat_qcodSimon. L2303 STS_004_MRIstat_meanGabor78078 STS_004_MRIstat_varGabor738707387 STS_004_MRIstat_skewLaws202 STS_004_MRIstat_skewGabor404 STS_004_MRIstat_skewCoif3 HHH L2303 STS_004_MRIstat_kurtGabor19019 STS_004_MRIstat_kurtCoif3 HHH L215015 STS_004_MRIstat_kurtSimon. L1808 STS_004_MRIstat_minSimon. L1-2400240 STS_004_MRIstat_minSimon. L2-2870287 STS_004_MRIstat_p90Laws1170117 STS_004_MRIstat_p90Gabor1570157 STS_004_MRIstat_maxLaws3290329 STS_004_MRIstat_maxGabor120301203 STS_004_MRIstat_maxSimon. L12900290 STS_004_MRIstat_maxSimon. L24320432 STS_004_MRIstat_iqrLaws48048 STS_004_MRIstat_iqrGabor59059 STS_004_MRIstat_rangeLaws3160316 STS_004_MRIstat_rangeGabor120301203 STS_004_MRIstat_rangeSimon. L15300530 STS_004_MRIstat_rangeSimon. L27190719 STS_004_MRIstat_madLaws31031 STS_004_MRIstat_madGabor52052 STS_004_MRIstat_madSimon. L227027 STS_004_MRIstat_rmadLaws20020 STS_004_MRIstat_rmadGabor26026 STS_004_MRIstat_medadLaws29029 STS_004_MRIstat_medadGabor47047 STS_004_MRIstat_medadSimon. L227027 STS_004_MRIstat_covLoG707 STS_004_MRIstat_covCoif3 LHH L16920692 STS_004_MRIstat_covSimon. L130030 STS_004_MRIstat_covSimon. L213013 STS_004_MRIstat_qcodLoG202 STS_004_MRIstat_qcodCoif3 LHH L11.748e+0401.748e+04 STS_004_MRIstat_qcodCoif3 HHH L2101 STS_004_MRIstat_qcodSimon. L126026 STS_004_MRIstat_qcodSimon. L210010 STS_004_MRIstat_rmsLaws75075 STS_004_MRIstat_rmsGabor1160116 STS_001_MRIstat_meanGabor1120112 STS_001_MRIstat_skewLaws202 STS_001_MRIstat_skewCoif3 HHH L2202 STS_001_MRIstat_kurtLaws12012 STS_001_MRIstat_kurtCoif3 HHH L210010 STS_001_MRIstat_medianGabor1020102 STS_001_MRIstat_minSimon. L1-1120112 STS_001_MRIstat_minSimon. L2-1450145 STS_001_MRIstat_p90Gabor1970197 STS_001_MRIstat_maxLaws1760176 STS_001_MRIstat_maxGabor8260826 STS_001_MRIstat_maxCoif3 HHH L24400440 STS_001_MRIstat_maxSimon. L11370137 STS_001_MRIstat_maxSimon. L21340134 STS_001_MRIstat_iqrGabor83083 STS_001_MRIstat_rangeLaws1440144 STS_001_MRIstat_rangeGabor8260826 STS_001_MRIstat_rangeCoif3 HHH L24760476 STS_001_MRIstat_rangeSimon. L12490249 STS_001_MRIstat_rangeSimon. L22790279 STS_001_MRIstat_madGabor50050 STS_001_MRIstat_rmadGabor35035 STS_001_MRIstat_medadGabor49049 STS_001_MRIstat_covLoG11011 STS_001_MRIstat_covCoif3 LHH L11.033e+0401.033e+04 STS_001_MRIstat_covSimon. L11180118 STS_001_MRIstat_covSimon. L2-33033 STS_001_MRIstat_qcodLoG606 STS_001_MRIstat_qcodCoif3 LHH L1-2150215 STS_001_MRIstat_qcodSimon. L163063 STS_001_MRIstat_qcodSimon. L2-48048 STS_001_MRIstat_rmsGabor1290129 STS_007_PETstat_covLoG-303 STS_007_PETstat_covCoif3 LHH L1-9570957 STS_007_PETstat_covSimon. L11160116 STS_007_PETstat_covSimon. L2606 STS_007_PETstat_qcodLoG-202 STS_007_PETstat_qcodCoif3 LHH L1-3430343 STS_007_PETstat_qcodSimon. L1-1870187 STS_007_PETstat_qcodSimon. L2404 STS_001_CTstat_meanGabor1700170 STS_001_CTstat_varGabor1.245e+0401.245e+04 STS_001_CTstat_medianGabor1470147 STS_001_CTstat_minSimon. L1-79079 STS_001_CTstat_p90Gabor3260326 STS_001_CTstat_maxGabor8630863 STS_001_CTstat_iqrGabor1490149 STS_001_CTstat_rangeGabor8630863 STS_001_CTstat_rangeSimon. L11570157 STS_001_CTstat_madGabor88088 STS_001_CTstat_rmadGabor62062 STS_001_CTstat_medadGabor87087 STS_001_CTstat_covLoG707 STS_001_CTstat_covCoif3 LHH L1-1.627e+0401.627e+04 STS_001_CTstat_covSimon. L1159801598 STS_001_CTstat_covSimon. L22440244 STS_001_CTstat_qcodLoG505 STS_001_CTstat_qcodCoif3 LHH L1-6730673 STS_001_CTstat_qcodSimon. L12000200 STS_001_CTstat_qcodSimon. L2201802018 STS_001_CTstat_rmsGabor2040204 STS_006_MRIstat_skewLoG202 STS_006_MRIstat_skewGabor202 STS_006_MRIstat_skewCoif3 HHH L2404 STS_006_MRIstat_kurtCoif3 HHH L219019 STS_006_MRIstat_minSimon. L1-1120112 STS_006_MRIstat_minSimon. L2-1700170 STS_006_MRIstat_maxLaws1260126 STS_006_MRIstat_maxSimon. L22350235 STS_006_MRIstat_rangeSimon. L12120212 STS_006_MRIstat_rangeSimon. L24050405 STS_006_MRIstat_covCoif3 LHH L14230423 STS_006_MRIstat_covSimon. L11360136 STS_006_MRIstat_covSimon. L2-39039 STS_006_MRIstat_qcodLoG101 STS_006_MRIstat_qcodLaws101 STS_006_MRIstat_qcodCoif3 LHH L1211002110 STS_006_MRIstat_qcodCoif3 HHH L2101 STS_006_MRIstat_qcodSimon. L11060106 STS_006_MRIstat_qcodSimon. L22800280 STS_005_MRIstat_meanGabor80080 STS_005_MRIstat_skewLaws202 STS_005_MRIstat_skewGabor303 STS_005_MRIstat_skewCoif3 HHH L2404 STS_005_MRIstat_skewSimon. L2-202 STS_005_MRIstat_kurtGabor20020 STS_005_MRIstat_kurtCoif3 HHH L230030 STS_005_MRIstat_kurtSimon. L1909 STS_005_MRIstat_minSimon. L1-2010201 STS_005_MRIstat_minSimon. L2-2470247 STS_005_MRIstat_p90Laws1130113 STS_005_MRIstat_p90Gabor1460146 STS_005_MRIstat_maxLaws3570357 STS_005_MRIstat_maxGabor113101131 STS_005_MRIstat_maxCoif3 HHH L25480548 STS_005_MRIstat_maxSimon. L12500250 STS_005_MRIstat_maxSimon. L22120212 STS_005_MRIstat_iqrGabor58058 STS_005_MRIstat_rangeLaws3330333 STS_005_MRIstat_rangeGabor113101131 STS_005_MRIstat_rangeCoif3 HHH L25750575 STS_005_MRIstat_rangeSimon. L14510451 STS_005_MRIstat_rangeSimon. L24590459 STS_005_MRIstat_madLaws27027 STS_005_MRIstat_madGabor44044 STS_005_MRIstat_madSimon. L223023 STS_005_MRIstat_rmadGabor25025 STS_005_MRIstat_medadLaws23023 STS_005_MRIstat_medadGabor42042 STS_005_MRIstat_medadSimon. L223023 STS_005_MRIstat_covCoif3 LHH L1136501365 STS_005_MRIstat_covSimon. L1-1990199 STS_005_MRIstat_covSimon. L2-909 STS_005_MRIstat_qcodLoG101 STS_005_MRIstat_qcodCoif3 LHH L1-158101581 STS_005_MRIstat_qcodSimon. L11100110 STS_005_MRIstat_qcodSimon. L2-14014 STS_005_MRIstat_rmsGabor1060106 STS_008_PETstat_covCoif3 LHH L166066 STS_008_PETstat_covSimon. L1808 STS_008_PETstat_qcodLoG-101 STS_008_PETstat_qcodCoif3 LHH L1-51051 STS_008_PETstat_qcodSimon. L1606 STS_008_PETstat_qcodSimon. L2101 STS_004_CTstat_meanGabor1660166 STS_004_CTstat_varGabor900609006 STS_004_CTstat_medianGabor1510151 STS_004_CTstat_minSimon. L1-99099 STS_004_CTstat_minSimon. L2-77077 STS_004_CTstat_p10Gabor57057 STS_004_CTstat_p90Gabor2980298 STS_004_CTstat_maxGabor6770677 STS_004_CTstat_iqrGabor1270127 STS_004_CTstat_rangeGabor6770677 STS_004_CTstat_rangeSimon. L11770177 STS_004_CTstat_rangeSimon. L21620162 STS_004_CTstat_madGabor75075 STS_004_CTstat_rmadGabor53053 STS_004_CTstat_medadGabor74074 STS_004_CTstat_covLoG-606 STS_004_CTstat_covCoif3 LHH L13.534e+0403.534e+04 STS_004_CTstat_covSimon. L1-3590359 STS_004_CTstat_covSimon. L21420142 STS_004_CTstat_qcodLoG-303 STS_004_CTstat_qcodCoif3 LHH L1-102301023 STS_004_CTstat_qcodSimon. L1-2690269 STS_004_CTstat_qcodSimon. L231031 STS_004_CTstat_rmsGabor1920192 STS_008_MRIstat_meanGabor81081 STS_008_MRIstat_varGabor637206372 STS_008_MRIstat_skewGabor303 STS_008_MRIstat_skewCoif3 HHH L2202 STS_008_MRIstat_kurtGabor13013 STS_008_MRIstat_kurtSimon. L1808 STS_008_MRIstat_minSimon. L1-1670167 STS_008_MRIstat_minSimon. L2-1820182 STS_008_MRIstat_p90Laws1170117 STS_008_MRIstat_p90Gabor1700170 STS_008_MRIstat_maxLaws2310231 STS_008_MRIstat_maxGabor8450845 STS_008_MRIstat_maxSimon. L11890189 STS_008_MRIstat_maxSimon. L23020302 STS_008_MRIstat_iqrLaws47047 STS_008_MRIstat_iqrGabor64064 STS_008_MRIstat_rangeLaws2150215 STS_008_MRIstat_rangeGabor8440844 STS_008_MRIstat_rangeSimon. L13560356 STS_008_MRIstat_rangeSimon. L24840484 STS_008_MRIstat_madLaws31031 STS_008_MRIstat_madGabor53053 STS_008_MRIstat_madSimon. L229029 STS_008_MRIstat_rmadLaws20020 STS_008_MRIstat_rmadGabor29029 STS_008_MRIstat_medadLaws29029 STS_008_MRIstat_medadGabor48048 STS_008_MRIstat_medadSimon. L229029 STS_008_MRIstat_covLoG303 STS_008_MRIstat_covCoif3 LHH L12890289 STS_008_MRIstat_covSimon. L160060 STS_008_MRIstat_covSimon. L2-62062 STS_008_MRIstat_qcodLoG101 STS_008_MRIstat_qcodCoif3 LHH L1-76076 STS_008_MRIstat_qcodCoif3 HHH L2101 STS_008_MRIstat_qcodSimon. L118018 STS_008_MRIstat_qcodSimon. L2-66066 STS_008_MRIstat_rmsGabor1140114 STS_006_CTstat_meanGabor2680268 STS_006_CTstat_varGabor1.999e+0401.999e+04 STS_006_CTstat_skewLoG-202 STS_006_CTstat_kurtLoG909 STS_006_CTstat_medianGabor2510251 STS_006_CTstat_minSimon. L1-1030103 STS_006_CTstat_minSimon. L2-80080 STS_006_CTstat_p10Laws52052 STS_006_CTstat_p10Gabor97097 STS_006_CTstat_p90Gabor4600460 STS_006_CTstat_maxGabor9320932 STS_006_CTstat_iqrGabor1960196 STS_006_CTstat_rangeGabor9320932 STS_006_CTstat_rangeSimon. L11890189 STS_006_CTstat_rangeSimon. L21600160 STS_006_CTstat_madGabor1140114 STS_006_CTstat_rmadGabor81081 STS_006_CTstat_medadGabor1130113 STS_006_CTstat_covLoG-28028 STS_006_CTstat_covCoif3 LHH L1-145601456 STS_006_CTstat_covSimon. L1-6500650 STS_006_CTstat_covSimon. L2-31031 STS_006_CTstat_qcodLoG13013 STS_006_CTstat_qcodCoif3 LHH L1-2100210 STS_006_CTstat_qcodSimon. L1-8400840 STS_006_CTstat_qcodSimon. L2-31031 STS_006_CTstat_rmsGabor3030303 STS_009_PETstat_skewLoG-202 STS_009_PETstat_skewLaws202 STS_009_PETstat_skewGabor202 STS_009_PETstat_skewCoif3 HHH L2202 STS_009_PETstat_kurtLoG808 STS_009_PETstat_kurtGabor808 STS_009_PETstat_kurtSimon. L114014 STS_009_PETstat_kurtSimon. L2808 STS_009_PETstat_covLoG-909 STS_009_PETstat_covCoif3 LHH L1-283202832 STS_009_PETstat_covSimon. L133033 STS_009_PETstat_covSimon. L212012 STS_009_PETstat_qcodLoG-14014 STS_009_PETstat_qcodCoif3 LHH L1-1310131 STS_009_PETstat_qcodSimon. L1-47047 STS_009_PETstat_qcodSimon. L214014 STS_005_CTstat_meanLaws81081 STS_005_CTstat_meanGabor2810281 STS_005_CTstat_varGabor2.541e+0402.541e+04 STS_005_CTstat_skewLoG202 STS_005_CTstat_kurtLoG24024 STS_005_CTstat_kurtLaws10010 STS_005_CTstat_kurtSimon. L210010 STS_005_CTstat_medianLaws80080 STS_005_CTstat_medianGabor2560256 STS_005_CTstat_minSimon. L1-1570157 STS_005_CTstat_minSimon. L2-3130313 STS_005_CTstat_p10Laws64064 STS_005_CTstat_p10Gabor99099 STS_005_CTstat_p90Laws1000100 STS_005_CTstat_p90Gabor4970497 STS_005_CTstat_maxLaws3090309 STS_005_CTstat_maxGabor129301293 STS_005_CTstat_maxCoif3 HHH L22300230 STS_005_CTstat_maxSimon. L11450145 STS_005_CTstat_maxSimon. L21370137 STS_005_CTstat_iqrGabor2080208 STS_005_CTstat_rangeLaws2620262 STS_005_CTstat_rangeGabor129201292 STS_005_CTstat_rangeCoif3 HHH L22570257 STS_005_CTstat_rangeSimon. L13020302 STS_005_CTstat_rangeSimon. L24500450 STS_005_CTstat_madGabor1250125 STS_005_CTstat_madSimon. L120020 STS_005_CTstat_rmadGabor87087 STS_005_CTstat_medadGabor1240124 STS_005_CTstat_medadSimon. L120020 STS_005_CTstat_covLoG-909 STS_005_CTstat_covCoif3 LHH L1143601436 STS_005_CTstat_covSimon. L1-80080 STS_005_CTstat_covSimon. L22410241 STS_005_CTstat_qcodLoG-505 STS_005_CTstat_qcodCoif3 LHH L13710371 STS_005_CTstat_qcodSimon. L1-73073 STS_005_CTstat_qcodSimon. L23600360 STS_005_CTstat_rmsLaws83083 STS_005_CTstat_rmsGabor3230323 STS_008_CTstat_meanGabor1140114 STS_008_CTstat_varGabor523905239 STS_008_CTstat_skewLoG303 STS_008_CTstat_skewLaws505 STS_008_CTstat_skewGabor202 STS_008_CTstat_skewCoif3 HHH L2505 STS_008_CTstat_skewSimon. L2-404 STS_008_CTstat_kurtLoG11011 STS_008_CTstat_kurtLaws28028 STS_008_CTstat_kurtGabor15015 STS_008_CTstat_kurtCoif3 HHH L233033 STS_008_CTstat_kurtSimon. L233033 STS_008_CTstat_medianGabor1020102 STS_008_CTstat_minSimon. L1-1870187 STS_008_CTstat_minSimon. L2-3130313 STS_008_CTstat_p90Gabor1990199 STS_008_CTstat_maxLaws3170317 STS_008_CTstat_maxGabor103701037 STS_008_CTstat_maxCoif3 HHH L21670167 STS_008_CTstat_maxSimon. L11340134 STS_008_CTstat_iqrGabor83083 STS_008_CTstat_rangeLaws2830283 STS_008_CTstat_rangeGabor103701037 STS_008_CTstat_rangeCoif3 HHH L21760176 STS_008_CTstat_rangeSimon. L13210321 STS_008_CTstat_rangeSimon. L24080408 STS_008_CTstat_madGabor52052 STS_008_CTstat_rmadGabor35035 STS_008_CTstat_medadGabor51051 STS_008_CTstat_covLoG404 STS_008_CTstat_covCoif3 LHH L1-3740374 STS_008_CTstat_covSimon. L138038 STS_008_CTstat_covSimon. L2-35035 STS_008_CTstat_qcodLoG404 STS_008_CTstat_qcodCoif3 LHH L1-1020102 STS_008_CTstat_qcodSimon. L145045 STS_008_CTstat_qcodSimon. L21060106 STS_008_CTstat_rmsGabor1350135 STS_010_PETstat_covLoG-303 STS_010_PETstat_covCoif3 LHH L15070507 STS_010_PETstat_covSimon. L117017 STS_010_PETstat_covSimon. L2505 STS_010_PETstat_qcodLoG-202 STS_010_PETstat_qcodCoif3 LHH L17650765 STS_010_PETstat_qcodSimon. L115015 STS_010_PETstat_qcodSimon. L2202 STS_011_PETstat_covLoG-505 STS_011_PETstat_covCoif3 LHH L1255402554 STS_011_PETstat_covSimon. L167067 STS_011_PETstat_covSimon. L211011 STS_011_PETstat_qcodLoG-404 STS_011_PETstat_qcodCoif3 LHH L1-4180418 STS_011_PETstat_qcodSimon. L1-89089 STS_011_PETstat_qcodSimon. L2707 STS_010_MRIstat_skewLoG404 STS_010_MRIstat_skewLaws404 STS_010_MRIstat_skewCoif3 HHH L2606 STS_010_MRIstat_kurtLoG17017 STS_010_MRIstat_kurtLaws18018 STS_010_MRIstat_kurtCoif3 HHH L260060 STS_010_MRIstat_minSimon. L1-1230123 STS_010_MRIstat_minSimon. L2-1110111 STS_010_MRIstat_maxLaws1790179 STS_010_MRIstat_maxGabor3370337 STS_010_MRIstat_rangeLaws1640164 STS_010_MRIstat_rangeGabor3370337 STS_010_MRIstat_rangeSimon. L12440244 STS_010_MRIstat_rangeSimon. L21980198 STS_010_MRIstat_madGabor24024 STS_010_MRIstat_medadGabor24024 STS_010_MRIstat_covLoG505 STS_010_MRIstat_covCoif3 LHH L18450845 STS_010_MRIstat_covSimon. L13610361 STS_010_MRIstat_covSimon. L27490749 STS_010_MRIstat_qcodLoG909 STS_010_MRIstat_qcodCoif3 LHH L16460646 STS_010_MRIstat_qcodSimon. L1-199401994 STS_010_MRIstat_qcodSimon. L2-33033 STS_007_MRIstat_skewLaws202 STS_007_MRIstat_skewGabor303 STS_007_MRIstat_skewCoif3 HHH L2303 STS_007_MRIstat_kurtGabor19019 STS_007_MRIstat_kurtCoif3 LHH L114014 STS_007_MRIstat_kurtCoif3 HHH L211011 STS_007_MRIstat_kurtSimon. L113013 STS_007_MRIstat_minSimon. L1-2100210 STS_007_MRIstat_minSimon. L2-2260226 STS_007_MRIstat_p90Laws1010101 STS_007_MRIstat_maxLaws3220322 STS_007_MRIstat_maxGabor2640264 STS_007_MRIstat_maxSimon. L12200220 STS_007_MRIstat_maxSimon. L22880288 STS_007_MRIstat_rangeLaws3130313 STS_007_MRIstat_rangeGabor2640264 STS_007_MRIstat_rangeSimon. L14310431 STS_007_MRIstat_rangeSimon. L25150515 STS_007_MRIstat_madLaws29029 STS_007_MRIstat_madSimon. L221021 STS_007_MRIstat_rmadLaws19019 STS_007_MRIstat_medadLaws26026 STS_007_MRIstat_medadSimon. L221021 STS_007_MRIstat_covCoif3 LHH L17530753 STS_007_MRIstat_covSimon. L14530453 STS_007_MRIstat_covSimon. L2-2380238 STS_007_MRIstat_qcodLoG101 STS_007_MRIstat_qcodLaws101 STS_007_MRIstat_qcodCoif3 LHH L1-169501695 STS_007_MRIstat_qcodCoif3 HHH L2101 STS_007_MRIstat_qcodSimon. L1-1850185 STS_007_MRIstat_qcodSimon. L255055 STS_012_PETstat_covCoif3 LHH L1-1620162 STS_012_PETstat_covSimon. L114014 STS_012_PETstat_covSimon. L2404 STS_012_PETstat_qcodLoG-101 STS_012_PETstat_qcodCoif3 LHH L1-4690469 STS_012_PETstat_qcodSimon. L119019 STS_012_PETstat_qcodSimon. L2202 STS_010_CTstat_skewLaws202 STS_010_CTstat_kurtLoG25025 STS_010_CTstat_kurtLaws35035 STS_010_CTstat_p90Gabor1010101 STS_010_CTstat_maxGabor2580258 STS_010_CTstat_iqrGabor42042 STS_010_CTstat_rangeGabor2570257 STS_010_CTstat_madGabor26026 STS_010_CTstat_rmadGabor18018 STS_010_CTstat_medadGabor25025 STS_010_CTstat_covLoG-1060106 STS_010_CTstat_covCoif3 LHH L1544505445 STS_010_CTstat_covSimon. L12280228 STS_010_CTstat_covSimon. L2-24024 STS_010_CTstat_qcodLoG808 STS_010_CTstat_qcodCoif3 LHH L1-3070307 STS_010_CTstat_qcodSimon. L12630263 STS_010_CTstat_qcodSimon. L2-26026 STS_007_CTstat_meanGabor2310231 STS_007_CTstat_varGabor1.657e+0401.657e+04 STS_007_CTstat_skewLaws505 STS_007_CTstat_skewCoif3 HHH L2303 STS_007_CTstat_kurtLoG10010 STS_007_CTstat_kurtLaws36036 STS_007_CTstat_kurtCoif3 HHH L232032 STS_007_CTstat_kurtSimon. L2808 STS_007_CTstat_medianGabor2120212 STS_007_CTstat_minSimon. L1-1780178 STS_007_CTstat_minSimon. L2-2590259 STS_007_CTstat_p10Laws61061 STS_007_CTstat_p10Gabor81081 STS_007_CTstat_p90Gabor4060406 STS_007_CTstat_maxLaws2330233 STS_007_CTstat_maxGabor100201002 STS_007_CTstat_maxCoif3 HHH L23520352 STS_007_CTstat_maxSimon. L21570157 STS_007_CTstat_iqrGabor1720172 STS_007_CTstat_rangeLaws1850185 STS_007_CTstat_rangeGabor100201002 STS_007_CTstat_rangeCoif3 HHH L23760376 STS_007_CTstat_rangeSimon. L12900290 STS_007_CTstat_rangeSimon. L24150415 STS_007_CTstat_madGabor1020102 STS_007_CTstat_rmadGabor71071 STS_007_CTstat_medadGabor1010101 STS_007_CTstat_covLoG-909 STS_007_CTstat_covCoif3 LHH L1635606356 STS_007_CTstat_covSimon. L1-1380138 STS_007_CTstat_covSimon. L2-22022 STS_007_CTstat_qcodLoG-404 STS_007_CTstat_qcodCoif3 LHH L14960496 STS_007_CTstat_qcodSimon. L1-2830283 STS_007_CTstat_qcodSimon. L2-8000800 STS_007_CTstat_rmsGabor2650265 STS_013_PETstat_covLoG-13013 STS_013_PETstat_covCoif3 LHH L1177201772 STS_013_PETstat_covSimon. L12920292 STS_013_PETstat_covSimon. L22840284 STS_013_PETstat_qcodLoG-28028 STS_013_PETstat_qcodCoif3 LHH L1-2870287 STS_013_PETstat_qcodSimon. L1-28028 STS_013_PETstat_qcodSimon. L2-1120112 STS_012_CTstat_meanGabor81081 STS_012_CTstat_minSimon. L1-93093 STS_012_CTstat_minSimon. L2-77077 STS_012_CTstat_p90Gabor1520152 STS_012_CTstat_maxGabor4130413 STS_012_CTstat_iqrGabor65065 STS_012_CTstat_rangeGabor4120412 STS_012_CTstat_rangeSimon. L11710171 STS_012_CTstat_rangeSimon. L21500150 STS_012_CTstat_madGabor40040 STS_012_CTstat_rmadGabor27027 STS_012_CTstat_medadGabor39039 STS_012_CTstat_covLoG-606 STS_012_CTstat_covCoif3 LHH L1-3.282e+0503.282e+05 STS_012_CTstat_covSimon. L1-2810281 STS_012_CTstat_covSimon. L2-47047 STS_012_CTstat_qcodLoG-202 STS_012_CTstat_qcodCoif3 LHH L1-1040104 STS_012_CTstat_qcodSimon. L1-1500150 STS_012_CTstat_qcodSimon. L2-33033 STS_012_CTstat_rmsGabor96096 STS_012_MRIstat_skewLoG-202 STS_012_MRIstat_skewGabor303 STS_012_MRIstat_skewCoif3 HHH L2202 STS_012_MRIstat_kurtGabor14014 STS_012_MRIstat_kurtCoif3 HHH L2808 STS_012_MRIstat_kurtSimon. L1808 STS_012_MRIstat_minSimon. L1-2070207 STS_012_MRIstat_minSimon. L2-2290229 STS_012_MRIstat_maxLaws1610161 STS_012_MRIstat_maxGabor1880188 STS_012_MRIstat_maxCoif3 HHH L21550155 STS_012_MRIstat_maxSimon. L11770177 STS_012_MRIstat_maxSimon. L22460246 STS_012_MRIstat_rangeLaws1510151 STS_012_MRIstat_rangeGabor1880188 STS_012_MRIstat_rangeCoif3 HHH L21820182 STS_012_MRIstat_rangeSimon. L13840384 STS_012_MRIstat_rangeSimon. L24750475 STS_012_MRIstat_madLaws21021 STS_012_MRIstat_madSimon. L222022 STS_012_MRIstat_medadLaws20020 STS_012_MRIstat_medadSimon. L222022 STS_012_MRIstat_covLoG-19019 STS_012_MRIstat_covCoif3 LHH L13850385 STS_012_MRIstat_covSimon. L142042 STS_012_MRIstat_covSimon. L213013 STS_012_MRIstat_qcodLoG303 STS_012_MRIstat_qcodCoif3 LHH L18290829 STS_012_MRIstat_qcodCoif3 HHH L2101 STS_012_MRIstat_qcodSimon. L135035 STS_012_MRIstat_qcodSimon. L230030 STS_014_PETstat_covLoG-303 STS_014_PETstat_covCoif3 LHH L14320432 STS_014_PETstat_covSimon. L119019 STS_014_PETstat_covSimon. L2505 STS_014_PETstat_qcodLoG-202 STS_014_PETstat_qcodCoif3 LHH L12050205 STS_014_PETstat_qcodSimon. L114014 STS_014_PETstat_qcodSimon. L2303 STS_011_MRIstat_skewLoG202 STS_011_MRIstat_skewLaws303 STS_011_MRIstat_skewGabor303 STS_011_MRIstat_skewCoif3 HHH L2505 STS_011_MRIstat_kurtLaws13013 STS_011_MRIstat_kurtGabor26026 STS_011_MRIstat_kurtCoif3 HHH L243043 STS_011_MRIstat_kurtSimon. L119019 STS_011_MRIstat_kurtSimon. L2808 STS_011_MRIstat_minSimon. L1-1650165 STS_011_MRIstat_minSimon. L2-2200220 STS_011_MRIstat_maxLaws1840184 STS_011_MRIstat_maxCoif3 HHH L21550155 STS_011_MRIstat_maxSimon. L11910191 STS_011_MRIstat_maxSimon. L22310231 STS_011_MRIstat_rangeLaws1770177 STS_011_MRIstat_rangeCoif3 HHH L21670167 STS_011_MRIstat_rangeSimon. L13560356 STS_011_MRIstat_rangeSimon. L24500450 STS_011_MRIstat_covLoG303 STS_011_MRIstat_covCoif3 LHH L1127701277 STS_011_MRIstat_covSimon. L1-4230423 STS_011_MRIstat_covSimon. L2-18018 STS_011_MRIstat_qcodLoG202 STS_011_MRIstat_qcodCoif3 LHH L13450345 STS_011_MRIstat_qcodSimon. L1-149201492 STS_011_MRIstat_qcodSimon. L2-16016 STS_009_MRIstat_skewLoG303 STS_009_MRIstat_skewLaws303 STS_009_MRIstat_skewGabor707 STS_009_MRIstat_skewCoif3 HHH L2606 STS_009_MRIstat_kurtLoG11011 STS_009_MRIstat_kurtLaws15015 STS_009_MRIstat_kurtGabor80080 STS_009_MRIstat_kurtCoif3 LHH L136036 STS_009_MRIstat_kurtCoif3 HHH L262062 STS_009_MRIstat_kurtSimon. L127027 STS_009_MRIstat_kurtSimon. L212012 STS_009_MRIstat_minSimon. L1-2730273 STS_009_MRIstat_minSimon. L2-3050305 STS_009_MRIstat_maxLaws4650465 STS_009_MRIstat_maxGabor9310931 STS_009_MRIstat_maxCoif3 HHH L22590259 STS_009_MRIstat_maxSimon. L12990299 STS_009_MRIstat_maxSimon. L23920392 STS_009_MRIstat_rangeLaws4580458 STS_009_MRIstat_rangeGabor9310931 STS_009_MRIstat_rangeCoif3 HHH L22670267 STS_009_MRIstat_rangeSimon. L15730573 STS_009_MRIstat_rangeSimon. L26960696 STS_009_MRIstat_madLaws21021 STS_009_MRIstat_covLoG404 STS_009_MRIstat_covCoif3 LHH L1-7230723 STS_009_MRIstat_covSimon. L1-1520152 STS_009_MRIstat_covSimon. L2-1520152 STS_009_MRIstat_qcodLoG303 STS_009_MRIstat_qcodCoif3 LHH L1-118301183 STS_009_MRIstat_qcodSimon. L1115701157 STS_009_MRIstat_qcodSimon. L21210121 STS_015_PETstat_covCoif3 LHH L161061 STS_015_PETstat_covSimon. L120020 STS_015_PETstat_covSimon. L2404 STS_015_PETstat_qcodLoG-101 STS_015_PETstat_qcodCoif3 LHH L11100110 STS_015_PETstat_qcodSimon. L128028 STS_015_PETstat_qcodSimon. L2404 STS_011_CTstat_meanLaws79079 STS_011_CTstat_kurtLoG11011 STS_011_CTstat_medianLaws78078 STS_011_CTstat_minSimon. L1-1050105 STS_011_CTstat_minSimon. L2-73073 STS_011_CTstat_p10Laws68068 STS_011_CTstat_p90Gabor1210121 STS_011_CTstat_maxGabor4590459 STS_011_CTstat_maxCoif3 HHH L23230323 STS_011_CTstat_iqrGabor50050 STS_011_CTstat_rangeGabor4590459 STS_011_CTstat_rangeCoif3 HHH L23620362 STS_011_CTstat_rangeSimon. L12190219 STS_011_CTstat_rangeSimon. L21780178 STS_011_CTstat_madGabor31031 STS_011_CTstat_rmadGabor21021 STS_011_CTstat_medadGabor31031 STS_011_CTstat_covLoG-14014 STS_011_CTstat_covCoif3 LHH L1685406854 STS_011_CTstat_covSimon. L1101501015 STS_011_CTstat_covSimon. L2-67067 STS_011_CTstat_qcodLoG-808 STS_011_CTstat_qcodCoif3 LHH L1-117501175 STS_011_CTstat_qcodSimon. L1116301163 STS_011_CTstat_qcodSimon. L2-40040 STS_011_CTstat_rmsLaws79079 STS_011_CTstat_rmsGabor80080 STS_015_MRIstat_meanLaws84084 STS_015_MRIstat_meanGabor1380138 STS_015_MRIstat_varGabor3.353e+0403.353e+04 STS_015_MRIstat_skewGabor303 STS_015_MRIstat_skewCoif3 HHH L2202 STS_015_MRIstat_kurtGabor10010 STS_015_MRIstat_kurtCoif3 HHH L2909 STS_015_MRIstat_medianLaws76076 STS_015_MRIstat_minSimon. L1-2430243 STS_015_MRIstat_minSimon. L2-2440244 STS_015_MRIstat_p10Simon. L2-80080 STS_015_MRIstat_p90Laws1660166 STS_015_MRIstat_p90Gabor3650365 STS_015_MRIstat_maxLaws3110311 STS_015_MRIstat_maxGabor190501905 STS_015_MRIstat_maxCoif3 HHH L24440444 STS_015_MRIstat_maxSimon. L13160316 STS_015_MRIstat_maxSimon. L23380338 STS_015_MRIstat_iqrLaws98098 STS_015_MRIstat_iqrGabor1470147 STS_015_MRIstat_iqrSimon. L253053 STS_015_MRIstat_rangeLaws3030303 STS_015_MRIstat_rangeGabor190501905 STS_015_MRIstat_rangeCoif3 HHH L24670467 STS_015_MRIstat_rangeSimon. L15600560 STS_015_MRIstat_rangeSimon. L25820582 STS_015_MRIstat_madLaws50050 STS_015_MRIstat_madGabor1250125 STS_015_MRIstat_madCoif3 HHH L225025 STS_015_MRIstat_madSimon. L123023 STS_015_MRIstat_madSimon. L241041 STS_015_MRIstat_rmadLaws39039 STS_015_MRIstat_rmadGabor69069 STS_015_MRIstat_rmadSimon. L224024 STS_015_MRIstat_medadLaws49049 STS_015_MRIstat_medadGabor1090109 STS_015_MRIstat_medadCoif3 HHH L224024 STS_015_MRIstat_medadSimon. L123023 STS_015_MRIstat_medadSimon. L241041 STS_015_MRIstat_covCoif3 LHH L1-154801548 STS_015_MRIstat_covSimon. L1-41041 STS_015_MRIstat_covSimon. L2-13013 STS_015_MRIstat_qcodLoG101 STS_015_MRIstat_qcodLaws101 STS_015_MRIstat_qcodGabor101 STS_015_MRIstat_qcodCoif3 LHH L12350235 STS_015_MRIstat_qcodCoif3 HHH L2101 STS_015_MRIstat_qcodSimon. L1-94094 STS_015_MRIstat_qcodSimon. L2-16016 STS_015_MRIstat_rmsLaws1030103 STS_015_MRIstat_rmsGabor2290229 STS_009_CTstat_meanLaws75075 STS_009_CTstat_meanGabor87087 STS_009_CTstat_varGabor581005810 STS_009_CTstat_skewLoG606 STS_009_CTstat_skewLaws303 STS_009_CTstat_skewGabor303 STS_009_CTstat_skewSimon. L2-202 STS_009_CTstat_kurtLoG45045 STS_009_CTstat_kurtLaws24024 STS_009_CTstat_kurtGabor13013 STS_009_CTstat_kurtSimon. L117017 STS_009_CTstat_kurtSimon. L240040 STS_009_CTstat_medianLaws76076 STS_009_CTstat_minSimon. L1-2870287 STS_009_CTstat_minSimon. L2-4280428 STS_009_CTstat_p90Gabor1740174 STS_009_CTstat_maxLaws3890389 STS_009_CTstat_maxGabor150901509 STS_009_CTstat_maxCoif3 HHH L22620262 STS_009_CTstat_maxSimon. L13410341 STS_009_CTstat_maxSimon. L23270327 STS_009_CTstat_iqrGabor69069 STS_009_CTstat_rangeLaws3640364 STS_009_CTstat_rangeGabor150901509 STS_009_CTstat_rangeCoif3 HHH L22980298 STS_009_CTstat_rangeSimon. L16270627 STS_009_CTstat_rangeSimon. L27550755 STS_009_CTstat_madGabor52052 STS_009_CTstat_rmadGabor30030 STS_009_CTstat_medadGabor49049 STS_009_CTstat_covLoG707 STS_009_CTstat_covCoif3 LHH L1-379003790 STS_009_CTstat_covSimon. L192092 STS_009_CTstat_covSimon. L2-1920192 STS_009_CTstat_qcodLoG63063 STS_009_CTstat_qcodCoif3 LHH L1364803648 STS_009_CTstat_qcodSimon. L11810181 STS_009_CTstat_qcodSimon. L26900690 STS_009_CTstat_rmsLaws79079 STS_009_CTstat_rmsGabor1160116 STS_016_PETstat_skewLoG-202 STS_016_PETstat_skewGabor202 STS_016_PETstat_skewSimon. L2303 STS_016_PETstat_kurtLoG13013 STS_016_PETstat_kurtSimon. L220020 STS_016_PETstat_covLoG-505 STS_016_PETstat_covCoif3 LHH L1-879508795 STS_016_PETstat_covSimon. L11070107 STS_016_PETstat_covSimon. L212012 STS_016_PETstat_qcodLoG-404 STS_016_PETstat_qcodCoif3 LHH L1-4370437 STS_016_PETstat_qcodSimon. L1-82082 STS_016_PETstat_qcodSimon. L216016 STS_015_CTstat_meanLaws73073 STS_015_CTstat_skewGabor202 STS_015_CTstat_minSimon. L1-98098 STS_015_CTstat_minSimon. L2-70070 STS_015_CTstat_p10Laws63063 STS_015_CTstat_maxGabor2710271 STS_015_CTstat_rangeGabor2710271 STS_015_CTstat_rangeSimon. L11920192 STS_015_CTstat_rangeSimon. L21510151 STS_015_CTstat_madGabor22022 STS_015_CTstat_medadGabor22022 STS_015_CTstat_covCoif3 LHH L11.458e+0401.458e+04 STS_015_CTstat_covSimon. L11710171 STS_015_CTstat_covSimon. L211011 STS_015_CTstat_qcodLoG-101 STS_015_CTstat_qcodCoif3 LHH L11470147 STS_015_CTstat_qcodSimon. L11220122 STS_015_CTstat_qcodSimon. L211011 STS_015_CTstat_rmsLaws73073 STS_014_MRIstat_skewLoG303 STS_014_MRIstat_skewLaws202 STS_014_MRIstat_skewGabor404 STS_014_MRIstat_skewCoif3 HHH L2404 STS_014_MRIstat_kurtLoG12012 STS_014_MRIstat_kurtLaws808 STS_014_MRIstat_kurtGabor27027 STS_014_MRIstat_kurtCoif3 LHH L1909 STS_014_MRIstat_kurtCoif3 HHH L221021 STS_014_MRIstat_kurtSimon. L116016 STS_014_MRIstat_kurtSimon. L2808 STS_014_MRIstat_minSimon. L1-1840184 STS_014_MRIstat_minSimon. L2-1660166 STS_014_MRIstat_p90Gabor1210121 STS_014_MRIstat_maxLaws2700270 STS_014_MRIstat_maxGabor9800980 STS_014_MRIstat_maxSimon. L11680168 STS_014_MRIstat_maxSimon. L21820182 STS_014_MRIstat_iqrGabor47047 STS_014_MRIstat_rangeLaws2560256 STS_014_MRIstat_rangeGabor9800980 STS_014_MRIstat_rangeSimon. L13520352 STS_014_MRIstat_rangeSimon. L23480348 STS_014_MRIstat_madGabor37037 STS_014_MRIstat_rmadGabor20020 STS_014_MRIstat_medadGabor34034 STS_014_MRIstat_covLoG404 STS_014_MRIstat_covCoif3 LHH L1-154501545 STS_014_MRIstat_covSimon. L1-1410141 STS_014_MRIstat_covSimon. L2-2480248 STS_014_MRIstat_qcodLoG303 STS_014_MRIstat_qcodCoif3 LHH L1217902179 STS_014_MRIstat_qcodSimon. L1101301013 STS_014_MRIstat_qcodSimon. L22110211 STS_014_MRIstat_rmsGabor87087 STS_014_CTstat_skewGabor202 STS_014_CTstat_minSimon. L1-86086 STS_014_CTstat_p10Laws51051 STS_014_CTstat_p90Gabor1170117 STS_014_CTstat_maxGabor4370437 STS_014_CTstat_iqrGabor51051 STS_014_CTstat_rangeGabor4370437 STS_014_CTstat_rangeSimon. L11780178 STS_014_CTstat_madGabor33033 STS_014_CTstat_rmadGabor22022 STS_014_CTstat_medadGabor31031 STS_014_CTstat_covLoG-33033 STS_014_CTstat_covCoif3 LHH L1-531405314 STS_014_CTstat_covSimon. L1271502715 STS_014_CTstat_covSimon. L2-3180318 STS_014_CTstat_qcodLoG-118801188 STS_014_CTstat_qcodCoif3 LHH L19900990 STS_014_CTstat_qcodSimon. L19850985 STS_014_CTstat_qcodSimon. L25260526 STS_014_CTstat_rmsGabor73073 STS_017_PETstat_covLoG-606 STS_017_PETstat_covCoif3 LHH L1-115201152 STS_017_PETstat_covSimon. L11680168 STS_017_PETstat_covSimon. L2909 STS_017_PETstat_qcodLoG-606 STS_017_PETstat_qcodGabor101 STS_017_PETstat_qcodCoif3 LHH L1-5070507 STS_017_PETstat_qcodSimon. L1-17017 STS_017_PETstat_qcodSimon. L2808 STS_018_PETstat_covCoif3 LHH L12570257 STS_018_PETstat_covSimon. L125025 STS_018_PETstat_covSimon. L2404 STS_018_PETstat_qcodLoG-202 STS_018_PETstat_qcodCoif3 LHH L198098 STS_018_PETstat_qcodSimon. L139039 STS_018_PETstat_qcodSimon. L2202 STS_013_MRIstat_meanLaws90090 STS_013_MRIstat_meanGabor74074 STS_013_MRIstat_skewGabor202 STS_013_MRIstat_skewCoif3 HHH L2202 STS_013_MRIstat_kurtGabor909 STS_013_MRIstat_kurtCoif3 HHH L2808 STS_013_MRIstat_medianLaws85085 STS_013_MRIstat_minSimon. L1-2960296 STS_013_MRIstat_minSimon. L2-3510351 STS_013_MRIstat_p10Simon. L2-65065 STS_013_MRIstat_p90Laws1380138 STS_013_MRIstat_p90Gabor1470147 STS_013_MRIstat_maxLaws3240324 STS_013_MRIstat_maxGabor7570757 STS_013_MRIstat_maxSimon. L13060306 STS_013_MRIstat_maxSimon. L23830383 STS_013_MRIstat_iqrLaws46046 STS_013_MRIstat_iqrGabor58058 STS_013_MRIstat_iqrSimon. L264064 STS_013_MRIstat_rangeLaws2980298 STS_013_MRIstat_rangeGabor7570757 STS_013_MRIstat_rangeSimon. L16020602 STS_013_MRIstat_rangeSimon. L27330733 STS_013_MRIstat_madLaws29029 STS_013_MRIstat_madGabor43043 STS_013_MRIstat_madSimon. L123023 STS_013_MRIstat_madSimon. L244044 STS_013_MRIstat_rmadLaws19019 STS_013_MRIstat_rmadGabor25025 STS_013_MRIstat_rmadSimon. L228028 STS_013_MRIstat_medadLaws28028 STS_013_MRIstat_medadGabor40040 STS_013_MRIstat_medadSimon. L123023 STS_013_MRIstat_medadSimon. L244044 STS_013_MRIstat_covLoG-404 STS_013_MRIstat_covCoif3 LHH L11.355e+0401.355e+04 STS_013_MRIstat_covSimon. L191091 STS_013_MRIstat_covSimon. L217017 STS_013_MRIstat_qcodLoG-303 STS_013_MRIstat_qcodCoif3 LHH L1-799007990 STS_013_MRIstat_qcodSimon. L11040104 STS_013_MRIstat_qcodSimon. L219019 STS_013_MRIstat_rmsLaws97097 STS_013_MRIstat_rmsGabor97097 STS_018_MRIstat_meanLaws94094 STS_018_MRIstat_medianLaws90090 STS_018_MRIstat_minSimon. L1-2820282 STS_018_MRIstat_minSimon. L2-3090309 STS_018_MRIstat_p10Laws54054 STS_018_MRIstat_p10Simon. L2-85085 STS_018_MRIstat_p90Laws1380138 STS_018_MRIstat_maxLaws2100210 STS_018_MRIstat_maxGabor1960196 STS_018_MRIstat_maxSimon. L12180218 STS_018_MRIstat_maxSimon. L24120412 STS_018_MRIstat_iqrLaws43043 STS_018_MRIstat_iqrSimon. L275075 STS_018_MRIstat_rangeLaws1810181 STS_018_MRIstat_rangeGabor1960196 STS_018_MRIstat_rangeSimon. L15000500 STS_018_MRIstat_rangeSimon. L27210721 STS_018_MRIstat_madLaws25025 STS_018_MRIstat_madGabor22022 STS_018_MRIstat_madSimon. L123023 STS_018_MRIstat_madSimon. L249049 STS_018_MRIstat_rmadLaws18018 STS_018_MRIstat_rmadSimon. L232032 STS_018_MRIstat_medadLaws25025 STS_018_MRIstat_medadGabor21021 STS_018_MRIstat_medadSimon. L123023 STS_018_MRIstat_medadSimon. L249049 STS_018_MRIstat_covLoG707 STS_018_MRIstat_covCoif3 LHH L1-2680268 STS_018_MRIstat_covSimon. L1-56056 STS_018_MRIstat_covSimon. L2-17017 STS_018_MRIstat_qcodLoG202 STS_018_MRIstat_qcodCoif3 LHH L1-1.476e+0401.476e+04 STS_018_MRIstat_qcodSimon. L1-58058 STS_018_MRIstat_qcodSimon. L2-808 STS_018_MRIstat_rmsLaws99099 STS_019_CTstat_varGabor543705437 STS_019_CTstat_skewLaws202 STS_019_CTstat_skewGabor707 STS_019_CTstat_skewCoif3 HHH L2303 STS_019_CTstat_kurtGabor65065 STS_019_CTstat_kurtCoif3 LHH L121021 STS_019_CTstat_kurtCoif3 HHH L217017 STS_019_CTstat_kurtSimon. L114014 STS_019_CTstat_minSimon. L1-1850185 STS_019_CTstat_minSimon. L2-1660166 STS_019_CTstat_p10Simon. L2-59059 STS_019_CTstat_maxLaws2540254 STS_019_CTstat_maxGabor117201172 STS_019_CTstat_maxCoif3 HHH L23080308 STS_019_CTstat_maxSimon. L22390239 STS_019_CTstat_iqrLaws43043 STS_019_CTstat_iqrSimon. L248048 STS_019_CTstat_rangeLaws2470247 STS_019_CTstat_rangeGabor117201172 STS_019_CTstat_rangeCoif3 HHH L23200320 STS_019_CTstat_rangeSimon. L12970297 STS_019_CTstat_rangeSimon. L24050405 STS_019_CTstat_madLaws27027 STS_019_CTstat_madGabor34034 STS_019_CTstat_madSimon. L233033 STS_019_CTstat_rmadLaws18018 STS_019_CTstat_rmadSimon. L221021 STS_019_CTstat_medadLaws25025 STS_019_CTstat_medadGabor27027 STS_019_CTstat_medadSimon. L233033 STS_019_CTstat_covCoif3 LHH L1255802558 STS_019_CTstat_covSimon. L1-24024 STS_019_CTstat_covSimon. L2-111701117 STS_019_CTstat_qcodLoG-101 STS_019_CTstat_qcodLaws101 STS_019_CTstat_qcodCoif3 LHH L11640164 STS_019_CTstat_qcodCoif3 HHH L2101 STS_019_CTstat_qcodSimon. L1-31031 STS_019_CTstat_qcodSimon. L2-13013 STS_019_CTstat_rmsGabor84084 STS_019_PETstat_covCoif3 LHH L175075 STS_019_PETstat_covSimon. L112012 STS_019_PETstat_qcodLoG-101 STS_019_PETstat_qcodCoif3 LHH L112012 STS_019_PETstat_qcodSimon. L1909 STS_019_PETstat_qcodSimon. L2101 STS_017_MRIstat_skewLaws202 STS_017_MRIstat_skewGabor303 STS_017_MRIstat_skewCoif3 HHH L2404 STS_017_MRIstat_kurtLaws808 STS_017_MRIstat_kurtGabor19019 STS_017_MRIstat_kurtCoif3 LHH L116016 STS_017_MRIstat_kurtCoif3 HHH L231031 STS_017_MRIstat_kurtSimon. L113013 STS_017_MRIstat_minSimon. L1-2560256 STS_017_MRIstat_minSimon. L2-2380238 STS_017_MRIstat_maxLaws4160416 STS_017_MRIstat_maxGabor5880588 STS_017_MRIstat_maxSimon. L12830283 STS_017_MRIstat_maxSimon. L22670267 STS_017_MRIstat_rangeLaws4050405 STS_017_MRIstat_rangeGabor5880588 STS_017_MRIstat_rangeSimon. L15390539 STS_017_MRIstat_rangeSimon. L25050505 STS_017_MRIstat_madLaws25025 STS_017_MRIstat_madGabor26026 STS_017_MRIstat_madSimon. L222022 STS_017_MRIstat_rmadLaws17017 STS_017_MRIstat_medadLaws23023 STS_017_MRIstat_medadGabor24024 STS_017_MRIstat_medadSimon. L222022 STS_017_MRIstat_covLoG808 STS_017_MRIstat_covCoif3 LHH L1-8760876 STS_017_MRIstat_covSimon. L14160416 STS_017_MRIstat_covSimon. L21150115 STS_017_MRIstat_qcodLoG303 STS_017_MRIstat_qcodCoif3 LHH L1-8060806 STS_017_MRIstat_qcodSimon. L1-2150215 STS_017_MRIstat_qcodSimon. L217017 STS_018_CTstat_meanGabor1020102 STS_018_CTstat_varGabor826108261 STS_018_CTstat_skewLoG-202 STS_018_CTstat_skewLaws404 STS_018_CTstat_skewGabor606 STS_018_CTstat_skewCoif3 HHH L2202 STS_018_CTstat_kurtLoG909 STS_018_CTstat_kurtLaws21021 STS_018_CTstat_kurtGabor56056 STS_018_CTstat_kurtCoif3 HHH L212012 STS_018_CTstat_kurtSimon. L1909 STS_018_CTstat_kurtSimon. L2909 STS_018_CTstat_medianGabor85085 STS_018_CTstat_minSimon. L1-2140214 STS_018_CTstat_minSimon. L2-2350235 STS_018_CTstat_p90Gabor1770177 STS_018_CTstat_maxLaws3000300 STS_018_CTstat_maxGabor177001770 STS_018_CTstat_maxCoif3 HHH L21790179 STS_018_CTstat_maxSimon. L11910191 STS_018_CTstat_maxSimon. L21450145 STS_018_CTstat_iqrGabor73073 STS_018_CTstat_rangeLaws2660266 STS_018_CTstat_rangeGabor177001770 STS_018_CTstat_rangeCoif3 HHH L21950195 STS_018_CTstat_rangeSimon. L14050405 STS_018_CTstat_rangeSimon. L23800380 STS_018_CTstat_madGabor52052 STS_018_CTstat_rmadGabor31031 STS_018_CTstat_medadGabor50050 STS_018_CTstat_covLoG-303 STS_018_CTstat_covCoif3 LHH L1-114701147 STS_018_CTstat_covSimon. L1-72072 STS_018_CTstat_covSimon. L2-41041 STS_018_CTstat_qcodLoG-101 STS_018_CTstat_qcodCoif3 LHH L11030103 STS_018_CTstat_qcodSimon. L11440144 STS_018_CTstat_qcodSimon. L210010 STS_018_CTstat_rmsGabor1370137 STS_016_MRIstat_skewLaws303 STS_016_MRIstat_skewGabor505 STS_016_MRIstat_skewCoif3 HHH L2404 STS_016_MRIstat_kurtLoG10010 STS_016_MRIstat_kurtLaws11011 STS_016_MRIstat_kurtGabor48048 STS_016_MRIstat_kurtCoif3 LHH L131031 STS_016_MRIstat_kurtCoif3 HHH L223023 STS_016_MRIstat_kurtSimon. L125025 STS_016_MRIstat_kurtSimon. L213013 STS_016_MRIstat_minSimon. L1-1740174 STS_016_MRIstat_minSimon. L2-1980198 STS_016_MRIstat_maxLaws2130213 STS_016_MRIstat_maxGabor1830183 STS_016_MRIstat_maxSimon. L12160216 STS_016_MRIstat_maxSimon. L21720172 STS_016_MRIstat_rangeLaws2060206 STS_016_MRIstat_rangeGabor1830183 STS_016_MRIstat_rangeSimon. L13900390 STS_016_MRIstat_rangeSimon. L23700370 STS_016_MRIstat_covLoG404 STS_016_MRIstat_covCoif3 LHH L1151301513 STS_016_MRIstat_covSimon. L11880188 STS_016_MRIstat_covSimon. L285085 STS_016_MRIstat_qcodLoG202 STS_016_MRIstat_qcodCoif3 LHH L13690369 STS_016_MRIstat_qcodSimon. L1-6100610 STS_016_MRIstat_qcodSimon. L236036 STS_019_MRIstat_meanLaws76076 STS_019_MRIstat_skewGabor202 STS_019_MRIstat_skewCoif3 HHH L2202 STS_019_MRIstat_kurtCoif3 HHH L2808 STS_019_MRIstat_minSimon. L1-1330133 STS_019_MRIstat_minSimon. L2-2430243 STS_019_MRIstat_p10Simon. L2-89089 STS_019_MRIstat_p90Laws1210121 STS_019_MRIstat_maxLaws2100210 STS_019_MRIstat_maxGabor3110311 STS_019_MRIstat_maxCoif3 HHH L21690169 STS_019_MRIstat_maxSimon. L21990199 STS_019_MRIstat_iqrLaws51051 STS_019_MRIstat_iqrSimon. L267067 STS_019_MRIstat_rangeLaws1870187 STS_019_MRIstat_rangeGabor3110311 STS_019_MRIstat_rangeCoif3 HHH L21780178 STS_019_MRIstat_rangeSimon. L12430243 STS_019_MRIstat_rangeSimon. L24420442 STS_019_MRIstat_madLaws28028 STS_019_MRIstat_madGabor27027 STS_019_MRIstat_madSimon. L243043 STS_019_MRIstat_rmadLaws21021 STS_019_MRIstat_rmadGabor17017 STS_019_MRIstat_rmadSimon. L229029 STS_019_MRIstat_medadLaws28028 STS_019_MRIstat_medadGabor25025 STS_019_MRIstat_medadSimon. L242042 STS_019_MRIstat_covLoG606 STS_019_MRIstat_covCoif3 LHH L1-1350135 STS_019_MRIstat_covSimon. L1-12012 STS_019_MRIstat_covSimon. L2-505 STS_019_MRIstat_qcodLoG202 STS_019_MRIstat_qcodCoif3 LHH L1-75075 STS_019_MRIstat_qcodSimon. L1-17017 STS_019_MRIstat_qcodSimon. L2-303 STS_019_MRIstat_rmsLaws83083 STS_016_CTstat_meanGabor94094 STS_016_CTstat_kurtLoG808 STS_016_CTstat_medianGabor80080 STS_016_CTstat_minSimon. L2-71071 STS_016_CTstat_p90Gabor1800180 STS_016_CTstat_maxGabor5940594 STS_016_CTstat_iqrGabor77077 STS_016_CTstat_rangeGabor5940594 STS_016_CTstat_madGabor49049 STS_016_CTstat_rmadGabor33033 STS_016_CTstat_medadGabor47047 STS_016_CTstat_covLoG-25025 STS_016_CTstat_covCoif3 LHH L1-2.528e+0402.528e+04 STS_016_CTstat_covSimon. L1-3970397 STS_016_CTstat_covSimon. L2-88088 STS_016_CTstat_qcodLoG-18018 STS_016_CTstat_qcodCoif3 LHH L1196401964 STS_016_CTstat_qcodSimon. L1-2510251 STS_016_CTstat_qcodSimon. L2-41041 STS_016_CTstat_rmsGabor1140114 STS_020_PETstat_skewLoG-404 STS_020_PETstat_skewLaws202 STS_020_PETstat_skewGabor404 STS_020_PETstat_skewCoif3 HHH L2202 STS_020_PETstat_skewSimon. L2202 STS_020_PETstat_kurtLoG27027 STS_020_PETstat_kurtGabor25025 STS_020_PETstat_kurtCoif3 HHH L2808 STS_020_PETstat_kurtSimon. L118018 STS_020_PETstat_kurtSimon. L221021 STS_020_PETstat_covLoG-149201492 STS_020_PETstat_covCoif3 LHH L1329703297 STS_020_PETstat_covSimon. L1122201222 STS_020_PETstat_covSimon. L2-2640264 STS_020_PETstat_qcodLoG202 STS_020_PETstat_qcodGabor101 STS_020_PETstat_qcodCoif3 LHH L16500650 STS_020_PETstat_qcodSimon. L1-23023 STS_020_PETstat_qcodSimon. L2-606 STS_021_PETstat_covLoG-909 STS_021_PETstat_covCoif3 LHH L1-6630663 STS_021_PETstat_covSimon. L1-2250225 STS_021_PETstat_covSimon. L221021 STS_021_PETstat_qcodLoG-22022 STS_021_PETstat_qcodGabor101 STS_021_PETstat_qcodCoif3 LHH L1-1130113 STS_021_PETstat_qcodSimon. L1-16016 STS_021_PETstat_qcodSimon. L2-5790579 STS_022_PETstat_covLoG-63063 STS_022_PETstat_covCoif3 LHH L1403404034 STS_022_PETstat_covSimon. L183083 STS_022_PETstat_covSimon. L2-30030 STS_022_PETstat_qcodLoG17017 STS_022_PETstat_qcodCoif3 LHH L1-1130113 STS_022_PETstat_qcodSimon. L1-20020 STS_022_PETstat_qcodSimon. L2-909 STS_013_CTstat_meanGabor90090 STS_013_CTstat_varGabor780507805 STS_013_CTstat_skewLoG303 STS_013_CTstat_skewLaws404 STS_013_CTstat_skewGabor808 STS_013_CTstat_skewCoif3 HHH L2808 STS_013_CTstat_skewSimon. L1-404 STS_013_CTstat_skewSimon. L2-303 STS_013_CTstat_kurtLoG10010 STS_013_CTstat_kurtLaws20020 STS_013_CTstat_kurtGabor1080108 STS_013_CTstat_kurtCoif3 LHH L118018 STS_013_CTstat_kurtCoif3 HHH L21140114 STS_013_CTstat_kurtSimon. L139039 STS_013_CTstat_kurtSimon. L219019 STS_013_CTstat_minSimon. L1-3040304 STS_013_CTstat_minSimon. L2-4650465 STS_013_CTstat_p90Gabor1530153 STS_013_CTstat_maxLaws5400540 STS_013_CTstat_maxGabor259302593 STS_013_CTstat_maxCoif3 HHH L23340334 STS_013_CTstat_maxSimon. L11410141 STS_013_CTstat_maxSimon. L22190219 STS_013_CTstat_iqrGabor64064 STS_013_CTstat_rangeLaws5270527 STS_013_CTstat_rangeGabor259302593 STS_013_CTstat_rangeCoif3 HHH L23470347 STS_013_CTstat_rangeSimon. L14450445 STS_013_CTstat_rangeSimon. L26840684 STS_013_CTstat_madLaws24024 STS_013_CTstat_madGabor46046 STS_013_CTstat_madSimon. L226026 STS_013_CTstat_rmadGabor27027 STS_013_CTstat_medadGabor44044 STS_013_CTstat_medadSimon. L226026 STS_013_CTstat_covLoG303 STS_013_CTstat_covCoif3 LHH L1-105101051 STS_013_CTstat_covSimon. L1-37037 STS_013_CTstat_covSimon. L2-12012 STS_013_CTstat_qcodLoG202 STS_013_CTstat_qcodCoif3 LHH L1-348203482 STS_013_CTstat_qcodSimon. L1-96096 STS_013_CTstat_qcodSimon. L2-1260126 STS_013_CTstat_rmsGabor1260126 STS_017_CTstat_meanGabor97097 STS_017_CTstat_varGabor590705907 STS_017_CTstat_skewLoG404 STS_017_CTstat_skewLaws202 STS_017_CTstat_skewGabor202 STS_017_CTstat_kurtLoG44044 STS_017_CTstat_kurtLaws22022 STS_017_CTstat_kurtSimon. L210010 STS_017_CTstat_medianGabor75075 STS_017_CTstat_minSimon. L1-80080 STS_017_CTstat_minSimon. L2-1770177 STS_017_CTstat_p90Gabor2030203 STS_017_CTstat_maxLaws2460246 STS_017_CTstat_maxGabor8090809 STS_017_CTstat_maxCoif3 HHH L21610161 STS_017_CTstat_maxSimon. L21340134 STS_017_CTstat_iqrGabor88088 STS_017_CTstat_rangeLaws2260226 STS_017_CTstat_rangeGabor8090809 STS_017_CTstat_rangeCoif3 HHH L21770177 STS_017_CTstat_rangeSimon. L11950195 STS_017_CTstat_rangeSimon. L23110311 STS_017_CTstat_madGabor58058 STS_017_CTstat_rmadGabor38038 STS_017_CTstat_medadGabor55055 STS_017_CTstat_covLoG-12012 STS_017_CTstat_covCoif3 LHH L1-938809388 STS_017_CTstat_covSimon. L1-6070607 STS_017_CTstat_covSimon. L251051 STS_017_CTstat_qcodLoG-202 STS_017_CTstat_qcodGabor101 STS_017_CTstat_qcodCoif3 LHH L1-1.093e+0401.093e+04 STS_017_CTstat_qcodSimon. L1-113901139 STS_017_CTstat_qcodSimon. L2-1260126 STS_017_CTstat_rmsGabor1240124 STS_023_PETstat_skewCoif3 HHH L2202 STS_023_PETstat_covLoG-404 STS_023_PETstat_covCoif3 LHH L12660266 STS_023_PETstat_covSimon. L114014 STS_023_PETstat_covSimon. L2606 STS_023_PETstat_qcodLoG-202 STS_023_PETstat_qcodGabor101 STS_023_PETstat_qcodCoif3 LHH L1-91091 STS_023_PETstat_qcodSimon. L113013 STS_023_PETstat_qcodSimon. L2303 STS_022_MRIstat_skewLoG404 STS_022_MRIstat_skewLaws404 STS_022_MRIstat_skewGabor404 STS_022_MRIstat_skewCoif3 HHH L2707 STS_022_MRIstat_skewSimon. L1-202 STS_022_MRIstat_kurtLoG22022 STS_022_MRIstat_kurtLaws20020 STS_022_MRIstat_kurtGabor22022 STS_022_MRIstat_kurtCoif3 LHH L151051 STS_022_MRIstat_kurtCoif3 HHH L269069 STS_022_MRIstat_kurtSimon. L155055 STS_022_MRIstat_kurtSimon. L214014 STS_022_MRIstat_minSimon. L1-2000200 STS_022_MRIstat_minSimon. L2-2140214 STS_022_MRIstat_maxLaws1900190 STS_022_MRIstat_maxSimon. L11430143 STS_022_MRIstat_maxSimon. L21630163 STS_022_MRIstat_rangeLaws1860186 STS_022_MRIstat_rangeSimon. L13430343 STS_022_MRIstat_rangeSimon. L23770377 STS_022_MRIstat_covLoG606 STS_022_MRIstat_covCoif3 LHH L13140314 STS_022_MRIstat_covSimon. L150050 STS_022_MRIstat_covSimon. L214014 STS_022_MRIstat_qcodLoG71071 STS_022_MRIstat_qcodCoif3 LHH L16980698 STS_022_MRIstat_qcodSimon. L11390139 STS_022_MRIstat_qcodSimon. L213013 STS_021_MRIstat_skewLoG202 STS_021_MRIstat_skewLaws303 STS_021_MRIstat_skewGabor303 STS_021_MRIstat_kurtLoG27027 STS_021_MRIstat_kurtLaws16016 STS_021_MRIstat_kurtGabor30030 STS_021_MRIstat_kurtSimon. L2909 STS_021_MRIstat_minSimon. L1-1240124 STS_021_MRIstat_minSimon. L2-1550155 STS_021_MRIstat_maxLaws1660166 STS_021_MRIstat_maxGabor7120712 STS_021_MRIstat_maxCoif3 HHH L22680268 STS_021_MRIstat_maxSimon. L11350135 STS_021_MRIstat_maxSimon. L22020202 STS_021_MRIstat_rangeLaws1450145 STS_021_MRIstat_rangeGabor7120712 STS_021_MRIstat_rangeCoif3 HHH L23030303 STS_021_MRIstat_rangeSimon. L12590259 STS_021_MRIstat_rangeSimon. L23560356 STS_021_MRIstat_covLoG-109701097 STS_021_MRIstat_covCoif3 LHH L15.033e+0405.033e+04 STS_021_MRIstat_covSimon. L1-112501125 STS_021_MRIstat_covSimon. L227027 STS_021_MRIstat_qcodLoG-808 STS_021_MRIstat_qcodCoif3 LHH L1-480704807 STS_021_MRIstat_qcodSimon. L1-114101141 STS_021_MRIstat_qcodSimon. L254054 STS_024_PETstat_covCoif3 LHH L1-2930293 STS_024_PETstat_covSimon. L123023 STS_024_PETstat_covSimon. L2505 STS_024_PETstat_qcodLoG-202 STS_024_PETstat_qcodGabor101 STS_024_PETstat_qcodCoif3 LHH L1-64064 STS_024_PETstat_qcodCoif3 HHH L2101 STS_024_PETstat_qcodSimon. L141041 STS_024_PETstat_qcodSimon. L2505 STS_023_MRIstat_skewLaws202 STS_023_MRIstat_skewGabor202 STS_023_MRIstat_skewCoif3 HHH L2303 STS_023_MRIstat_kurtGabor909 STS_023_MRIstat_kurtCoif3 HHH L218018 STS_023_MRIstat_kurtSimon. L113013 STS_023_MRIstat_minSimon. L1-1180118 STS_023_MRIstat_minSimon. L2-1240124 STS_023_MRIstat_maxLaws2150215 STS_023_MRIstat_maxSimon. L12190219 STS_023_MRIstat_maxSimon. L21910191 STS_023_MRIstat_rangeLaws2040204 STS_023_MRIstat_rangeSimon. L13370337 STS_023_MRIstat_rangeSimon. L23150315 STS_023_MRIstat_covLoG-606 STS_023_MRIstat_covCoif3 LHH L1-5030503 STS_023_MRIstat_covSimon. L13470347 STS_023_MRIstat_covSimon. L2-41041 STS_023_MRIstat_qcodLoG-505 STS_023_MRIstat_qcodCoif3 LHH L1-2500250 STS_023_MRIstat_qcodCoif3 HHH L2101 STS_023_MRIstat_qcodSimon. L1-62062 STS_023_MRIstat_qcodSimon. L2-12012 STS_022_CTstat_meanLaws76076 STS_022_CTstat_meanGabor3070307 STS_022_CTstat_varGabor2.893e+0402.893e+04 STS_022_CTstat_skewSimon. L2404 STS_022_CTstat_kurtLoG12012 STS_022_CTstat_kurtSimon. L1909 STS_022_CTstat_kurtSimon. L258058 STS_022_CTstat_medianLaws76076 STS_022_CTstat_medianGabor2830283 STS_022_CTstat_minSimon. L1-2220222 STS_022_CTstat_minSimon. L2-2240224 STS_022_CTstat_p10Laws66066 STS_022_CTstat_p10Gabor1080108 STS_022_CTstat_p90Gabor5380538 STS_022_CTstat_maxLaws1540154 STS_022_CTstat_maxGabor131301313 STS_022_CTstat_maxCoif3 HHH L21370137 STS_022_CTstat_maxSimon. L13080308 STS_022_CTstat_maxSimon. L23190319 STS_022_CTstat_iqrGabor2270227 STS_022_CTstat_rangeGabor131301313 STS_022_CTstat_rangeCoif3 HHH L21650165 STS_022_CTstat_rangeSimon. L15290529 STS_022_CTstat_rangeSimon. L25430543 STS_022_CTstat_madGabor1350135 STS_022_CTstat_rmadGabor94094 STS_022_CTstat_medadGabor1330133 STS_022_CTstat_covLoG11011 STS_022_CTstat_covCoif3 LHH L1-559305593 STS_022_CTstat_covSimon. L196096 STS_022_CTstat_covSimon. L294094 STS_022_CTstat_qcodLoG909 STS_022_CTstat_qcodCoif3 LHH L1-119501195 STS_022_CTstat_qcodSimon. L14910491 STS_022_CTstat_qcodSimon. L2-2630263 STS_022_CTstat_energyGabor1.007e+1101.007e+11 STS_022_CTstat_rmsLaws76076 STS_022_CTstat_rmsGabor3510351 STS_025_PETstat_covCoif3 LHH L153053 STS_025_PETstat_covSimon. L1505 STS_025_PETstat_qcodLoG-101 STS_025_PETstat_qcodCoif3 LHH L127027 STS_025_PETstat_qcodSimon. L1303 STS_025_PETstat_qcodSimon. L2101 STS_023_CTstat_meanGabor1660166 STS_023_CTstat_varGabor989209892 STS_023_CTstat_skewLoG404 STS_023_CTstat_skewLaws404 STS_023_CTstat_kurtLoG13013 STS_023_CTstat_kurtLaws19019 STS_023_CTstat_kurtCoif3 LHH L1808 STS_023_CTstat_kurtSimon. L213013 STS_023_CTstat_medianGabor1500150 STS_023_CTstat_minSimon. L1-2260226 STS_023_CTstat_minSimon. L2-3470347 STS_023_CTstat_p10Gabor53053 STS_023_CTstat_p90Gabor2990299 STS_023_CTstat_maxLaws3590359 STS_023_CTstat_maxGabor106801068 STS_023_CTstat_maxSimon. L11280128 STS_023_CTstat_maxSimon. L21510151 STS_023_CTstat_iqrGabor1340134 STS_023_CTstat_rangeLaws3310331 STS_023_CTstat_rangeGabor106801068 STS_023_CTstat_rangeSimon. L13540354 STS_023_CTstat_rangeSimon. L24980498 STS_023_CTstat_madGabor79079 STS_023_CTstat_madSimon. L222022 STS_023_CTstat_rmadGabor55055 STS_023_CTstat_medadGabor78078 STS_023_CTstat_medadSimon. L222022 STS_023_CTstat_covLoG303 STS_023_CTstat_covCoif3 LHH L1-3950395 STS_023_CTstat_covSimon. L181081 STS_023_CTstat_covSimon. L2909 STS_023_CTstat_qcodLoG202 STS_023_CTstat_qcodCoif3 LHH L1-2470247 STS_023_CTstat_qcodSimon. L1-6180618 STS_023_CTstat_qcodSimon. L233033 STS_023_CTstat_rmsGabor1940194 STS_024_MRIstat_varGabor600906009 STS_024_MRIstat_skewLaws202 STS_024_MRIstat_skewGabor404 STS_024_MRIstat_skewCoif3 HHH L2303 STS_024_MRIstat_skewSimon. L2-202 STS_024_MRIstat_kurtGabor30030 STS_024_MRIstat_kurtCoif3 LHH L1909 STS_024_MRIstat_kurtCoif3 HHH L217017 STS_024_MRIstat_kurtSimon. L112012 STS_024_MRIstat_minSimon. L1-1630163 STS_024_MRIstat_minSimon. L2-2360236 STS_024_MRIstat_p90Laws1170117 STS_024_MRIstat_p90Gabor1320132 STS_024_MRIstat_maxLaws2600260 STS_024_MRIstat_maxGabor121301213 STS_024_MRIstat_maxCoif3 HHH L21740174 STS_024_MRIstat_maxSimon. L12090209 STS_024_MRIstat_maxSimon. L21930193 STS_024_MRIstat_iqrGabor48048 STS_024_MRIstat_rangeLaws2470247 STS_024_MRIstat_rangeGabor121301213 STS_024_MRIstat_rangeCoif3 HHH L21780178 STS_024_MRIstat_rangeSimon. L13720372 STS_024_MRIstat_rangeSimon. L24290429 STS_024_MRIstat_madLaws32032 STS_024_MRIstat_madGabor45045 STS_024_MRIstat_madSimon. L225025 STS_024_MRIstat_rmadLaws20020 STS_024_MRIstat_rmadGabor22022 STS_024_MRIstat_medadLaws28028 STS_024_MRIstat_medadGabor40040 STS_024_MRIstat_medadSimon. L225025 STS_024_MRIstat_covCoif3 LHH L17650765 STS_024_MRIstat_covSimon. L1-75075 STS_024_MRIstat_covSimon. L2-909 STS_024_MRIstat_qcodLoG101 STS_024_MRIstat_qcodCoif3 LHH L1-6630663 STS_024_MRIstat_qcodCoif3 HHH L2101 STS_024_MRIstat_qcodSimon. L12440244 STS_024_MRIstat_qcodSimon. L23330333 STS_024_MRIstat_rmsGabor1010101 STS_025_MRIstat_meanLaws1120112 STS_025_MRIstat_meanGabor2580258 STS_025_MRIstat_varGabor6.212e+0406.212e+04 STS_025_MRIstat_skewGabor202 STS_025_MRIstat_skewCoif3 HHH L2606 STS_025_MRIstat_kurtCoif3 HHH L268068 STS_025_MRIstat_medianLaws1010101 STS_025_MRIstat_medianGabor1700170 STS_025_MRIstat_minSimon. L1-2910291 STS_025_MRIstat_minSimon. L2-3240324 STS_025_MRIstat_p10Laws53053 STS_025_MRIstat_p10Simon. L2-75075 STS_025_MRIstat_p90Laws1880188 STS_025_MRIstat_p90Gabor5910591 STS_025_MRIstat_maxLaws3220322 STS_025_MRIstat_maxGabor177001770 STS_025_MRIstat_maxCoif3 HHH L23460346 STS_025_MRIstat_maxSimon. L12220222 STS_025_MRIstat_maxSimon. L22370237 STS_025_MRIstat_iqrLaws76076 STS_025_MRIstat_iqrGabor2540254 STS_025_MRIstat_iqrSimon. L253053 STS_025_MRIstat_rangeLaws2880288 STS_025_MRIstat_rangeGabor176901769 STS_025_MRIstat_rangeCoif3 HHH L23560356 STS_025_MRIstat_rangeSimon. L15130513 STS_025_MRIstat_rangeSimon. L25610561 STS_025_MRIstat_madLaws43043 STS_025_MRIstat_madGabor1830183 STS_025_MRIstat_madSimon. L124024 STS_025_MRIstat_madSimon. L240040 STS_025_MRIstat_rmadLaws31031 STS_025_MRIstat_rmadGabor1120112 STS_025_MRIstat_rmadSimon. L224024 STS_025_MRIstat_medadLaws42042 STS_025_MRIstat_medadGabor1680168 STS_025_MRIstat_medadSimon. L124024 STS_025_MRIstat_medadSimon. L239039 STS_025_MRIstat_covLoG78078 STS_025_MRIstat_covCoif3 LHH L1-187801878 STS_025_MRIstat_covSimon. L1-47047 STS_025_MRIstat_covSimon. L2-808 STS_025_MRIstat_qcodLoG505 STS_025_MRIstat_qcodGabor101 STS_025_MRIstat_qcodCoif3 LHH L1-1010101 STS_025_MRIstat_qcodSimon. L1-78078 STS_025_MRIstat_qcodSimon. L2-13013 STS_025_MRIstat_rmsLaws1230123 STS_025_MRIstat_rmsGabor3590359 STS_026_PETstat_covLoG-505 STS_026_PETstat_covCoif3 LHH L1188401884 STS_026_PETstat_covSimon. L158058 STS_026_PETstat_covSimon. L2808 STS_026_PETstat_qcodLoG-404 STS_026_PETstat_qcodCoif3 LHH L1-510505105 STS_026_PETstat_qcodSimon. L1-59059 STS_026_PETstat_qcodSimon. L2707 STS_021_CTstat_meanGabor1430143 STS_021_CTstat_varGabor598905989 STS_021_CTstat_medianGabor1320132 STS_021_CTstat_minSimon. L1-1240124 STS_021_CTstat_minSimon. L2-84084 STS_021_CTstat_p10Laws61061 STS_021_CTstat_p10Gabor51051 STS_021_CTstat_p90Gabor2480248 STS_021_CTstat_maxGabor6160616 STS_021_CTstat_iqrGabor1050105 STS_021_CTstat_rangeGabor6160616 STS_021_CTstat_rangeCoif3 HHH L21330133 STS_021_CTstat_rangeSimon. L12400240 STS_021_CTstat_rangeSimon. L21710171 STS_021_CTstat_madGabor62062 STS_021_CTstat_rmadGabor43043 STS_021_CTstat_medadGabor61061 STS_021_CTstat_covLoG80080 STS_021_CTstat_covCoif3 LHH L1-4.246e+0404.246e+04 STS_021_CTstat_covSimon. L1251302513 STS_021_CTstat_covSimon. L2719007190 STS_021_CTstat_qcodLoG1000100 STS_021_CTstat_qcodCoif3 LHH L14500450 STS_021_CTstat_qcodSimon. L11.846e+0501.846e+05 STS_021_CTstat_qcodSimon. L25190519 STS_021_CTstat_rmsGabor1620162 STS_024_CTstat_meanGabor92092 STS_024_CTstat_skewLoG-505 STS_024_CTstat_skewLaws505 STS_024_CTstat_skewCoif3 HHH L2909 STS_024_CTstat_kurtLoG34034 STS_024_CTstat_kurtLaws40040 STS_024_CTstat_kurtCoif3 HHH L21120112 STS_024_CTstat_kurtSimon. L2808 STS_024_CTstat_medianGabor83083 STS_024_CTstat_minSimon. L2-1280128 STS_024_CTstat_p90Gabor1620162 STS_024_CTstat_maxLaws1620162 STS_024_CTstat_maxGabor3850385 STS_024_CTstat_maxCoif3 HHH L22290229 STS_024_CTstat_iqrGabor68068 STS_024_CTstat_rangeLaws1440144 STS_024_CTstat_rangeGabor3850385 STS_024_CTstat_rangeCoif3 HHH L22360236 STS_024_CTstat_rangeSimon. L21830183 STS_024_CTstat_madGabor41041 STS_024_CTstat_rmadGabor28028 STS_024_CTstat_medadGabor41041 STS_024_CTstat_covCoif3 LHH L1-4230423 STS_024_CTstat_covSimon. L173073 STS_024_CTstat_covSimon. L2-80080 STS_024_CTstat_qcodLoG-101 STS_024_CTstat_qcodCoif3 LHH L1-3470347 STS_024_CTstat_qcodSimon. L11720172 STS_024_CTstat_qcodSimon. L221021 STS_024_CTstat_rmsGabor1060106 STS_025_CTstat_skewLoG-202 STS_025_CTstat_skewLaws303 STS_025_CTstat_skewCoif3 HHH L2202 STS_025_CTstat_kurtCoif3 HHH L2909 STS_025_CTstat_kurtSimon. L111011 STS_025_CTstat_minSimon. L1-61061 STS_025_CTstat_minSimon. L2-1050105 STS_025_CTstat_maxLaws1550155 STS_025_CTstat_maxGabor3530353 STS_025_CTstat_rangeLaws1430143 STS_025_CTstat_rangeGabor3520352 STS_025_CTstat_rangeSimon. L11680168 STS_025_CTstat_rangeSimon. L22110211 STS_025_CTstat_madGabor20020 STS_025_CTstat_medadGabor20020 STS_025_CTstat_covCoif3 LHH L12890289 STS_025_CTstat_covSimon. L1-25025 STS_025_CTstat_covSimon. L2-10010 STS_025_CTstat_qcodLoG-101 STS_025_CTstat_qcodCoif3 LHH L13820382 STS_025_CTstat_qcodSimon. L1-43043 STS_025_CTstat_qcodSimon. L228028 STS_027_PETstat_covLoG-303 STS_027_PETstat_covCoif3 LHH L14660466 STS_027_PETstat_covSimon. L115015 STS_027_PETstat_covSimon. L2606 STS_027_PETstat_qcodLoG-202 STS_027_PETstat_qcodCoif3 LHH L1-116501165 STS_027_PETstat_qcodSimon. L115015 STS_027_PETstat_qcodSimon. L2404 STS_026_MRIstat_skewLaws202 STS_026_MRIstat_skewGabor404 STS_026_MRIstat_skewCoif3 HHH L2303 STS_026_MRIstat_kurtGabor21021 STS_026_MRIstat_kurtCoif3 HHH L211011 STS_026_MRIstat_kurtSimon. L114014 STS_026_MRIstat_minSimon. L1-2110211 STS_026_MRIstat_minSimon. L2-2460246 STS_026_MRIstat_maxLaws2120212 STS_026_MRIstat_maxGabor5020502 STS_026_MRIstat_maxCoif3 HHH L23540354 STS_026_MRIstat_maxSimon. L11640164 STS_026_MRIstat_maxSimon. L22070207 STS_026_MRIstat_rangeLaws1970197 STS_026_MRIstat_rangeGabor5020502 STS_026_MRIstat_rangeCoif3 HHH L23970397 STS_026_MRIstat_rangeSimon. L13760376 STS_026_MRIstat_rangeSimon. L24530453 STS_026_MRIstat_madLaws22022 STS_026_MRIstat_madGabor22022 STS_026_MRIstat_madSimon. L223023 STS_026_MRIstat_medadLaws21021 STS_026_MRIstat_medadGabor20020 STS_026_MRIstat_medadSimon. L223023 STS_026_MRIstat_covLoG606 STS_026_MRIstat_covCoif3 LHH L1-189501895 STS_026_MRIstat_covSimon. L1-31031 STS_026_MRIstat_covSimon. L2-14014 STS_026_MRIstat_qcodLoG202 STS_026_MRIstat_qcodCoif3 LHH L1-3010301 STS_026_MRIstat_qcodSimon. L1-47047 STS_026_MRIstat_qcodSimon. L2-12012 STS_028_PETstat_covLoG-303 STS_028_PETstat_covCoif3 LHH L1-3880388 STS_028_PETstat_covSimon. L1-1410141 STS_028_PETstat_covSimon. L2808 STS_028_PETstat_qcodLoG-202 STS_028_PETstat_qcodCoif3 LHH L1-90090 STS_028_PETstat_qcodSimon. L1-16016 STS_028_PETstat_qcodSimon. L2707 STS_026_CTstat_meanGabor2200220 STS_026_CTstat_varGabor1.384e+0401.384e+04 STS_026_CTstat_skewLoG303 STS_026_CTstat_skewSimon. L1202 STS_026_CTstat_skewSimon. L2505 STS_026_CTstat_kurtLoG43043 STS_026_CTstat_kurtSimon. L125025 STS_026_CTstat_kurtSimon. L249049 STS_026_CTstat_medianGabor2040204 STS_026_CTstat_minSimon. L1-1470147 STS_026_CTstat_minSimon. L2-1350135 STS_026_CTstat_p10Laws53053 STS_026_CTstat_p10Gabor80080 STS_026_CTstat_p90Gabor3800380 STS_026_CTstat_maxLaws1470147 STS_026_CTstat_maxGabor9030903 STS_026_CTstat_maxCoif3 HHH L21320132 STS_026_CTstat_maxSimon. L13040304 STS_026_CTstat_maxSimon. L23460346 STS_026_CTstat_iqrGabor1600160 STS_026_CTstat_rangeGabor9030903 STS_026_CTstat_rangeCoif3 HHH L21510151 STS_026_CTstat_rangeSimon. L14510451 STS_026_CTstat_rangeSimon. L24810481 STS_026_CTstat_madGabor94094 STS_026_CTstat_rmadGabor66066 STS_026_CTstat_medadGabor93093 STS_026_CTstat_covLoG-1630163 STS_026_CTstat_covCoif3 LHH L1-9.46e+0409.46e+04 STS_026_CTstat_covSimon. L144044 STS_026_CTstat_covSimon. L224024 STS_026_CTstat_qcodLoG-27027 STS_026_CTstat_qcodCoif3 LHH L18020802 STS_026_CTstat_qcodSimon. L12600260 STS_026_CTstat_qcodSimon. L21580158 STS_026_CTstat_rmsGabor2490249 STS_028_MRIstat_skewLoG202 STS_028_MRIstat_skewLaws202 STS_028_MRIstat_skewGabor404 STS_028_MRIstat_skewCoif3 HHH L2303 STS_028_MRIstat_skewSimon. L1-202 STS_028_MRIstat_kurtGabor23023 STS_028_MRIstat_kurtCoif3 LHH L111011 STS_028_MRIstat_kurtCoif3 HHH L211011 STS_028_MRIstat_kurtSimon. L118018 STS_028_MRIstat_minSimon. L1-1670167 STS_028_MRIstat_minSimon. L2-1840184 STS_028_MRIstat_maxLaws2330233 STS_028_MRIstat_maxGabor3620362 STS_028_MRIstat_maxCoif3 HHH L22680268 STS_028_MRIstat_maxSimon. L11770177 STS_028_MRIstat_maxSimon. L21900190 STS_028_MRIstat_rangeLaws2270227 STS_028_MRIstat_rangeGabor3620362 STS_028_MRIstat_rangeCoif3 HHH L22860286 STS_028_MRIstat_rangeSimon. L13440344 STS_028_MRIstat_rangeSimon. L23730373 STS_028_MRIstat_madLaws26026 STS_028_MRIstat_medadLaws22022 STS_028_MRIstat_covCoif3 LHH L1-212502125 STS_028_MRIstat_covSimon. L1-50050 STS_028_MRIstat_covSimon. L2-20020 STS_028_MRIstat_qcodLoG101 STS_028_MRIstat_qcodLaws101 STS_028_MRIstat_qcodCoif3 LHH L1-3190319 STS_028_MRIstat_qcodCoif3 HHH L2101 STS_028_MRIstat_qcodSimon. L11580158 STS_028_MRIstat_qcodSimon. L2-20020 STS_029_PETstat_covLoG-505 STS_029_PETstat_covCoif3 LHH L1110301103 STS_029_PETstat_covSimon. L198098 STS_029_PETstat_covSimon. L219019 STS_029_PETstat_qcodLoG-505 STS_029_PETstat_qcodCoif3 LHH L14170417 STS_029_PETstat_qcodSimon. L1-25025 STS_029_PETstat_qcodSimon. L2-52052 STS_028_CTstat_skewLaws404 STS_028_CTstat_skewCoif3 HHH L2404 STS_028_CTstat_kurtLoG26026 STS_028_CTstat_kurtLaws21021 STS_028_CTstat_kurtCoif3 HHH L227027 STS_028_CTstat_kurtSimon. L112012 STS_028_CTstat_kurtSimon. L2909 STS_028_CTstat_minSimon. L1-93093 STS_028_CTstat_minSimon. L2-1050105 STS_028_CTstat_maxLaws1310131 STS_028_CTstat_maxGabor2560256 STS_028_CTstat_maxCoif3 HHH L21320132 STS_028_CTstat_rangeGabor2550255 STS_028_CTstat_rangeCoif3 HHH L21400140 STS_028_CTstat_rangeSimon. L11690169 STS_028_CTstat_rangeSimon. L22260226 STS_028_CTstat_madGabor21021 STS_028_CTstat_medadGabor20020 STS_028_CTstat_covLoG-10010 STS_028_CTstat_covCoif3 LHH L12.898e+0402.898e+04 STS_028_CTstat_covSimon. L1-44044 STS_028_CTstat_covSimon. L2-15015 STS_028_CTstat_qcodLoG-16016 STS_028_CTstat_qcodCoif3 LHH L1-4550455 STS_028_CTstat_qcodSimon. L1-1520152 STS_028_CTstat_qcodSimon. L2-909 STS_020_MRIstat_skewLaws404 STS_020_MRIstat_skewGabor505 STS_020_MRIstat_skewCoif3 HHH L2606 STS_020_MRIstat_skewSimon. L1202 STS_020_MRIstat_kurtLoG12012 STS_020_MRIstat_kurtLaws26026 STS_020_MRIstat_kurtGabor46046 STS_020_MRIstat_kurtCoif3 LHH L112012 STS_020_MRIstat_kurtCoif3 HHH L265065 STS_020_MRIstat_kurtSimon. L139039 STS_020_MRIstat_kurtSimon. L220020 STS_020_MRIstat_minSimon. L1-1670167 STS_020_MRIstat_minSimon. L2-2800280 STS_020_MRIstat_maxLaws2370237 STS_020_MRIstat_maxGabor1260126 STS_020_MRIstat_maxCoif3 HHH L21770177 STS_020_MRIstat_maxSimon. L12180218 STS_020_MRIstat_maxSimon. L23290329 STS_020_MRIstat_rangeLaws2350235 STS_020_MRIstat_rangeGabor1260126 STS_020_MRIstat_rangeCoif3 HHH L22190219 STS_020_MRIstat_rangeSimon. L13850385 STS_020_MRIstat_rangeSimon. L26090609 STS_020_MRIstat_covLoG10010 STS_020_MRIstat_covCoif3 LHH L1-132401324 STS_020_MRIstat_covSimon. L154054 STS_020_MRIstat_covSimon. L228028 STS_020_MRIstat_qcodLoG-707 STS_020_MRIstat_qcodLaws101 STS_020_MRIstat_qcodCoif3 LHH L1-9450945 STS_020_MRIstat_qcodCoif3 HHH L2101 STS_020_MRIstat_qcodSimon. L13350335 STS_020_MRIstat_qcodSimon. L227027 STS_030_PETstat_covLoG-404 STS_030_PETstat_covCoif3 LHH L1-252002520 STS_030_PETstat_covSimon. L179079 STS_030_PETstat_covSimon. L2808 STS_030_PETstat_qcodLoG-202 STS_030_PETstat_qcodCoif3 LHH L1-78078 STS_030_PETstat_qcodSimon. L1-44044 STS_030_PETstat_qcodSimon. L2303 STS_029_CTstat_meanGabor2170217 STS_029_CTstat_varGabor1.63e+0401.63e+04 STS_029_CTstat_medianGabor1960196 STS_029_CTstat_minSimon. L1-82082 STS_029_CTstat_minSimon. L2-69069 STS_029_CTstat_p10Laws61061 STS_029_CTstat_p10Gabor74074 STS_029_CTstat_p90Gabor3880388 STS_029_CTstat_maxGabor103601036 STS_029_CTstat_iqrGabor1620162 STS_029_CTstat_rangeGabor103601036 STS_029_CTstat_rangeSimon. L11700170 STS_029_CTstat_rangeSimon. L21750175 STS_029_CTstat_madGabor99099 STS_029_CTstat_rmadGabor68068 STS_029_CTstat_medadGabor98098 STS_029_CTstat_covLoG-404 STS_029_CTstat_covCoif3 LHH L1-1.87e+0401.87e+04 STS_029_CTstat_covSimon. L1330703307 STS_029_CTstat_covSimon. L2-1090109 STS_029_CTstat_qcodLoG-303 STS_029_CTstat_qcodCoif3 LHH L1-175601756 STS_029_CTstat_qcodSimon. L12970297 STS_029_CTstat_qcodSimon. L2-2090209 STS_029_CTstat_rmsGabor2520252 STS_020_CTstat_meanLaws73073 STS_020_CTstat_meanGabor2460246 STS_020_CTstat_varGabor2.131e+0402.131e+04 STS_020_CTstat_skewLoG505 STS_020_CTstat_skewLaws404 STS_020_CTstat_skewCoif3 HHH L2303 STS_020_CTstat_skewSimon. L2-202 STS_020_CTstat_kurtLoG28028 STS_020_CTstat_kurtLaws29029 STS_020_CTstat_kurtCoif3 HHH L229029 STS_020_CTstat_kurtSimon. L112012 STS_020_CTstat_kurtSimon. L236036 STS_020_CTstat_medianGabor2230223 STS_020_CTstat_minSimon. L1-3770377 STS_020_CTstat_minSimon. L2-4700470 STS_020_CTstat_p10Laws54054 STS_020_CTstat_p10Gabor79079 STS_020_CTstat_p90Gabor4430443 STS_020_CTstat_maxLaws4190419 STS_020_CTstat_maxGabor143801438 STS_020_CTstat_maxCoif3 HHH L24880488 STS_020_CTstat_maxSimon. L13090309 STS_020_CTstat_maxSimon. L23110311 STS_020_CTstat_iqrGabor1930193 STS_020_CTstat_rangeLaws3910391 STS_020_CTstat_rangeGabor143701437 STS_020_CTstat_rangeCoif3 HHH L25240524 STS_020_CTstat_rangeSimon. L16860686 STS_020_CTstat_rangeSimon. L27810781 STS_020_CTstat_madGabor1150115 STS_020_CTstat_rmadGabor80080 STS_020_CTstat_medadGabor1140114 STS_020_CTstat_covLoG505 STS_020_CTstat_covCoif3 LHH L11.103e+0401.103e+04 STS_020_CTstat_covSimon. L11330133 STS_020_CTstat_covSimon. L2-1900190 STS_020_CTstat_qcodLoG707 STS_020_CTstat_qcodCoif3 LHH L1-198101981 STS_020_CTstat_qcodSimon. L15920592 STS_020_CTstat_qcodSimon. L2-3040304 STS_020_CTstat_energyGabor1.901e+1101.901e+11 STS_020_CTstat_rmsLaws75075 STS_020_CTstat_rmsGabor2860286 STS_031_PETstat_covLoG-303 STS_031_PETstat_covCoif3 LHH L19920992 STS_031_PETstat_covSimon. L158058 STS_031_PETstat_covSimon. L2505 STS_031_PETstat_qcodLoG-303 STS_031_PETstat_qcodCoif3 LHH L1100601006 STS_031_PETstat_qcodSimon. L1-66066 STS_031_PETstat_qcodSimon. L2404 STS_027_MRIstat_skewLaws303 STS_027_MRIstat_skewGabor404 STS_027_MRIstat_skewCoif3 HHH L2303 STS_027_MRIstat_kurtLaws13013 STS_027_MRIstat_kurtGabor32032 STS_027_MRIstat_kurtCoif3 HHH L216016 STS_027_MRIstat_kurtSimon. L2909 STS_027_MRIstat_minSimon. L1-1330133 STS_027_MRIstat_minSimon. L2-2090209 STS_027_MRIstat_maxLaws1410141 STS_027_MRIstat_maxGabor1370137 STS_027_MRIstat_maxSimon. L11290129 STS_027_MRIstat_maxSimon. L22680268 STS_027_MRIstat_rangeLaws1290129 STS_027_MRIstat_rangeGabor1370137 STS_027_MRIstat_rangeSimon. L12620262 STS_027_MRIstat_rangeSimon. L24760476 STS_027_MRIstat_covLoG61061 STS_027_MRIstat_covCoif3 LHH L12.19e+0402.19e+04 STS_027_MRIstat_covSimon. L1-126501265 STS_027_MRIstat_covSimon. L2-2600260 STS_027_MRIstat_qcodLoG-606 STS_027_MRIstat_qcodCoif3 LHH L1126401264 STS_027_MRIstat_qcodSimon. L1-1900190 STS_027_MRIstat_qcodSimon. L2-72072 STS_032_PETstat_covCoif3 LHH L1-154701547 STS_032_PETstat_covSimon. L110010 STS_032_PETstat_covSimon. L2303 STS_032_PETstat_qcodLoG-101 STS_032_PETstat_qcodCoif3 LHH L1-1750175 STS_032_PETstat_qcodSimon. L111011 STS_032_PETstat_qcodSimon. L2202 STS_029_MRIstat_skewLaws202 STS_029_MRIstat_skewGabor505 STS_029_MRIstat_skewCoif3 HHH L2303 STS_029_MRIstat_kurtGabor36036 STS_029_MRIstat_kurtCoif3 LHH L114014 STS_029_MRIstat_kurtCoif3 HHH L214014 STS_029_MRIstat_kurtSimon. L112012 STS_029_MRIstat_minSimon. L1-2150215 STS_029_MRIstat_minSimon. L2-2270227 STS_029_MRIstat_p90Laws1090109 STS_029_MRIstat_maxLaws3300330 STS_029_MRIstat_maxGabor6480648 STS_029_MRIstat_maxCoif3 HHH L23030303 STS_029_MRIstat_maxSimon. L11870187 STS_029_MRIstat_maxSimon. L22700270 STS_029_MRIstat_iqrLaws43043 STS_029_MRIstat_rangeLaws3180318 STS_029_MRIstat_rangeGabor6480648 STS_029_MRIstat_rangeCoif3 HHH L23220322 STS_029_MRIstat_rangeSimon. L14010401 STS_029_MRIstat_rangeSimon. L24970497 STS_029_MRIstat_madLaws31031 STS_029_MRIstat_madGabor26026 STS_029_MRIstat_madSimon. L222022 STS_029_MRIstat_rmadLaws20020 STS_029_MRIstat_medadLaws28028 STS_029_MRIstat_medadGabor22022 STS_029_MRIstat_medadSimon. L222022 STS_029_MRIstat_covCoif3 LHH L1-5470547 STS_029_MRIstat_covSimon. L1-1220122 STS_029_MRIstat_covSimon. L2-18018 STS_029_MRIstat_qcodLoG101 STS_029_MRIstat_qcodLaws101 STS_029_MRIstat_qcodCoif3 LHH L12700270 STS_029_MRIstat_qcodCoif3 HHH L2101 STS_029_MRIstat_qcodSimon. L1634506345 STS_029_MRIstat_qcodSimon. L22420242 STS_027_CTstat_meanLaws76076 STS_027_CTstat_meanGabor2390239 STS_027_CTstat_varGabor2.724e+0402.724e+04 STS_027_CTstat_skewLoG404 STS_027_CTstat_skewLaws404 STS_027_CTstat_skewSimon. L2-404 STS_027_CTstat_kurtLoG19019 STS_027_CTstat_kurtLaws21021 STS_027_CTstat_kurtSimon. L111011 STS_027_CTstat_kurtSimon. L229029 STS_027_CTstat_medianGabor2010201 STS_027_CTstat_minSimon. L1-3000300 STS_027_CTstat_minSimon. L2-4000400 STS_027_CTstat_p10Gabor69069 STS_027_CTstat_p90Gabor4650465 STS_027_CTstat_maxLaws4840484 STS_027_CTstat_maxGabor214902149 STS_027_CTstat_maxCoif3 HHH L21860186 STS_027_CTstat_maxSimon. L11650165 STS_027_CTstat_maxSimon. L21640164 STS_027_CTstat_iqrGabor2030203 STS_027_CTstat_rangeLaws4580458 STS_027_CTstat_rangeGabor214902149 STS_027_CTstat_rangeCoif3 HHH L22230223 STS_027_CTstat_rangeSimon. L14650465 STS_027_CTstat_rangeSimon. L25640564 STS_027_CTstat_madLaws21021 STS_027_CTstat_madGabor1270127 STS_027_CTstat_rmadGabor86086 STS_027_CTstat_medadLaws21021 STS_027_CTstat_medadGabor1230123 STS_027_CTstat_covLoG404 STS_027_CTstat_covCoif3 LHH L1569305693 STS_027_CTstat_covSimon. L1-94094 STS_027_CTstat_covSimon. L2-20020 STS_027_CTstat_qcodLoG606 STS_027_CTstat_qcodCoif3 LHH L1109401094 STS_027_CTstat_qcodSimon. L14100410 STS_027_CTstat_qcodSimon. L2-88088 STS_027_CTstat_rmsLaws85085 STS_027_CTstat_rmsGabor2910291 STS_032_MRIstat_meanGabor87087 STS_032_MRIstat_varGabor670206702 STS_032_MRIstat_skewLaws202 STS_032_MRIstat_skewGabor303 STS_032_MRIstat_skewCoif3 HHH L2303 STS_032_MRIstat_skewSimon. L1-202 STS_032_MRIstat_skewSimon. L2-202 STS_032_MRIstat_kurtLoG808 STS_032_MRIstat_kurtGabor13013 STS_032_MRIstat_kurtCoif3 LHH L114014 STS_032_MRIstat_kurtCoif3 HHH L217017 STS_032_MRIstat_kurtSimon. L122022 STS_032_MRIstat_kurtSimon. L2808 STS_032_MRIstat_minSimon. L1-3620362 STS_032_MRIstat_minSimon. L2-3870387 STS_032_MRIstat_p90Laws1580158 STS_032_MRIstat_p90Gabor1750175 STS_032_MRIstat_maxLaws5030503 STS_032_MRIstat_maxGabor116601166 STS_032_MRIstat_maxCoif3 HHH L23630363 STS_032_MRIstat_maxSimon. L13150315 STS_032_MRIstat_maxSimon. L23390339 STS_032_MRIstat_iqrLaws52052 STS_032_MRIstat_iqrGabor61061 STS_032_MRIstat_rangeLaws4820482 STS_032_MRIstat_rangeGabor116601166 STS_032_MRIstat_rangeCoif3 HHH L23860386 STS_032_MRIstat_rangeSimon. L16770677 STS_032_MRIstat_rangeSimon. L27260726 STS_032_MRIstat_madLaws45045 STS_032_MRIstat_madGabor53053 STS_032_MRIstat_madSimon. L229029 STS_032_MRIstat_rmadLaws25025 STS_032_MRIstat_rmadGabor28028 STS_032_MRIstat_medadLaws39039 STS_032_MRIstat_medadGabor48048 STS_032_MRIstat_medadSimon. L229029 STS_032_MRIstat_covLoG303 STS_032_MRIstat_covCoif3 LHH L1-5780578 STS_032_MRIstat_covSimon. L1-67067 STS_032_MRIstat_covSimon. L2-808 STS_032_MRIstat_qcodLoG202 STS_032_MRIstat_qcodCoif3 LHH L1-4200420 STS_032_MRIstat_qcodCoif3 HHH L2101 STS_032_MRIstat_qcodSimon. L156056 STS_032_MRIstat_qcodSimon. L2-59059 STS_032_MRIstat_rmsLaws94094 STS_032_MRIstat_rmsGabor1200120 STS_033_PETstat_covLoG-404 STS_033_PETstat_covCoif3 LHH L1-6670667 STS_033_PETstat_covSimon. L141041 STS_033_PETstat_covSimon. L2505 STS_033_PETstat_qcodLoG-404 STS_033_PETstat_qcodGabor101 STS_033_PETstat_qcodCoif3 LHH L1-85085 STS_033_PETstat_qcodSimon. L1-21021 STS_033_PETstat_qcodSimon. L2404 STS_032_CTstat_skewLoG-505 STS_032_CTstat_skewLaws606 STS_032_CTstat_skewCoif3 HHH L2707 STS_032_CTstat_kurtLoG36036 STS_032_CTstat_kurtLaws45045 STS_032_CTstat_kurtCoif3 HHH L287087 STS_032_CTstat_kurtSimon. L2909 STS_032_CTstat_minSimon. L2-1540154 STS_032_CTstat_maxLaws1280128 STS_032_CTstat_maxGabor2240224 STS_032_CTstat_maxCoif3 HHH L21920192 STS_032_CTstat_iqrGabor42042 STS_032_CTstat_rangeGabor2240224 STS_032_CTstat_rangeCoif3 HHH L22000200 STS_032_CTstat_rangeSimon. L22190219 STS_032_CTstat_madGabor25025 STS_032_CTstat_rmadGabor17017 STS_032_CTstat_medadGabor25025 STS_032_CTstat_covCoif3 LHH L1-3030303 STS_032_CTstat_covSimon. L1-1580158 STS_032_CTstat_covSimon. L24290429 STS_032_CTstat_qcodLoG-101 STS_032_CTstat_qcodCoif3 LHH L15610561 STS_032_CTstat_qcodSimon. L1-38038 STS_032_CTstat_qcodSimon. L220020 STS_034_PETstat_covLoG-505 STS_034_PETstat_covCoif3 LHH L1187501875 STS_034_PETstat_covSimon. L116016 STS_034_PETstat_covSimon. L2707 STS_034_PETstat_qcodLoG-404 STS_034_PETstat_qcodCoif3 LHH L1-97097 STS_034_PETstat_qcodSimon. L141041 STS_034_PETstat_qcodSimon. L2505 STS_030_CTstat_meanLaws77077 STS_030_CTstat_skewGabor202 STS_030_CTstat_medianLaws77077 STS_030_CTstat_minSimon. L1-1180118 STS_030_CTstat_minSimon. L2-77077 STS_030_CTstat_p10Laws62062 STS_030_CTstat_maxGabor3490349 STS_030_CTstat_maxCoif3 HHH L21300130 STS_030_CTstat_rangeGabor3480348 STS_030_CTstat_rangeCoif3 HHH L21700170 STS_030_CTstat_rangeSimon. L12390239 STS_030_CTstat_rangeSimon. L21630163 STS_030_CTstat_madGabor22022 STS_030_CTstat_medadGabor22022 STS_030_CTstat_covLoG-10010 STS_030_CTstat_covCoif3 LHH L1-610206102 STS_030_CTstat_covSimon. L1129901299 STS_030_CTstat_covSimon. L2-1110111 STS_030_CTstat_qcodLoG-404 STS_030_CTstat_qcodCoif3 LHH L1155401554 STS_030_CTstat_qcodSimon. L16430643 STS_030_CTstat_qcodSimon. L2-62062 STS_030_CTstat_rmsLaws78078 STS_031_MRIstat_skewLaws202 STS_031_MRIstat_skewGabor505 STS_031_MRIstat_skewCoif3 HHH L2404 STS_031_MRIstat_kurtGabor44044 STS_031_MRIstat_kurtCoif3 LHH L112012 STS_031_MRIstat_kurtCoif3 HHH L222022 STS_031_MRIstat_kurtSimon. L118018 STS_031_MRIstat_minSimon. L1-2710271 STS_031_MRIstat_minSimon. L2-2040204 STS_031_MRIstat_p90Laws1020102 STS_031_MRIstat_p90Gabor1140114 STS_031_MRIstat_maxLaws3480348 STS_031_MRIstat_maxGabor127701277 STS_031_MRIstat_maxCoif3 HHH L25210521 STS_031_MRIstat_maxSimon. L12550255 STS_031_MRIstat_maxSimon. L22680268 STS_031_MRIstat_iqrGabor46046 STS_031_MRIstat_rangeLaws3330333 STS_031_MRIstat_rangeGabor127701277 STS_031_MRIstat_rangeCoif3 HHH L25460546 STS_031_MRIstat_rangeSimon. L15260526 STS_031_MRIstat_rangeSimon. L24710471 STS_031_MRIstat_madLaws26026 STS_031_MRIstat_madGabor38038 STS_031_MRIstat_rmadGabor20020 STS_031_MRIstat_medadLaws23023 STS_031_MRIstat_medadGabor35035 STS_031_MRIstat_covCoif3 LHH L1-121201212 STS_031_MRIstat_covSimon. L1-1760176 STS_031_MRIstat_covSimon. L2-1580158 STS_031_MRIstat_qcodLoG101 STS_031_MRIstat_qcodCoif3 LHH L1-9060906 STS_031_MRIstat_qcodSimon. L1189601896 STS_031_MRIstat_qcodSimon. L221021 STS_031_MRIstat_rmsGabor91091 STS_035_PETstat_covCoif3 LHH L1-1140114 STS_035_PETstat_covSimon. L1909 STS_035_PETstat_qcodLoG-101 STS_035_PETstat_qcodCoif3 LHH L1-30030 STS_035_PETstat_qcodSimon. L110010 STS_035_PETstat_qcodSimon. L2101 STS_031_CTstat_meanLaws77077 STS_031_CTstat_meanGabor2680268 STS_031_CTstat_varGabor2.451e+0402.451e+04 STS_031_CTstat_skewLoG404 STS_031_CTstat_kurtLoG39039 STS_031_CTstat_kurtSimon. L216016 STS_031_CTstat_medianLaws78078 STS_031_CTstat_medianGabor2420242 STS_031_CTstat_minSimon. L1-1770177 STS_031_CTstat_minSimon. L2-1980198 STS_031_CTstat_p10Laws59059 STS_031_CTstat_p10Gabor90090 STS_031_CTstat_p90Gabor4830483 STS_031_CTstat_maxLaws2140214 STS_031_CTstat_maxGabor107101071 STS_031_CTstat_maxCoif3 HHH L22390239 STS_031_CTstat_maxSimon. L21340134 STS_031_CTstat_iqrGabor2080208 STS_031_CTstat_rangeLaws1700170 STS_031_CTstat_rangeGabor107001070 STS_031_CTstat_rangeCoif3 HHH L22650265 STS_031_CTstat_rangeSimon. L12860286 STS_031_CTstat_rangeSimon. L23330333 STS_031_CTstat_madGabor1240124 STS_031_CTstat_rmadGabor87087 STS_031_CTstat_medadGabor1220122 STS_031_CTstat_covLoG-13013 STS_031_CTstat_covCoif3 LHH L1-583305833 STS_031_CTstat_covSimon. L1-8500850 STS_031_CTstat_covSimon. L2-35035 STS_031_CTstat_qcodLoG-303 STS_031_CTstat_qcodCoif3 LHH L1714107141 STS_031_CTstat_qcodSimon. L1-149701497 STS_031_CTstat_qcodSimon. L2-27027 STS_031_CTstat_rmsLaws78078 STS_031_CTstat_rmsGabor3110311 STS_033_MRIstat_skewLaws202 STS_033_MRIstat_skewGabor404 STS_033_MRIstat_skewCoif3 HHH L2303 STS_033_MRIstat_kurtGabor25025 STS_033_MRIstat_kurtCoif3 LHH L113013 STS_033_MRIstat_kurtCoif3 HHH L216016 STS_033_MRIstat_kurtSimon. L115015 STS_033_MRIstat_kurtSimon. L2808 STS_033_MRIstat_minSimon. L1-2840284 STS_033_MRIstat_minSimon. L2-3660366 STS_033_MRIstat_maxLaws2300230 STS_033_MRIstat_maxGabor4800480 STS_033_MRIstat_maxCoif3 HHH L22820282 STS_033_MRIstat_maxSimon. L12970297 STS_033_MRIstat_maxSimon. L22780278 STS_033_MRIstat_rangeLaws2210221 STS_033_MRIstat_rangeGabor4800480 STS_033_MRIstat_rangeCoif3 HHH L23180318 STS_033_MRIstat_rangeSimon. L15810581 STS_033_MRIstat_rangeSimon. L26440644 STS_033_MRIstat_madLaws24024 STS_033_MRIstat_medadLaws21021 STS_033_MRIstat_covLoG303 STS_033_MRIstat_covCoif3 LHH L1-9320932 STS_033_MRIstat_covSimon. L1-73073 STS_033_MRIstat_covSimon. L2-18018 STS_033_MRIstat_qcodLoG101 STS_033_MRIstat_qcodCoif3 LHH L16140614 STS_033_MRIstat_qcodCoif3 HHH L2101 STS_033_MRIstat_qcodSimon. L1-84084 STS_033_MRIstat_qcodSimon. L2-80080 STS_030_MRIstat_meanLaws87087 STS_030_MRIstat_meanGabor99099 STS_030_MRIstat_varGabor984009840 STS_030_MRIstat_skewGabor303 STS_030_MRIstat_skewCoif3 HHH L2303 STS_030_MRIstat_kurtGabor10010 STS_030_MRIstat_kurtCoif3 HHH L217017 STS_030_MRIstat_minSimon. L1-2570257 STS_030_MRIstat_minSimon. L2-3580358 STS_030_MRIstat_p10Simon. L2-56056 STS_030_MRIstat_p90Laws1610161 STS_030_MRIstat_p90Gabor2150215 STS_030_MRIstat_maxLaws3650365 STS_030_MRIstat_maxGabor102401024 STS_030_MRIstat_maxSimon. L12920292 STS_030_MRIstat_maxSimon. L22820282 STS_030_MRIstat_iqrLaws67067 STS_030_MRIstat_iqrGabor80080 STS_030_MRIstat_iqrSimon. L251051 STS_030_MRIstat_rangeLaws3530353 STS_030_MRIstat_rangeGabor102401024 STS_030_MRIstat_rangeSimon. L15490549 STS_030_MRIstat_rangeSimon. L26400640 STS_030_MRIstat_madLaws41041 STS_030_MRIstat_madGabor67067 STS_030_MRIstat_madSimon. L121021 STS_030_MRIstat_madSimon. L235035 STS_030_MRIstat_rmadLaws28028 STS_030_MRIstat_rmadGabor37037 STS_030_MRIstat_rmadSimon. L222022 STS_030_MRIstat_medadLaws39039 STS_030_MRIstat_medadGabor60060 STS_030_MRIstat_medadSimon. L121021 STS_030_MRIstat_medadSimon. L235035 STS_030_MRIstat_covLoG505 STS_030_MRIstat_covCoif3 LHH L11.755e+0501.755e+05 STS_030_MRIstat_covSimon. L14260426 STS_030_MRIstat_covSimon. L2-39039 STS_030_MRIstat_qcodLoG303 STS_030_MRIstat_qcodGabor101 STS_030_MRIstat_qcodCoif3 LHH L17760776 STS_030_MRIstat_qcodCoif3 HHH L2101 STS_030_MRIstat_qcodSimon. L14820482 STS_030_MRIstat_qcodSimon. L2-22022 STS_030_MRIstat_rmsLaws1010101 STS_030_MRIstat_rmsGabor1400140 STS_033_CTstat_meanGabor1140114 STS_033_CTstat_skewLoG-707 STS_033_CTstat_skewLaws909 STS_033_CTstat_skewCoif3 HHH L2808 STS_033_CTstat_kurtLoG66066 STS_033_CTstat_kurtLaws1060106 STS_033_CTstat_kurtCoif3 HHH L21130113 STS_033_CTstat_kurtSimon. L110010 STS_033_CTstat_kurtSimon. L233033 STS_033_CTstat_medianGabor1030103 STS_033_CTstat_minSimon. L1-1080108 STS_033_CTstat_minSimon. L2-1480148 STS_033_CTstat_p90Gabor2060206 STS_033_CTstat_maxLaws2660266 STS_033_CTstat_maxGabor4980498 STS_033_CTstat_maxCoif3 HHH L22290229 STS_033_CTstat_maxSimon. L11700170 STS_033_CTstat_maxSimon. L22260226 STS_033_CTstat_iqrGabor89089 STS_033_CTstat_rangeLaws2450245 STS_033_CTstat_rangeGabor4980498 STS_033_CTstat_rangeCoif3 HHH L22380238 STS_033_CTstat_rangeSimon. L12790279 STS_033_CTstat_rangeSimon. L23740374 STS_033_CTstat_madGabor53053 STS_033_CTstat_rmadGabor37037 STS_033_CTstat_medadGabor52052 STS_033_CTstat_covLoG-404 STS_033_CTstat_covCoif3 LHH L1-187401874 STS_033_CTstat_covSimon. L1-2890289 STS_033_CTstat_covSimon. L23410341 STS_033_CTstat_qcodLoG-202 STS_033_CTstat_qcodCoif3 LHH L1-133901339 STS_033_CTstat_qcodSimon. L1-2060206 STS_033_CTstat_qcodSimon. L291091 STS_033_CTstat_rmsGabor1320132 STS_036_PETstat_covLoG-404 STS_036_PETstat_covCoif3 LHH L1-582805828 STS_036_PETstat_covSimon. L143043 STS_036_PETstat_covSimon. L2606 STS_036_PETstat_qcodLoG-404 STS_036_PETstat_qcodCoif3 LHH L1-7940794 STS_036_PETstat_qcodSimon. L1-63063 STS_036_PETstat_qcodSimon. L2404 STS_037_PETstat_covLoG-10010 STS_037_PETstat_covCoif3 LHH L1-112701127 STS_037_PETstat_covSimon. L1-1250125 STS_037_PETstat_covSimon. L246046 STS_037_PETstat_qcodLoG17017 STS_037_PETstat_qcodGabor101 STS_037_PETstat_qcodCoif3 LHH L12490249 STS_037_PETstat_qcodSimon. L1-20020 STS_037_PETstat_qcodSimon. L2-53053 STS_034_MRIstat_skewLaws202 STS_034_MRIstat_skewGabor404 STS_034_MRIstat_skewCoif3 HHH L2404 STS_034_MRIstat_skewSimon. L1202 STS_034_MRIstat_kurtGabor26026 STS_034_MRIstat_kurtCoif3 LHH L114014 STS_034_MRIstat_kurtCoif3 HHH L227027 STS_034_MRIstat_kurtSimon. L115015 STS_034_MRIstat_minSimon. L1-1540154 STS_034_MRIstat_minSimon. L2-2040204 STS_034_MRIstat_maxLaws2270227 STS_034_MRIstat_maxGabor1780178 STS_034_MRIstat_maxSimon. L12220222 STS_034_MRIstat_maxSimon. L22750275 STS_034_MRIstat_rangeLaws2230223 STS_034_MRIstat_rangeGabor1780178 STS_034_MRIstat_rangeSimon. L13760376 STS_034_MRIstat_rangeSimon. L24790479 STS_034_MRIstat_madLaws20020 STS_034_MRIstat_covLoG404 STS_034_MRIstat_covCoif3 LHH L15430543 STS_034_MRIstat_covSimon. L125025 STS_034_MRIstat_covSimon. L274074 STS_034_MRIstat_qcodLoG202 STS_034_MRIstat_qcodLaws101 STS_034_MRIstat_qcodCoif3 LHH L1-441504415 STS_034_MRIstat_qcodCoif3 HHH L2101 STS_034_MRIstat_qcodSimon. L178078 STS_034_MRIstat_qcodSimon. L262062 STS_035_MRIstat_meanLaws94094 STS_035_MRIstat_varSimon. L2520905209 STS_035_MRIstat_skewGabor202 STS_035_MRIstat_skewCoif3 HHH L2202 STS_035_MRIstat_kurtCoif3 HHH L2808 STS_035_MRIstat_medianLaws94094 STS_035_MRIstat_minSimon. L1-1550155 STS_035_MRIstat_minSimon. L2-2510251 STS_035_MRIstat_p10Simon. L2-1070107 STS_035_MRIstat_p90Laws1600160 STS_035_MRIstat_maxLaws2880288 STS_035_MRIstat_maxGabor3420342 STS_035_MRIstat_maxSimon. L12210221 STS_035_MRIstat_maxSimon. L22820282 STS_035_MRIstat_iqrLaws76076 STS_035_MRIstat_iqrSimon. L284084 STS_035_MRIstat_rangeLaws2780278 STS_035_MRIstat_rangeGabor3420342 STS_035_MRIstat_rangeSimon. L13760376 STS_035_MRIstat_rangeSimon. L25330533 STS_035_MRIstat_madLaws42042 STS_035_MRIstat_madGabor29029 STS_035_MRIstat_madSimon. L124024 STS_035_MRIstat_madSimon. L255055 STS_035_MRIstat_rmadLaws31031 STS_035_MRIstat_rmadGabor18018 STS_035_MRIstat_rmadSimon. L236036 STS_035_MRIstat_medadLaws42042 STS_035_MRIstat_medadGabor27027 STS_035_MRIstat_medadSimon. L124024 STS_035_MRIstat_medadSimon. L255055 STS_035_MRIstat_covCoif3 LHH L1145601456 STS_035_MRIstat_covSimon. L1-30030 STS_035_MRIstat_covSimon. L2-14014 STS_035_MRIstat_qcodLoG101 STS_035_MRIstat_qcodCoif3 LHH L148048 STS_035_MRIstat_qcodCoif3 HHH L2101 STS_035_MRIstat_qcodSimon. L1-32032 STS_035_MRIstat_qcodSimon. L2-1150115 STS_035_MRIstat_rmsLaws1060106 STS_035_CTstat_meanLaws1220122 STS_035_CTstat_meanGabor86086 STS_035_CTstat_varLaws1.275e+0401.275e+04 STS_035_CTstat_varGabor2.263e+0402.263e+04 STS_035_CTstat_varSimon. L21.69e+0401.69e+04 STS_035_CTstat_skewGabor606 STS_035_CTstat_skewCoif3 HHH L2303 STS_035_CTstat_skewSimon. L1-202 STS_035_CTstat_kurtGabor43043 STS_035_CTstat_kurtCoif3 HHH L219019 STS_035_CTstat_minSimon. L1-3860386 STS_035_CTstat_minSimon. L2-4340434 STS_035_CTstat_p10Simon. L2-2750275 STS_035_CTstat_p90Laws3060306 STS_035_CTstat_p90Gabor1600160 STS_035_CTstat_maxLaws5120512 STS_035_CTstat_maxGabor181701817 STS_035_CTstat_maxCoif3 HHH L22160216 STS_035_CTstat_maxSimon. L11530153 STS_035_CTstat_maxSimon. L21490149 STS_035_CTstat_iqrLaws1620162 STS_035_CTstat_iqrGabor50050 STS_035_CTstat_iqrSimon. L294094 STS_035_CTstat_rangeLaws4900490 STS_035_CTstat_rangeGabor181701817 STS_035_CTstat_rangeCoif3 HHH L22290229 STS_035_CTstat_rangeSimon. L15400540 STS_035_CTstat_rangeSimon. L25830583 STS_035_CTstat_madLaws95095 STS_035_CTstat_madGabor71071 STS_035_CTstat_madSimon. L137037 STS_035_CTstat_madSimon. L299099 STS_035_CTstat_rmadLaws71071 STS_035_CTstat_rmadGabor24024 STS_035_CTstat_rmadSimon. L117017 STS_035_CTstat_rmadSimon. L260060 STS_035_CTstat_medadLaws86086 STS_035_CTstat_medadGabor58058 STS_035_CTstat_medadSimon. L137037 STS_035_CTstat_medadSimon. L289089 STS_035_CTstat_covCoif3 LHH L181081 STS_035_CTstat_covSimon. L1-17017 STS_035_CTstat_covSimon. L2-404 STS_035_CTstat_qcodLoG101 STS_035_CTstat_qcodLaws101 STS_035_CTstat_qcodCoif3 LHH L136036 STS_035_CTstat_qcodSimon. L1-41041 STS_035_CTstat_qcodSimon. L2404 STS_035_CTstat_rmsLaws1660166 STS_035_CTstat_rmsGabor1730173 STS_035_CTstat_rmsSimon. L21340134 STS_038_PETstat_covLoG-505 STS_038_PETstat_covCoif3 LHH L1-5080508 STS_038_PETstat_covSimon. L11980198 STS_038_PETstat_covSimon. L210010 STS_038_PETstat_qcodLoG-505 STS_038_PETstat_qcodCoif3 LHH L1-2400240 STS_038_PETstat_qcodSimon. L1-25025 STS_038_PETstat_qcodSimon. L217017 STS_034_CTstat_meanGabor2180218 STS_034_CTstat_varGabor2.749e+0402.749e+04 STS_034_CTstat_skewLoG606 STS_034_CTstat_skewLaws404 STS_034_CTstat_skewCoif3 HHH L2202 STS_034_CTstat_skewSimon. L1202 STS_034_CTstat_skewSimon. L2404 STS_034_CTstat_kurtLoG56056 STS_034_CTstat_kurtLaws28028 STS_034_CTstat_kurtCoif3 HHH L2909 STS_034_CTstat_kurtSimon. L118018 STS_034_CTstat_kurtSimon. L229029 STS_034_CTstat_medianGabor1780178 STS_034_CTstat_minSimon. L1-1730173 STS_034_CTstat_minSimon. L2-1970197 STS_034_CTstat_p10Laws51051 STS_034_CTstat_p90Gabor4460446 STS_034_CTstat_maxLaws2400240 STS_034_CTstat_maxGabor137501375 STS_034_CTstat_maxCoif3 HHH L21930193 STS_034_CTstat_maxSimon. L13290329 STS_034_CTstat_maxSimon. L23270327 STS_034_CTstat_iqrGabor2140214 STS_034_CTstat_rangeLaws2070207 STS_034_CTstat_rangeGabor137501375 STS_034_CTstat_rangeCoif3 HHH L22110211 STS_034_CTstat_rangeSimon. L15020502 STS_034_CTstat_rangeSimon. L25240524 STS_034_CTstat_madGabor1290129 STS_034_CTstat_rmadGabor89089 STS_034_CTstat_medadGabor1250125 STS_034_CTstat_covLoG48048 STS_034_CTstat_covCoif3 LHH L1-322103221 STS_034_CTstat_covSimon. L148048 STS_034_CTstat_covSimon. L212012 STS_034_CTstat_qcodLoG-404 STS_034_CTstat_qcodGabor101 STS_034_CTstat_qcodCoif3 LHH L1131101311 STS_034_CTstat_qcodSimon. L1-1530153 STS_034_CTstat_qcodSimon. L21410141 STS_034_CTstat_rmsGabor2740274 STS_039_PETstat_skewLoG-202 STS_039_PETstat_skewGabor202 STS_039_PETstat_skewCoif3 HHH L2202 STS_039_PETstat_skewSimon. L2202 STS_039_PETstat_kurtLoG10010 STS_039_PETstat_covLoG-25025 STS_039_PETstat_covCoif3 LHH L1-5260526 STS_039_PETstat_covSimon. L1-82082 STS_039_PETstat_covSimon. L278078 STS_039_PETstat_qcodLoG303 STS_039_PETstat_qcodGabor101 STS_039_PETstat_qcodCoif3 LHH L1-53053 STS_039_PETstat_qcodSimon. L1-10010 STS_039_PETstat_qcodSimon. L2-505 STS_038_MRIstat_skewLaws202 STS_038_MRIstat_skewGabor404 STS_038_MRIstat_skewCoif3 HHH L2505 STS_038_MRIstat_kurtLaws808 STS_038_MRIstat_kurtGabor29029 STS_038_MRIstat_kurtCoif3 LHH L113013 STS_038_MRIstat_kurtCoif3 HHH L266066 STS_038_MRIstat_kurtSimon. L118018 STS_038_MRIstat_minSimon. L1-2770277 STS_038_MRIstat_minSimon. L2-2550255 STS_038_MRIstat_p90Gabor1120112 STS_038_MRIstat_maxLaws2920292 STS_038_MRIstat_maxGabor8990899 STS_038_MRIstat_maxCoif3 HHH L21700170 STS_038_MRIstat_maxSimon. L12230223 STS_038_MRIstat_maxSimon. L23240324 STS_038_MRIstat_iqrGabor44044 STS_038_MRIstat_rangeLaws2780278 STS_038_MRIstat_rangeGabor8990899 STS_038_MRIstat_rangeCoif3 HHH L21760176 STS_038_MRIstat_rangeSimon. L15000500 STS_038_MRIstat_rangeSimon. L25790579 STS_038_MRIstat_madGabor37037 STS_038_MRIstat_madSimon. L220020 STS_038_MRIstat_rmadGabor19019 STS_038_MRIstat_medadGabor34034 STS_038_MRIstat_medadSimon. L220020 STS_038_MRIstat_covLoG303 STS_038_MRIstat_covCoif3 LHH L1-4250425 STS_038_MRIstat_covSimon. L1-63063 STS_038_MRIstat_covSimon. L2-24024 STS_038_MRIstat_qcodLoG101 STS_038_MRIstat_qcodCoif3 LHH L1-7210721 STS_038_MRIstat_qcodCoif3 HHH L2101 STS_038_MRIstat_qcodSimon. L1-1820182 STS_038_MRIstat_qcodSimon. L2-80080 STS_038_MRIstat_rmsGabor87087 STS_036_MRIstat_skewGabor303 STS_036_MRIstat_skewCoif3 HHH L2202 STS_036_MRIstat_kurtGabor16016 STS_036_MRIstat_kurtCoif3 LHH L110010 STS_036_MRIstat_kurtSimon. L110010 STS_036_MRIstat_minSimon. L1-2750275 STS_036_MRIstat_minSimon. L2-2980298 STS_036_MRIstat_p10Simon. L2-50050 STS_036_MRIstat_p90Laws1320132 STS_036_MRIstat_maxLaws2690269 STS_036_MRIstat_maxGabor2950295 STS_036_MRIstat_maxCoif3 HHH L23210321 STS_036_MRIstat_maxSimon. L13140314 STS_036_MRIstat_maxSimon. L23370337 STS_036_MRIstat_iqrLaws67067 STS_036_MRIstat_rangeLaws2590259 STS_036_MRIstat_rangeGabor2950295 STS_036_MRIstat_rangeCoif3 HHH L23720372 STS_036_MRIstat_rangeSimon. L15890589 STS_036_MRIstat_rangeSimon. L26350635 STS_036_MRIstat_madLaws40040 STS_036_MRIstat_madCoif3 HHH L221021 STS_036_MRIstat_madSimon. L229029 STS_036_MRIstat_rmadLaws29029 STS_036_MRIstat_medadLaws37037 STS_036_MRIstat_medadSimon. L229029 STS_036_MRIstat_covLoG707 STS_036_MRIstat_covCoif3 LHH L1126201262 STS_036_MRIstat_covSimon. L1-1.632e+0401.632e+04 STS_036_MRIstat_covSimon. L2-39039 STS_036_MRIstat_qcodLoG202 STS_036_MRIstat_qcodLaws101 STS_036_MRIstat_qcodGabor101 STS_036_MRIstat_qcodCoif3 LHH L16320632 STS_036_MRIstat_qcodCoif3 HHH L2101 STS_036_MRIstat_qcodSimon. L14730473 STS_036_MRIstat_qcodSimon. L244044 STS_036_MRIstat_rmsLaws74074 STS_040_PETstat_skewLoG-303 STS_040_PETstat_skewGabor303 STS_040_PETstat_skewCoif3 HHH L2202 STS_040_PETstat_skewSimon. L1202 STS_040_PETstat_skewSimon. L2202 STS_040_PETstat_kurtLoG18018 STS_040_PETstat_kurtGabor13013 STS_040_PETstat_kurtSimon. L114014 STS_040_PETstat_covLoG-23023 STS_040_PETstat_covCoif3 LHH L1111101111 STS_040_PETstat_covSimon. L1-89089 STS_040_PETstat_covSimon. L224024 STS_040_PETstat_qcodLoG303 STS_040_PETstat_qcodGabor101 STS_040_PETstat_qcodCoif3 LHH L1-1100110 STS_040_PETstat_qcodCoif3 HHH L2101 STS_040_PETstat_qcodSimon. L1-909 STS_040_PETstat_qcodSimon. L2-10010 STS_036_CTstat_meanGabor1320132 STS_036_CTstat_varGabor675606756 STS_036_CTstat_skewLoG-505 STS_036_CTstat_skewLaws505 STS_036_CTstat_skewCoif3 HHH L2404 STS_036_CTstat_skewSimon. L1202 STS_036_CTstat_skewSimon. L2303 STS_036_CTstat_kurtLoG25025 STS_036_CTstat_kurtLaws36036 STS_036_CTstat_kurtCoif3 HHH L236036 STS_036_CTstat_kurtSimon. L117017 STS_036_CTstat_kurtSimon. L228028 STS_036_CTstat_medianGabor1150115 STS_036_CTstat_minSimon. L1-99099 STS_036_CTstat_minSimon. L2-1260126 STS_036_CTstat_p90Gabor2440244 STS_036_CTstat_maxLaws2730273 STS_036_CTstat_maxGabor7080708 STS_036_CTstat_maxCoif3 HHH L22160216 STS_036_CTstat_maxSimon. L12110211 STS_036_CTstat_maxSimon. L22870287 STS_036_CTstat_iqrGabor1080108 STS_036_CTstat_rangeLaws2520252 STS_036_CTstat_rangeGabor7080708 STS_036_CTstat_rangeCoif3 HHH L22320232 STS_036_CTstat_rangeSimon. L13100310 STS_036_CTstat_rangeSimon. L24140414 STS_036_CTstat_madGabor65065 STS_036_CTstat_rmadGabor45045 STS_036_CTstat_medadGabor64064 STS_036_CTstat_covLoG-303 STS_036_CTstat_covCoif3 LHH L1511005110 STS_036_CTstat_covSimon. L160060 STS_036_CTstat_covSimon. L236036 STS_036_CTstat_qcodLoG-101 STS_036_CTstat_qcodCoif3 LHH L17390739 STS_036_CTstat_qcodSimon. L12780278 STS_036_CTstat_qcodSimon. L243043 STS_036_CTstat_rmsGabor1550155 STS_038_CTstat_meanGabor1470147 STS_038_CTstat_varGabor654606546 STS_038_CTstat_skewLoG10010 STS_038_CTstat_skewLaws707 STS_038_CTstat_skewSimon. L1202 STS_038_CTstat_skewSimon. L2303 STS_038_CTstat_kurtLoG1380138 STS_038_CTstat_kurtLaws1040104 STS_038_CTstat_kurtSimon. L133033 STS_038_CTstat_kurtSimon. L258058 STS_038_CTstat_medianGabor1360136 STS_038_CTstat_minSimon. L1-1670167 STS_038_CTstat_minSimon. L2-3230323 STS_038_CTstat_p10Gabor52052 STS_038_CTstat_p90Gabor2580258 STS_038_CTstat_maxLaws2860286 STS_038_CTstat_maxGabor6870687 STS_038_CTstat_maxSimon. L13010301 STS_038_CTstat_maxSimon. L23180318 STS_038_CTstat_iqrGabor1090109 STS_038_CTstat_rangeLaws2660266 STS_038_CTstat_rangeGabor6870687 STS_038_CTstat_rangeSimon. L14680468 STS_038_CTstat_rangeSimon. L26400640 STS_038_CTstat_madGabor65065 STS_038_CTstat_rmadGabor45045 STS_038_CTstat_medadGabor64064 STS_038_CTstat_covLoG27027 STS_038_CTstat_covCoif3 LHH L11.289e+0401.289e+04 STS_038_CTstat_covSimon. L11070107 STS_038_CTstat_covSimon. L281081 STS_038_CTstat_qcodLoG-12012 STS_038_CTstat_qcodCoif3 LHH L13810381 STS_038_CTstat_qcodSimon. L11.217e+0501.217e+05 STS_038_CTstat_qcodSimon. L2-21021 STS_038_CTstat_rmsGabor1680168 STS_041_PETstat_skewSimon. L2202 STS_041_PETstat_covCoif3 LHH L12570257 STS_041_PETstat_covSimon. L113013 STS_041_PETstat_covSimon. L2303 STS_041_PETstat_qcodLoG-101 STS_041_PETstat_qcodCoif3 LHH L1-133901339 STS_041_PETstat_qcodSimon. L110010 STS_041_PETstat_qcodSimon. L2202 STS_040_MRIstat_meanGabor95095 STS_040_MRIstat_varGabor843608436 STS_040_MRIstat_skewLaws202 STS_040_MRIstat_skewGabor303 STS_040_MRIstat_skewCoif3 HHH L2202 STS_040_MRIstat_kurtGabor15015 STS_040_MRIstat_kurtCoif3 HHH L2909 STS_040_MRIstat_minSimon. L1-2260226 STS_040_MRIstat_minSimon. L2-2670267 STS_040_MRIstat_p90Laws1310131 STS_040_MRIstat_p90Gabor1940194 STS_040_MRIstat_maxLaws3560356 STS_040_MRIstat_maxGabor125601256 STS_040_MRIstat_maxCoif3 HHH L22970297 STS_040_MRIstat_maxSimon. L12340234 STS_040_MRIstat_maxSimon. L22330233 STS_040_MRIstat_iqrLaws53053 STS_040_MRIstat_iqrGabor76076 STS_040_MRIstat_rangeLaws3370337 STS_040_MRIstat_rangeGabor125601256 STS_040_MRIstat_rangeCoif3 HHH L23170317 STS_040_MRIstat_rangeSimon. L14600460 STS_040_MRIstat_rangeSimon. L24990499 STS_040_MRIstat_madLaws33033 STS_040_MRIstat_madGabor60060 STS_040_MRIstat_madSimon. L227027 STS_040_MRIstat_rmadLaws22022 STS_040_MRIstat_rmadGabor34034 STS_040_MRIstat_medadLaws31031 STS_040_MRIstat_medadGabor56056 STS_040_MRIstat_medadSimon. L227027 STS_040_MRIstat_covLoG404 STS_040_MRIstat_covCoif3 LHH L14590459 STS_040_MRIstat_covSimon. L12480248 STS_040_MRIstat_covSimon. L2-41041 STS_040_MRIstat_qcodLoG202 STS_040_MRIstat_qcodCoif3 LHH L1156401564 STS_040_MRIstat_qcodCoif3 HHH L2101 STS_040_MRIstat_qcodSimon. L13470347 STS_040_MRIstat_qcodSimon. L2-25025 STS_040_MRIstat_rmsLaws84084 STS_040_MRIstat_rmsGabor1320132 STS_039_MRIstat_meanGabor1170117 STS_039_MRIstat_varGabor1.511e+0401.511e+04 STS_039_MRIstat_skewLaws303 STS_039_MRIstat_skewGabor505 STS_039_MRIstat_skewCoif3 HHH L2404 STS_039_MRIstat_kurtLaws10010 STS_039_MRIstat_kurtGabor46046 STS_039_MRIstat_kurtCoif3 HHH L237037 STS_039_MRIstat_kurtSimon. L113013 STS_039_MRIstat_medianGabor85085 STS_039_MRIstat_minSimon. L1-3300330 STS_039_MRIstat_minSimon. L2-2940294 STS_039_MRIstat_p90Laws1260126 STS_039_MRIstat_p90Gabor2260226 STS_039_MRIstat_maxLaws4490449 STS_039_MRIstat_maxGabor279802798 STS_039_MRIstat_maxCoif3 HHH L21980198 STS_039_MRIstat_maxSimon. L13040304 STS_039_MRIstat_maxSimon. L22960296 STS_039_MRIstat_iqrGabor87087 STS_039_MRIstat_rangeLaws4330433 STS_039_MRIstat_rangeGabor279802798 STS_039_MRIstat_rangeCoif3 HHH L22050205 STS_039_MRIstat_rangeSimon. L16340634 STS_039_MRIstat_rangeSimon. L25890589 STS_039_MRIstat_madLaws32032 STS_039_MRIstat_madGabor72072 STS_039_MRIstat_madSimon. L227027 STS_039_MRIstat_rmadLaws17017 STS_039_MRIstat_rmadGabor38038 STS_039_MRIstat_medadLaws29029 STS_039_MRIstat_medadGabor67067 STS_039_MRIstat_medadSimon. L227027 STS_039_MRIstat_covLoG-505 STS_039_MRIstat_covCoif3 LHH L1193901939 STS_039_MRIstat_covSimon. L1-5520552 STS_039_MRIstat_covSimon. L250050 STS_039_MRIstat_qcodLoG-303 STS_039_MRIstat_qcodCoif3 LHH L1-2420242 STS_039_MRIstat_qcodSimon. L1-69069 STS_039_MRIstat_qcodSimon. L24680468 STS_039_MRIstat_rmsLaws86086 STS_039_MRIstat_rmsGabor1690169 STS_037_MRIstat_varGabor923009230 STS_037_MRIstat_skewLaws303 STS_037_MRIstat_skewGabor909 STS_037_MRIstat_skewCoif3 HHH L2404 STS_037_MRIstat_skewSimon. L2-202 STS_037_MRIstat_kurtLaws12012 STS_037_MRIstat_kurtGabor1100110 STS_037_MRIstat_kurtCoif3 LHH L130030 STS_037_MRIstat_kurtCoif3 HHH L230030 STS_037_MRIstat_kurtSimon. L119019 STS_037_MRIstat_kurtSimon. L211011 STS_037_MRIstat_minSimon. L1-2280228 STS_037_MRIstat_minSimon. L2-3220322 STS_037_MRIstat_p90Gabor1170117 STS_037_MRIstat_maxLaws3590359 STS_037_MRIstat_maxGabor241002410 STS_037_MRIstat_maxCoif3 HHH L21960196 STS_037_MRIstat_maxSimon. L12360236 STS_037_MRIstat_maxSimon. L22230223 STS_037_MRIstat_iqrGabor46046 STS_037_MRIstat_rangeLaws3460346 STS_037_MRIstat_rangeGabor241002410 STS_037_MRIstat_rangeCoif3 HHH L22080208 STS_037_MRIstat_rangeSimon. L14650465 STS_037_MRIstat_rangeSimon. L25450545 STS_037_MRIstat_madLaws23023 STS_037_MRIstat_madGabor43043 STS_037_MRIstat_madSimon. L221021 STS_037_MRIstat_rmadGabor20020 STS_037_MRIstat_medadLaws20020 STS_037_MRIstat_medadGabor39039 STS_037_MRIstat_medadSimon. L221021 STS_037_MRIstat_covLoG303 STS_037_MRIstat_covCoif3 LHH L1-353603536 STS_037_MRIstat_covSimon. L198098 STS_037_MRIstat_covSimon. L2-22022 STS_037_MRIstat_qcodLoG202 STS_037_MRIstat_qcodCoif3 LHH L15440544 STS_037_MRIstat_qcodSimon. L11000100 STS_037_MRIstat_qcodSimon. L21770177 STS_037_MRIstat_rmsGabor1170117 STS_042_PETstat_covCoif3 LHH L1-712007120 STS_042_PETstat_covSimon. L1505 STS_042_PETstat_qcodLoG-101 STS_042_PETstat_qcodCoif3 LHH L1-4320432 STS_042_PETstat_qcodSimon. L1303 STS_042_PETstat_qcodSimon. L2101 STS_041_CTstat_meanGabor1620162 STS_041_CTstat_varGabor939309393 STS_041_CTstat_medianGabor1430143 STS_041_CTstat_minSimon. L1-93093 STS_041_CTstat_minSimon. L2-77077 STS_041_CTstat_p10Gabor54054 STS_041_CTstat_p90Gabor2970297 STS_041_CTstat_maxGabor7730773 STS_041_CTstat_iqrGabor1230123 STS_041_CTstat_rangeGabor7720772 STS_041_CTstat_rangeSimon. L11720172 STS_041_CTstat_rangeSimon. L21490149 STS_041_CTstat_madGabor76076 STS_041_CTstat_rmadGabor52052 STS_041_CTstat_medadGabor74074 STS_041_CTstat_covCoif3 LHH L1-7540754 STS_041_CTstat_covSimon. L11050105 STS_041_CTstat_covSimon. L2909 STS_041_CTstat_qcodLoG-101 STS_041_CTstat_qcodCoif3 LHH L1138701387 STS_041_CTstat_qcodSimon. L162062 STS_041_CTstat_qcodSimon. L2707 STS_041_CTstat_rmsGabor1880188 STS_043_PETstat_covLoG-505 STS_043_PETstat_covCoif3 LHH L13530353 STS_043_PETstat_covSimon. L158058 STS_043_PETstat_covSimon. L2505 STS_043_PETstat_qcodLoG-404 STS_043_PETstat_qcodCoif3 LHH L196096 STS_043_PETstat_qcodSimon. L1-43043 STS_043_PETstat_qcodSimon. L2404 STS_041_MRIstat_meanLaws89089 STS_041_MRIstat_meanGabor90090 STS_041_MRIstat_varGabor702607026 STS_041_MRIstat_skewGabor404 STS_041_MRIstat_skewCoif3 HHH L2202 STS_041_MRIstat_kurtGabor25025 STS_041_MRIstat_medianLaws82082 STS_041_MRIstat_minSimon. L1-2450245 STS_041_MRIstat_minSimon. L2-3230323 STS_041_MRIstat_p10Simon. L2-1000100 STS_041_MRIstat_p90Laws1570157 STS_041_MRIstat_p90Gabor1680168 STS_041_MRIstat_maxLaws3100310 STS_041_MRIstat_maxGabor114601146 STS_041_MRIstat_maxSimon. L12130213 STS_041_MRIstat_maxSimon. L22200220 STS_041_MRIstat_iqrLaws77077 STS_041_MRIstat_iqrGabor68068 STS_041_MRIstat_iqrSimon. L268068 STS_041_MRIstat_rangeLaws2840284 STS_041_MRIstat_rangeGabor114601146 STS_041_MRIstat_rangeCoif3 HHH L21370137 STS_041_MRIstat_rangeSimon. L14590459 STS_041_MRIstat_rangeSimon. L25430543 STS_041_MRIstat_madLaws41041 STS_041_MRIstat_madGabor52052 STS_041_MRIstat_madSimon. L122022 STS_041_MRIstat_madSimon. L245045 STS_041_MRIstat_rmadLaws31031 STS_041_MRIstat_rmadGabor29029 STS_041_MRIstat_rmadSimon. L230030 STS_041_MRIstat_medadLaws40040 STS_041_MRIstat_medadGabor49049 STS_041_MRIstat_medadSimon. L121021 STS_041_MRIstat_medadSimon. L244044 STS_041_MRIstat_covCoif3 LHH L17800780 STS_041_MRIstat_covSimon. L1-10010 STS_041_MRIstat_covSimon. L2-404 STS_041_MRIstat_qcodLoG101 STS_041_MRIstat_qcodCoif3 LHH L1-7350735 STS_041_MRIstat_qcodCoif3 HHH L2101 STS_041_MRIstat_qcodSimon. L1-31031 STS_041_MRIstat_qcodSimon. L2-404 STS_041_MRIstat_rmsLaws1020102 STS_041_MRIstat_rmsGabor1230123 STS_042_MRIstat_meanLaws76076 STS_042_MRIstat_meanGabor84084 STS_042_MRIstat_varGabor921509215 STS_042_MRIstat_skewGabor303 STS_042_MRIstat_skewCoif3 HHH L2202 STS_042_MRIstat_kurtGabor808 STS_042_MRIstat_kurtCoif3 HHH L2909 STS_042_MRIstat_minSimon. L1-1480148 STS_042_MRIstat_minSimon. L2-2500250 STS_042_MRIstat_p10Simon. L2-71071 STS_042_MRIstat_p90Laws1550155 STS_042_MRIstat_p90Gabor2030203 STS_042_MRIstat_maxLaws3420342 STS_042_MRIstat_maxGabor7810781 STS_042_MRIstat_maxSimon. L11900190 STS_042_MRIstat_maxSimon. L23180318 STS_042_MRIstat_iqrLaws84084 STS_042_MRIstat_iqrGabor76076 STS_042_MRIstat_iqrSimon. L266066 STS_042_MRIstat_rangeLaws3330333 STS_042_MRIstat_rangeGabor7810781 STS_042_MRIstat_rangeSimon. L13380338 STS_042_MRIstat_rangeSimon. L25680568 STS_042_MRIstat_madLaws46046 STS_042_MRIstat_madGabor65065 STS_042_MRIstat_madSimon. L244044 STS_042_MRIstat_rmadLaws34034 STS_042_MRIstat_rmadGabor35035 STS_042_MRIstat_rmadSimon. L228028 STS_042_MRIstat_medadLaws45045 STS_042_MRIstat_medadGabor58058 STS_042_MRIstat_medadSimon. L244044 STS_042_MRIstat_covCoif3 LHH L1-148401484 STS_042_MRIstat_covSimon. L117017 STS_042_MRIstat_covSimon. L2-2310231 STS_042_MRIstat_qcodLoG101 STS_042_MRIstat_qcodLaws101 STS_042_MRIstat_qcodGabor101 STS_042_MRIstat_qcodCoif3 LHH L1-1120112 STS_042_MRIstat_qcodCoif3 HHH L2101 STS_042_MRIstat_qcodSimon. L110010 STS_042_MRIstat_qcodSimon. L210010 STS_042_MRIstat_rmsLaws94094 STS_042_MRIstat_rmsGabor1280128 STS_037_CTstat_meanGabor1780178 STS_037_CTstat_varGabor1.184e+0401.184e+04 STS_037_CTstat_skewLoG202 STS_037_CTstat_skewLaws202 STS_037_CTstat_skewCoif3 HHH L2404 STS_037_CTstat_skewSimon. L2-202 STS_037_CTstat_kurtLoG15015 STS_037_CTstat_kurtLaws808 STS_037_CTstat_kurtCoif3 HHH L236036 STS_037_CTstat_kurtSimon. L214014 STS_037_CTstat_medianGabor1590159 STS_037_CTstat_minSimon. L1-1950195 STS_037_CTstat_minSimon. L2-2920292 STS_037_CTstat_p10Gabor56056 STS_037_CTstat_p90Gabor3270327 STS_037_CTstat_maxLaws2280228 STS_037_CTstat_maxGabor142701427 STS_037_CTstat_maxCoif3 HHH L23780378 STS_037_CTstat_maxSimon. L21490149 STS_037_CTstat_iqrGabor1440144 STS_037_CTstat_rangeLaws2090209 STS_037_CTstat_rangeGabor142701427 STS_037_CTstat_rangeCoif3 HHH L24230423 STS_037_CTstat_rangeSimon. L13070307 STS_037_CTstat_rangeSimon. L24400440 STS_037_CTstat_madGabor86086 STS_037_CTstat_rmadGabor60060 STS_037_CTstat_medadGabor85085 STS_037_CTstat_covLoG11011 STS_037_CTstat_covCoif3 LHH L1-416704167 STS_037_CTstat_covSimon. L1-84084 STS_037_CTstat_covSimon. L2-23023 STS_037_CTstat_qcodLoG-60060 STS_037_CTstat_qcodCoif3 LHH L1-8560856 STS_037_CTstat_qcodSimon. L1-1540154 STS_037_CTstat_qcodSimon. L2-2320232 STS_037_CTstat_rmsGabor2090209 STS_039_CTstat_meanGabor1260126 STS_039_CTstat_varGabor851108511 STS_039_CTstat_skewLoG707 STS_039_CTstat_skewGabor202 STS_039_CTstat_kurtLoG1180118 STS_039_CTstat_kurtSimon. L210010 STS_039_CTstat_medianGabor1030103 STS_039_CTstat_minSimon. L1-80080 STS_039_CTstat_minSimon. L2-62062 STS_039_CTstat_p90Gabor2510251 STS_039_CTstat_maxLaws1460146 STS_039_CTstat_maxGabor8880888 STS_039_CTstat_iqrGabor1070107 STS_039_CTstat_rangeLaws1280128 STS_039_CTstat_rangeGabor8880888 STS_039_CTstat_rangeSimon. L11590159 STS_039_CTstat_rangeSimon. L21750175 STS_039_CTstat_madGabor70070 STS_039_CTstat_rmadGabor46046 STS_039_CTstat_medadGabor67067 STS_039_CTstat_covLoG16016 STS_039_CTstat_covCoif3 LHH L1-767307673 STS_039_CTstat_covSimon. L1-6700670 STS_039_CTstat_covSimon. L21890189 STS_039_CTstat_qcodLoG11011 STS_039_CTstat_qcodCoif3 LHH L1-570105701 STS_039_CTstat_qcodSimon. L1-5580558 STS_039_CTstat_qcodSimon. L21620162 STS_039_CTstat_rmsGabor1560156 STS_040_CTstat_meanGabor1820182 STS_040_CTstat_varGabor1.226e+0401.226e+04 STS_040_CTstat_skewLaws707 STS_040_CTstat_skewCoif3 HHH L2707 STS_040_CTstat_skewSimon. L2-202 STS_040_CTstat_kurtLoG808 STS_040_CTstat_kurtLaws73073 STS_040_CTstat_kurtCoif3 HHH L291091 STS_040_CTstat_kurtSimon. L219019 STS_040_CTstat_medianGabor1610161 STS_040_CTstat_minSimon. L1-1880188 STS_040_CTstat_minSimon. L2-2310231 STS_040_CTstat_p10Gabor59059 STS_040_CTstat_p90Gabor3340334 STS_040_CTstat_maxLaws2670267 STS_040_CTstat_maxGabor8460846 STS_040_CTstat_maxCoif3 HHH L23260326 STS_040_CTstat_iqrGabor1450145 STS_040_CTstat_rangeLaws2290229 STS_040_CTstat_rangeGabor8460846 STS_040_CTstat_rangeCoif3 HHH L23480348 STS_040_CTstat_rangeSimon. L12970297 STS_040_CTstat_rangeSimon. L23310331 STS_040_CTstat_madGabor87087 STS_040_CTstat_rmadGabor60060 STS_040_CTstat_medadGabor86086 STS_040_CTstat_covLoG-707 STS_040_CTstat_covCoif3 LHH L1-144501445 STS_040_CTstat_covSimon. L1-1040104 STS_040_CTstat_covSimon. L2-28028 STS_040_CTstat_qcodLoG-202 STS_040_CTstat_qcodCoif3 LHH L1-7080708 STS_040_CTstat_qcodSimon. L1-2960296 STS_040_CTstat_qcodSimon. L2-88088 STS_040_CTstat_rmsGabor2130213 STS_044_PETstat_covLoG-808 STS_044_PETstat_covCoif3 LHH L14030403 STS_044_PETstat_covSimon. L197097 STS_044_PETstat_covSimon. L213013 STS_044_PETstat_qcodLoG-707 STS_044_PETstat_qcodCoif3 LHH L12880288 STS_044_PETstat_qcodSimon. L1-37037 STS_044_PETstat_qcodSimon. L211011 STS_045_PETstat_skewLoG-202 STS_045_PETstat_skewGabor303 STS_045_PETstat_skewCoif3 HHH L2202 STS_045_PETstat_kurtLoG12012 STS_045_PETstat_kurtGabor17017 STS_045_PETstat_kurtSimon. L1808 STS_045_PETstat_covLoG-11011 STS_045_PETstat_covCoif3 LHH L1-178001780 STS_045_PETstat_covSimon. L16480648 STS_045_PETstat_covSimon. L214014 STS_045_PETstat_qcodLoG-2640264 STS_045_PETstat_qcodGabor101 STS_045_PETstat_qcodCoif3 LHH L1172501725 STS_045_PETstat_qcodSimon. L1-11011 STS_045_PETstat_qcodSimon. L233033 STS_042_CTstat_meanGabor83083 STS_042_CTstat_skewLoG202 STS_042_CTstat_skewLaws505 STS_042_CTstat_skewCoif3 HHH L2404 STS_042_CTstat_kurtLoG909 STS_042_CTstat_kurtLaws34034 STS_042_CTstat_kurtCoif3 HHH L232032 STS_042_CTstat_medianGabor76076 STS_042_CTstat_minSimon. L1-83083 STS_042_CTstat_minSimon. L2-1890189 STS_042_CTstat_p90Gabor1430143 STS_042_CTstat_maxLaws2380238 STS_042_CTstat_maxGabor3240324 STS_042_CTstat_iqrGabor60060 STS_042_CTstat_rangeLaws2240224 STS_042_CTstat_rangeGabor3230323 STS_042_CTstat_rangeSimon. L11850185 STS_042_CTstat_rangeSimon. L22740274 STS_042_CTstat_madGabor36036 STS_042_CTstat_rmadGabor25025 STS_042_CTstat_medadGabor36036 STS_042_CTstat_covLoG-303 STS_042_CTstat_covCoif3 LHH L1-7400740 STS_042_CTstat_covSimon. L1-6810681 STS_042_CTstat_covSimon. L211011 STS_042_CTstat_qcodLoG-101 STS_042_CTstat_qcodCoif3 LHH L1-1350135 STS_042_CTstat_qcodSimon. L1-36036 STS_042_CTstat_qcodSimon. L219019 STS_042_CTstat_rmsGabor95095 STS_046_PETstat_skewGabor202 STS_046_PETstat_covLoG-505 STS_046_PETstat_covCoif3 LHH L1203902039 STS_046_PETstat_covSimon. L1151101511 STS_046_PETstat_covSimon. L210010 STS_046_PETstat_qcodLoG-404 STS_046_PETstat_qcodCoif3 LHH L11850185 STS_046_PETstat_qcodSimon. L1-22022 STS_046_PETstat_qcodSimon. L2505 STS_043_MRIstat_meanGabor73073 STS_043_MRIstat_skewLoG303 STS_043_MRIstat_skewLaws303 STS_043_MRIstat_skewGabor202 STS_043_MRIstat_skewCoif3 HHH L2303 STS_043_MRIstat_kurtLoG12012 STS_043_MRIstat_kurtLaws11011 STS_043_MRIstat_kurtGabor10010 STS_043_MRIstat_kurtCoif3 HHH L217017 STS_043_MRIstat_kurtSimon. L113013 STS_043_MRIstat_kurtSimon. L2808 STS_043_MRIstat_minSimon. L1-1780178 STS_043_MRIstat_minSimon. L2-1510151 STS_043_MRIstat_p90Gabor1280128 STS_043_MRIstat_maxLaws3400340 STS_043_MRIstat_maxGabor6890689 STS_043_MRIstat_maxSimon. L12120212 STS_043_MRIstat_maxSimon. L22330233 STS_043_MRIstat_iqrGabor53053 STS_043_MRIstat_rangeLaws3170317 STS_043_MRIstat_rangeGabor6890689 STS_043_MRIstat_rangeCoif3 HHH L21260126 STS_043_MRIstat_rangeSimon. L13900390 STS_043_MRIstat_rangeSimon. L23830383 STS_043_MRIstat_madGabor34034 STS_043_MRIstat_rmadGabor22022 STS_043_MRIstat_medadGabor33033 STS_043_MRIstat_covLoG303 STS_043_MRIstat_covCoif3 LHH L1-1.382e+0501.382e+05 STS_043_MRIstat_covSimon. L1-71071 STS_043_MRIstat_covSimon. L2-27027 STS_043_MRIstat_qcodLoG101 STS_043_MRIstat_qcodCoif3 LHH L15870587 STS_043_MRIstat_qcodSimon. L1-92092 STS_043_MRIstat_qcodSimon. L2-16016 STS_043_MRIstat_rmsGabor87087 STS_043_CTstat_meanGabor1350135 STS_043_CTstat_varGabor607206072 STS_043_CTstat_medianGabor1230123 STS_043_CTstat_minSimon. L1-65065 STS_043_CTstat_p90Gabor2390239 STS_043_CTstat_maxGabor8470847 STS_043_CTstat_iqrGabor1020102 STS_043_CTstat_rangeGabor8470847 STS_043_CTstat_madGabor61061 STS_043_CTstat_rmadGabor42042 STS_043_CTstat_medadGabor60060 STS_043_CTstat_covLoG404 STS_043_CTstat_covCoif3 LHH L1-134101341 STS_043_CTstat_covSimon. L12490249 STS_043_CTstat_covSimon. L2-30030 STS_043_CTstat_qcodLoG202 STS_043_CTstat_qcodCoif3 LHH L13290329 STS_043_CTstat_qcodSimon. L12340234 STS_043_CTstat_qcodSimon. L2-1330133 STS_043_CTstat_rmsGabor1560156 STS_047_PETstat_covCoif3 LHH L11.046e+0401.046e+04 STS_047_PETstat_covSimon. L118018 STS_047_PETstat_covSimon. L2303 STS_047_PETstat_qcodLoG-202 STS_047_PETstat_qcodCoif3 LHH L1-2370237 STS_047_PETstat_qcodSimon. L117017 STS_047_PETstat_qcodSimon. L2202 STS_044_MRIstat_skewGabor202 STS_044_MRIstat_kurtGabor808 STS_044_MRIstat_minSimon. L1-1840184 STS_044_MRIstat_minSimon. L2-2120212 STS_044_MRIstat_maxLaws2090209 STS_044_MRIstat_maxGabor2090209 STS_044_MRIstat_maxSimon. L11800180 STS_044_MRIstat_maxSimon. L23090309 STS_044_MRIstat_iqrSimon. L243043 STS_044_MRIstat_rangeLaws1970197 STS_044_MRIstat_rangeGabor2090209 STS_044_MRIstat_rangeSimon. L13650365 STS_044_MRIstat_rangeSimon. L25220522 STS_044_MRIstat_madSimon. L228028 STS_044_MRIstat_rmadSimon. L218018 STS_044_MRIstat_medadSimon. L228028 STS_044_MRIstat_covLoG11011 STS_044_MRIstat_covCoif3 LHH L1186301863 STS_044_MRIstat_covSimon. L1-73073 STS_044_MRIstat_covSimon. L2-40040 STS_044_MRIstat_qcodLoG505 STS_044_MRIstat_qcodCoif3 LHH L1-449804498 STS_044_MRIstat_qcodCoif3 HHH L2101 STS_044_MRIstat_qcodSimon. L1-65065 STS_044_MRIstat_qcodSimon. L2-15015 STS_045_MRIstat_meanGabor1060106 STS_045_MRIstat_varGabor963309633 STS_045_MRIstat_skewLaws202 STS_045_MRIstat_skewGabor303 STS_045_MRIstat_skewCoif3 HHH L2303 STS_045_MRIstat_kurtLoG808 STS_045_MRIstat_kurtGabor15015 STS_045_MRIstat_kurtCoif3 HHH L217017 STS_045_MRIstat_kurtSimon. L111011 STS_045_MRIstat_medianGabor78078 STS_045_MRIstat_minSimon. L1-2600260 STS_045_MRIstat_minSimon. L2-2610261 STS_045_MRIstat_p90Laws1250125 STS_045_MRIstat_p90Gabor2150215 STS_045_MRIstat_maxLaws3720372 STS_045_MRIstat_maxGabor129701297 STS_045_MRIstat_maxCoif3 HHH L21710171 STS_045_MRIstat_maxSimon. L13740374 STS_045_MRIstat_maxSimon. L23000300 STS_045_MRIstat_iqrLaws49049 STS_045_MRIstat_iqrGabor81081 STS_045_MRIstat_rangeLaws3520352 STS_045_MRIstat_rangeGabor129701297 STS_045_MRIstat_rangeCoif3 HHH L21860186 STS_045_MRIstat_rangeSimon. L16340634 STS_045_MRIstat_rangeSimon. L25610561 STS_045_MRIstat_madLaws33033 STS_045_MRIstat_madGabor65065 STS_045_MRIstat_rmadLaws21021 STS_045_MRIstat_rmadGabor36036 STS_045_MRIstat_medadLaws30030 STS_045_MRIstat_medadGabor60060 STS_045_MRIstat_covLoG-6420642 STS_045_MRIstat_covCoif3 LHH L1409904099 STS_045_MRIstat_covSimon. L1-2390239 STS_045_MRIstat_covSimon. L2-46046 STS_045_MRIstat_qcodLoG606 STS_045_MRIstat_qcodCoif3 LHH L1-9530953 STS_045_MRIstat_qcodSimon. L1-80080 STS_045_MRIstat_qcodSimon. L2-41041 STS_045_MRIstat_rmsLaws77077 STS_045_MRIstat_rmsGabor1440144 STS_048_PETstat_covLoG-404 STS_048_PETstat_covCoif3 LHH L1-8060806 STS_048_PETstat_covSimon. L151051 STS_048_PETstat_covSimon. L2505 STS_048_PETstat_qcodLoG-404 STS_048_PETstat_qcodCoif3 LHH L1-7370737 STS_048_PETstat_qcodSimon. L1-33033 STS_048_PETstat_qcodSimon. L2404 STS_044_CTstat_meanGabor1720172 STS_044_CTstat_varGabor1.032e+0401.032e+04 STS_044_CTstat_skewLaws202 STS_044_CTstat_skewCoif3 HHH L2303 STS_044_CTstat_kurtLoG17017 STS_044_CTstat_kurtLaws19019 STS_044_CTstat_kurtCoif3 HHH L229029 STS_044_CTstat_kurtSimon. L212012 STS_044_CTstat_medianGabor1550155 STS_044_CTstat_minSimon. L1-1770177 STS_044_CTstat_minSimon. L2-1690169 STS_044_CTstat_p10Gabor57057 STS_044_CTstat_p90Gabor3090309 STS_044_CTstat_maxLaws1830183 STS_044_CTstat_maxGabor8310831 STS_044_CTstat_maxCoif3 HHH L23080308 STS_044_CTstat_maxSimon. L21720172 STS_044_CTstat_iqrGabor1320132 STS_044_CTstat_rangeLaws1550155 STS_044_CTstat_rangeGabor8300830 STS_044_CTstat_rangeCoif3 HHH L23250325 STS_044_CTstat_rangeSimon. L12880288 STS_044_CTstat_rangeSimon. L23410341 STS_044_CTstat_madGabor80080 STS_044_CTstat_rmadGabor55055 STS_044_CTstat_medadGabor78078 STS_044_CTstat_covLoG505 STS_044_CTstat_covCoif3 LHH L1904509045 STS_044_CTstat_covSimon. L1-1040104 STS_044_CTstat_covSimon. L2-36036 STS_044_CTstat_qcodLoG404 STS_044_CTstat_qcodCoif3 LHH L15930593 STS_044_CTstat_qcodSimon. L1-3680368 STS_044_CTstat_qcodSimon. L2-3250325 STS_044_CTstat_rmsGabor2000200 STS_047_MRIstat_meanLaws79079 STS_047_MRIstat_meanGabor1340134 STS_047_MRIstat_varGabor2.385e+0402.385e+04 STS_047_MRIstat_skewGabor404 STS_047_MRIstat_skewCoif3 HHH L2303 STS_047_MRIstat_kurtGabor29029 STS_047_MRIstat_kurtCoif3 HHH L211011 STS_047_MRIstat_kurtSimon. L1808 STS_047_MRIstat_medianGabor86086 STS_047_MRIstat_minSimon. L1-2410241 STS_047_MRIstat_minSimon. L2-3210321 STS_047_MRIstat_p10Simon. L2-63063 STS_047_MRIstat_p90Laws1580158 STS_047_MRIstat_p90Gabor2900290 STS_047_MRIstat_maxLaws3810381 STS_047_MRIstat_maxGabor273102731 STS_047_MRIstat_maxCoif3 HHH L23080308 STS_047_MRIstat_maxSimon. L12890289 STS_047_MRIstat_maxSimon. L24030403 STS_047_MRIstat_iqrLaws75075 STS_047_MRIstat_iqrGabor1100110 STS_047_MRIstat_iqrSimon. L242042 STS_047_MRIstat_rangeLaws3580358 STS_047_MRIstat_rangeGabor273102731 STS_047_MRIstat_rangeCoif3 HHH L23400340 STS_047_MRIstat_rangeSimon. L15300530 STS_047_MRIstat_rangeSimon. L27240724 STS_047_MRIstat_madLaws44044 STS_047_MRIstat_madGabor96096 STS_047_MRIstat_madSimon. L234034 STS_047_MRIstat_rmadLaws32032 STS_047_MRIstat_rmadGabor50050 STS_047_MRIstat_rmadSimon. L219019 STS_047_MRIstat_medadLaws42042 STS_047_MRIstat_medadGabor85085 STS_047_MRIstat_medadSimon. L234034 STS_047_MRIstat_covLoG303 STS_047_MRIstat_covCoif3 LHH L12.372e+0402.372e+04 STS_047_MRIstat_covSimon. L1-25025 STS_047_MRIstat_covSimon. L2-16016 STS_047_MRIstat_qcodLoG101 STS_047_MRIstat_qcodLaws101 STS_047_MRIstat_qcodGabor101 STS_047_MRIstat_qcodCoif3 LHH L1-2520252 STS_047_MRIstat_qcodCoif3 HHH L2101 STS_047_MRIstat_qcodSimon. L1-51051 STS_047_MRIstat_qcodSimon. L2-12012 STS_047_MRIstat_rmsLaws96096 STS_047_MRIstat_rmsGabor2040204 STS_046_MRIstat_skewLaws202 STS_046_MRIstat_skewGabor303 STS_046_MRIstat_skewCoif3 HHH L2404 STS_046_MRIstat_kurtGabor13013 STS_046_MRIstat_kurtCoif3 LHH L111011 STS_046_MRIstat_kurtCoif3 HHH L222022 STS_046_MRIstat_kurtSimon. L124024 STS_046_MRIstat_kurtSimon. L211011 STS_046_MRIstat_minSimon. L1-2020202 STS_046_MRIstat_minSimon. L2-3020302 STS_046_MRIstat_maxLaws2850285 STS_046_MRIstat_maxGabor1640164 STS_046_MRIstat_maxSimon. L13350335 STS_046_MRIstat_maxSimon. L23510351 STS_046_MRIstat_rangeLaws2790279 STS_046_MRIstat_rangeGabor1640164 STS_046_MRIstat_rangeSimon. L15370537 STS_046_MRIstat_rangeSimon. L26530653 STS_046_MRIstat_covLoG404 STS_046_MRIstat_covCoif3 LHH L19190919 STS_046_MRIstat_covSimon. L17880788 STS_046_MRIstat_covSimon. L256056 STS_046_MRIstat_qcodLoG303 STS_046_MRIstat_qcodGabor101 STS_046_MRIstat_qcodCoif3 LHH L11140114 STS_046_MRIstat_qcodCoif3 HHH L2101 STS_046_MRIstat_qcodSimon. L1-1490149 STS_046_MRIstat_qcodSimon. L21530153 STS_047_CTstat_skewLoG404 STS_047_CTstat_skewLaws707 STS_047_CTstat_skewGabor909 STS_047_CTstat_skewCoif3 HHH L2808 STS_047_CTstat_skewSimon. L1303 STS_047_CTstat_skewSimon. L2-303 STS_047_CTstat_kurtLoG24024 STS_047_CTstat_kurtLaws63063 STS_047_CTstat_kurtGabor1330133 STS_047_CTstat_kurtCoif3 HHH L21190119 STS_047_CTstat_kurtSimon. L133033 STS_047_CTstat_kurtSimon. L240040 STS_047_CTstat_minSimon. L1-1710171 STS_047_CTstat_minSimon. L2-3370337 STS_047_CTstat_maxLaws3160316 STS_047_CTstat_maxGabor150601506 STS_047_CTstat_maxCoif3 HHH L22520252 STS_047_CTstat_maxSimon. L11270127 STS_047_CTstat_rangeLaws3060306 STS_047_CTstat_rangeGabor150601506 STS_047_CTstat_rangeCoif3 HHH L22600260 STS_047_CTstat_rangeSimon. L12980298 STS_047_CTstat_rangeSimon. L24370437 STS_047_CTstat_madGabor25025 STS_047_CTstat_medadGabor24024 STS_047_CTstat_covLoG35035 STS_047_CTstat_covCoif3 LHH L1-3.454e+0403.454e+04 STS_047_CTstat_covSimon. L129029 STS_047_CTstat_covSimon. L213013 STS_047_CTstat_qcodLoG-202 STS_047_CTstat_qcodCoif3 LHH L1-7410741 STS_047_CTstat_qcodSimon. L176076 STS_047_CTstat_qcodSimon. L2909 STS_049_PETstat_covLoG-505 STS_049_PETstat_covCoif3 LHH L1305603056 STS_049_PETstat_covSimon. L172072 STS_049_PETstat_covSimon. L215015 STS_049_PETstat_qcodLoG-404 STS_049_PETstat_qcodCoif3 LHH L1-1100110 STS_049_PETstat_qcodSimon. L1-83083 STS_049_PETstat_qcodSimon. L210010 STS_050_PETstat_skewLoG-202 STS_050_PETstat_skewGabor202 STS_050_PETstat_kurtLoG808 STS_050_PETstat_covLoG505 STS_050_PETstat_covCoif3 LHH L17750775 STS_050_PETstat_covSimon. L1-16016 STS_050_PETstat_covSimon. L2-505 STS_050_PETstat_qcodLoG101 STS_050_PETstat_qcodGabor101 STS_050_PETstat_qcodCoif3 LHH L1-1400140 STS_050_PETstat_qcodSimon. L1-707 STS_050_PETstat_qcodSimon. L2-202 STS_045_CTstat_meanGabor1510151 STS_045_CTstat_varGabor990609906 STS_045_CTstat_skewLoG404 STS_045_CTstat_skewLaws404 STS_045_CTstat_skewCoif3 HHH L2202 STS_045_CTstat_skewSimon. L2-303 STS_045_CTstat_kurtLoG21021 STS_045_CTstat_kurtLaws25025 STS_045_CTstat_kurtCoif3 HHH L214014 STS_045_CTstat_kurtSimon. L223023 STS_045_CTstat_medianGabor1300130 STS_045_CTstat_minSimon. L1-2350235 STS_045_CTstat_minSimon. L2-3190319 STS_045_CTstat_p90Gabor2870287 STS_045_CTstat_maxLaws2980298 STS_045_CTstat_maxGabor125101251 STS_045_CTstat_maxCoif3 HHH L22170217 STS_045_CTstat_maxSimon. L11330133 STS_045_CTstat_maxSimon. L21600160 STS_045_CTstat_iqrGabor1220122 STS_045_CTstat_rangeLaws2710271 STS_045_CTstat_rangeGabor125101251 STS_045_CTstat_rangeCoif3 HHH L22360236 STS_045_CTstat_rangeSimon. L13680368 STS_045_CTstat_rangeSimon. L24780478 STS_045_CTstat_madGabor77077 STS_045_CTstat_rmadGabor52052 STS_045_CTstat_medadGabor75075 STS_045_CTstat_covLoG505 STS_045_CTstat_covCoif3 LHH L1-283302833 STS_045_CTstat_covSimon. L1-2320232 STS_045_CTstat_covSimon. L2-37037 STS_045_CTstat_qcodLoG505 STS_045_CTstat_qcodCoif3 LHH L1-6430643 STS_045_CTstat_qcodSimon. L1-2100210 STS_045_CTstat_qcodSimon. L2-60060 STS_045_CTstat_rmsGabor1810181 STS_048_MRIstat_skewLaws303 STS_048_MRIstat_skewGabor303 STS_048_MRIstat_skewCoif3 HHH L2505 STS_048_MRIstat_kurtLoG16016 STS_048_MRIstat_kurtLaws11011 STS_048_MRIstat_kurtGabor21021 STS_048_MRIstat_kurtCoif3 LHH L136036 STS_048_MRIstat_kurtCoif3 HHH L229029 STS_048_MRIstat_kurtSimon. L125025 STS_048_MRIstat_kurtSimon. L212012 STS_048_MRIstat_minSimon. L1-1940194 STS_048_MRIstat_minSimon. L2-1920192 STS_048_MRIstat_maxLaws3260326 STS_048_MRIstat_maxGabor4450445 STS_048_MRIstat_maxCoif3 HHH L21360136 STS_048_MRIstat_maxSimon. L12430243 STS_048_MRIstat_maxSimon. L22960296 STS_048_MRIstat_rangeLaws3140314 STS_048_MRIstat_rangeGabor4450445 STS_048_MRIstat_rangeCoif3 HHH L21410141 STS_048_MRIstat_rangeSimon. L14380438 STS_048_MRIstat_rangeSimon. L24880488 STS_048_MRIstat_madLaws21021 STS_048_MRIstat_madGabor20020 STS_048_MRIstat_covLoG505 STS_048_MRIstat_covCoif3 LHH L1-3500350 STS_048_MRIstat_covSimon. L1-1850185 STS_048_MRIstat_covSimon. L240040 STS_048_MRIstat_qcodLoG202 STS_048_MRIstat_qcodCoif3 LHH L1-236902369 STS_048_MRIstat_qcodSimon. L12360236 STS_048_MRIstat_qcodSimon. L2-74074 STS_051_PETstat_skewLoG-202 STS_051_PETstat_skewGabor202 STS_051_PETstat_skewCoif3 HHH L2202 STS_051_PETstat_covLoG-303 STS_051_PETstat_covCoif3 LHH L15260526 STS_051_PETstat_covSimon. L121021 STS_051_PETstat_covSimon. L2505 STS_051_PETstat_qcodLoG-202 STS_051_PETstat_qcodCoif3 LHH L1-1990199 STS_051_PETstat_qcodSimon. L11690169 STS_051_PETstat_qcodSimon. L2303 STS_048_CTstat_meanGabor1600160 STS_048_CTstat_varGabor751407514 STS_048_CTstat_skewLoG909 STS_048_CTstat_skewLaws404 STS_048_CTstat_skewSimon. L2202 STS_048_CTstat_kurtLoG1370137 STS_048_CTstat_kurtLaws45045 STS_048_CTstat_kurtSimon. L216016 STS_048_CTstat_medianGabor1490149 STS_048_CTstat_minSimon. L1-85085 STS_048_CTstat_minSimon. L2-1410141 STS_048_CTstat_p10Gabor58058 STS_048_CTstat_p90Gabor2790279 STS_048_CTstat_maxLaws2050205 STS_048_CTstat_maxGabor6300630 STS_048_CTstat_maxSimon. L21590159 STS_048_CTstat_iqrGabor1160116 STS_048_CTstat_rangeLaws1730173 STS_048_CTstat_rangeGabor6300630 STS_048_CTstat_rangeCoif3 HHH L21370137 STS_048_CTstat_rangeSimon. L11630163 STS_048_CTstat_rangeSimon. L23000300 STS_048_CTstat_madGabor69069 STS_048_CTstat_rmadGabor48048 STS_048_CTstat_medadGabor68068 STS_048_CTstat_covLoG-749307493 STS_048_CTstat_covCoif3 LHH L1-336503365 STS_048_CTstat_covSimon. L1-7410741 STS_048_CTstat_covSimon. L277077 STS_048_CTstat_qcodLoG-606 STS_048_CTstat_qcodCoif3 LHH L1-692206922 STS_048_CTstat_qcodSimon. L1-9650965 STS_048_CTstat_qcodSimon. L2-42042 STS_048_CTstat_rmsGabor1820182 STS_046_CTstat_skewLoG606 STS_046_CTstat_skewLaws707 STS_046_CTstat_skewGabor303 STS_046_CTstat_kurtLoG51051 STS_046_CTstat_kurtLaws79079 STS_046_CTstat_kurtGabor21021 STS_046_CTstat_kurtSimon. L230030 STS_046_CTstat_minSimon. L1-2610261 STS_046_CTstat_minSimon. L2-3600360 STS_046_CTstat_p90Gabor1230123 STS_046_CTstat_maxLaws3530353 STS_046_CTstat_maxGabor107301073 STS_046_CTstat_maxSimon. L11360136 STS_046_CTstat_maxSimon. L21450145 STS_046_CTstat_iqrGabor48048 STS_046_CTstat_rangeLaws3310331 STS_046_CTstat_rangeGabor107301073 STS_046_CTstat_rangeCoif3 HHH L21400140 STS_046_CTstat_rangeSimon. L13970397 STS_046_CTstat_rangeSimon. L25050505 STS_046_CTstat_madGabor38038 STS_046_CTstat_rmadGabor21021 STS_046_CTstat_medadGabor35035 STS_046_CTstat_covLoG707 STS_046_CTstat_covCoif3 LHH L16750675 STS_046_CTstat_covSimon. L1-2010201 STS_046_CTstat_covSimon. L217017 STS_046_CTstat_qcodLoG11011 STS_046_CTstat_qcodCoif3 LHH L1437404374 STS_046_CTstat_qcodSimon. L1-1010101 STS_046_CTstat_qcodSimon. L2-132801328 STS_046_CTstat_rmsGabor83083 STS_050_MRIstat_skewLoG202 STS_050_MRIstat_skewLaws303 STS_050_MRIstat_skewGabor303 STS_050_MRIstat_skewCoif3 HHH L2505 STS_050_MRIstat_kurtLoG808 STS_050_MRIstat_kurtLaws10010 STS_050_MRIstat_kurtGabor21021 STS_050_MRIstat_kurtCoif3 LHH L111011 STS_050_MRIstat_kurtCoif3 HHH L248048 STS_050_MRIstat_kurtSimon. L113013 STS_050_MRIstat_minSimon. L1-1310131 STS_050_MRIstat_minSimon. L2-1150115 STS_050_MRIstat_maxLaws2140214 STS_050_MRIstat_maxGabor6620662 STS_050_MRIstat_maxCoif3 HHH L21960196 STS_050_MRIstat_maxSimon. L11410141 STS_050_MRIstat_maxSimon. L21460146 STS_050_MRIstat_rangeLaws1970197 STS_050_MRIstat_rangeGabor6620662 STS_050_MRIstat_rangeCoif3 HHH L22000200 STS_050_MRIstat_rangeSimon. L12720272 STS_050_MRIstat_rangeSimon. L22610261 STS_050_MRIstat_madGabor28028 STS_050_MRIstat_rmadGabor17017 STS_050_MRIstat_medadGabor27027 STS_050_MRIstat_covLoG-1810181 STS_050_MRIstat_covCoif3 LHH L16730673 STS_050_MRIstat_covSimon. L1-55055 STS_050_MRIstat_covSimon. L222022 STS_050_MRIstat_qcodLoG-505 STS_050_MRIstat_qcodCoif3 LHH L15140514 STS_050_MRIstat_qcodSimon. L1-55055 STS_050_MRIstat_qcodSimon. L2-13013 STS_049_MRIstat_varGabor1.06e+0401.06e+04 STS_049_MRIstat_skewLaws202 STS_049_MRIstat_skewGabor505 STS_049_MRIstat_skewCoif3 HHH L2606 STS_049_MRIstat_kurtGabor40040 STS_049_MRIstat_kurtCoif3 LHH L117017 STS_049_MRIstat_kurtCoif3 HHH L255055 STS_049_MRIstat_kurtSimon. L118018 STS_049_MRIstat_kurtSimon. L2808 STS_049_MRIstat_minSimon. L1-1990199 STS_049_MRIstat_minSimon. L2-2880288 STS_049_MRIstat_p90Gabor1460146 STS_049_MRIstat_maxLaws3450345 STS_049_MRIstat_maxGabor189101891 STS_049_MRIstat_maxCoif3 HHH L21750175 STS_049_MRIstat_maxSimon. L12940294 STS_049_MRIstat_maxSimon. L22790279 STS_049_MRIstat_iqrGabor50050 STS_049_MRIstat_rangeLaws3380338 STS_049_MRIstat_rangeGabor189101891 STS_049_MRIstat_rangeCoif3 HHH L21840184 STS_049_MRIstat_rangeSimon. L14930493 STS_049_MRIstat_rangeSimon. L25670567 STS_049_MRIstat_madLaws24024 STS_049_MRIstat_madGabor55055 STS_049_MRIstat_rmadGabor23023 STS_049_MRIstat_medadLaws21021 STS_049_MRIstat_medadGabor47047 STS_049_MRIstat_covLoG81081 STS_049_MRIstat_covCoif3 LHH L1246102461 STS_049_MRIstat_covSimon. L1-184801848 STS_049_MRIstat_covSimon. L233033 STS_049_MRIstat_qcodLoG172401724 STS_049_MRIstat_qcodGabor101 STS_049_MRIstat_qcodCoif3 LHH L12970297 STS_049_MRIstat_qcodCoif3 HHH L2101 STS_049_MRIstat_qcodSimon. L1-1030103 STS_049_MRIstat_qcodSimon. L2-82082 STS_049_MRIstat_rmsGabor1250125 STS_050_CTstat_meanGabor2030203 STS_050_CTstat_varGabor1.317e+0401.317e+04 STS_050_CTstat_skewLoG606 STS_050_CTstat_skewLaws606 STS_050_CTstat_kurtLoG34034 STS_050_CTstat_kurtLaws39039 STS_050_CTstat_kurtCoif3 LHH L1909 STS_050_CTstat_kurtSimon. L222022 STS_050_CTstat_medianGabor1860186 STS_050_CTstat_minSimon. L1-86086 STS_050_CTstat_minSimon. L2-3180318 STS_050_CTstat_p10Gabor72072 STS_050_CTstat_p90Gabor3570357 STS_050_CTstat_maxLaws2980298 STS_050_CTstat_maxGabor8380838 STS_050_CTstat_maxSimon. L11530153 STS_050_CTstat_maxSimon. L21270127 STS_050_CTstat_iqrGabor1510151 STS_050_CTstat_rangeLaws2730273 STS_050_CTstat_rangeGabor8380838 STS_050_CTstat_rangeSimon. L12390239 STS_050_CTstat_rangeSimon. L24440444 STS_050_CTstat_madGabor90090 STS_050_CTstat_rmadGabor63063 STS_050_CTstat_medadGabor89089 STS_050_CTstat_covLoG404 STS_050_CTstat_covCoif3 LHH L19390939 STS_050_CTstat_covSimon. L11360136 STS_050_CTstat_covSimon. L210010 STS_050_CTstat_qcodLoG202 STS_050_CTstat_qcodCoif3 LHH L13.174e+0403.174e+04 STS_050_CTstat_qcodSimon. L1-2430243 STS_050_CTstat_qcodSimon. L225025 STS_050_CTstat_rmsGabor2340234 STS_051_MRIstat_meanGabor81081 STS_051_MRIstat_varGabor1.032e+0401.032e+04 STS_051_MRIstat_skewLaws202 STS_051_MRIstat_skewGabor505 STS_051_MRIstat_skewCoif3 HHH L2303 STS_051_MRIstat_kurtGabor33033 STS_051_MRIstat_kurtCoif3 LHH L110010 STS_051_MRIstat_kurtCoif3 HHH L214014 STS_051_MRIstat_kurtSimon. L1808 STS_051_MRIstat_minSimon. L1-1790179 STS_051_MRIstat_minSimon. L2-3730373 STS_051_MRIstat_p90Gabor1590159 STS_051_MRIstat_maxLaws2910291 STS_051_MRIstat_maxGabor158801588 STS_051_MRIstat_maxSimon. L11530153 STS_051_MRIstat_maxSimon. L21840184 STS_051_MRIstat_iqrGabor57057 STS_051_MRIstat_rangeLaws2780278 STS_051_MRIstat_rangeGabor158801588 STS_051_MRIstat_rangeCoif3 HHH L21300130 STS_051_MRIstat_rangeSimon. L13320332 STS_051_MRIstat_rangeSimon. L25570557 STS_051_MRIstat_madLaws25025 STS_051_MRIstat_madGabor56056 STS_051_MRIstat_madSimon. L227027 STS_051_MRIstat_rmadGabor26026 STS_051_MRIstat_medadLaws23023 STS_051_MRIstat_medadGabor50050 STS_051_MRIstat_medadSimon. L227027 STS_051_MRIstat_covCoif3 LHH L1-504205042 STS_051_MRIstat_covSimon. L1-66066 STS_051_MRIstat_covSimon. L2-13013 STS_051_MRIstat_qcodLoG101 STS_051_MRIstat_qcodCoif3 LHH L1108101081 STS_051_MRIstat_qcodCoif3 HHH L2101 STS_051_MRIstat_qcodSimon. L1-91091 STS_051_MRIstat_qcodSimon. L2-19019 STS_051_MRIstat_rmsGabor1300130 STS_051_CTstat_skewLoG-303 STS_051_CTstat_skewLaws303 STS_051_CTstat_skewCoif3 HHH L2505 STS_051_CTstat_kurtLoG13013 STS_051_CTstat_kurtLaws22022 STS_051_CTstat_kurtCoif3 HHH L266066 STS_051_CTstat_minSimon. L2-94094 STS_051_CTstat_maxGabor2380238 STS_051_CTstat_maxCoif3 HHH L21410141 STS_051_CTstat_rangeGabor2380238 STS_051_CTstat_rangeCoif3 HHH L21480148 STS_051_CTstat_rangeSimon. L21570157 STS_051_CTstat_covLoG-303 STS_051_CTstat_covCoif3 LHH L1138001380 STS_051_CTstat_covSimon. L174074 STS_051_CTstat_covSimon. L213013 STS_051_CTstat_qcodLoG-101 STS_051_CTstat_qcodCoif3 LHH L1-4220422 STS_051_CTstat_qcodSimon. L12510251 STS_051_CTstat_qcodSimon. L211011 STS_049_CTstat_varGabor611106111 STS_049_CTstat_skewLoG-202 STS_049_CTstat_skewLaws606 STS_049_CTstat_skewGabor10010 STS_049_CTstat_skewCoif3 HHH L2909 STS_049_CTstat_skewSimon. L1-505 STS_049_CTstat_kurtLoG12012 STS_049_CTstat_kurtLaws44044 STS_049_CTstat_kurtGabor1570157 STS_049_CTstat_kurtCoif3 LHH L154054 STS_049_CTstat_kurtCoif3 HHH L297097 STS_049_CTstat_kurtSimon. L199099 STS_049_CTstat_kurtSimon. L225025 STS_049_CTstat_minSimon. L1-3350335 STS_049_CTstat_minSimon. L2-4430443 STS_049_CTstat_maxLaws5470547 STS_049_CTstat_maxGabor248002480 STS_049_CTstat_maxCoif3 HHH L23040304 STS_049_CTstat_maxSimon. L11660166 STS_049_CTstat_maxSimon. L22470247 STS_049_CTstat_rangeLaws5380538 STS_049_CTstat_rangeGabor248002480 STS_049_CTstat_rangeCoif3 HHH L23420342 STS_049_CTstat_rangeSimon. L15010501 STS_049_CTstat_rangeSimon. L26900690 STS_049_CTstat_madGabor31031 STS_049_CTstat_medadGabor28028 STS_049_CTstat_covLoG-606 STS_049_CTstat_covCoif3 LHH L1271602716 STS_049_CTstat_covSimon. L1-52052 STS_049_CTstat_covSimon. L2-1640164 STS_049_CTstat_qcodLoG-404 STS_049_CTstat_qcodCoif3 LHH L1777807778 STS_049_CTstat_qcodSimon. L1-2390239 STS_049_CTstat_qcodSimon. L246046 STS_049_CTstat_rmsGabor93093"},{"location":"ibsi2_phase3_compliance/#usz","title":"USZ","text":"Patient Feature Configuration Pictologics Value Team Value Error STS_004_MRI stat_qcod Coif3 LHH L1 1.748e+04 2.543e+04 7950 STS_015_CT stat_cov Coif3 LHH L1 1.458e+04 1.602e+05 1.456e+05 STS_026_PET stat_qcod Coif3 LHH L1 -5105 -194.8 4911 STS_044_MRI stat_min Mean 216 150.2 65.8"},{"location":"ibsi2_phase3_compliance/#udes","title":"UdeS","text":"Patient Feature Configuration Pictologics Value Team Value Error STS_002_MRI stat_min Simon. L1 -162 0 162 STS_002_MRI stat_min Simon. L2 -249 0 249 STS_002_MRI stat_p10 Simon. L2 -58 0 58 STS_002_MRI stat_iqr Simon. L2 42 0 42 STS_002_MRI stat_range Simon. L1 338 0 338 STS_002_MRI stat_range Simon. L2 447 0 447 STS_002_MRI stat_mad Simon. L2 30 0 30 STS_002_MRI stat_rmad Simon. L2 18 0 18 STS_002_MRI stat_medad Simon. L2 30 0 30 STS_003_MRI stat_min Simon. L1 -137 0 137 Show 604 more mismatches... PatientFeatureConfigurationPictologics ValueTeam ValueError STS_003_MRIstat_minSimon. L2-2460246 STS_003_MRIstat_p10Simon. L2-54054 STS_003_MRIstat_iqrSimon. L242042 STS_003_MRIstat_rangeSimon. L13010301 STS_003_MRIstat_rangeSimon. L24970497 STS_003_MRIstat_madSimon. L229029 STS_003_MRIstat_rmadSimon. L218018 STS_003_MRIstat_medadSimon. L228028 STS_002_CTstat_minSimon. L1-1170117 STS_002_CTstat_minSimon. L2-93093 STS_002_CTstat_madSimon. L122022 STS_002_CTstat_medadSimon. L122022 STS_003_CTstat_minSimon. L1-1310131 STS_003_CTstat_minSimon. L2-85085 STS_003_CTstat_madSimon. L124024 STS_003_CTstat_medadSimon. L124024 STS_004_MRIstat_minSimon. L1-2400240 STS_004_MRIstat_minSimon. L2-2870287 STS_004_MRIstat_maxSimon. L12900290 STS_004_MRIstat_maxSimon. L24320432 STS_004_MRIstat_rangeSimon. L15300530 STS_004_MRIstat_rangeSimon. L27190719 STS_004_MRIstat_madSimon. L227027 STS_004_MRIstat_medadSimon. L227027 STS_001_MRIstat_minSimon. L1-1120112 STS_001_MRIstat_minSimon. L2-1450145 STS_001_CTstat_minSimon. L1-79079 STS_006_MRIstat_minSimon. L1-1120112 STS_006_MRIstat_minSimon. L2-1700170 STS_006_MRIstat_rangeSimon. L24050405 STS_005_MRIstat_skewSimon. L2-202 STS_005_MRIstat_minSimon. L1-2010201 STS_005_MRIstat_minSimon. L2-2470247 STS_005_MRIstat_rangeSimon. L14510451 STS_005_MRIstat_rangeSimon. L24590459 STS_005_MRIstat_madSimon. L223023 STS_005_MRIstat_medadSimon. L223023 STS_004_CTstat_minSimon. L1-99099 STS_004_CTstat_minSimon. L2-77077 STS_008_MRIstat_minSimon. L1-1670167 STS_008_MRIstat_minSimon. L2-1820182 STS_008_MRIstat_maxSimon. L23020302 STS_008_MRIstat_rangeSimon. L13560356 STS_008_MRIstat_rangeSimon. L24840484 STS_008_MRIstat_madSimon. L229029 STS_008_MRIstat_medadSimon. L229029 STS_006_CTstat_minSimon. L1-1030103 STS_006_CTstat_minSimon. L2-80080 STS_005_CTstat_minSimon. L1-1570157 STS_005_CTstat_minSimon. L2-3130313 STS_005_CTstat_rangeSimon. L13020302 STS_005_CTstat_rangeSimon. L24500450 STS_005_CTstat_madSimon. L120020 STS_005_CTstat_medadSimon. L120020 STS_008_CTstat_skewSimon. L2-404 STS_008_CTstat_kurtSimon. L233033 STS_008_CTstat_minSimon. L1-1870187 STS_008_CTstat_minSimon. L2-3130313 STS_008_CTstat_rangeSimon. L13210321 STS_008_CTstat_rangeSimon. L24080408 STS_010_MRIstat_minSimon. L1-1230123 STS_010_MRIstat_minSimon. L2-1110111 STS_007_MRIstat_minSimon. L1-2100210 STS_007_MRIstat_minSimon. L2-2260226 STS_007_MRIstat_maxSimon. L22880288 STS_007_MRIstat_rangeSimon. L14310431 STS_007_MRIstat_rangeSimon. L25150515 STS_007_MRIstat_madSimon. L221021 STS_007_MRIstat_medadSimon. L221021 STS_007_CTstat_minSimon. L1-1780178 STS_007_CTstat_minSimon. L2-2590259 STS_007_CTstat_rangeSimon. L12900290 STS_007_CTstat_rangeSimon. L24150415 STS_012_CTstat_minSimon. L1-93093 STS_012_CTstat_minSimon. L2-77077 STS_012_MRIstat_minSimon. L1-2070207 STS_012_MRIstat_minSimon. L2-2290229 STS_012_MRIstat_rangeSimon. L13840384 STS_012_MRIstat_rangeSimon. L24750475 STS_012_MRIstat_madSimon. L222022 STS_012_MRIstat_medadSimon. L222022 STS_011_MRIstat_kurtSimon. L119019 STS_011_MRIstat_minSimon. L1-1650165 STS_011_MRIstat_minSimon. L2-2200220 STS_011_MRIstat_rangeSimon. L13560356 STS_011_MRIstat_rangeSimon. L24500450 STS_009_MRIstat_kurtCoif3 HHH L261.9641.9919.97 STS_009_MRIstat_kurtSimon. L127027 STS_009_MRIstat_minSimon. L1-2730273 STS_009_MRIstat_minSimon. L2-3050305 STS_009_MRIstat_maxSimon. L12990299 STS_009_MRIstat_maxSimon. L23920392 STS_009_MRIstat_rangeSimon. L15730573 STS_009_MRIstat_rangeSimon. L26960696 STS_011_CTstat_minSimon. L1-1050105 STS_011_CTstat_minSimon. L2-73073 STS_015_MRIstat_minSimon. L1-2430243 STS_015_MRIstat_minSimon. L2-2440244 STS_015_MRIstat_p10Simon. L2-80080 STS_015_MRIstat_maxSimon. L13160316 STS_015_MRIstat_maxSimon. L23380338 STS_015_MRIstat_iqrSimon. L253053 STS_015_MRIstat_rangeSimon. L15600560 STS_015_MRIstat_rangeSimon. L25820582 STS_015_MRIstat_madSimon. L123023 STS_015_MRIstat_madSimon. L241041 STS_015_MRIstat_rmadSimon. L224024 STS_015_MRIstat_medadSimon. L123023 STS_015_MRIstat_medadSimon. L241041 STS_009_CTstat_skewSimon. L2-202 STS_009_CTstat_kurtSimon. L117017 STS_009_CTstat_kurtSimon. L240040 STS_009_CTstat_minSimon. L1-2870287 STS_009_CTstat_minSimon. L2-4280428 STS_009_CTstat_maxSimon. L13410341 STS_009_CTstat_maxSimon. L23270327 STS_009_CTstat_rangeSimon. L16270627 STS_009_CTstat_rangeSimon. L27550755 STS_016_PETstat_skewSimon. L2303 STS_016_PETstat_kurtSimon. L220020 STS_015_CTstat_minSimon. L1-98098 STS_015_CTstat_minSimon. L2-70070 STS_014_MRIstat_kurtSimon. L116016 STS_014_MRIstat_minSimon. L1-1840184 STS_014_MRIstat_minSimon. L2-1660166 STS_014_MRIstat_rangeSimon. L13520352 STS_014_MRIstat_rangeSimon. L23480348 STS_014_CTstat_minSimon. L1-86086 STS_013_MRIstat_minSimon. L1-2960296 STS_013_MRIstat_minSimon. L2-3510351 STS_013_MRIstat_p10Simon. L2-65065 STS_013_MRIstat_maxSimon. L13060306 STS_013_MRIstat_maxSimon. L23830383 STS_013_MRIstat_iqrSimon. L264064 STS_013_MRIstat_rangeSimon. L16020602 STS_013_MRIstat_rangeSimon. L27330733 STS_013_MRIstat_madSimon. L123023 STS_013_MRIstat_madSimon. L244044 STS_013_MRIstat_rmadSimon. L228028 STS_013_MRIstat_medadSimon. L123023 STS_013_MRIstat_medadSimon. L244044 STS_018_MRIstat_minSimon. L1-2820282 STS_018_MRIstat_minSimon. L2-3090309 STS_018_MRIstat_p10Simon. L2-85085 STS_018_MRIstat_maxSimon. L24120412 STS_018_MRIstat_iqrSimon. L275075 STS_018_MRIstat_rangeSimon. L15000500 STS_018_MRIstat_rangeSimon. L27210721 STS_018_MRIstat_madSimon. L123023 STS_018_MRIstat_madSimon. L249049 STS_018_MRIstat_rmadSimon. L232032 STS_018_MRIstat_medadSimon. L123023 STS_018_MRIstat_medadSimon. L249049 STS_019_CTstat_minSimon. L1-1850185 STS_019_CTstat_minSimon. L2-1660166 STS_019_CTstat_p10Simon. L2-59059 STS_019_CTstat_iqrSimon. L248048 STS_019_CTstat_rangeSimon. L12970297 STS_019_CTstat_rangeSimon. L24050405 STS_019_CTstat_madSimon. L233033 STS_019_CTstat_rmadSimon. L221021 STS_019_CTstat_medadSimon. L233033 STS_017_MRIstat_minSimon. L1-2560256 STS_017_MRIstat_minSimon. L2-2380238 STS_017_MRIstat_maxSimon. L12830283 STS_017_MRIstat_rangeSimon. L15390539 STS_017_MRIstat_rangeSimon. L25050505 STS_017_MRIstat_madSimon. L222022 STS_017_MRIstat_medadSimon. L222022 STS_018_CTstat_minSimon. L1-2140214 STS_018_CTstat_minSimon. L2-2350235 STS_018_CTstat_rangeSimon. L14050405 STS_018_CTstat_rangeSimon. L23800380 STS_016_MRIstat_kurtSimon. L125025 STS_016_MRIstat_minSimon. L1-1740174 STS_016_MRIstat_minSimon. L2-1980198 STS_016_MRIstat_rangeSimon. L13900390 STS_016_MRIstat_rangeSimon. L23700370 STS_019_MRIstat_minSimon. L1-1330133 STS_019_MRIstat_minSimon. L2-2430243 STS_019_MRIstat_p10Simon. L2-89089 STS_019_MRIstat_iqrSimon. L267067 STS_019_MRIstat_rangeSimon. L24420442 STS_019_MRIstat_madSimon. L243043 STS_019_MRIstat_rmadSimon. L229029 STS_019_MRIstat_medadSimon. L242042 STS_016_CTstat_minSimon. L2-71071 STS_020_PETstat_skewSimon. L2202 STS_020_PETstat_kurtSimon. L118018 STS_020_PETstat_kurtSimon. L221021 STS_020_PETstat_covLoG-1492-1.592e+051.578e+05 STS_013_CTstat_skewSimon. L1-404 STS_013_CTstat_skewSimon. L2-303 STS_013_CTstat_kurtSimon. L139039 STS_013_CTstat_kurtSimon. L219019 STS_013_CTstat_minSimon. L1-3040304 STS_013_CTstat_minSimon. L2-4650465 STS_013_CTstat_rangeSimon. L14450445 STS_013_CTstat_rangeSimon. L26840684 STS_013_CTstat_madSimon. L226026 STS_013_CTstat_medadSimon. L226026 STS_017_CTstat_minSimon. L1-80080 STS_017_CTstat_minSimon. L2-1770177 STS_017_CTstat_rangeSimon. L23110311 STS_017_CTstat_qcodCoif3 LHH L1-1.093e+04-7.389e+046.296e+04 STS_022_MRIstat_skewSimon. L1-202 STS_022_MRIstat_kurtSimon. L155055 STS_022_MRIstat_minSimon. L1-2000200 STS_022_MRIstat_minSimon. L2-2140214 STS_022_MRIstat_rangeSimon. L13430343 STS_022_MRIstat_rangeSimon. L23770377 STS_021_MRIstat_minSimon. L1-1240124 STS_021_MRIstat_minSimon. L2-1550155 STS_021_MRIstat_rangeSimon. L23560356 STS_023_MRIstat_minSimon. L1-1180118 STS_023_MRIstat_minSimon. L2-1240124 STS_023_MRIstat_rangeSimon. L13370337 STS_023_MRIstat_rangeSimon. L23150315 STS_022_CTstat_skewSimon. L2404 STS_022_CTstat_kurtSimon. L258058 STS_022_CTstat_minSimon. L1-2220222 STS_022_CTstat_minSimon. L2-2240224 STS_022_CTstat_maxSimon. L13080308 STS_022_CTstat_maxSimon. L23190319 STS_022_CTstat_rangeSimon. L15290529 STS_022_CTstat_rangeSimon. L25430543 STS_023_CTstat_minSimon. L1-2260226 STS_023_CTstat_minSimon. L2-3470347 STS_023_CTstat_rangeSimon. L13540354 STS_023_CTstat_rangeSimon. L24980498 STS_023_CTstat_madSimon. L222022 STS_023_CTstat_medadSimon. L222022 STS_024_MRIstat_skewSimon. L2-202 STS_024_MRIstat_minSimon. L1-1630163 STS_024_MRIstat_minSimon. L2-2360236 STS_024_MRIstat_rangeSimon. L13720372 STS_024_MRIstat_rangeSimon. L24290429 STS_024_MRIstat_madSimon. L225025 STS_024_MRIstat_medadSimon. L225025 STS_025_MRIstat_minSimon. L1-2910291 STS_025_MRIstat_minSimon. L2-3240324 STS_025_MRIstat_p10Simon. L2-75075 STS_025_MRIstat_iqrSimon. L253053 STS_025_MRIstat_rangeSimon. L15130513 STS_025_MRIstat_rangeSimon. L25610561 STS_025_MRIstat_madSimon. L124024 STS_025_MRIstat_madSimon. L240040 STS_025_MRIstat_rmadSimon. L224024 STS_025_MRIstat_medadSimon. L124024 STS_025_MRIstat_medadSimon. L239039 STS_021_CTstat_minSimon. L1-1240124 STS_021_CTstat_minSimon. L2-84084 STS_021_CTstat_qcodSimon. L11.846e+0501.846e+05 STS_024_CTstat_minSimon. L2-1280128 STS_025_CTstat_minSimon. L1-61061 STS_025_CTstat_minSimon. L2-1050105 STS_026_MRIstat_minSimon. L1-2110211 STS_026_MRIstat_minSimon. L2-2460246 STS_026_MRIstat_rangeSimon. L13760376 STS_026_MRIstat_rangeSimon. L24530453 STS_026_MRIstat_madSimon. L223023 STS_026_MRIstat_medadSimon. L223023 STS_026_CTstat_skewSimon. L1202 STS_026_CTstat_skewSimon. L2505 STS_026_CTstat_kurtSimon. L125025 STS_026_CTstat_kurtSimon. L249049 STS_026_CTstat_minSimon. L1-1470147 STS_026_CTstat_minSimon. L2-1350135 STS_026_CTstat_maxSimon. L13040304 STS_026_CTstat_maxSimon. L23460346 STS_026_CTstat_rangeSimon. L14510451 STS_026_CTstat_rangeSimon. L24810481 STS_026_CTstat_covCoif3 LHH L1-9.46e+04-2.47e+046.99e+04 STS_028_MRIstat_skewSimon. L1-202 STS_028_MRIstat_kurtSimon. L118018 STS_028_MRIstat_minSimon. L1-1670167 STS_028_MRIstat_minSimon. L2-1840184 STS_028_MRIstat_rangeSimon. L13440344 STS_028_MRIstat_rangeSimon. L23730373 STS_028_CTstat_minSimon. L1-93093 STS_028_CTstat_minSimon. L2-1050105 STS_020_MRIstat_skewSimon. L1202 STS_020_MRIstat_kurtSimon. L139039 STS_020_MRIstat_kurtSimon. L220020 STS_020_MRIstat_minSimon. L1-1670167 STS_020_MRIstat_minSimon. L2-2800280 STS_020_MRIstat_maxSimon. L23290329 STS_020_MRIstat_rangeSimon. L13850385 STS_020_MRIstat_rangeSimon. L26090609 STS_029_CTstat_minSimon. L1-82082 STS_029_CTstat_minSimon. L2-69069 STS_020_CTstat_skewSimon. L2-202 STS_020_CTstat_kurtSimon. L236036 STS_020_CTstat_minSimon. L1-3770377 STS_020_CTstat_minSimon. L2-4700470 STS_020_CTstat_maxSimon. L13090309 STS_020_CTstat_maxSimon. L23110311 STS_020_CTstat_rangeSimon. L16860686 STS_020_CTstat_rangeSimon. L27810781 STS_027_MRIstat_minSimon. L1-1330133 STS_027_MRIstat_minSimon. L2-2090209 STS_027_MRIstat_rangeSimon. L24760476 STS_029_MRIstat_minSimon. L1-2150215 STS_029_MRIstat_minSimon. L2-2270227 STS_029_MRIstat_rangeSimon. L14010401 STS_029_MRIstat_rangeSimon. L24970497 STS_029_MRIstat_madSimon. L222022 STS_029_MRIstat_medadSimon. L222022 STS_027_CTstat_skewSimon. L2-404 STS_027_CTstat_kurtSimon. L229029 STS_027_CTstat_minSimon. L1-3000300 STS_027_CTstat_minSimon. L2-4000400 STS_027_CTstat_rangeSimon. L14650465 STS_027_CTstat_rangeSimon. L25640564 STS_032_MRIstat_skewSimon. L1-202 STS_032_MRIstat_skewSimon. L2-202 STS_032_MRIstat_kurtSimon. L122022 STS_032_MRIstat_minSimon. L1-3620362 STS_032_MRIstat_minSimon. L2-3870387 STS_032_MRIstat_maxSimon. L13150315 STS_032_MRIstat_maxSimon. L23390339 STS_032_MRIstat_rangeSimon. L16770677 STS_032_MRIstat_rangeSimon. L27260726 STS_032_MRIstat_madSimon. L229029 STS_032_MRIstat_medadSimon. L229029 STS_032_CTstat_minSimon. L2-1540154 STS_030_CTstat_minSimon. L1-1180118 STS_030_CTstat_minSimon. L2-77077 STS_031_MRIstat_kurtSimon. L118018 STS_031_MRIstat_minSimon. L1-2710271 STS_031_MRIstat_minSimon. L2-2040204 STS_031_MRIstat_rangeSimon. L15260526 STS_031_MRIstat_rangeSimon. L24710471 STS_031_CTstat_kurtSimon. L216016 STS_031_CTstat_minSimon. L1-1770177 STS_031_CTstat_minSimon. L2-1980198 STS_031_CTstat_rangeSimon. L12860286 STS_031_CTstat_rangeSimon. L23330333 STS_033_MRIstat_minSimon. L1-2840284 STS_033_MRIstat_minSimon. L2-3660366 STS_033_MRIstat_maxSimon. L12970297 STS_033_MRIstat_rangeSimon. L15810581 STS_033_MRIstat_rangeSimon. L26440644 STS_030_MRIstat_minSimon. L1-2570257 STS_030_MRIstat_minSimon. L2-3580358 STS_030_MRIstat_p10Simon. L2-56056 STS_030_MRIstat_maxSimon. L12920292 STS_030_MRIstat_maxSimon. L22820282 STS_030_MRIstat_iqrSimon. L251051 STS_030_MRIstat_rangeSimon. L15490549 STS_030_MRIstat_rangeSimon. L26400640 STS_030_MRIstat_madSimon. L121021 STS_030_MRIstat_madSimon. L235035 STS_030_MRIstat_rmadSimon. L222022 STS_030_MRIstat_medadSimon. L121021 STS_030_MRIstat_medadSimon. L235035 STS_033_CTstat_kurtSimon. L233033 STS_033_CTstat_minSimon. L1-1080108 STS_033_CTstat_minSimon. L2-1480148 STS_033_CTstat_rangeSimon. L23740374 STS_034_MRIstat_skewSimon. L1202 STS_034_MRIstat_minSimon. L1-1540154 STS_034_MRIstat_minSimon. L2-2040204 STS_034_MRIstat_rangeSimon. L13760376 STS_034_MRIstat_rangeSimon. L24790479 STS_034_MRIstat_qcodCoif3 LHH L1-44153.407e+043.849e+04 STS_035_MRIstat_minSimon. L1-1550155 STS_035_MRIstat_minSimon. L2-2510251 STS_035_MRIstat_p10Simon. L2-1070107 STS_035_MRIstat_maxSimon. L22820282 STS_035_MRIstat_iqrSimon. L284084 STS_035_MRIstat_rangeSimon. L13760376 STS_035_MRIstat_rangeSimon. L25330533 STS_035_MRIstat_madSimon. L124024 STS_035_MRIstat_madSimon. L255055 STS_035_MRIstat_rmadSimon. L236036 STS_035_MRIstat_medadSimon. L124024 STS_035_MRIstat_medadSimon. L255055 STS_035_CTstat_varSimon. L21.69e+0401.69e+04 STS_035_CTstat_skewSimon. L1-202 STS_035_CTstat_minSimon. L1-3860386 STS_035_CTstat_minSimon. L2-4340434 STS_035_CTstat_p10Simon. L2-2750275 STS_035_CTstat_iqrSimon. L294094 STS_035_CTstat_rangeSimon. L15400540 STS_035_CTstat_rangeSimon. L25830583 STS_035_CTstat_madSimon. L137037 STS_035_CTstat_madSimon. L299099 STS_035_CTstat_rmadSimon. L117017 STS_035_CTstat_rmadSimon. L260060 STS_035_CTstat_medadSimon. L137037 STS_035_CTstat_medadSimon. L289089 STS_035_CTstat_rmsSimon. L21340134 STS_034_CTstat_skewSimon. L1202 STS_034_CTstat_skewSimon. L2404 STS_034_CTstat_kurtSimon. L118018 STS_034_CTstat_kurtSimon. L229029 STS_034_CTstat_minSimon. L1-1730173 STS_034_CTstat_minSimon. L2-1970197 STS_034_CTstat_maxSimon. L13290329 STS_034_CTstat_maxSimon. L23270327 STS_034_CTstat_rangeSimon. L15020502 STS_034_CTstat_rangeSimon. L25240524 STS_039_PETstat_skewSimon. L2202 STS_038_MRIstat_kurtSimon. L118018 STS_038_MRIstat_minSimon. L1-2770277 STS_038_MRIstat_minSimon. L2-2550255 STS_038_MRIstat_maxSimon. L23240324 STS_038_MRIstat_rangeSimon. L15000500 STS_038_MRIstat_rangeSimon. L25790579 STS_038_MRIstat_madSimon. L220020 STS_038_MRIstat_medadSimon. L220020 STS_036_MRIstat_minSimon. L1-2750275 STS_036_MRIstat_minSimon. L2-2980298 STS_036_MRIstat_p10Simon. L2-50050 STS_036_MRIstat_maxSimon. L13140314 STS_036_MRIstat_maxSimon. L23370337 STS_036_MRIstat_rangeSimon. L15890589 STS_036_MRIstat_rangeSimon. L26350635 STS_036_MRIstat_madSimon. L229029 STS_036_MRIstat_medadSimon. L229029 STS_040_PETstat_skewSimon. L1202 STS_040_PETstat_skewSimon. L2202 STS_036_CTstat_skewSimon. L1202 STS_036_CTstat_skewSimon. L2303 STS_036_CTstat_kurtSimon. L117017 STS_036_CTstat_kurtSimon. L228028 STS_036_CTstat_minSimon. L1-99099 STS_036_CTstat_minSimon. L2-1260126 STS_036_CTstat_maxSimon. L22870287 STS_036_CTstat_rangeSimon. L13100310 STS_036_CTstat_rangeSimon. L24140414 STS_038_CTstat_skewSimon. L1202 STS_038_CTstat_skewSimon. L2303 STS_038_CTstat_kurtSimon. L133033 STS_038_CTstat_kurtSimon. L258058 STS_038_CTstat_minSimon. L1-1670167 STS_038_CTstat_minSimon. L2-3230323 STS_038_CTstat_maxSimon. L13010301 STS_038_CTstat_maxSimon. L23180318 STS_038_CTstat_rangeSimon. L14680468 STS_038_CTstat_rangeSimon. L26400640 STS_038_CTstat_qcodSimon. L11.217e+0501.217e+05 STS_041_PETstat_skewSimon. L2202 STS_040_MRIstat_minSimon. L1-2260226 STS_040_MRIstat_minSimon. L2-2670267 STS_040_MRIstat_rangeSimon. L14600460 STS_040_MRIstat_rangeSimon. L24990499 STS_040_MRIstat_madSimon. L227027 STS_040_MRIstat_medadSimon. L227027 STS_039_MRIstat_minSimon. L1-3300330 STS_039_MRIstat_minSimon. L2-2940294 STS_039_MRIstat_maxSimon. L13040304 STS_039_MRIstat_maxSimon. L22960296 STS_039_MRIstat_rangeSimon. L16340634 STS_039_MRIstat_rangeSimon. L25890589 STS_039_MRIstat_madSimon. L227027 STS_039_MRIstat_medadSimon. L227027 STS_037_MRIstat_skewSimon. L2-202 STS_037_MRIstat_kurtSimon. L119019 STS_037_MRIstat_minSimon. L1-2280228 STS_037_MRIstat_minSimon. L2-3220322 STS_037_MRIstat_rangeSimon. L14650465 STS_037_MRIstat_rangeSimon. L25450545 STS_037_MRIstat_madSimon. L221021 STS_037_MRIstat_medadSimon. L221021 STS_041_CTstat_minSimon. L1-93093 STS_041_CTstat_minSimon. L2-77077 STS_041_MRIstat_minSimon. L1-2450245 STS_041_MRIstat_minSimon. L2-3230323 STS_041_MRIstat_p10Simon. L2-1000100 STS_041_MRIstat_iqrSimon. L268068 STS_041_MRIstat_rangeSimon. L14590459 STS_041_MRIstat_rangeSimon. L25430543 STS_041_MRIstat_madSimon. L122022 STS_041_MRIstat_madSimon. L245045 STS_041_MRIstat_rmadSimon. L230030 STS_041_MRIstat_medadSimon. L121021 STS_041_MRIstat_medadSimon. L244044 STS_042_MRIstat_minSimon. L1-1480148 STS_042_MRIstat_minSimon. L2-2500250 STS_042_MRIstat_p10Simon. L2-71071 STS_042_MRIstat_maxSimon. L23180318 STS_042_MRIstat_iqrSimon. L266066 STS_042_MRIstat_rangeSimon. L13380338 STS_042_MRIstat_rangeSimon. L25680568 STS_042_MRIstat_madSimon. L244044 STS_042_MRIstat_rmadSimon. L228028 STS_042_MRIstat_medadSimon. L244044 STS_037_CTstat_skewSimon. L2-202 STS_037_CTstat_minSimon. L1-1950195 STS_037_CTstat_minSimon. L2-2920292 STS_037_CTstat_rangeSimon. L13070307 STS_037_CTstat_rangeSimon. L24400440 STS_039_CTstat_minSimon. L1-80080 STS_039_CTstat_minSimon. L2-62062 STS_040_CTstat_skewSimon. L2-202 STS_040_CTstat_kurtSimon. L219019 STS_040_CTstat_minSimon. L1-1880188 STS_040_CTstat_minSimon. L2-2310231 STS_040_CTstat_rangeSimon. L12970297 STS_040_CTstat_rangeSimon. L23310331 STS_042_CTstat_minSimon. L1-83083 STS_042_CTstat_minSimon. L2-1890189 STS_043_MRIstat_minSimon. L1-1780178 STS_043_MRIstat_minSimon. L2-1510151 STS_043_MRIstat_rangeSimon. L13900390 STS_043_MRIstat_rangeSimon. L23830383 STS_043_MRIstat_covCoif3 LHH L1-1.382e+053.86e+041.768e+05 STS_043_CTstat_minSimon. L1-65065 STS_044_MRIstat_minSimon. L1-1840184 STS_044_MRIstat_minSimon. L2-2120212 STS_044_MRIstat_maxSimon. L23090309 STS_044_MRIstat_iqrSimon. L243043 STS_044_MRIstat_rangeSimon. L13650365 STS_044_MRIstat_rangeSimon. L25220522 STS_044_MRIstat_madSimon. L228028 STS_044_MRIstat_rmadSimon. L218018 STS_044_MRIstat_medadSimon. L228028 STS_045_MRIstat_minSimon. L1-2600260 STS_045_MRIstat_minSimon. L2-2610261 STS_045_MRIstat_maxSimon. L13740374 STS_045_MRIstat_maxSimon. L23000300 STS_045_MRIstat_rangeSimon. L16340634 STS_045_MRIstat_rangeSimon. L25610561 STS_044_CTstat_minSimon. L1-1770177 STS_044_CTstat_minSimon. L2-1690169 STS_044_CTstat_rangeSimon. L12880288 STS_044_CTstat_rangeSimon. L23410341 STS_047_MRIstat_minSimon. L1-2410241 STS_047_MRIstat_minSimon. L2-3210321 STS_047_MRIstat_p10Simon. L2-63063 STS_047_MRIstat_maxSimon. L12890289 STS_047_MRIstat_maxSimon. L24030403 STS_047_MRIstat_iqrSimon. L242042 STS_047_MRIstat_rangeSimon. L15300530 STS_047_MRIstat_rangeSimon. L27240724 STS_047_MRIstat_madSimon. L234034 STS_047_MRIstat_rmadSimon. L219019 STS_047_MRIstat_medadSimon. L234034 STS_046_MRIstat_kurtSimon. L124024 STS_046_MRIstat_minSimon. L1-2020202 STS_046_MRIstat_minSimon. L2-3020302 STS_046_MRIstat_maxSimon. L13350335 STS_046_MRIstat_maxSimon. L23510351 STS_046_MRIstat_rangeSimon. L15370537 STS_046_MRIstat_rangeSimon. L26530653 STS_047_CTstat_skewSimon. L1303 STS_047_CTstat_skewSimon. L2-303 STS_047_CTstat_kurtSimon. L133033 STS_047_CTstat_kurtSimon. L240040 STS_047_CTstat_minSimon. L1-1710171 STS_047_CTstat_minSimon. L2-3370337 STS_047_CTstat_rangeSimon. L12980298 STS_047_CTstat_rangeSimon. L24370437 STS_045_CTstat_skewSimon. L2-303 STS_045_CTstat_kurtSimon. L223023 STS_045_CTstat_minSimon. L1-2350235 STS_045_CTstat_minSimon. L2-3190319 STS_045_CTstat_rangeSimon. L13680368 STS_045_CTstat_rangeSimon. L24780478 STS_048_MRIstat_kurtSimon. L125025 STS_048_MRIstat_minSimon. L1-1940194 STS_048_MRIstat_minSimon. L2-1920192 STS_048_MRIstat_maxSimon. L22960296 STS_048_MRIstat_rangeSimon. L14380438 STS_048_MRIstat_rangeSimon. L24880488 STS_048_CTstat_skewSimon. L2202 STS_048_CTstat_kurtSimon. L216016 STS_048_CTstat_minSimon. L1-85085 STS_048_CTstat_minSimon. L2-1410141 STS_048_CTstat_rangeSimon. L23000300 STS_046_CTstat_kurtSimon. L230030 STS_046_CTstat_minSimon. L1-2610261 STS_046_CTstat_minSimon. L2-3600360 STS_046_CTstat_rangeSimon. L13970397 STS_046_CTstat_rangeSimon. L25050505 STS_050_MRIstat_minSimon. L1-1310131 STS_050_MRIstat_minSimon. L2-1150115 STS_049_MRIstat_kurtSimon. L118018 STS_049_MRIstat_minSimon. L1-1990199 STS_049_MRIstat_minSimon. L2-2880288 STS_049_MRIstat_maxSimon. L12940294 STS_049_MRIstat_rangeSimon. L14930493 STS_049_MRIstat_rangeSimon. L25670567 STS_050_CTstat_kurtSimon. L222022 STS_050_CTstat_minSimon. L1-86086 STS_050_CTstat_minSimon. L2-3180318 STS_050_CTstat_rangeSimon. L24440444 STS_050_CTstat_qcodCoif3 LHH L13.174e+0468242.492e+04 STS_051_MRIstat_minSimon. L1-1790179 STS_051_MRIstat_minSimon. L2-3730373 STS_051_MRIstat_rangeSimon. L13320332 STS_051_MRIstat_rangeSimon. L25570557 STS_051_MRIstat_madSimon. L227027 STS_051_MRIstat_medadSimon. L227027 STS_051_CTstat_minSimon. L2-94094 STS_049_CTstat_skewSimon. L1-505 STS_049_CTstat_kurtSimon. L199099 STS_049_CTstat_kurtSimon. L225025 STS_049_CTstat_minSimon. L1-3350335 STS_049_CTstat_minSimon. L2-4430443 STS_049_CTstat_rangeSimon. L15010501 STS_049_CTstat_rangeSimon. L26900690"},{"location":"ibsi2_phase3_compliance/#veneto-institute-of-oncology","title":"Veneto Institute of Oncology","text":"Patient Feature Configuration Pictologics Value Team Value Error STS_002_MRI stat_min Simon. L1 -162.1 -248.1 86 STS_002_MRI stat_p10 Simon. L2 -58.49 -153.3 94.83 STS_002_MRI stat_iqr Simon. L2 42.45 130.4 87.93 STS_002_MRI stat_mad Simon. L2 30.24 70.97 40.73 STS_002_MRI stat_rmad Simon. L2 18.35 53.14 34.79 STS_002_MRI stat_medad Simon. L2 29.91 70.92 41.02 STS_003_MRI stat_min Simon. L1 -136.6 -245.4 108.8 STS_003_MRI stat_p10 Simon. L2 -53.76 -149.2 95.45 STS_003_MRI stat_iqr Simon. L2 42.36 133 90.65 STS_003_MRI stat_mad Simon. L2 28.55 71.27 42.72 Show 308 more mismatches... PatientFeatureConfigurationPictologics ValueTeam ValueError STS_003_MRIstat_rmadSimon. L218.153.135 STS_003_MRIstat_medadSimon. L228.3571.2742.91 STS_006_MRIstat_minSimon. L2-170.3-264.794.38 STS_006_MRIstat_p10Simon. L2-27.21-113.986.73 STS_006_MRIstat_iqrSimon. L223.8182.6158.8 STS_006_MRIstat_madSimon. L218.8955.5236.64 STS_006_MRIstat_rmadSimon. L210.4635.8225.36 STS_006_MRIstat_medadSimon. L218.8855.3936.51 STS_005_MRIstat_p10Simon. L2-34.38-107.172.73 STS_005_MRIstat_madSimon. L223.0446.0122.97 STS_005_MRIstat_medadSimon. L222.7144.621.88 STS_008_MRIstat_iqrSimon. L237.8784.846.93 STS_008_MRIstat_madSimon. L228.6652.4923.83 STS_008_MRIstat_rmadSimon. L216.2135.7619.55 STS_008_MRIstat_medadSimon. L228.6452.2523.61 STS_005_CTstat_minSimon. L1-156.7-312.6155.9 STS_005_CTstat_minSimon. L2-313-209.3103.7 STS_008_CTstat_skewSimon. L10.3132-3.9794.292 STS_008_CTstat_skewSimon. L2-4.346-1.9672.378 STS_008_CTstat_kurtSimon. L16.21830.0623.84 STS_008_CTstat_kurtSimon. L233.065.38327.68 STS_008_CTstat_minSimon. L1-187-316.1129.1 STS_008_CTstat_minSimon. L2-313.1-186.2126.9 STS_010_MRIstat_minSimon. L2-111-216105 STS_010_MRIstat_iqrSimon. L216.8568.5751.72 STS_010_MRIstat_madSimon. L211.0437.6126.57 STS_010_MRIstat_rmadSimon. L27.12127.720.58 STS_010_MRIstat_medadSimon. L211.0437.5126.47 STS_007_MRIstat_minSimon. L2-226.3-308.582.25 STS_007_MRIstat_madSimon. L220.9442.921.96 STS_007_MRIstat_medadSimon. L220.9442.1321.19 STS_010_CTstat_minSimon. L2-36.69-160123.4 STS_007_CTstat_minSimon. L1-177.7-258.981.19 STS_007_CTstat_minSimon. L2-258.7-109.5149.2 STS_011_MRIstat_minSimon. L2-219.7-326.6106.9 STS_011_CTstat_minSimon. L2-72.74-183.5110.8 STS_015_MRIstat_varSimon. L233091.08e+047491 STS_015_MRIstat_minSimon. L2-244.1-400.9156.9 STS_015_MRIstat_p10Simon. L2-80.22-176.696.41 STS_015_MRIstat_iqrSimon. L252.75158.6105.9 STS_015_MRIstat_madSimon. L240.8486.4245.58 STS_015_MRIstat_rmadSimon. L22464.5940.59 STS_015_MRIstat_medadSimon. L240.5385.9545.43 STS_009_CTstat_skewSimon. L10.9229-5.1816.104 STS_009_CTstat_skewSimon. L2-2.12-4.9492.829 STS_009_CTstat_kurtSimon. L117.2654.5937.33 STS_009_CTstat_minSimon. L1-286.6-428.3141.7 STS_016_PETstat_skewSimon. L10.44912.4922.043 STS_016_PETstat_skewSimon. L22.5320.6281.904 STS_016_PETstat_kurtSimon. L12.38919.3116.92 STS_016_PETstat_kurtSimon. L219.572.31117.26 STS_014_MRIstat_minSimon. L2-166-257.691.57 STS_014_MRIstat_madSimon. L212.2334.5822.35 STS_014_MRIstat_medadSimon. L212.2334.5822.36 STS_013_MRIstat_minSimon. L2-350.5-458.2107.7 STS_013_MRIstat_iqrSimon. L264.3611651.62 STS_013_MRIstat_madSimon. L122.8243.520.68 STS_013_MRIstat_madSimon. L243.5771.8228.25 STS_013_MRIstat_rmadSimon. L227.5848.6321.05 STS_013_MRIstat_medadSimon. L122.8243.4520.63 STS_013_MRIstat_medadSimon. L243.5271.5728.05 STS_018_MRIstat_madSimon. L123.1949.3326.14 STS_018_MRIstat_madSimon. L249.0574.125.05 STS_018_MRIstat_rmadSimon. L114.7132.0917.38 STS_018_MRIstat_rmadSimon. L231.7948.8717.07 STS_018_MRIstat_medadSimon. L123.1949.3226.12 STS_018_MRIstat_medadSimon. L249.0473.9924.95 STS_019_CTstat_varSimon. L2204694617415 STS_019_CTstat_skewSimon. L1-1.3520.59091.943 STS_019_CTstat_p90Simon. L244.16158.7114.6 STS_019_CTstat_iqrSimon. L248.25107.659.34 STS_019_CTstat_madSimon. L110.1933.3823.19 STS_019_CTstat_madSimon. L233.1776.7943.62 STS_019_CTstat_rmadSimon. L221.0552.2831.23 STS_019_CTstat_medadSimon. L110.1833.3423.16 STS_019_CTstat_medadSimon. L233.1374.5541.42 STS_017_MRIstat_madSimon. L222.0242.5420.51 STS_017_MRIstat_medadSimon. L222.0242.5220.5 STS_018_CTstat_skewSimon. L2-1.1440.95822.103 STS_018_CTstat_minSimon. L2-234.9-102.5132.4 STS_019_MRIstat_minSimon. L1-133.2-244.6111.4 STS_019_MRIstat_iqrSimon. L123.1266.6343.51 STS_019_MRIstat_iqrSimon. L267.11138.171 STS_019_MRIstat_madSimon. L116.6242.7226.09 STS_019_MRIstat_madSimon. L242.8378.1935.36 STS_019_MRIstat_rmadSimon. L19.88528.4118.52 STS_019_MRIstat_rmadSimon. L228.5757.1828.62 STS_019_MRIstat_medadSimon. L116.5941.8225.23 STS_019_MRIstat_medadSimon. L241.9577.8435.89 STS_013_CTstat_kurtSimon. L139.0219.1519.88 STS_013_CTstat_minSimon. L1-304.5-465.8161.4 STS_013_CTstat_p10Simon. L2-29.12-97.4968.37 STS_013_CTstat_madSimon. L226.3650.0223.66 STS_013_CTstat_medadSimon. L225.9546.9220.97 STS_017_CTstat_skewSimon. L21.28-3.234.51 STS_017_CTstat_kurtSimon. L29.69136.1926.49 STS_017_CTstat_minSimon. L1-80.3-176.195.83 STS_017_CTstat_minSimon. L2-176.7-359.5182.8 STS_022_MRIstat_skewSimon. L1-1.8041.2733.077 STS_022_MRIstat_skewSimon. L20.3835-1.6011.985 STS_022_MRIstat_kurtSimon. L155.1514.0641.09 STS_022_MRIstat_minSimon. L2-214.2-405.3191.1 STS_023_MRIstat_minSimon. L2-123.9-235.9112 STS_022_CTstat_skewSimon. L24.0171.2342.782 STS_022_CTstat_kurtSimon. L257.94.63253.27 STS_022_CTstat_minSimon. L1-221.9-84.75137.1 STS_022_CTstat_rangeSimon. L1529.4207.5321.9 STS_022_CTstat_rangeSimon. L2542.8244.8298 STS_023_CTstat_skewSimon. L2-0.7073-2.71.992 STS_023_CTstat_minSimon. L1-226.5-345.7119.2 STS_023_CTstat_madSimon. L221.9844.4422.46 STS_024_MRIstat_minSimon. L2-235.6-311.475.78 STS_024_MRIstat_p10Simon. L2-44.01-151107 STS_024_MRIstat_iqrSimon. L228.98105.576.5 STS_024_MRIstat_madSimon. L225.4867.6642.18 STS_024_MRIstat_rmadSimon. L213.1445.9132.77 STS_024_MRIstat_medadSimon. L225.1866.841.62 STS_025_MRIstat_iqrSimon. L253.23125.372.08 STS_025_MRIstat_madSimon. L239.8371.6431.81 STS_025_MRIstat_rmadSimon. L223.5550.9527.39 STS_025_MRIstat_medadSimon. L239.1871.2532.07 STS_021_CTstat_qcodSimon. L11.846e+05572.21.84e+05 STS_024_CTstat_minSimon. L1-44.84-126.481.6 STS_025_CTstat_iqrSimon. L221.7871.1949.42 STS_025_CTstat_madSimon. L216.1343.2827.15 STS_025_CTstat_rmadSimon. L29.75928.8919.13 STS_025_CTstat_medadSimon. L216.0542.8226.77 STS_026_CTstat_skewSimon. L24.569-1.5986.167 STS_026_CTstat_kurtSimon. L124.726.26418.45 STS_026_CTstat_kurtSimon. L248.5326.8321.7 STS_026_CTstat_minSimon. L2-134.8-336.8202 STS_026_CTstat_covCoif3 LHH L1-9.46e+04-4.209e+045.25e+04 STS_028_MRIstat_minSimon. L2-183.9-312.5128.7 STS_028_MRIstat_p10Simon. L2-26.01-112.386.31 STS_028_MRIstat_iqrSimon. L22294.7172.71 STS_028_MRIstat_madSimon. L218.2959.5341.23 STS_028_MRIstat_rmadSimon. L29.59439.5929.99 STS_028_MRIstat_medadSimon. L218.2759.341.03 STS_028_CTstat_skewSimon. L20.7936-1.0651.859 STS_028_CTstat_minSimon. L2-105.1-220.1115.1 STS_020_MRIstat_skewSimon. L11.619-0.13411.754 STS_020_MRIstat_kurtSimon. L139.2821.1918.09 STS_020_MRIstat_minSimon. L1-167-280.4113.4 STS_020_CTstat_skewSimon. L10.5691-3.7344.303 STS_020_CTstat_kurtSimon. L112.137.525.4 STS_020_CTstat_kurtSimon. L235.7618.8716.89 STS_020_CTstat_minSimon. L2-470.2-383.386.85 STS_027_MRIstat_minSimon. L1-133.3-208.975.64 STS_027_CTstat_skewSimon. L1-1.224-4.0462.822 STS_027_CTstat_kurtSimon. L111.1128.9817.87 STS_027_CTstat_minSimon. L1-300-401.4101.4 STS_032_MRIstat_minSimon. L2-386.9-281.6105.3 STS_032_MRIstat_p10Simon. L2-44.43-130.586.11 STS_032_MRIstat_madSimon. L229.4854.3524.86 STS_032_MRIstat_rmadSimon. L21534.3519.35 STS_032_MRIstat_medadSimon. L229.0352.9723.94 STS_032_CTstat_minSimon. L1-50.07-154.4104.4 STS_032_CTstat_iqrSimon. L214.1261.2947.17 STS_032_CTstat_madSimon. L210.3634.8324.47 STS_032_CTstat_rmadSimon. L25.98625.2919.31 STS_032_CTstat_medadSimon. L210.3634.6624.3 STS_031_CTstat_kurtSimon. L10.289816.2715.98 STS_033_MRIstat_minSimon. L1-283.9-36581.1 STS_033_MRIstat_madSimon. L218.842.323.5 STS_033_MRIstat_rmadSimon. L29.21725.2916.07 STS_033_MRIstat_medadSimon. L218.7341.9523.23 STS_030_MRIstat_minSimon. L1-257.3-355.898.53 STS_030_MRIstat_iqrSimon. L250.6292.5541.94 STS_030_MRIstat_madSimon. L234.658.223.59 STS_030_MRIstat_rmadSimon. L221.6238.8717.26 STS_030_MRIstat_medadSimon. L234.658.223.59 STS_033_CTstat_kurtSimon. L11031.8321.83 STS_033_CTstat_kurtSimon. L233.0210.4722.54 STS_034_MRIstat_skewSimon. L11.504-0.44441.948 STS_034_MRIstat_minSimon. L2-204.1-282.378.23 STS_034_MRIstat_madSimon. L218.7338.5419.8 STS_034_MRIstat_medadSimon. L218.7338.5119.78 STS_035_MRIstat_varSimon. L252091.164e+046435 STS_035_MRIstat_minSimon. L1-154.6-247.492.8 STS_035_MRIstat_minSimon. L2-251.3-382.6131.2 STS_035_MRIstat_p10Simon. L2-107.2-202.194.82 STS_035_MRIstat_iqrSimon. L131.7682.5150.75 STS_035_MRIstat_iqrSimon. L284.31155.971.57 STS_035_MRIstat_madSimon. L124.0654.3230.26 STS_035_MRIstat_madSimon. L254.9789.2734.31 STS_035_MRIstat_rmadSimon. L114.2635.3821.12 STS_035_MRIstat_rmadSimon. L236.0164.8528.84 STS_035_MRIstat_medadSimon. L124.0654.1330.08 STS_035_MRIstat_medadSimon. L254.7887.4432.66 STS_035_CTstat_varSimon. L139091.691e+041.3e+04 STS_035_CTstat_p10Simon. L1-38.56-274.7236.1 STS_035_CTstat_iqrSimon. L135.1694.8659.69 STS_035_CTstat_iqrSimon. L293.81212.5118.7 STS_035_CTstat_madSimon. L137.0298.3261.3 STS_035_CTstat_rmadSimon. L116.7859.4442.66 STS_035_CTstat_rmadSimon. L259.6486.1626.52 STS_035_CTstat_medadSimon. L136.7889.0252.24 STS_034_CTstat_skewSimon. L23.757-4.1327.888 STS_034_CTstat_minSimon. L2-196.6-356.3159.7 STS_038_MRIstat_kurtCoif3 HHH L265.9232.2133.71 STS_038_MRIstat_madSimon. L220.4147.0926.68 STS_038_MRIstat_rmadSimon. L211.4628.0516.59 STS_038_MRIstat_medadSimon. L220.3746.8826.51 STS_036_MRIstat_minSimon. L2-297.9-423.3125.4 STS_036_CTstat_kurtSimon. L227.518.67618.83 STS_038_CTstat_skewSimon. L22.921-6.058.971 STS_038_CTstat_minSimon. L1-167.5-321.4153.9 STS_038_CTstat_qcodSimon. L11.217e+05-23.141.217e+05 STS_037_MRIstat_minSimon. L1-228.1-321.593.4 STS_037_MRIstat_p10Simon. L2-28.68-112.884.15 STS_037_MRIstat_madSimon. L220.747.7127.01 STS_037_MRIstat_rmadSimon. L210.2526.4816.23 STS_037_MRIstat_medadSimon. L220.6146.1625.55 STS_041_MRIstat_varSimon. L2346299696508 STS_041_MRIstat_minSimon. L1-245.2-323.778.51 STS_041_MRIstat_p10Simon. L2-100.2-172.272 STS_041_MRIstat_iqrSimon. L125.0467.9642.91 STS_041_MRIstat_iqrSimon. L267.94152.584.54 STS_041_MRIstat_madSimon. L121.6345.423.77 STS_041_MRIstat_madSimon. L245.2683.1237.86 STS_041_MRIstat_rmadSimon. L111.2829.8318.56 STS_041_MRIstat_rmadSimon. L229.6862.0232.34 STS_041_MRIstat_medadSimon. L121.4244.5723.15 STS_041_MRIstat_medadSimon. L244.483.0738.68 STS_042_MRIstat_varSimon. L234341.767e+041.424e+04 STS_042_MRIstat_minSimon. L1-148-258.1110.1 STS_042_MRIstat_minSimon. L2-249.7-396.2146.5 STS_042_MRIstat_p10Simon. L2-70.57-236.9166.3 STS_042_MRIstat_iqrSimon. L266.19239.3173.2 STS_042_MRIstat_madSimon. L118.641.3422.74 STS_042_MRIstat_madSimon. L243.76117.373.53 STS_042_MRIstat_rmadSimon. L19.2525.9316.68 STS_042_MRIstat_rmadSimon. L228.295.3267.12 STS_042_MRIstat_medadSimon. L118.5741.3422.77 STS_042_MRIstat_medadSimon. L243.75114.170.37 STS_042_MRIstat_rmsSimon. L258.6140.682.03 STS_037_CTstat_minSimon. L1-194.7-293.398.58 STS_037_CTstat_minSimon. L2-291.9-191.9100 STS_039_CTstat_minSimon. L2-62.47-219.8157.3 STS_040_CTstat_skewSimon. L1-0.2984-2.1471.849 STS_040_CTstat_skewSimon. L2-2.17-0.33621.834 STS_040_CTstat_kurtSimon. L12.08418.8416.76 STS_040_CTstat_kurtSimon. L219.092.71716.37 STS_040_CTstat_minSimon. L2-230.7-147.683.14 STS_045_PETstat_skewSimon. L20.8012.9412.14 STS_042_CTstat_minSimon. L1-83.35-186.7103.4 STS_042_CTstat_iqrSimon. L225.874.8949.09 STS_042_CTstat_madSimon. L217.7338.4420.71 STS_042_CTstat_rmadSimon. L211.3829.3818 STS_042_CTstat_medadSimon. L217.5938.2120.62 STS_043_MRIstat_minSimon. L2-150.6-242.892.12 STS_043_MRIstat_madSimon. L214.2236.6722.44 STS_043_MRIstat_rmadSimon. L27.80823.916.09 STS_043_MRIstat_medadSimon. L214.2136.5122.29 STS_043_MRIstat_covCoif3 LHH L1-1.382e+053.861e+041.769e+05 STS_043_CTstat_qcodSimon. L1233.9-2.371e+042.395e+04 STS_047_MRIstat_minSimon. L1-240.7-326.285.5 STS_047_MRIstat_p10Simon. L2-63.49-135.371.82 STS_047_MRIstat_iqrSimon. L241.994.3852.48 STS_047_MRIstat_madSimon. L234.1363.9329.8 STS_047_MRIstat_rmadSimon. L218.9841.3122.33 STS_047_MRIstat_medadSimon. L234.0562.628.55 STS_046_MRIstat_minSimon. L1-202-300.998.91 STS_046_MRIstat_madSimon. L217.6238.7621.14 STS_046_MRIstat_medadSimon. L217.6138.7621.15 STS_047_CTstat_skewSimon. L12.862-2.8555.717 STS_047_CTstat_kurtSimon. L239.8210.1529.67 STS_047_CTstat_minSimon. L1-170.8-333.2162.4 STS_045_CTstat_skewSimon. L1-0.5661-2.6352.069 STS_045_CTstat_kurtSimon. L16.61722.8416.22 STS_045_CTstat_minSimon. L1-234.6-317.683.06 STS_048_CTstat_skewSimon. L1-0.0142.1072.121 STS_048_CTstat_skewSimon. L22.111-3.2455.356 STS_048_CTstat_kurtSimon. L10.220316.2516.03 STS_048_CTstat_kurtSimon. L216.2443.5827.33 STS_048_CTstat_minSimon. L2-140.7-364.3223.6 STS_046_CTstat_skewSimon. L20.0237-3.8773.901 STS_046_CTstat_kurtSimon. L12.21227.7125.5 STS_046_CTstat_minSimon. L1-261.4-361.7100.3 STS_050_MRIstat_iqrSimon. L218.0464.2846.24 STS_050_MRIstat_madSimon. L212.4135.7423.33 STS_050_MRIstat_rmadSimon. L27.57926.218.62 STS_050_MRIstat_medadSimon. L212.3735.6623.29 STS_049_MRIstat_minSimon. L1-198.9-289.190.18 STS_049_MRIstat_minSimon. L2-287.7-365.377.67 STS_049_MRIstat_madSimon. L218.3740.1421.77 STS_049_MRIstat_medadSimon. L218.3539.9721.62 STS_050_CTstat_skewSimon. L20.0976-3.3763.473 STS_050_CTstat_kurtSimon. L13.61621.7218.11 STS_050_CTstat_minSimon. L1-86.22-316.1229.9 STS_050_CTstat_qcodCoif3 LHH L13.174e+04-1.262e+051.579e+05 STS_051_MRIstat_minSimon. L1-178.9-384.5205.6 STS_051_MRIstat_p10Simon. L2-39.96-128.488.45 STS_051_MRIstat_iqrSimon. L235.2689.6654.4 STS_051_MRIstat_madSimon. L226.960.3833.48 STS_051_MRIstat_rmadSimon. L215.1739.724.52 STS_051_MRIstat_medadSimon. L226.8359.2432.41 STS_049_CTstat_skewSimon. L1-4.806-1.3453.461 STS_049_CTstat_skewSimon. L2-1.3640.54981.913 STS_049_CTstat_kurtSimon. L198.9324.6374.3 STS_049_CTstat_kurtSimon. L224.793.99920.79 STS_049_CTstat_minSimon. L1-335.5-441.9106.4 STS_049_CTstat_minSimon. L2-443.2-354.788.45 STS_049_CTstat_iqrSimon. L216.1859.1142.94 STS_049_CTstat_madSimon. L217.1544.3227.16 STS_049_CTstat_rmadSimon. L27.31225.5218.21 STS_049_CTstat_medadSimon. L217.1544.2127.05 STS_049_CTstat_qcodCoif3 LHH L177781.1e+051.023e+05"},{"location":"quality/","title":"Code Quality Report","text":""},{"location":"quality/#test-coverage","title":"Test Coverage","text":"<p>Status: \u2705 Pass (100.00% Coverage)</p>"},{"location":"quality/#static-type-checking-mypy","title":"Static Type Checking (Mypy)","text":"<p>Status: \u2705 Pass (0 Errors)</p>"},{"location":"quality/#linting-ruff","title":"Linting (Ruff)","text":"<p>Status: \u2705 Pass (0 Issues)</p>"},{"location":"api/filters/","title":"Filters API","text":"<p>Pictologics provides IBSI 2-compliant convolutional filters for image response map generation.</p>"},{"location":"api/filters/#overview","title":"Overview","text":"<p>All filters can be used via the <code>RadiomicsPipeline</code> <code>filter</code> step or called directly:</p> <pre><code># Pipeline usage\n{\"step\": \"filter\", \"params\": {\"type\": \"log\", \"sigma_mm\": 1.5}}\n\n# Direct usage\nfrom pictologics.filters import laplacian_of_gaussian, BoundaryCondition\nresponse = laplacian_of_gaussian(image.array, sigma_mm=1.5, spacing_mm=image.spacing)\n</code></pre>"},{"location":"api/filters/#available-filters","title":"Available Filters","text":"Filter Function Use Case Mean <code>mean_filter</code> Local averaging LoG <code>laplacian_of_gaussian</code> Edge/blob detection Laws <code>laws_filter</code> Texture energy Gabor <code>gabor_filter</code> Directional patterns Wavelet <code>wavelet_transform</code> Multi-resolution analysis Simoncelli <code>simoncelli_wavelet</code> Non-separable wavelet Riesz <code>riesz_transform</code>, <code>riesz_log</code>, <code>riesz_simoncelli</code> Rotation-equivariant transforms"},{"location":"api/filters/#boundary-conditions","title":"Boundary Conditions","text":""},{"location":"api/filters/#pictologics.filters.BoundaryCondition","title":"<code>pictologics.filters.BoundaryCondition</code>","text":"<p>               Bases: <code>Enum</code></p> <p>IBSI 2 boundary conditions for image padding (GBYQ).</p> <p>Maps to scipy.ndimage mode parameter values.</p> Source code in <code>pictologics/filters/base.py</code> <pre><code>class BoundaryCondition(Enum):\n    \"\"\"\n    IBSI 2 boundary conditions for image padding (GBYQ).\n\n    Maps to scipy.ndimage mode parameter values.\n    \"\"\"\n\n    ZERO = \"constant\"  # Zero padding (Z3VE)\n    NEAREST = \"nearest\"  # Nearest value padding (SIJG)\n    PERIODIC = \"wrap\"  # Periodic/wrap padding (Z7YO)\n    MIRROR = \"reflect\"  # Mirror/symmetric padding (ZDTV)\n</code></pre>"},{"location":"api/filters/#pictologics.filters.FilterResult","title":"<code>pictologics.filters.FilterResult</code>  <code>dataclass</code>","text":"<p>Container for filter response maps and metadata.</p> Source code in <code>pictologics/filters/base.py</code> <pre><code>@dataclass\nclass FilterResult:\n    \"\"\"Container for filter response maps and metadata.\"\"\"\n\n    response_map: npt.NDArray[np.floating[Any]]\n    filter_name: str\n    filter_params: Dict[str, Any]\n\n    @property\n    def shape(self) -&gt; tuple[int, ...]:\n        \"\"\"Shape of the response map.\"\"\"\n        return self.response_map.shape  # type: ignore[no-any-return]\n\n    @property\n    def dtype(self) -&gt; np.dtype[Any]:\n        \"\"\"Data type of the response map.\"\"\"\n        return self.response_map.dtype  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/filters/#pictologics.filters.FilterResult.dtype","title":"<code>dtype</code>  <code>property</code>","text":"<p>Data type of the response map.</p>"},{"location":"api/filters/#pictologics.filters.FilterResult.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Shape of the response map.</p>"},{"location":"api/filters/#pictologics.filters.LAWS_KERNELS","title":"<code>pictologics.filters.LAWS_KERNELS = _LAWS_KERNELS</code>  <code>module-attribute</code>","text":"<p>Dictionary of normalized Laws kernels (IBSI 2 Table 6).</p>"},{"location":"api/filters/#filter-functions","title":"Filter Functions","text":""},{"location":"api/filters/#pictologics.filters.mean_filter","title":"<code>pictologics.filters.mean_filter(image, support=15, boundary=BoundaryCondition.ZERO)</code>","text":"<p>Apply 3D mean filter (IBSI code: S60F).</p> <p>The mean filter computes the average intensity over an M\u00d7M\u00d7M spatial support. Per IBSI 2 Eq. 2.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray[floating[Any]]</code> <p>3D input image array</p> required <code>support</code> <code>int</code> <p>Filter support M in voxels (must be odd, YNOF)</p> <code>15</code> <code>boundary</code> <code>Union[BoundaryCondition, str]</code> <p>Boundary condition for padding (GBYQ)</p> <code>ZERO</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>Response map with same dimensions as input</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If support is not an odd positive integer</p> Example <p>Apply Mean filter with 15-voxel support:</p> <pre><code>import numpy as np\nfrom pictologics.filters import mean_filter\n\n# Create dummy 3D image\nimage = np.random.rand(50, 50, 50)\n\n# Apply filter\nresponse = mean_filter(image, support=15, boundary=\"zero\")\n</code></pre> Note <p>Support M is defined in voxel units as per IBSI specification.</p> Source code in <code>pictologics/filters/mean.py</code> <pre><code>def mean_filter(\n    image: npt.NDArray[np.floating[Any]],\n    support: int = 15,\n    boundary: Union[BoundaryCondition, str] = BoundaryCondition.ZERO,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Apply 3D mean filter (IBSI code: S60F).\n\n    The mean filter computes the average intensity over an M\u00d7M\u00d7M\n    spatial support. Per IBSI 2 Eq. 2.\n\n    Args:\n        image: 3D input image array\n        support: Filter support M in voxels (must be odd, YNOF)\n        boundary: Boundary condition for padding (GBYQ)\n\n    Returns:\n        Response map with same dimensions as input\n\n    Raises:\n        ValueError: If support is not an odd positive integer\n\n    Example:\n        Apply Mean filter with 15-voxel support:\n\n        ```python\n        import numpy as np\n        from pictologics.filters import mean_filter\n\n        # Create dummy 3D image\n        image = np.random.rand(50, 50, 50)\n\n        # Apply filter\n        response = mean_filter(image, support=15, boundary=\"zero\")\n        ```\n\n    Note:\n        Support M is defined in voxel units as per IBSI specification.\n    \"\"\"\n    # Validate support\n    if support &lt; 1 or support % 2 == 0:\n        raise ValueError(f\"Support must be an odd positive integer, got {support}\")\n\n    # Convert to float32 as required by IBSI\n    image = ensure_float32(image)\n\n    # Handle string boundary condition\n    if isinstance(boundary, str):\n        boundary = BoundaryCondition[boundary.upper()]\n\n    mode = get_scipy_mode(boundary)\n\n    # Apply uniform filter (3D)\n    return uniform_filter(image, size=support, mode=mode)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/filters/#pictologics.filters.laplacian_of_gaussian","title":"<code>pictologics.filters.laplacian_of_gaussian(image, sigma_mm, spacing_mm=1.0, truncate=4.0, boundary=BoundaryCondition.ZERO)</code>","text":"<p>Apply 3D Laplacian of Gaussian filter (IBSI code: L6PA).</p> <p>The LoG is a band-pass, spherically symmetric operator. Per IBSI 2 Eq. 3.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray[floating[Any]]</code> <p>3D input image array</p> required <code>sigma_mm</code> <code>float</code> <p>Standard deviation in mm (\u03c3*, 41LN)</p> required <code>spacing_mm</code> <code>Union[float, Tuple[float, float, float]]</code> <p>Voxel spacing in mm (scalar for isotropic, or tuple)</p> <code>1.0</code> <code>truncate</code> <code>float</code> <p>Filter size cutoff in \u03c3 units (default 4.0, WGPM)</p> <code>4.0</code> <code>boundary</code> <code>Union[BoundaryCondition, str]</code> <p>Boundary condition for padding (GBYQ)</p> <code>ZERO</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>Response map with same dimensions as input</p> Example <p>Apply LoG filter with 5.0mm sigma on an image with 2.0mm spacing:</p> <pre><code>import numpy as np\nfrom pictologics.filters import laplacian_of_gaussian\n\n# Create dummy 3D image\nimage = np.random.rand(50, 50, 50)\n\n# Apply filter\nresponse = laplacian_of_gaussian(\n    image,\n    sigma_mm=5.0,\n    spacing_mm=(2.0, 2.0, 2.0),\n    truncate=4.0\n)\n</code></pre> Note <ul> <li>\u03c3 is converted from mm to voxels: \u03c3_voxels = \u03c3_mm / spacing_mm</li> <li>Filter size: M = 1 + 2\u230ad\u00d7\u03c3 + 0.5\u230b where d=truncate</li> <li>The kernel should sum to approximately 0 (zero-mean)</li> </ul> Source code in <code>pictologics/filters/log.py</code> <pre><code>def laplacian_of_gaussian(\n    image: npt.NDArray[np.floating[Any]],\n    sigma_mm: float,\n    spacing_mm: Union[float, Tuple[float, float, float]] = 1.0,\n    truncate: float = 4.0,\n    boundary: Union[BoundaryCondition, str] = BoundaryCondition.ZERO,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Apply 3D Laplacian of Gaussian filter (IBSI code: L6PA).\n\n    The LoG is a band-pass, spherically symmetric operator. Per IBSI 2 Eq. 3.\n\n    Args:\n        image: 3D input image array\n        sigma_mm: Standard deviation in mm (\u03c3*, 41LN)\n        spacing_mm: Voxel spacing in mm (scalar for isotropic, or tuple)\n        truncate: Filter size cutoff in \u03c3 units (default 4.0, WGPM)\n        boundary: Boundary condition for padding (GBYQ)\n\n    Returns:\n        Response map with same dimensions as input\n\n    Example:\n        Apply LoG filter with 5.0mm sigma on an image with 2.0mm spacing:\n\n        ```python\n        import numpy as np\n        from pictologics.filters import laplacian_of_gaussian\n\n        # Create dummy 3D image\n        image = np.random.rand(50, 50, 50)\n\n        # Apply filter\n        response = laplacian_of_gaussian(\n            image,\n            sigma_mm=5.0,\n            spacing_mm=(2.0, 2.0, 2.0),\n            truncate=4.0\n        )\n        ```\n\n    Note:\n        - \u03c3 is converted from mm to voxels: \u03c3_voxels = \u03c3_mm / spacing_mm\n        - Filter size: M = 1 + 2\u230ad\u00d7\u03c3 + 0.5\u230b where d=truncate\n        - The kernel should sum to approximately 0 (zero-mean)\n    \"\"\"\n    # Convert to float32 as required by IBSI\n    image = ensure_float32(image)\n\n    # Handle scalar spacing\n    if isinstance(spacing_mm, (int, float)):\n        spacing_mm = (float(spacing_mm),) * 3\n\n    # Convert sigma from mm to voxels for each axis\n    sigma_voxels = tuple(sigma_mm / s for s in spacing_mm)\n\n    # Handle string boundary condition\n    if isinstance(boundary, str):\n        boundary = BoundaryCondition[boundary.upper()]\n\n    mode = get_scipy_mode(boundary)\n\n    # Apply Laplacian of Gaussian\n    # scipy.ndimage.gaussian_laplace already implements LoG correctly\n    return gaussian_laplace(image, sigma=sigma_voxels, mode=mode, truncate=truncate)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/filters/#pictologics.filters.laws_filter","title":"<code>pictologics.filters.laws_filter(image, kernels, boundary=BoundaryCondition.ZERO, rotation_invariant=False, pooling='max', compute_energy=False, energy_distance=7, use_parallel=None)</code>","text":"<p>Apply 3D Laws kernel filter (IBSI code: JTXT).</p> <p>Laws kernels detect texture patterns via separable 1D filters combined into 2D/3D filters via outer products.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray[floating[Any]]</code> <p>3D input image array</p> required <code>kernels</code> <code>str</code> <p>Kernel specification as string, e.g., \"E5L5S5\" for 3D</p> required <code>boundary</code> <code>Union[BoundaryCondition, str]</code> <p>Boundary condition for padding (GBYQ)</p> <code>ZERO</code> <code>rotation_invariant</code> <code>bool</code> <p>If True, apply pseudo-rotational invariance (O1AQ)                 using max pooling over 24 right-angle rotations</p> <code>False</code> <code>pooling</code> <code>str</code> <p>Pooling method for rotation invariance (\"max\", \"average\", \"min\")</p> <code>'max'</code> <code>compute_energy</code> <code>bool</code> <p>If True, compute texture energy image (PQSD)</p> <code>False</code> <code>energy_distance</code> <code>int</code> <p>Chebyshev distance \u03b4 for energy computation (I176)</p> <code>7</code> <code>use_parallel</code> <code>Union[bool, None]</code> <p>If True, use parallel processing for rotation_invariant mode. If None (default), auto-enables for images &gt; ~128\u00b3 voxels. Only affects rotation_invariant mode.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>Response map (or energy image if compute_energy=True)</p> Example <p>Apply Laws E5L5S5 kernel with rotation invariance and texture energy:</p> <pre><code>import numpy as np\nfrom pictologics.filters import laws_filter\n\n# Create dummy 3D image\nimage = np.random.rand(50, 50, 50)\n\n# Apply filter\nresponse = laws_filter(\n    image,\n    \"E5L5S5\",\n    rotation_invariant=True,\n    pooling=\"max\",\n    compute_energy=True,\n    energy_distance=7\n)\n</code></pre> Note <ul> <li>Kernels are normalized (deviate from Laws' original unnormalized)</li> <li>Energy is computed as: mean(|h|) over \u03b4 neighborhood</li> <li>For rotation invariance, energy is computed after pooling</li> <li>Uses separable 1D convolutions for ~8x speedup over full 3D</li> </ul> Source code in <code>pictologics/filters/laws.py</code> <pre><code>def laws_filter(\n    image: npt.NDArray[np.floating[Any]],\n    kernels: str,\n    boundary: Union[BoundaryCondition, str] = BoundaryCondition.ZERO,\n    rotation_invariant: bool = False,\n    pooling: str = \"max\",\n    compute_energy: bool = False,\n    energy_distance: int = 7,\n    use_parallel: Union[bool, None] = None,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Apply 3D Laws kernel filter (IBSI code: JTXT).\n\n    Laws kernels detect texture patterns via separable 1D filters combined\n    into 2D/3D filters via outer products.\n\n    Args:\n        image: 3D input image array\n        kernels: Kernel specification as string, e.g., \"E5L5S5\" for 3D\n        boundary: Boundary condition for padding (GBYQ)\n        rotation_invariant: If True, apply pseudo-rotational invariance (O1AQ)\n                            using max pooling over 24 right-angle rotations\n        pooling: Pooling method for rotation invariance (\"max\", \"average\", \"min\")\n        compute_energy: If True, compute texture energy image (PQSD)\n        energy_distance: Chebyshev distance \u03b4 for energy computation (I176)\n        use_parallel: If True, use parallel processing for rotation_invariant mode.\n            If None (default), auto-enables for images &gt; ~128\u00b3 voxels.\n            Only affects rotation_invariant mode.\n\n    Returns:\n        Response map (or energy image if compute_energy=True)\n\n    Example:\n        Apply Laws E5L5S5 kernel with rotation invariance and texture energy:\n\n        ```python\n        import numpy as np\n        from pictologics.filters import laws_filter\n\n        # Create dummy 3D image\n        image = np.random.rand(50, 50, 50)\n\n        # Apply filter\n        response = laws_filter(\n            image,\n            \"E5L5S5\",\n            rotation_invariant=True,\n            pooling=\"max\",\n            compute_energy=True,\n            energy_distance=7\n        )\n        ```\n\n    Note:\n        - Kernels are normalized (deviate from Laws' original unnormalized)\n        - Energy is computed as: mean(|h|) over \u03b4 neighborhood\n        - For rotation invariance, energy is computed after pooling\n        - Uses separable 1D convolutions for ~8x speedup over full 3D\n    \"\"\"\n\n    # Convert to float32\n    image = ensure_float32(image)\n\n    # Parse kernel names (e.g., \"E5L5S5\" -&gt; [\"E5\", \"L5\", \"S5\"])\n    kernel_names = _parse_kernel_string(kernels)\n    if len(kernel_names) != 3:\n        raise ValueError(\n            f\"Expected 3 kernel names for 3D, got {len(kernel_names)}: {kernel_names}\"\n        )\n\n    # Handle boundary condition\n    if isinstance(boundary, str):\n        boundary = BoundaryCondition[boundary.upper()]\n    mode = get_scipy_mode(boundary)\n\n    # Validate pooling method if used\n    if rotation_invariant and pooling not in (\"max\", \"average\", \"min\"):\n        raise ValueError(f\"Unknown pooling method: {pooling}\")\n\n    # Get 1D kernels for separable convolution\n    g1 = LAWS_KERNELS[kernel_names[0]].astype(np.float32)\n    g2 = LAWS_KERNELS[kernel_names[1]].astype(np.float32)\n    g3 = LAWS_KERNELS[kernel_names[2]].astype(np.float32)\n\n    # Auto-detect parallel mode based on image size\n    if use_parallel is None:\n        use_parallel = image.size &gt; _PARALLEL_THRESHOLD\n\n    if rotation_invariant:\n        rotations = _get_rotation_permutations_3d()\n\n        def apply_rotated_convolution(\n            rotation: Tuple[Tuple[int, int, int], Tuple[bool, bool, bool]],\n        ) -&gt; npt.NDArray[np.floating[Any]]:\n            \"\"\"Apply separable convolution with rotated kernels.\"\"\"\n            perm, flips = rotation\n            # Permute kernel order to match rotation\n            rotated_kernels = [g1, g2, g3]\n            rotated_kernels = [rotated_kernels[p] for p in perm]\n            # Flip kernels as needed\n            for i, do_flip in enumerate(flips):\n                if do_flip:\n                    rotated_kernels[i] = rotated_kernels[i][::-1].copy()\n            # Apply separable convolution\n            return _separable_convolve_3d(\n                image, rotated_kernels[0], rotated_kernels[1], rotated_kernels[2], mode\n            )\n\n        if use_parallel:\n            # Parallel processing for large images\n            # Use as_completed to process results as they finish, avoiding\n            # holding all 24 response maps in memory at once.\n            from concurrent.futures import ThreadPoolExecutor, as_completed\n\n            with ThreadPoolExecutor() as executor:\n                # Submit all rotation tasks\n                future_to_rot = {\n                    executor.submit(apply_rotated_convolution, rot): rot\n                    for rot in rotations\n                }\n\n                # Pool responses incrementally\n                result: npt.NDArray[np.floating[Any]] | None = None\n                for _, future in enumerate(as_completed(future_to_rot)):\n                    response = future.result()\n\n                    if result is None:\n                        # Initialize accumulator with first result\n                        result = (\n                            response.astype(np.float64)\n                            if pooling == \"average\"\n                            else response.copy()\n                        )\n                    else:\n                        if result is None:  # pragma: no cover\n                            raise RuntimeError(\"Result should not be None\")\n\n                        res = result\n\n                        if pooling == \"max\":\n                            np.maximum(res, response, out=res)\n                        elif pooling == \"average\":\n                            res += response\n                        elif pooling == \"min\":\n                            np.minimum(res, response, out=res)\n                        else:\n                            raise ValueError(\n                                f\"Unknown pooling method: {pooling}\"\n                            )  # pragma: no cover\n\n                    # Explicitly delete response to free memory\n                    del response\n        else:\n            # Sequential processing for small images (avoid thread overhead)\n            result = None\n            for i, rotation in enumerate(rotations):\n                response = apply_rotated_convolution(rotation)\n\n                if i == 0:\n                    result = (\n                        response.astype(np.float64)\n                        if pooling == \"average\"\n                        else response.copy()\n                    )\n                else:\n                    if result is None:  # pragma: no cover\n                        raise RuntimeError(\"Result should not be None\")\n\n                    res = result\n\n                    if pooling == \"max\":\n                        np.maximum(res, response, out=res)\n                    elif pooling == \"average\":\n                        res += response\n                    elif pooling == \"min\":\n                        np.minimum(res, response, out=res)\n                    else:\n                        raise ValueError(\n                            f\"Unknown pooling method: {pooling}\"\n                        )  # pragma: no cover\n\n        # Finalize average pooling\n        if pooling == \"average\" and result is not None:\n            result /= len(rotations)\n    else:\n        # Non-rotation-invariant: single separable convolution\n        result = _separable_convolve_3d(image, g1, g2, g3, mode)\n\n    # Compute energy image if requested\n    if compute_energy:\n        if result is None:  # pragma: no cover\n            raise RuntimeError(\"Result should not be None\")\n\n        # Energy = mean of absolute values over \u03b4 neighborhood\n        # This is equivalent to uniform_filter on |result|\n        abs_result = np.abs(result)\n        energy_support = 2 * energy_distance + 1\n        result = uniform_filter(abs_result, size=energy_support, mode=mode)\n\n    if result is None:  # pragma: no cover\n        raise RuntimeError(\"Result should not be None\")\n\n    return result  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/filters/#pictologics.filters.gabor_filter","title":"<code>pictologics.filters.gabor_filter(image, sigma_mm, lambda_mm, gamma=1.0, theta=0.0, spacing_mm=1.0, boundary=BoundaryCondition.ZERO, rotation_invariant=False, delta_theta=None, pooling='average', average_over_planes=False, use_parallel=None)</code>","text":"<p>Apply 2D Gabor filter to 3D image (IBSI code: Q88H).</p> <p>The Gabor filter is applied in the axial plane (k1, k2) and optionally averaged over orthogonal planes. Per IBSI 2 Eq. 9.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray[floating[Any]]</code> <p>3D input image array</p> required <code>sigma_mm</code> <code>float</code> <p>Standard deviation of Gaussian envelope in mm (41LN)</p> required <code>lambda_mm</code> <code>float</code> <p>Wavelength in mm (S4N6)</p> required <code>gamma</code> <code>float</code> <p>Spatial aspect ratio (GDR5), typically 0.5 to 2.0</p> <code>1.0</code> <code>theta</code> <code>float</code> <p>Orientation angle in radians (FQER), clockwise in (k1,k2)</p> <code>0.0</code> <code>spacing_mm</code> <code>Union[float, Tuple[float, float, float]]</code> <p>Voxel spacing in mm (scalar or tuple)</p> <code>1.0</code> <code>boundary</code> <code>Union[BoundaryCondition, str]</code> <p>Boundary condition for padding (GBYQ)</p> <code>ZERO</code> <code>rotation_invariant</code> <code>bool</code> <p>If True, average over orientations</p> <code>False</code> <code>delta_theta</code> <code>Optional[float]</code> <p>Orientation step for rotation invariance (XTGK)</p> <code>None</code> <code>pooling</code> <code>str</code> <p>Pooling method (\"average\", \"max\", \"min\")</p> <code>'average'</code> <code>average_over_planes</code> <code>bool</code> <p>If True, average 2D responses over 3 orthogonal planes</p> <code>False</code> <code>use_parallel</code> <code>Union[bool, None]</code> <p>If True, process slices in parallel. If None (default), auto-enables for images &gt; ~80\u00b3 voxels.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>Response map (modulus of complex response)</p> Example <p>Apply Gabor filter with rotation invariance over orthogonal planes:</p> <pre><code>import numpy as np\nfrom pictologics.filters import gabor_filter\n\n# Create dummy 3D image\nimage = np.random.rand(50, 50, 50)\n\n# Apply filter\nresponse = gabor_filter(\n    image,\n    sigma_mm=10.0,\n    lambda_mm=4.0,\n    gamma=0.5,\n    rotation_invariant=True,\n    delta_theta=np.pi/4,\n    average_over_planes=True\n)\n</code></pre> Note <ul> <li>Returns modulus |h| = |g \u2297 f| for feature extraction</li> <li>2D filter applied slice-by-slice, then optionally over planes</li> <li>Uses single complex FFT convolution for ~2x speedup</li> </ul> Source code in <code>pictologics/filters/gabor.py</code> <pre><code>def gabor_filter(\n    image: npt.NDArray[np.floating[Any]],\n    sigma_mm: float,\n    lambda_mm: float,\n    gamma: float = 1.0,\n    theta: float = 0.0,\n    spacing_mm: Union[float, Tuple[float, float, float]] = 1.0,\n    boundary: Union[BoundaryCondition, str] = BoundaryCondition.ZERO,\n    rotation_invariant: bool = False,\n    delta_theta: Optional[float] = None,\n    pooling: str = \"average\",\n    average_over_planes: bool = False,\n    use_parallel: Union[bool, None] = None,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Apply 2D Gabor filter to 3D image (IBSI code: Q88H).\n\n    The Gabor filter is applied in the axial plane (k1, k2) and optionally\n    averaged over orthogonal planes. Per IBSI 2 Eq. 9.\n\n    Args:\n        image: 3D input image array\n        sigma_mm: Standard deviation of Gaussian envelope in mm (41LN)\n        lambda_mm: Wavelength in mm (S4N6)\n        gamma: Spatial aspect ratio (GDR5), typically 0.5 to 2.0\n        theta: Orientation angle in radians (FQER), clockwise in (k1,k2)\n        spacing_mm: Voxel spacing in mm (scalar or tuple)\n        boundary: Boundary condition for padding (GBYQ)\n        rotation_invariant: If True, average over orientations\n        delta_theta: Orientation step for rotation invariance (XTGK)\n        pooling: Pooling method (\"average\", \"max\", \"min\")\n        average_over_planes: If True, average 2D responses over 3 orthogonal planes\n        use_parallel: If True, process slices in parallel. If None (default),\n            auto-enables for images &gt; ~80\u00b3 voxels.\n\n    Returns:\n        Response map (modulus of complex response)\n\n    Example:\n        Apply Gabor filter with rotation invariance over orthogonal planes:\n\n        ```python\n        import numpy as np\n        from pictologics.filters import gabor_filter\n\n        # Create dummy 3D image\n        image = np.random.rand(50, 50, 50)\n\n        # Apply filter\n        response = gabor_filter(\n            image,\n            sigma_mm=10.0,\n            lambda_mm=4.0,\n            gamma=0.5,\n            rotation_invariant=True,\n            delta_theta=np.pi/4,\n            average_over_planes=True\n        )\n        ```\n\n    Note:\n        - Returns modulus |h| = |g \u2297 f| for feature extraction\n        - 2D filter applied slice-by-slice, then optionally over planes\n        - Uses single complex FFT convolution for ~2x speedup\n    \"\"\"\n    # Convert to float32\n    image = ensure_float32(image)\n\n    # Handle spacing\n    if isinstance(spacing_mm, (int, float)):\n        spacing_mm = (float(spacing_mm),) * 3\n\n    # Convert mm to voxels (use in-plane spacing for 2D filter)\n    sigma_voxels = sigma_mm / spacing_mm[0]  # Assume isotropic in-plane\n    lambda_voxels = lambda_mm / spacing_mm[0]\n\n    # Handle boundary\n    if isinstance(boundary, str):\n        boundary = BoundaryCondition[boundary.upper()]\n    mode = get_scipy_mode(boundary)\n\n    # Validate pooling parameter early\n    valid_poolings = (\"max\", \"average\", \"min\")\n    if pooling not in valid_poolings:\n        raise ValueError(f\"Unknown pooling: {pooling}. Must be one of {valid_poolings}\")\n\n    # Auto-detect parallel mode based on image size\n    if use_parallel is None:\n        use_parallel = image.size &gt; _PARALLEL_THRESHOLD\n\n    if rotation_invariant and delta_theta is not None:\n        # Generate orientations from 0 to 2\u03c0\n        n_orientations = int(np.ceil(2 * np.pi / delta_theta))\n        thetas = [i * delta_theta for i in range(n_orientations)]\n    else:\n        thetas = [theta]\n\n    if average_over_planes:\n        # Apply to all 3 orthogonal planes and average with in-place aggregation\n        result: npt.NDArray[np.floating[Any]] | None = None\n        for plane_axis in range(3):\n            plane_response = _apply_gabor_to_plane(\n                image,\n                sigma_voxels,\n                lambda_voxels,\n                gamma,\n                thetas,\n                plane_axis,\n                mode,\n                pooling,\n                use_parallel,\n            )\n            if result is None:\n                result = plane_response.astype(np.float64)\n            else:\n                result += plane_response\n\n        if result is None:  # pragma: no cover\n            raise RuntimeError(\"Result should not be None after plane loop\")\n\n        return (result / 3.0).astype(np.float32)  # type: ignore[union-attr]\n    else:\n        # Apply only to axial plane (axis 2 = k3 slices)\n        return _apply_gabor_to_plane(\n            image,\n            sigma_voxels,\n            lambda_voxels,\n            gamma,\n            thetas,\n            plane_axis=2,\n            mode=mode,\n            pooling=pooling,\n            use_parallel=use_parallel,\n        )\n</code></pre>"},{"location":"api/filters/#pictologics.filters.wavelet_transform","title":"<code>pictologics.filters.wavelet_transform(image, wavelet='db2', level=1, decomposition='LHL', boundary=BoundaryCondition.ZERO, rotation_invariant=False, pooling='average', use_parallel=None)</code>","text":"<p>Apply 3D separable wavelet transform (undecimated/stationary).</p> <p>Uses the \u00e0 trous algorithm for undecimated wavelet decomposition. The transform is translation-invariant (unlike decimated transform).</p> Supported wavelets <ul> <li>\"haar\" (UOUE): Haar wavelet</li> <li>\"db2\", \"db3\": Daubechies wavelets</li> <li>\"coif1\": Coiflet wavelet</li> </ul> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray[floating[Any]]</code> <p>3D input image array</p> required <code>wavelet</code> <code>str</code> <p>Wavelet name (e.g., \"db2\", \"coif1\", \"haar\")</p> <code>'db2'</code> <code>level</code> <code>int</code> <p>Decomposition level (GCEK)</p> <code>1</code> <code>decomposition</code> <code>str</code> <p>Which response map to return, e.g., \"LHL\", \"HHH\"</p> <code>'LHL'</code> <code>boundary</code> <code>Union[BoundaryCondition, str]</code> <p>Boundary condition for padding</p> <code>ZERO</code> <code>rotation_invariant</code> <code>bool</code> <p>If True, average over 24 rotations</p> <code>False</code> <code>pooling</code> <code>str</code> <p>Pooling method for rotation invariance</p> <code>'average'</code> <code>use_parallel</code> <code>Union[bool, None]</code> <p>If True, use parallel processing for rotation_invariant mode. If None (default), auto-enables for images &gt; ~128\u00b3 voxels.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>Response map for the specified decomposition</p> Example <p>Apply Daubechies 2 wavelet transform at level 1, returning LHL coefficients:</p> <pre><code>import numpy as np\nfrom pictologics.filters import wavelet_transform\n\n# Create dummy 3D image\nimage = np.random.rand(50, 50, 50)\n\n# Apply transform\nresponse = wavelet_transform(\n    image,\n    wavelet=\"db2\",\n    level=1,\n    decomposition=\"LHL\"\n)\n</code></pre> Source code in <code>pictologics/filters/wavelets.py</code> <pre><code>def wavelet_transform(\n    image: npt.NDArray[np.floating[Any]],\n    wavelet: str = \"db2\",\n    level: int = 1,\n    decomposition: str = \"LHL\",\n    boundary: Union[BoundaryCondition, str] = BoundaryCondition.ZERO,\n    rotation_invariant: bool = False,\n    pooling: str = \"average\",\n    use_parallel: Union[bool, None] = None,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Apply 3D separable wavelet transform (undecimated/stationary).\n\n    Uses the \u00e0 trous algorithm for undecimated wavelet decomposition.\n    The transform is translation-invariant (unlike decimated transform).\n\n    Supported wavelets:\n        - \"haar\" (UOUE): Haar wavelet\n        - \"db2\", \"db3\": Daubechies wavelets\n        - \"coif1\": Coiflet wavelet\n\n    Args:\n        image: 3D input image array\n        wavelet: Wavelet name (e.g., \"db2\", \"coif1\", \"haar\")\n        level: Decomposition level (GCEK)\n        decomposition: Which response map to return, e.g., \"LHL\", \"HHH\"\n        boundary: Boundary condition for padding\n        rotation_invariant: If True, average over 24 rotations\n        pooling: Pooling method for rotation invariance\n        use_parallel: If True, use parallel processing for rotation_invariant mode.\n            If None (default), auto-enables for images &gt; ~128\u00b3 voxels.\n\n    Returns:\n        Response map for the specified decomposition\n\n    Example:\n        Apply Daubechies 2 wavelet transform at level 1, returning LHL coefficients:\n\n        ```python\n        import numpy as np\n        from pictologics.filters import wavelet_transform\n\n        # Create dummy 3D image\n        image = np.random.rand(50, 50, 50)\n\n        # Apply transform\n        response = wavelet_transform(\n            image,\n            wavelet=\"db2\",\n            level=1,\n            decomposition=\"LHL\"\n        )\n        ```\n    \"\"\"\n    from concurrent.futures import ThreadPoolExecutor\n\n    # Convert to float32\n    image = ensure_float32(image)\n\n    # Handle boundary\n    if isinstance(boundary, str):\n        boundary = BoundaryCondition[boundary.upper()]\n    mode = get_scipy_mode(boundary)\n\n    # Get wavelet filters\n    w = pywt.Wavelet(wavelet)\n    lo = np.array(w.dec_lo, dtype=np.float32)  # Low-pass decomposition filter\n    hi = np.array(w.dec_hi, dtype=np.float32)  # High-pass decomposition filter\n\n    # Auto-detect parallel mode based on image size\n    if use_parallel is None:\n        use_parallel = image.size &gt; _PARALLEL_THRESHOLD\n\n    if rotation_invariant:\n        rotations = _get_rotation_perms()\n\n        def apply_rotated_wavelet(\n            rotation: Tuple[Tuple[int, int, int], Tuple[bool, bool, bool]],\n        ) -&gt; npt.NDArray[np.floating[Any]]:\n            \"\"\"Apply wavelet transform with rotated image.\"\"\"\n            perm, flips = rotation\n            # Permute and flip image\n            rotated = np.transpose(image, perm)\n            for axis, flip in enumerate(flips):\n                if flip:\n                    rotated = np.flip(rotated, axis=axis)\n\n            # Apply wavelet\n            response = _apply_undecimated_wavelet_3d(\n                rotated, lo, hi, level, decomposition, mode\n            )\n\n            # Undo rotation for response\n            for axis, flip in enumerate(flips):\n                if flip:\n                    response = np.flip(response, axis=axis)\n            inv_perm = tuple(np.argsort(perm))\n            return np.transpose(response, inv_perm)\n\n        if use_parallel:\n            from concurrent.futures import ThreadPoolExecutor, as_completed\n\n            with ThreadPoolExecutor() as executor:\n                # Submit all rotation tasks\n                future_to_rot = {\n                    executor.submit(apply_rotated_wavelet, rot): rot\n                    for rot in rotations\n                }\n\n                # Pool responses incrementally\n                result: npt.NDArray[np.floating[Any]] | None = None\n                for _, future in enumerate(as_completed(future_to_rot)):\n                    response = future.result()\n\n                    if result is None:\n                        # Initialize accumulator with first result\n                        result = (\n                            response.astype(np.float64)\n                            if pooling == \"average\"\n                            else response.copy()\n                        )\n                    else:\n                        if result is None:  # pragma: no cover\n                            raise RuntimeError(\"Result should not be None\")\n\n                        # Fix mypy narrowing issue\n                        # Assert was sufficient\n                        res = result\n\n                        if pooling == \"max\":\n                            np.maximum(res, response, out=res)\n                        elif pooling == \"average\":\n                            res += response\n                        elif pooling == \"min\":\n                            np.minimum(res, response, out=res)\n                        else:\n                            raise ValueError(\n                                f\"Unknown pooling: {pooling}\"\n                            )  # pragma: no cover\n\n                    # Explicitly delete response\n                    del response\n        else:\n            # Sequential processing for small images\n            result = None\n            for i, rotation in enumerate(rotations):\n                response = apply_rotated_wavelet(rotation)\n\n                if i == 0:\n                    result = (\n                        response.astype(np.float64)\n                        if pooling == \"average\"\n                        else response.copy()\n                    )\n                else:\n                    if result is None:  # pragma: no cover\n                        raise RuntimeError(\"Result should not be None\")\n\n                    res_seq = result\n\n                    if pooling == \"max\":\n                        np.maximum(res_seq, response, out=res_seq)\n                    elif pooling == \"average\":\n                        res_seq += response\n                    elif pooling == \"min\":\n                        np.minimum(res_seq, response, out=res_seq)\n                    else:\n                        raise ValueError(\n                            f\"Unknown pooling: {pooling}\"\n                        )  # pragma: no cover\n\n        # Finalize average pooling\n        if pooling == \"average\" and result is not None:\n            result /= len(rotations)\n        return result.astype(np.float32)  # type: ignore[union-attr]\n    else:\n        return _apply_undecimated_wavelet_3d(image, lo, hi, level, decomposition, mode)\n</code></pre>"},{"location":"api/filters/#pictologics.filters.simoncelli_wavelet","title":"<code>pictologics.filters.simoncelli_wavelet(image, level=1, boundary=BoundaryCondition.PERIODIC)</code>","text":"<p>Apply Simoncelli non-separable wavelet (IBSI code: PRT7).</p> <p>The Simoncelli wavelet is isotropic (spherically symmetric) and implemented in the Fourier domain. Per IBSI 2 Eq. 27.</p> <p>For decomposition level N, the frequency band is scaled by j = N-1:     - Level 1 (j=0): band [\u03c0/4, \u03c0] (highest frequencies)     - Level 2 (j=1): band [\u03c0/8, \u03c0/2]     - Level 3 (j=2): band [\u03c0/16, \u03c0/4]</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray[floating[Any]]</code> <p>3D input image array</p> required <code>level</code> <code>int</code> <p>Decomposition level (1 = highest frequency band)</p> <code>1</code> <code>boundary</code> <code>Union[BoundaryCondition, str]</code> <p>Boundary condition (FFT is inherently periodic)</p> <code>PERIODIC</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>Band-pass response map (B map) for the specified level</p> Example <p>Apply first-level Simoncelli wavelet (highest frequency band):</p> <pre><code>import numpy as np\nfrom pictologics.filters import simoncelli_wavelet\n\n# Create dummy 3D image\nimage = np.random.rand(50, 50, 50)\n\n# Apply wavelet\nresponse = simoncelli_wavelet(image, level=1)\n</code></pre> Source code in <code>pictologics/filters/wavelets.py</code> <pre><code>def simoncelli_wavelet(\n    image: npt.NDArray[np.floating[Any]],\n    level: int = 1,\n    boundary: Union[BoundaryCondition, str] = BoundaryCondition.PERIODIC,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Apply Simoncelli non-separable wavelet (IBSI code: PRT7).\n\n    The Simoncelli wavelet is isotropic (spherically symmetric) and\n    implemented in the Fourier domain. Per IBSI 2 Eq. 27.\n\n    For decomposition level N, the frequency band is scaled by j = N-1:\n        - Level 1 (j=0): band [\u03c0/4, \u03c0] (highest frequencies)\n        - Level 2 (j=1): band [\u03c0/8, \u03c0/2]\n        - Level 3 (j=2): band [\u03c0/16, \u03c0/4]\n\n    Args:\n        image: 3D input image array\n        level: Decomposition level (1 = highest frequency band)\n        boundary: Boundary condition (FFT is inherently periodic)\n\n    Returns:\n        Band-pass response map (B map) for the specified level\n\n    Example:\n        Apply first-level Simoncelli wavelet (highest frequency band):\n\n        ```python\n        import numpy as np\n        from pictologics.filters import simoncelli_wavelet\n\n        # Create dummy 3D image\n        image = np.random.rand(50, 50, 50)\n\n        # Apply wavelet\n        response = simoncelli_wavelet(image, level=1)\n        ```\n    \"\"\"\n    # Convert to float32\n    image = ensure_float32(image)\n\n    shape = image.shape\n    ndim = len(shape)\n\n    # IBSI level N corresponds to j = N-1\n    # Level 1 = j=0 \u2192 max_freq = 1.0 (normalized Nyquist)\n    j = level - 1\n    # Normalized max frequency for this level (relative to Nyquist=1.0)\n    max_freq = 1.0 / (2**j)\n\n    # Use centered grid coordinates [-1, 1] relative to geometric center (N-1)/2\n    center = (np.array(shape) - 1.0) / 2.0\n\n    # Generate value grid for each dimension\n    grids = []\n    for i, s in enumerate(shape):\n        dim_grid = np.arange(s)\n        # Normalize to [-1, 1] relative to center\n        grids.append((dim_grid - center[i]) / center[i])\n\n    # Optimize: Use rfftn (Real FFT) logic\n    # The output of rfftn has shape (N1, N2, ..., N d//2 + 1)\n    # We must construct the frequency grid to match this specific shape\n\n    # Adjust the last dimension grid for rfftn\n    # rfftn frequencies correspond to the first N//2 + 1 elements\n    grids[-1] = grids[-1][: shape[-1] // 2 + 1]\n\n    # Compute Euclidean distance via broadcasting (lazy evaluation)\n    # meshgrid with sparse=True returns coordinate vectors that broadcast\n    mesh_vectors = np.meshgrid(*grids, indexing=\"ij\", sparse=True)\n\n    # Compute dist^2 via broadcasting\n    dist_sq = np.asarray(sum(g**2 for g in mesh_vectors), dtype=np.float64)\n    dist = np.sqrt(dist_sq)\n\n    # Avoid log(0) and divide by zero\n    val = 2.0 * dist / max_freq\n    log_arg = np.where(val &gt; 0, val, 1.0)\n\n    with np.errstate(all=\"ignore\"):\n        g_sim = np.cos(np.pi / 2.0 * np.log2(log_arg))\n\n    # Apply band-pass mask\n    mask = (dist &gt;= max_freq / 4.0) &amp; (dist &lt;= max_freq)\n    g_sim = np.where(mask, g_sim, 0.0)\n\n    grids = []\n    for i, s in enumerate(shape):\n        dim_grid = np.arange(s)\n        # Normalize to [-1, 1] relative to center\n        grid_norm = (dim_grid - center[i]) / center[i]\n\n        # Shift to move the \"center\" (DC-like area) to array start/corner\n        grid_shifted = np.fft.ifftshift(grid_norm)\n        grids.append(grid_shifted)\n\n    # Use broadcasting for full 3D grid\n    mesh_vectors = np.meshgrid(*grids, indexing=\"ij\", sparse=True)\n    dist_sq = np.asarray(sum(g**2 for g in mesh_vectors), dtype=np.float64)\n    dist = np.sqrt(dist_sq)\n\n    # Calculate transfer function (same as before)\n    val = 2.0 * dist / max_freq\n    log_arg = np.where(val &gt; 0, val, 1.0)\n\n    with np.errstate(all=\"ignore\"):\n        g_sim = np.cos(np.pi / 2.0 * np.log2(log_arg))\n\n    # Apply band-pass mask\n    mask = (dist &gt;= max_freq / 4.0) &amp; (dist &lt;= max_freq)\n    g_sim = np.where(mask, g_sim, 0.0)\n\n    # Apply filter in frequency domain using full FFT\n    F = np.fft.fftn(image)\n\n    # Explicitly specify axes to avoid NumPy 2.0 DeprecationWarning\n    axes = tuple(range(ndim))\n    response = np.fft.ifftn(F * g_sim, s=shape, axes=axes)\n\n    return np.real(response).astype(np.float32)\n</code></pre>"},{"location":"api/filters/#pictologics.filters.riesz_transform","title":"<code>pictologics.filters.riesz_transform(image, order)</code>","text":"<p>Apply Riesz transform (IBSI code: AYRS).</p> <p>The Riesz transform computes higher-order all-pass image derivatives in the Fourier domain. Per IBSI 2 Eq. 34.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray[floating[Any]]</code> <p>3D input image array</p> required <code>order</code> <code>Tuple[int, ...]</code> <p>Tuple (l1, l2, l3) specifying derivative order per axis    e.g., (1,0,0) = first-order along k1 (gradient-like)          (2,0,0), (1,1,0), (0,2,0) = second-order (Hessian-like)</p> required <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>Riesz-transformed image (real part)</p> Example <p>Compute first-order Riesz transform along the k1 axis:</p> <pre><code>import numpy as np\nfrom pictologics.filters import riesz_transform\n\n# Create dummy 3D image\nimage = np.random.rand(50, 50, 50)\n\n# Apply transform (gradient-like along axis 0)\nresponse = riesz_transform(image, order=(1, 0, 0))\n</code></pre> Note <ul> <li>First-order Riesz components form the image gradient</li> <li>Second-order Riesz components form the image Hessian</li> <li>All-pass: doesn't amplify high frequencies like regular derivatives</li> </ul> Source code in <code>pictologics/filters/riesz.py</code> <pre><code>def riesz_transform(\n    image: npt.NDArray[np.floating[Any]],\n    order: Tuple[int, ...],\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Apply Riesz transform (IBSI code: AYRS).\n\n    The Riesz transform computes higher-order all-pass image derivatives\n    in the Fourier domain. Per IBSI 2 Eq. 34.\n\n    Args:\n        image: 3D input image array\n        order: Tuple (l1, l2, l3) specifying derivative order per axis\n               e.g., (1,0,0) = first-order along k1 (gradient-like)\n                     (2,0,0), (1,1,0), (0,2,0) = second-order (Hessian-like)\n\n    Returns:\n        Riesz-transformed image (real part)\n\n    Example:\n        Compute first-order Riesz transform along the k1 axis:\n\n        ```python\n        import numpy as np\n        from pictologics.filters import riesz_transform\n\n        # Create dummy 3D image\n        image = np.random.rand(50, 50, 50)\n\n        # Apply transform (gradient-like along axis 0)\n        response = riesz_transform(image, order=(1, 0, 0))\n        ```\n\n    Note:\n        - First-order Riesz components form the image gradient\n        - Second-order Riesz components form the image Hessian\n        - All-pass: doesn't amplify high frequencies like regular derivatives\n    \"\"\"\n    # Convert to float32\n    image = ensure_float32(image)\n\n    L = sum(order)  # Total order\n\n    if L == 0:\n        raise ValueError(\"At least one order component must be &gt; 0\")\n\n    shape = image.shape\n    ndim = len(shape)\n\n    # Generate frequency coordinates appropriately for rfftn\n    # Last dimension uses rfftfreq, others use fftfreq\n    freqs = []\n    for i, s in enumerate(shape):\n        if i == ndim - 1:\n            # Last dimension for rfftn is non-negative frequencies only\n            freqs.append(np.fft.rfftfreq(s) * 2 * np.pi)\n        else:\n            freqs.append(np.fft.fftfreq(s) * 2 * np.pi)\n\n    # Create grid using broadcasting (lazy evaluation) to avoid huge meshgrid matching input size\n    # meshgrid with sparse=True returns coordinate vectors that broadcast\n    nu_vectors = np.meshgrid(*freqs, indexing=\"ij\", sparse=True)\n\n    # Compute ||\u03bd||^2 via broadcasting\n    nu_sq_norm = np.asarray(sum(n**2 for n in nu_vectors), dtype=np.float64)\n    nu_norm = np.sqrt(nu_sq_norm)\n\n    # Avoid division by zero at DC\n    nu_norm_safe = np.where(nu_norm &gt; 0, nu_norm, 1.0)\n\n    # Compute normalization factor\n    norm_factor = sqrt(factorial(L) / np.prod([factorial(o) for o in order]))\n\n    # Compute numerator via broadcasting\n    numerator = np.ones(nu_norm.shape, dtype=np.float64)\n    for i, ord_val in enumerate(order):\n        if ord_val &gt; 0:\n            numerator *= nu_vectors[i] ** ord_val\n\n    # Riesz transfer function\n    phase = np.exp(-1j * np.pi * L / 2)\n\n    transfer = phase * norm_factor * numerator / (nu_norm_safe**L)\n    transfer = np.where(nu_norm &gt; 0, transfer, 0)  # Set DC to 0\n\n    # Apply in frequency domain using Real FFT\n    F = np.fft.rfftn(image)\n\n    # Verify shapes match (should match due to rfftfreq logic)\n    # F has shape (N1, N2, N3//2 + 1)\n    # transfer should have same shape or broadcastable\n\n    # Explicitly specify axes to avoid NumPy 2.0 DeprecationWarning\n    axes = tuple(range(ndim))\n    response = np.fft.irfftn(F * transfer, s=shape, axes=axes)\n\n    return response.astype(np.float32)\n</code></pre>"},{"location":"api/filters/#pictologics.filters.riesz_log","title":"<code>pictologics.filters.riesz_log(image, sigma_mm, spacing_mm=1.0, order=(1, 0, 0), truncate=4.0)</code>","text":"<p>Apply Riesz transform to LoG-filtered image.</p> <p>Combines multi-scale analysis (LoG) with directional analysis (Riesz). First applies LoG filtering, then applies Riesz transform.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray[floating[Any]]</code> <p>3D input image array</p> required <code>sigma_mm</code> <code>float</code> <p>LoG scale in mm</p> required <code>spacing_mm</code> <code>Union[float, Tuple[float, float, float]]</code> <p>Voxel spacing in mm</p> <code>1.0</code> <code>order</code> <code>Tuple[int, ...]</code> <p>Riesz order tuple (l1, l2, l3)</p> <code>(1, 0, 0)</code> <code>truncate</code> <code>float</code> <p>LoG truncation parameter</p> <code>4.0</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>Riesz-transformed LoG response</p> Example <p>Compute first-order Riesz transform of LoG-filtered image at 5mm scale:</p> <pre><code>import numpy as np\nfrom pictologics.filters import riesz_log\n\n# Create dummy 3D image\nimage = np.random.rand(50, 50, 50)\n\n# Apply filter\nresponse = riesz_log(\n    image,\n    sigma_mm=5.0,\n    spacing_mm=(2.0, 2.0, 2.0),\n    order=(1, 0, 0)\n)\n</code></pre> Source code in <code>pictologics/filters/riesz.py</code> <pre><code>def riesz_log(\n    image: npt.NDArray[np.floating[Any]],\n    sigma_mm: float,\n    spacing_mm: Union[float, Tuple[float, float, float]] = 1.0,\n    order: Tuple[int, ...] = (1, 0, 0),\n    truncate: float = 4.0,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Apply Riesz transform to LoG-filtered image.\n\n    Combines multi-scale analysis (LoG) with directional analysis (Riesz).\n    First applies LoG filtering, then applies Riesz transform.\n\n    Args:\n        image: 3D input image array\n        sigma_mm: LoG scale in mm\n        spacing_mm: Voxel spacing in mm\n        order: Riesz order tuple (l1, l2, l3)\n        truncate: LoG truncation parameter\n\n    Returns:\n        Riesz-transformed LoG response\n\n    Example:\n        Compute first-order Riesz transform of LoG-filtered image at 5mm scale:\n\n        ```python\n        import numpy as np\n        from pictologics.filters import riesz_log\n\n        # Create dummy 3D image\n        image = np.random.rand(50, 50, 50)\n\n        # Apply filter\n        response = riesz_log(\n            image,\n            sigma_mm=5.0,\n            spacing_mm=(2.0, 2.0, 2.0),\n            order=(1, 0, 0)\n        )\n        ```\n    \"\"\"\n    from .log import laplacian_of_gaussian\n\n    # First apply LoG\n    log_response = laplacian_of_gaussian(\n        image, sigma_mm=sigma_mm, spacing_mm=spacing_mm, truncate=truncate\n    )\n\n    # Then apply Riesz transform\n    return riesz_transform(log_response, order=order)\n</code></pre>"},{"location":"api/filters/#pictologics.filters.riesz_simoncelli","title":"<code>pictologics.filters.riesz_simoncelli(image, level=1, order=(1, 0, 0))</code>","text":"<p>Apply Riesz transform to Simoncelli wavelet-filtered image.</p> <p>Combines isotropic multi-scale analysis (Simoncelli) with directional analysis (Riesz) for rotation-invariant directional features.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>NDArray[floating[Any]]</code> <p>3D input image array</p> required <code>level</code> <code>int</code> <p>Simoncelli decomposition level</p> <code>1</code> <code>order</code> <code>Tuple[int, ...]</code> <p>Riesz order tuple (l1, l2, l3)</p> <code>(1, 0, 0)</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>Riesz-transformed Simoncelli response</p> Example <p>Compute second-order Riesz transform (Hessian-like) of Simoncelli level 2:</p> <pre><code>import numpy as np\nfrom pictologics.filters import riesz_simoncelli\n\n# Create dummy 3D image\nimage = np.random.rand(50, 50, 50)\n\n# Apply filter\nresponse = riesz_simoncelli(\n    image,\n    level=2,\n    order=(2, 0, 0)\n)\n</code></pre> Source code in <code>pictologics/filters/riesz.py</code> <pre><code>def riesz_simoncelli(\n    image: npt.NDArray[np.floating[Any]],\n    level: int = 1,\n    order: Tuple[int, ...] = (1, 0, 0),\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Apply Riesz transform to Simoncelli wavelet-filtered image.\n\n    Combines isotropic multi-scale analysis (Simoncelli) with\n    directional analysis (Riesz) for rotation-invariant directional features.\n\n    Args:\n        image: 3D input image array\n        level: Simoncelli decomposition level\n        order: Riesz order tuple (l1, l2, l3)\n\n    Returns:\n        Riesz-transformed Simoncelli response\n\n    Example:\n        Compute second-order Riesz transform (Hessian-like) of Simoncelli level 2:\n\n        ```python\n        import numpy as np\n        from pictologics.filters import riesz_simoncelli\n\n        # Create dummy 3D image\n        image = np.random.rand(50, 50, 50)\n\n        # Apply filter\n        response = riesz_simoncelli(\n            image,\n            level=2,\n            order=(2, 0, 0)\n        )\n        ```\n    \"\"\"\n    from .wavelets import simoncelli_wavelet\n\n    # First apply Simoncelli\n    sim_response = simoncelli_wavelet(image, level=level)\n\n    # Then apply Riesz transform\n    return riesz_transform(sim_response, order=order)\n</code></pre>"},{"location":"api/loader/","title":"Loaders API","text":""},{"location":"api/loader/#pictologics.loader","title":"<code>pictologics.loader</code>","text":""},{"location":"api/loader/#pictologics.loader--image-loading-module","title":"Image Loading Module","text":"<p>This module handles the loading of medical images from various formats (NIfTI, DICOM) into a standardized <code>Image</code> class. It abstracts away file format differences to provide a consistent interface for the rest of the library.</p>"},{"location":"api/loader/#pictologics.loader--key-features","title":"Key Features:","text":"<ul> <li>Unified Image Class: Stores 3D data, spacing, origin, direction, and modality.</li> <li>Format Support:<ul> <li>NIfTI (.nii, .nii.gz) via <code>nibabel</code>.</li> <li>DICOM Series (directory of DICOM files) via <code>pydicom</code>.</li> <li>Single DICOM files.</li> </ul> </li> <li>Automatic Detection: <code>load_image</code> automatically detects format and dimensionality.</li> <li>Robust DICOM Sorting: Sorts slices based on spatial position and orientation.</li> </ul>"},{"location":"api/loader/#pictologics.loader--axis-conventions","title":"Axis Conventions:","text":"<p>All image arrays are stored in (X, Y, Z) order to match ITK/SimpleITK conventions:</p> <ul> <li>X (axis 0): Left-Right direction (columns in DICOM terminology)</li> <li>Y (axis 1): Anterior-Posterior direction (rows in DICOM terminology)</li> <li>Z (axis 2): Superior-Inferior direction (slices)</li> </ul> <p>This differs from raw DICOM and matplotlib conventions:</p> <ul> <li>DICOM pixel_array: Returns (Rows, Columns) = (Y, X) for 2D slices</li> <li>Matplotlib imshow: Expects (height, width) = (Y, X)</li> </ul> <p>The loaders handle the necessary axis transformations automatically. When using visualization utilities like <code>visualize_mask_overlay()</code>, slices are internally transposed for correct display.</p>"},{"location":"api/loader/#pictologics.loader.Image","title":"<code>Image</code>  <code>dataclass</code>","text":"<p>A standardized container for 3D medical image data and metadata.</p> <p>This class serves as the common interface for all image processing operations in the library, abstracting away the differences between file formats like DICOM and NIfTI.</p> <p>Attributes:</p> Name Type Description <code>array</code> <code>NDArray[floating[Any]]</code> <p>The 3D image data with shape (x, y, z).</p> <code>spacing</code> <code>tuple[float, float, float]</code> <p>Voxel spacing in millimeters (mm) along the (x, y, z) axes.</p> <code>origin</code> <code>tuple[float, float, float]</code> <p>World coordinates of the image origin (center of the first voxel) in millimeters (mm).</p> <code>direction</code> <code>Optional[NDArray[floating[Any]]]</code> <p>3x3 direction cosine matrix defining the orientation of the image axes in world space. Defaults to identity matrix.</p> <code>modality</code> <code>str</code> <p>The imaging modality (e.g., 'CT', 'MR', 'PT'). Defaults to 'Unknown'.</p> Source code in <code>pictologics/loader.py</code> <pre><code>@dataclass\nclass Image:\n    \"\"\"\n    A standardized container for 3D medical image data and metadata.\n\n    This class serves as the common interface for all image processing operations\n    in the library, abstracting away the differences between file formats like\n    DICOM and NIfTI.\n\n    Attributes:\n        array (npt.NDArray[np.floating[Any]]): The 3D image data with shape (x, y, z).\n        spacing (tuple[float, float, float]): Voxel spacing in millimeters (mm)\n            along the (x, y, z) axes.\n        origin (tuple[float, float, float]): World coordinates of the image origin\n            (center of the first voxel) in millimeters (mm).\n        direction (Optional[npt.NDArray[np.floating[Any]]]): 3x3 direction cosine matrix defining the\n            orientation of the image axes in world space. Defaults to identity matrix.\n        modality (str): The imaging modality (e.g., 'CT', 'MR', 'PT'). Defaults to 'Unknown'.\n    \"\"\"\n\n    array: npt.NDArray[np.floating[Any]]\n    spacing: tuple[float, float, float]\n    origin: tuple[float, float, float]\n    direction: Optional[npt.NDArray[np.floating[Any]]] = None\n    modality: str = \"Unknown\"\n</code></pre>"},{"location":"api/loader/#pictologics.loader.load_image","title":"<code>load_image(path, dataset_index=0, recursive=False, reference_image=None, transpose_axes=None, fill_value=0.0, apply_rescale=True)</code>","text":"<p>Load a medical image from a file path or directory.</p> <p>This is the main entry point for loading data. It automatically detects whether the input is a NIfTI file, DICOM directory/file (single DICOM or series), or a DICOM Segmentation (SEG) object and standardizes it into an <code>Image</code> object.</p> <p>The resulting image array is always 3D with dimensions (x, y, z).</p> Note <p>For DICOM SEG files, this function uses :func:<code>pictologics.loaders.load_seg</code> internally. For more control over segment extraction (e.g., selecting specific segments or extracting them separately), use <code>load_seg()</code> directly.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The absolute or relative path to the image file (e.g., .nii.gz, .dcm or file with no extension) or the directory containing DICOM files.</p> required <code>dataset_index</code> <code>int</code> <p>For multi-volume datasets, specifies which volume to extract (0-indexed). This works for:</p> <ul> <li>4D NIfTI files: Selects which time point/volume to load.</li> <li>Multi-phase DICOM series: Selects which phase to load (e.g., cardiac   phases, temporal positions, echo numbers). Use   :func:<code>pictologics.utilities.get_dicom_phases</code> to discover available phases.</li> </ul> <p>Defaults to 0 (the first volume/phase).</p> <code>0</code> <code>recursive</code> <code>bool</code> <p>If True and <code>path</code> is a directory, recursively searches subdirectories and loads the DICOM series from the folder containing the most DICOM files. Defaults to False.</p> <code>False</code> <code>reference_image</code> <code>Optional[Image]</code> <p>If provided and the loaded image has different dimensions than the reference, it will be repositioned into the reference coordinate space using spatial metadata (origin, spacing). This is useful for loading cropped segmentation masks that need to match a full-sized image.</p> <code>None</code> <code>transpose_axes</code> <code>tuple[int, int, int] | None</code> <p>Optional axis transposition to apply before repositioning. Use this if the mask's axis order differs from the reference. E.g., (0, 2, 1) swaps Y and Z axes. Only used when reference_image is provided.</p> <code>None</code> <code>fill_value</code> <code>float</code> <p>Fill value for regions outside the loaded image when repositioning (default: 0.0). Only used when reference_image is provided.</p> <code>0.0</code> <code>apply_rescale</code> <code>bool</code> <p>If True (default), apply RescaleSlope and RescaleIntercept transformation for DICOM files to convert stored pixel values to real-world values (e.g., Hounsfield Units for CT). NIfTI files always apply their scaling factors via nibabel's get_fdata(). Set to False if you need raw stored values.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Image</code> <code>Image</code> <p>An <code>Image</code> object containing the 3D numpy array and metadata (spacing, origin, etc.).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path does not exist, the file format is not supported, or the file is corrupt/unreadable.</p> Example <p>Loading a NIfTI file: <pre><code>from pictologics.loader import load_image\n\n# Load a standard brain scan\nimg = load_image(\"data/brain.nii.gz\")\nprint(f\"Image shape: {img.array.shape}\")\n# Output: Image shape: (256, 256, 128)\n</code></pre></p> <p>Loading a DICOM series: <pre><code># Load a CT scan from a folder of DICOM files\nimg_ct = load_image(\"data/patients/001/CT_scan/\")\nprint(f\"Voxel spacing: {img_ct.spacing}\")\n# Output: Voxel spacing: (0.97, 0.97, 2.5)\n</code></pre></p> <p>Loading a single DICOM file: <pre><code># Load a single DICOM file (even without .dcm extension)\nimg_slice = load_image(\"data/slice_001\")\nprint(f\"Modality: {img_slice.modality}\")\n</code></pre></p> <p>Recursive DICOM loading: <pre><code># Finds the deep subfolder with actual DICOM files\nimg = load_image(\"data/patients/001/\", recursive=True)\n</code></pre></p> <p>Loading a specific volume from a 4D file: <pre><code># Load the 5th time point from a 4D fMRI file\nfmri_vol = load_image(\"data/fmri.nii.gz\", dataset_index=4)\n</code></pre></p> <p>Loading a cropped mask and repositioning to match main image: <pre><code>main_img = load_image(\"ct_scan/\")\nmask = load_image(\"cropped_mask.dcm\", reference_image=main_img)\n# mask now has same shape as main_img\n</code></pre></p> <p>Loading a DICOM SEG file (auto-detected): <pre><code># DICOM SEG files are automatically detected and loaded\nseg = load_image(\"segmentation.dcm\")\nprint(f\"Modality: {seg.modality}\")  # Output: Modality: SEG\n# Segments are combined into a label image by default\n</code></pre></p> <p>Loading a specific phase from a multi-phase DICOM series: <pre><code>from pictologics.utilities import get_dicom_phases\n\n# Discover available phases\nphases = get_dicom_phases(\"cardiac_ct/\")\nprint(f\"Found {len(phases)} phases\")\nfor p in phases:\n    print(f\"  {p.index}: {p.label} ({p.num_slices} slices)\")\n\n# Load the 5th phase (40%)\nimg = load_image(\"cardiac_ct/\", dataset_index=4)\n</code></pre></p> Source code in <code>pictologics/loader.py</code> <pre><code>def load_image(\n    path: str,\n    dataset_index: int = 0,\n    recursive: bool = False,\n    reference_image: Optional[Image] = None,\n    transpose_axes: tuple[int, int, int] | None = None,\n    fill_value: float = 0.0,\n    apply_rescale: bool = True,\n) -&gt; Image:\n    \"\"\"\n    Load a medical image from a file path or directory.\n\n    This is the main entry point for loading data. It automatically detects whether\n    the input is a NIfTI file, DICOM directory/file (single DICOM or series), or\n    a DICOM Segmentation (SEG) object and standardizes it into an `Image` object.\n\n    The resulting image array is always 3D with dimensions (x, y, z).\n\n    Note:\n        For DICOM SEG files, this function uses :func:`pictologics.loaders.load_seg`\n        internally. For more control over segment extraction (e.g., selecting specific\n        segments or extracting them separately), use ``load_seg()`` directly.\n\n    Args:\n        path (str): The absolute or relative path to the image file (e.g., .nii.gz,\n            .dcm or file with no extension) or the directory containing DICOM files.\n        dataset_index (int, optional): For multi-volume datasets, specifies which\n            volume to extract (0-indexed). This works for:\n\n            - **4D NIfTI files**: Selects which time point/volume to load.\n            - **Multi-phase DICOM series**: Selects which phase to load (e.g., cardiac\n              phases, temporal positions, echo numbers). Use\n              :func:`pictologics.utilities.get_dicom_phases` to discover available phases.\n\n            Defaults to 0 (the first volume/phase).\n        recursive (bool, optional): If True and `path` is a directory, recursively searches\n            subdirectories and loads the DICOM series from the folder containing the most\n            DICOM files. Defaults to False.\n        reference_image (Optional[Image]): If provided and the loaded image has different\n            dimensions than the reference, it will be repositioned into the reference\n            coordinate space using spatial metadata (origin, spacing). This is useful for\n            loading cropped segmentation masks that need to match a full-sized image.\n        transpose_axes (tuple[int, int, int] | None): Optional axis transposition to apply\n            before repositioning. Use this if the mask's axis order differs from the reference.\n            E.g., (0, 2, 1) swaps Y and Z axes. Only used when reference_image is provided.\n        fill_value (float): Fill value for regions outside the loaded image when\n            repositioning (default: 0.0). Only used when reference_image is provided.\n        apply_rescale (bool): If True (default), apply RescaleSlope and RescaleIntercept\n            transformation for DICOM files to convert stored pixel values to real-world\n            values (e.g., Hounsfield Units for CT). NIfTI files always apply their scaling\n            factors via nibabel's get_fdata(). Set to False if you need raw stored values.\n\n    Returns:\n        Image: An `Image` object containing the 3D numpy array and metadata (spacing, origin, etc.).\n\n    Raises:\n        ValueError: If the path does not exist, the file format is not supported,\n            or the file is corrupt/unreadable.\n\n    Example:\n        **Loading a NIfTI file:**\n        ```python\n        from pictologics.loader import load_image\n\n        # Load a standard brain scan\n        img = load_image(\"data/brain.nii.gz\")\n        print(f\"Image shape: {img.array.shape}\")\n        # Output: Image shape: (256, 256, 128)\n        ```\n\n        **Loading a DICOM series:**\n        ```python\n        # Load a CT scan from a folder of DICOM files\n        img_ct = load_image(\"data/patients/001/CT_scan/\")\n        print(f\"Voxel spacing: {img_ct.spacing}\")\n        # Output: Voxel spacing: (0.97, 0.97, 2.5)\n        ```\n\n        **Loading a single DICOM file:**\n        ```python\n        # Load a single DICOM file (even without .dcm extension)\n        img_slice = load_image(\"data/slice_001\")\n        print(f\"Modality: {img_slice.modality}\")\n        ```\n\n        **Recursive DICOM loading:**\n        ```python\n        # Finds the deep subfolder with actual DICOM files\n        img = load_image(\"data/patients/001/\", recursive=True)\n        ```\n\n        **Loading a specific volume from a 4D file:**\n        ```python\n        # Load the 5th time point from a 4D fMRI file\n        fmri_vol = load_image(\"data/fmri.nii.gz\", dataset_index=4)\n        ```\n\n        **Loading a cropped mask and repositioning to match main image:**\n        ```python\n        main_img = load_image(\"ct_scan/\")\n        mask = load_image(\"cropped_mask.dcm\", reference_image=main_img)\n        # mask now has same shape as main_img\n        ```\n\n        **Loading a DICOM SEG file (auto-detected):**\n        ```python\n        # DICOM SEG files are automatically detected and loaded\n        seg = load_image(\"segmentation.dcm\")\n        print(f\"Modality: {seg.modality}\")  # Output: Modality: SEG\n        # Segments are combined into a label image by default\n        ```\n\n        **Loading a specific phase from a multi-phase DICOM series:**\n        ```python\n        from pictologics.utilities import get_dicom_phases\n\n        # Discover available phases\n        phases = get_dicom_phases(\"cardiac_ct/\")\n        print(f\"Found {len(phases)} phases\")\n        for p in phases:\n            print(f\"  {p.index}: {p.label} ({p.num_slices} slices)\")\n\n        # Load the 5th phase (40%)\n        img = load_image(\"cardiac_ct/\", dataset_index=4)\n        ```\n    \"\"\"\n    path_obj = Path(path)\n    if not path_obj.exists():\n        raise ValueError(f\"The specified path does not exist: {path}\")\n\n    try:\n        if path_obj.is_dir():\n            target_path = path_obj\n            if recursive:\n                target_path = _find_best_dicom_series_dir(path_obj)\n            loaded_image = _load_dicom_series(target_path, dataset_index, apply_rescale)\n        elif path.lower().endswith((\".nii\", \".nii.gz\")):\n            loaded_image = _load_nifti(path, dataset_index)\n        else:\n            # Attempt to load as a single DICOM file if extension is not NIfTI\n            # Check if it's a DICOM SEG file first\n            if _is_dicom_seg(path):\n                from pictologics.loaders.seg_loader import load_seg\n\n                seg_result = load_seg(path, reference_image=reference_image)\n                # load_seg can return dict when combine_segments=False, but here we use default\n                if isinstance(seg_result, dict):\n                    # Should not happen with default args, but handle gracefully\n                    return next(iter(seg_result.values()))\n                # Return early since reference alignment is handled by load_seg\n                return seg_result\n\n            try:\n                loaded_image = _load_dicom_file(path, apply_rescale)\n            except Exception:\n                raise ValueError(\n                    f\"Unsupported file format or unable to read file: {path}\"\n                ) from None\n    except Exception as e:\n        # Re-raise ValueErrors directly, wrap others\n        if isinstance(e, ValueError):\n            raise e\n        raise ValueError(f\"Failed to load image from '{path}': {e}\") from e\n\n    # Apply repositioning if reference_image is provided and shapes differ\n    if reference_image is not None:\n        if loaded_image.array.shape != reference_image.array.shape:\n            loaded_image = _position_in_reference(\n                loaded_image, reference_image, fill_value, transpose_axes\n            )\n\n    return loaded_image\n</code></pre>"},{"location":"api/loader/#pictologics.loader.load_and_merge_images","title":"<code>load_and_merge_images(image_paths, reference_image=None, conflict_resolution='max', dataset_index=0, recursive=False, binarize=None, reposition_to_reference=False, transpose_axes=None, fill_value=0.0, relabel_masks=False, apply_rescale=True)</code>","text":"<p>Load multiple images (e.g., masks or partial scans) and merge them into a single image.</p> <p>This function loads images from the provided paths, validates that they all share the same geometry (dimensions, spacing, origin, direction), and merges them according to the specified conflict resolution strategy.</p> <p>Use Cases: - Merging multiple segmentation masks into a single ROI. - Merging split image volumes (though typically less common than mask merging). - Merging cropped/bounding-box segmentation masks (with <code>reposition_to_reference=True</code>).</p> <p>Format &amp; Path Support: Since this function uses <code>load_image</code> internally for each path, it supports: - NIfTI files (.nii, .nii.gz). - DICOM series (directories containing DICOM files). - Single DICOM files (with or without .dcm extension). - Nested directories (if paths point to folders containing DICOMs).</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>list[str]</code> <p>List of absolute or relative paths to the images. These can be file paths or directory paths.</p> required <code>reference_image</code> <code>Optional[Image]</code> <p>An optional reference image (e.g., the scan corresponding to the masks). If provided, the merged image is validated against this image's geometry. Required when <code>reposition_to_reference=True</code>.</p> <code>None</code> <code>conflict_resolution</code> <code>str</code> <p>Strategy to resolve voxel values when multiple images have non-zero values at the same location. Options: - 'max': Use the maximum value (default). - 'min': Use the minimum value. - 'first': Keep the value from the first image encountered (earlier in list). - 'last': Overwrite with the value from the last image encountered (later in list).</p> <code>'max'</code> <code>dataset_index</code> <code>int</code> <p>For multi-volume datasets, specifies which volume to extract for all images (0-indexed). This works for:</p> <ul> <li>4D NIfTI files: Selects which time point/volume to load.</li> <li>Multi-phase DICOM series: Selects which phase to load (e.g., cardiac   phases, temporal positions, echo numbers). Use   :func:<code>pictologics.utilities.get_dicom_phases</code> to discover available phases.</li> </ul> <p>Defaults to 0 (the first volume/phase).</p> <code>0</code> <code>recursive</code> <code>bool</code> <p>If True, recursively searches subdirectories for each path in <code>image_paths</code>. Defaults to False.</p> <code>False</code> <code>binarize</code> <code>bool | int | list[int] | tuple[int, int] | None</code> <p>Rules for binarizing the merged image. - <code>None</code> (default): No binarization. - <code>True</code>: Sets all voxels &gt; 0 to 1, others to 0. - <code>int</code> (e.g., 2): Sets voxels == value to 1, others to 0. - <code>list[int]</code> (e.g., [1, 2]): Sets voxels in list to 1, others to 0. - <code>tuple[int, int]</code> (e.g., (1, 10)): Sets voxels in inclusive range to 1, others to 0.</p> <code>None</code> <code>reposition_to_reference</code> <code>bool</code> <p>If True and reference_image is provided, each loaded image will be repositioned into the reference coordinate space before merging. This is required when loading cropped segmentation masks that have different dimensions than the reference. Geometry validation is performed AFTER repositioning. Defaults to False.</p> <code>False</code> <code>transpose_axes</code> <code>tuple[int, int, int] | None</code> <p>Axis transposition to apply when repositioning. E.g., (0, 2, 1) swaps Y and Z axes. Only used when <code>reposition_to_reference=True</code>.</p> <code>None</code> <code>fill_value</code> <code>float</code> <p>Fill value for regions outside cropped masks when repositioning (default: 0.0). Only used when <code>reposition_to_reference=True</code>.</p> <code>0.0</code> <code>relabel_masks</code> <code>bool</code> <p>If True, assigns unique label values (1, 2, 3, ...) to each mask file based on its order in <code>image_paths</code>. This converts binary [0,1] masks into multi-label masks where each file gets a distinct label, useful for visualization with different colors. Label assignment respects the order of <code>image_paths</code>. Defaults to False.</p> <code>False</code> <code>apply_rescale</code> <code>bool</code> <p>If True (default), apply RescaleSlope and RescaleIntercept transformation for DICOM files to convert stored pixel values to real-world values (e.g., Hounsfield Units for CT). Set to False if you need raw stored values.</p> <code>True</code> Note <p>The <code>binarize</code> parameter is intended for mask filtering (e.g., selecting specific ROI labels). To filter image intensity values (e.g., HU ranges), use the preprocessing steps in the radiomics pipeline configuration instead.</p> <p>Returns:</p> Name Type Description <code>Image</code> <code>Image</code> <p>A new <code>Image</code> object containing the merged data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>image_paths</code> is empty, if an invalid <code>conflict_resolution</code> is provided, if <code>reposition_to_reference=True</code> but <code>reference_image</code> is not provided, or if the images (or reference) have mismatched geometries.</p> Example <p>Merging cropped segmentation masks: <pre><code>main_img = load_image(\"ct_scan/\", recursive=True)\nseg_paths = [str(f) for f in Path(\"masks/\").glob(\"*.dcm\")]\n\nmerged = load_and_merge_images(\n    seg_paths,\n    reference_image=main_img,\n    reposition_to_reference=True,\n    conflict_resolution=\"max\",\n)\n</code></pre></p> Source code in <code>pictologics/loader.py</code> <pre><code>def load_and_merge_images(\n    image_paths: list[str],\n    reference_image: Optional[Image] = None,\n    conflict_resolution: str = \"max\",\n    dataset_index: int = 0,\n    recursive: bool = False,\n    binarize: bool | int | list[int] | tuple[int, int] | None = None,\n    reposition_to_reference: bool = False,\n    transpose_axes: tuple[int, int, int] | None = None,\n    fill_value: float = 0.0,\n    relabel_masks: bool = False,\n    apply_rescale: bool = True,\n) -&gt; Image:\n    \"\"\"\n    Load multiple images (e.g., masks or partial scans) and merge them into a single image.\n\n    This function loads images from the provided paths, validates that they all share\n    the same geometry (dimensions, spacing, origin, direction), and merges them\n    according to the specified conflict resolution strategy.\n\n    **Use Cases:**\n    - Merging multiple segmentation masks into a single ROI.\n    - Merging split image volumes (though typically less common than mask merging).\n    - Merging cropped/bounding-box segmentation masks (with `reposition_to_reference=True`).\n\n    **Format &amp; Path Support:**\n    Since this function uses `load_image` internally for each path, it supports:\n    - **NIfTI files** (.nii, .nii.gz).\n    - **DICOM series** (directories containing DICOM files).\n    - **Single DICOM files** (with or without .dcm extension).\n    - **Nested directories** (if paths point to folders containing DICOMs).\n\n    Args:\n        image_paths (list[str]): List of absolute or relative paths to the images.\n            These can be file paths or directory paths.\n        reference_image (Optional[Image]): An optional reference image (e.g., the scan\n            corresponding to the masks). If provided, the merged image is validated\n            against this image's geometry. Required when `reposition_to_reference=True`.\n        conflict_resolution (str): Strategy to resolve voxel values when multiple images\n            have non-zero values at the same location. Options:\n            - 'max': Use the maximum value (default).\n            - 'min': Use the minimum value.\n            - 'first': Keep the value from the first image encountered (earlier in list).\n            - 'last': Overwrite with the value from the last image encountered (later in list).\n        dataset_index (int, optional): For multi-volume datasets, specifies which\n            volume to extract for all images (0-indexed). This works for:\n\n            - **4D NIfTI files**: Selects which time point/volume to load.\n            - **Multi-phase DICOM series**: Selects which phase to load (e.g., cardiac\n              phases, temporal positions, echo numbers). Use\n              :func:`pictologics.utilities.get_dicom_phases` to discover available phases.\n\n            Defaults to 0 (the first volume/phase).\n        recursive (bool, optional): If True, recursively searches subdirectories\n            for each path in `image_paths`. Defaults to False.\n        binarize (bool | int | list[int] | tuple[int, int] | None, optional):\n            Rules for binarizing the merged image.\n            - `None` (default): No binarization.\n            - `True`: Sets all voxels &gt; 0 to 1, others to 0.\n            - `int` (e.g., 2): Sets voxels == value to 1, others to 0.\n            - `list[int]` (e.g., [1, 2]): Sets voxels in list to 1, others to 0.\n            - `tuple[int, int]` (e.g., (1, 10)): Sets voxels in inclusive range to 1, others to 0.\n        reposition_to_reference (bool): If True and reference_image is provided,\n            each loaded image will be repositioned into the reference coordinate\n            space before merging. This is required when loading cropped segmentation\n            masks that have different dimensions than the reference. Geometry validation\n            is performed AFTER repositioning. Defaults to False.\n        transpose_axes (tuple[int, int, int] | None): Axis transposition to apply\n            when repositioning. E.g., (0, 2, 1) swaps Y and Z axes.\n            Only used when `reposition_to_reference=True`.\n        fill_value (float): Fill value for regions outside cropped masks when\n            repositioning (default: 0.0). Only used when `reposition_to_reference=True`.\n        relabel_masks (bool): If True, assigns unique label values (1, 2, 3, ...)\n            to each mask file based on its order in `image_paths`. This converts\n            binary [0,1] masks into multi-label masks where each file gets a\n            distinct label, useful for visualization with different colors.\n            Label assignment respects the order of `image_paths`. Defaults to False.\n        apply_rescale (bool): If True (default), apply RescaleSlope and RescaleIntercept\n            transformation for DICOM files to convert stored pixel values to real-world\n            values (e.g., Hounsfield Units for CT). Set to False if you need raw stored values.\n\n    Note:\n        The `binarize` parameter is intended for **mask filtering** (e.g., selecting specific ROI labels).\n        To filter image intensity values (e.g., HU ranges), use the preprocessing steps in the\n        radiomics pipeline configuration instead.\n\n    Returns:\n        Image: A new `Image` object containing the merged data.\n\n    Raises:\n        ValueError: If `image_paths` is empty, if an invalid `conflict_resolution` is provided,\n            if `reposition_to_reference=True` but `reference_image` is not provided,\n            or if the images (or reference) have mismatched geometries.\n\n    Example:\n        **Merging cropped segmentation masks:**\n        ```python\n        main_img = load_image(\"ct_scan/\", recursive=True)\n        seg_paths = [str(f) for f in Path(\"masks/\").glob(\"*.dcm\")]\n\n        merged = load_and_merge_images(\n            seg_paths,\n            reference_image=main_img,\n            reposition_to_reference=True,\n            conflict_resolution=\"max\",\n        )\n        ```\n    \"\"\"\n    if not image_paths:\n        raise ValueError(\"image_paths cannot be empty.\")\n\n    valid_strategies = {\"max\", \"min\", \"first\", \"last\"}\n    if conflict_resolution not in valid_strategies:\n        raise ValueError(\n            f\"Invalid conflict_resolution '{conflict_resolution}'. \"\n            f\"Must be one of {valid_strategies}.\"\n        )\n\n    if reposition_to_reference and reference_image is None:\n        raise ValueError(\n            \"reference_image must be provided when reposition_to_reference=True.\"\n        )\n\n    # Geometry validation helper\n    def _validate_geometry(target: Image, ref: Image, name: str, ref_name: str) -&gt; None:\n        if target.array.shape != ref.array.shape:\n            raise ValueError(\n                f\"Dimension mismatch between {name} {target.array.shape} \"\n                f\"and {ref_name} {ref.array.shape}.\"\n            )\n        if not np.allclose(target.spacing, ref.spacing, atol=1e-5):\n            raise ValueError(\n                f\"Spacing mismatch between {name} {target.spacing} \"\n                f\"and {ref_name} {ref.spacing}.\"\n            )\n        if not np.allclose(target.origin, ref.origin, atol=1e-5):\n            raise ValueError(\n                f\"Origin mismatch between {name} {target.origin} \"\n                f\"and {ref_name} {ref.origin}.\"\n            )\n        if target.direction is not None and ref.direction is not None:\n            if not np.allclose(target.direction, ref.direction, atol=1e-5):\n                raise ValueError(f\"Direction mismatch between {name} and {ref_name}.\")\n\n    if reposition_to_reference:\n        # Mode: Reposition each image to reference space, then merge\n        assert reference_image is not None  # Already validated above\n\n        # Initialize merged array with reference geometry\n        merged_array = np.full(\n            reference_image.array.shape, fill_value, dtype=np.float64\n        )\n\n        for i, path in enumerate(image_paths):\n            try:\n                current_image = load_image(\n                    path,\n                    dataset_index=dataset_index,\n                    recursive=recursive,\n                    apply_rescale=apply_rescale,\n                )\n            except Exception as e:\n                raise ValueError(f\"Failed to load image '{path}': {e}\") from e\n\n            # Reposition to reference space\n            repositioned = _position_in_reference(\n                current_image, reference_image, fill_value, transpose_axes\n            )\n\n            # Validate geometry after repositioning\n            _validate_geometry(\n                repositioned,\n                reference_image,\n                f\"repositioned image '{path}'\",\n                \"reference image\",\n            )\n\n            current_array = repositioned.array\n\n            # Apply relabeling: replace all non-zero values with mask index + 1\n            if relabel_masks:\n                label_value = i + 1  # 1-indexed labels\n                current_array = np.where(\n                    current_array != fill_value, label_value, fill_value\n                )\n\n            # Merge with conflict resolution\n            if i == 0:\n                # First image: just copy non-fill values\n                non_fill_mask = current_array != fill_value\n                merged_array[non_fill_mask] = current_array[non_fill_mask]\n            else:\n                # Subsequent images: apply conflict resolution\n                # Overlap: non-fill in both\n                overlap_mask = (merged_array != fill_value) &amp; (\n                    current_array != fill_value\n                )\n                # New data: fill in merged, non-fill in current\n                new_data_mask = (merged_array == fill_value) &amp; (\n                    current_array != fill_value\n                )\n\n                # Apply new data\n                merged_array[new_data_mask] = current_array[new_data_mask]\n\n                # Resolve conflicts\n                if np.any(overlap_mask):\n                    if conflict_resolution == \"max\":\n                        merged_array[overlap_mask] = np.maximum(\n                            merged_array[overlap_mask], current_array[overlap_mask]\n                        )\n                    elif conflict_resolution == \"min\":\n                        merged_array[overlap_mask] = np.minimum(\n                            merged_array[overlap_mask], current_array[overlap_mask]\n                        )\n                    elif conflict_resolution == \"last\":\n                        merged_array[overlap_mask] = current_array[overlap_mask]\n                    elif conflict_resolution == \"first\":\n                        pass  # Keep existing values\n\n        # Use reference geometry for output\n        consensus_spacing = reference_image.spacing\n        consensus_origin = reference_image.origin\n        consensus_direction = reference_image.direction\n\n    else:\n        # Mode: Standard merging with strict geometry validation\n        # Load the first image to serve as the consensus geometry\n        try:\n            consensus_image = load_image(\n                image_paths[0],\n                dataset_index=dataset_index,\n                recursive=recursive,\n                apply_rescale=apply_rescale,\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Failed to load first image '{image_paths[0]}': {e}\"\n            ) from e\n\n        merged_array = consensus_image.array.astype(np.float64)\n\n        # Apply relabeling for the first image\n        if relabel_masks:\n            merged_array = np.where(merged_array != 0, 1, 0).astype(merged_array.dtype)\n\n        # Iterate through remaining images\n        for idx, path in enumerate(image_paths[1:], start=2):\n            try:\n                current_image = load_image(\n                    path,\n                    dataset_index=dataset_index,\n                    recursive=recursive,\n                    apply_rescale=apply_rescale,\n                )\n            except Exception as e:\n                raise ValueError(f\"Failed to load image '{path}': {e}\") from e\n\n            _validate_geometry(\n                current_image, consensus_image, f\"image '{path}'\", \"consensus image\"\n            )\n\n            current_array = current_image.array\n\n            # Apply relabeling: replace all non-zero values with mask index\n            if relabel_masks:\n                label_value = idx  # idx starts at 2 for second file\n                current_array = np.where(current_array != 0, label_value, 0).astype(\n                    current_array.dtype\n                )\n\n            # Identify regions\n            overlap_mask = (merged_array != 0) &amp; (current_array != 0)\n            new_data_mask = (merged_array == 0) &amp; (current_array != 0)\n\n            # Apply new data\n            merged_array[new_data_mask] = current_array[new_data_mask]\n\n            # Resolve conflicts\n            if np.any(overlap_mask):\n                if conflict_resolution == \"max\":\n                    merged_array[overlap_mask] = np.maximum(\n                        merged_array[overlap_mask], current_array[overlap_mask]\n                    )\n                elif conflict_resolution == \"min\":\n                    merged_array[overlap_mask] = np.minimum(\n                        merged_array[overlap_mask], current_array[overlap_mask]\n                    )\n                elif conflict_resolution == \"last\":\n                    merged_array[overlap_mask] = current_array[overlap_mask]\n                elif conflict_resolution == \"first\":\n                    pass  # Already have the 'first' value\n\n        consensus_spacing = consensus_image.spacing\n        consensus_origin = consensus_image.origin\n        consensus_direction = consensus_image.direction\n\n        # Validate against reference image if provided (for non-reposition mode)\n        if reference_image is not None:\n            final_merged_image = Image(\n                array=merged_array,\n                spacing=consensus_spacing,\n                origin=consensus_origin,\n                direction=consensus_direction,\n                modality=\"Image\",\n            )\n            _validate_geometry(\n                final_merged_image, reference_image, \"merged image\", \"reference image\"\n            )\n\n    # Apply binarization if requested\n    if binarize is not None:\n        mask_out: npt.NDArray[np.floating[Any]] = np.zeros_like(\n            merged_array, dtype=np.uint8\n        )\n        if isinstance(binarize, bool) and binarize is True:\n            mask_out[merged_array &gt; 0] = 1\n        elif isinstance(binarize, int) and not isinstance(binarize, bool):\n            mask_out[merged_array == binarize] = 1\n        elif isinstance(binarize, list):\n            mask_out[np.isin(merged_array, binarize)] = 1\n        elif isinstance(binarize, tuple) and len(binarize) == 2:\n            mask_out[(merged_array &gt;= binarize[0]) &amp; (merged_array &lt;= binarize[1])] = 1\n        else:\n            if binarize is not False:\n                raise ValueError(f\"Unsupported binarize value: {binarize}\")\n            mask_out = merged_array\n\n        if binarize is not False:\n            merged_array = mask_out.astype(np.float64)\n\n    return Image(\n        array=merged_array,\n        spacing=consensus_spacing,\n        origin=consensus_origin,\n        direction=consensus_direction,\n        modality=\"MergedImage\",\n    )\n</code></pre>"},{"location":"api/loader/#pictologics.loader.create_full_mask","title":"<code>create_full_mask(reference_image, dtype=np.uint8)</code>","text":"<p>Create a whole-image ROI mask matching a reference image.</p> <p>This utility is primarily used when a user does not provide a segmentation mask. The returned mask has the same geometry (shape, spacing, origin, direction) as the reference image and contains a value of 1 for every voxel.</p> <p>Parameters:</p> Name Type Description Default <code>reference_image</code> <code>Image</code> <p>Image whose geometry should be copied.</p> required <code>dtype</code> <code>DTypeLike</code> <p>Numpy dtype to use for the mask array. Defaults to <code>np.uint8</code>.</p> <code>uint8</code> <p>Returns:</p> Type Description <code>Image</code> <p>An <code>Image</code> mask with <code>array == 1</code> everywhere.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reference image does not have a valid 3D array.</p> Source code in <code>pictologics/loader.py</code> <pre><code>def create_full_mask(reference_image: Image, dtype: DTypeLike = np.uint8) -&gt; Image:\n    \"\"\"Create a whole-image ROI mask matching a reference image.\n\n    This utility is primarily used when a user does not provide a segmentation mask.\n    The returned mask has the same geometry (shape, spacing, origin, direction) as\n    the reference image and contains a value of 1 for every voxel.\n\n    Args:\n        reference_image: Image whose geometry should be copied.\n        dtype: Numpy dtype to use for the mask array. Defaults to `np.uint8`.\n\n    Returns:\n        An `Image` mask with `array == 1` everywhere.\n\n    Raises:\n        ValueError: If the reference image does not have a valid 3D array.\n    \"\"\"\n    if reference_image.array.ndim != 3:\n        raise ValueError(\n            f\"reference_image.array must be 3D, got shape {reference_image.array.shape}\"\n        )\n\n    mask_array = np.ones(reference_image.array.shape, dtype=dtype)\n    return Image(\n        array=mask_array,\n        spacing=reference_image.spacing,\n        origin=reference_image.origin,\n        direction=reference_image.direction,\n        modality=\"mask\",\n    )\n</code></pre>"},{"location":"api/loader/#pictologics.loaders.seg_loader","title":"<code>pictologics.loaders.seg_loader</code>","text":""},{"location":"api/loader/#pictologics.loaders.seg_loader--dicom-segmentation-seg-loader","title":"DICOM Segmentation (SEG) Loader","text":"<p>This module provides functionality for loading DICOM Segmentation objects as pictologics Image instances. SEG files are specialized DICOM objects that store segmentation masks with multi-segment support.</p> <p>Uses highdicom for robust SEG parsing and extraction.</p>"},{"location":"api/loader/#pictologics.loaders.seg_loader.load_seg","title":"<code>load_seg(path, segment_numbers=None, combine_segments=True, reference_image=None)</code>","text":"<p>Load a DICOM SEG file as a mask Image.</p> <p>This function loads a DICOM Segmentation object and converts it to the standard pictologics Image format. The resulting Image has the same structure as images returned by load_image():</p> <ul> <li>array: npt.NDArray[np.floating[Any]] with shape (X, Y, Z)</li> <li>spacing: tuple[float, float, float] in mm</li> <li>origin: tuple[float, float, float] in mm</li> <li>direction: Optional[npt.NDArray[np.floating[Any]]] - 3x3 direction cosines</li> <li>modality: str - set to \"SEG\"</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the DICOM SEG file.</p> required <code>segment_numbers</code> <code>list[int] | None</code> <p>Specific segment numbers to extract. If None, all segments are extracted. Segment numbers are 1-indexed as per DICOM convention.</p> <code>None</code> <code>combine_segments</code> <code>bool</code> <p>Controls how segments are returned:</p> <ul> <li> <p>True (default): Returns a single Image where each segment   is encoded as its segment number (1, 2, 3...) in the voxel values.   Background voxels are 0. This is useful when you want a single   label map for visualization or when segments are mutually exclusive   (e.g., organ segmentation where each voxel belongs to one structure).</p> </li> <li> <p>False: Returns a dict mapping segment numbers to individual   binary Image masks. Each mask contains only 0s and 1s. This is   useful when:</p> </li> <li> <p>Segments may overlap (e.g., nested structures like tumor     within organ)</p> </li> <li>You need to process each segment independently (e.g., extract     radiomics from each segment separately)</li> <li>You want to select specific segments for different analyses</li> </ul> <code>True</code> <code>reference_image</code> <code>'Image | None'</code> <p>Optional reference Image for geometry alignment. When provided, the output mask will be resampled/repositioned to match the reference geometry.</p> <code>None</code> <p>Returns:</p> Type Description <code>'Image | dict[int, Image]'</code> <p>If combine_segments is True: A single Image with segment labels.</p> <code>'Image | dict[int, Image]'</code> <p>If combine_segments is False: A dict of {segment_number: Image}.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file is not a valid DICOM SEG object.</p> <code>FileNotFoundError</code> <p>If the file does not exist.</p> Example <p>Load a SEG file with all segments combined (label map):</p> <pre><code>from pictologics.loaders import load_seg\nimport numpy as np\n\nmask = load_seg(\"segmentation.dcm\")\nprint(mask.array.shape)  # (X, Y, Z)\nprint(np.unique(mask.array))  # [0, 1, 2, ...]\n</code></pre> <p>Load specific segments as separate binary masks:</p> <pre><code>masks = load_seg(\"segmentation.dcm\", segment_numbers=[1, 2], combine_segments=False)\nfor seg_num, mask in masks.items():\n    print(f\"Segment {seg_num}: {mask.array.sum()} voxels\")\n</code></pre> <p>Align mask to a reference CT image:</p> <pre><code>from pictologics import load_image\n\nct = load_image(\"ct_scan/\")\nmask = load_seg(\"segmentation.dcm\", reference_image=ct)\nassert mask.array.shape == ct.array.shape\n</code></pre> Source code in <code>pictologics/loaders/seg_loader.py</code> <pre><code>def load_seg(\n    path: str | Path,\n    segment_numbers: list[int] | None = None,\n    combine_segments: bool = True,\n    reference_image: \"Image | None\" = None,\n) -&gt; \"Image | dict[int, Image]\":\n    \"\"\"Load a DICOM SEG file as a mask Image.\n\n    This function loads a DICOM Segmentation object and converts it to\n    the standard pictologics Image format. The resulting Image has the\n    same structure as images returned by load_image():\n\n    - array: npt.NDArray[np.floating[Any]] with shape (X, Y, Z)\n    - spacing: tuple[float, float, float] in mm\n    - origin: tuple[float, float, float] in mm\n    - direction: Optional[npt.NDArray[np.floating[Any]]] - 3x3 direction cosines\n    - modality: str - set to \"SEG\"\n\n    Args:\n        path: Path to the DICOM SEG file.\n        segment_numbers: Specific segment numbers to extract. If None, all\n            segments are extracted. Segment numbers are 1-indexed as per\n            DICOM convention.\n        combine_segments: Controls how segments are returned:\n\n            - **True (default)**: Returns a single Image where each segment\n              is encoded as its segment number (1, 2, 3...) in the voxel values.\n              Background voxels are 0. This is useful when you want a single\n              label map for visualization or when segments are mutually exclusive\n              (e.g., organ segmentation where each voxel belongs to one structure).\n\n            - **False**: Returns a dict mapping segment numbers to individual\n              binary Image masks. Each mask contains only 0s and 1s. This is\n              useful when:\n\n              - Segments may overlap (e.g., nested structures like tumor\n                within organ)\n              - You need to process each segment independently (e.g., extract\n                radiomics from each segment separately)\n              - You want to select specific segments for different analyses\n\n        reference_image: Optional reference Image for geometry alignment.\n            When provided, the output mask will be resampled/repositioned\n            to match the reference geometry.\n\n    Returns:\n        If combine_segments is True: A single Image with segment labels.\n        If combine_segments is False: A dict of {segment_number: Image}.\n\n    Raises:\n        ValueError: If the file is not a valid DICOM SEG object.\n        FileNotFoundError: If the file does not exist.\n\n    Example:\n        Load a SEG file with all segments combined (label map):\n\n        ```python\n        from pictologics.loaders import load_seg\n        import numpy as np\n\n        mask = load_seg(\"segmentation.dcm\")\n        print(mask.array.shape)  # (X, Y, Z)\n        print(np.unique(mask.array))  # [0, 1, 2, ...]\n        ```\n\n        Load specific segments as separate binary masks:\n\n        ```python\n        masks = load_seg(\"segmentation.dcm\", segment_numbers=[1, 2], combine_segments=False)\n        for seg_num, mask in masks.items():\n            print(f\"Segment {seg_num}: {mask.array.sum()} voxels\")\n        ```\n\n        Align mask to a reference CT image:\n\n        ```python\n        from pictologics import load_image\n\n        ct = load_image(\"ct_scan/\")\n        mask = load_seg(\"segmentation.dcm\", reference_image=ct)\n        assert mask.array.shape == ct.array.shape\n        ```\n    \"\"\"\n    import highdicom as hd\n\n    from pictologics.loader import Image\n\n    path_obj = Path(path)\n    if not path_obj.exists():\n        raise FileNotFoundError(f\"SEG file not found: {path}\")\n\n    # Load the DICOM SEG using highdicom\n    try:\n        seg = hd.seg.segread(str(path_obj))\n    except Exception as e:\n        raise ValueError(f\"Failed to load DICOM SEG file: {e}\") from e\n\n    # Verify it's a SEG object\n    if not hasattr(seg, \"SegmentSequence\"):\n        raise ValueError(f\"File is not a valid DICOM SEG object: {path}\")\n\n    # Get available segment numbers\n    available_segments = [s.SegmentNumber for s in seg.SegmentSequence]\n\n    # Determine which segments to extract\n    if segment_numbers is None:\n        target_segments = available_segments\n    else:\n        # Validate requested segments exist\n        for seg_num in segment_numbers:\n            if seg_num not in available_segments:\n                raise ValueError(\n                    f\"Segment {seg_num} not found. \"\n                    f\"Available segments: {available_segments}\"\n                )\n        target_segments = segment_numbers\n\n    # Extract geometry information from the SEG\n    spacing, origin, direction = _extract_seg_geometry(seg)\n\n    # Extract pixel array - shape is typically (frames, rows, cols)\n    pixel_array = seg.pixel_array\n\n    # Get the number of segments and frames\n    n_frames = pixel_array.shape[0] if pixel_array.ndim == 3 else 1\n\n    if combine_segments:\n        # Create combined label image\n        combined_array = _extract_combined_segments(\n            seg, pixel_array, target_segments, n_frames\n        )\n\n        # Reorder axes from (Z, Y, X) or (frames, rows, cols) to (X, Y, Z)\n        combined_array = np.transpose(combined_array, (2, 1, 0))\n\n        result = Image(\n            array=combined_array,\n            spacing=spacing,\n            origin=origin,\n            direction=direction,\n            modality=\"SEG\",\n        )\n\n        # Align to reference if provided\n        if reference_image is not None:\n            result = _align_to_reference(result, reference_image)\n\n        return result\n    else:\n        # Return dict of individual segment masks\n        result_dict: dict[int, Image] = {}\n\n        for seg_num in target_segments:\n            mask_array = _extract_single_segment(seg, pixel_array, seg_num, n_frames)\n\n            # Reorder axes from (Z, Y, X) to (X, Y, Z)\n            mask_array = np.transpose(mask_array, (2, 1, 0))\n\n            mask_image = Image(\n                array=mask_array.astype(np.uint8),\n                spacing=spacing,\n                origin=origin,\n                direction=direction,\n                modality=\"SEG\",\n            )\n\n            # Align to reference if provided\n            if reference_image is not None:\n                mask_image = _align_to_reference(mask_image, reference_image)\n\n            result_dict[seg_num] = mask_image\n\n        return result_dict\n</code></pre>"},{"location":"api/loader/#pictologics.loaders.seg_loader.get_segment_info","title":"<code>get_segment_info(path)</code>","text":"<p>Get information about segments in a DICOM SEG file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the DICOM SEG file.</p> required <p>Returns:</p> Type Description <code>list[dict[str, str | int]]</code> <p>List of dicts with segment information:</p> <code>list[dict[str, str | int]]</code> <ul> <li>segment_number: int</li> </ul> <code>list[dict[str, str | int]]</code> <ul> <li>segment_label: str</li> </ul> <code>list[dict[str, str | int]]</code> <ul> <li>segment_description: str (if available)</li> </ul> <code>list[dict[str, str | int]]</code> <ul> <li>algorithm_type: str (if available)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file is not a valid DICOM SEG object.</p> Source code in <code>pictologics/loaders/seg_loader.py</code> <pre><code>def get_segment_info(path: str | Path) -&gt; list[dict[str, str | int]]:\n    \"\"\"Get information about segments in a DICOM SEG file.\n\n    Args:\n        path: Path to the DICOM SEG file.\n\n    Returns:\n        List of dicts with segment information:\n        - segment_number: int\n        - segment_label: str\n        - segment_description: str (if available)\n        - algorithm_type: str (if available)\n\n    Raises:\n        ValueError: If the file is not a valid DICOM SEG object.\n    \"\"\"\n    import highdicom as hd\n\n    path_obj = Path(path)\n    if not path_obj.exists():\n        raise FileNotFoundError(f\"SEG file not found: {path}\")\n\n    try:\n        seg = hd.seg.segread(str(path_obj))\n    except Exception as e:\n        raise ValueError(f\"Failed to load DICOM SEG file: {e}\") from e\n\n    if not hasattr(seg, \"SegmentSequence\"):\n        raise ValueError(f\"File is not a valid DICOM SEG object: {path}\")\n\n    segments = []\n    for segment in seg.SegmentSequence:\n        info: dict[str, str | int] = {\n            \"segment_number\": segment.SegmentNumber,\n            \"segment_label\": getattr(segment, \"SegmentLabel\", \"\"),\n        }\n\n        if hasattr(segment, \"SegmentDescription\"):\n            info[\"segment_description\"] = segment.SegmentDescription\n\n        if hasattr(segment, \"SegmentAlgorithmType\"):\n            info[\"algorithm_type\"] = segment.SegmentAlgorithmType\n\n        segments.append(info)\n\n    return segments\n</code></pre>"},{"location":"api/pipeline/","title":"Pipeline API","text":""},{"location":"api/pipeline/#pictologics.pipeline","title":"<code>pictologics.pipeline</code>","text":""},{"location":"api/pipeline/#pictologics.pipeline--radiomics-pipeline-module","title":"Radiomics Pipeline Module","text":"<p>This module provides a flexible, configurable pipeline for executing radiomic feature extraction workflows. It allows users to define sequences of preprocessing steps and feature extraction tasks.</p>"},{"location":"api/pipeline/#pictologics.pipeline--key-features","title":"Key Features:","text":"<ul> <li>Configurable Workflows: Define steps like resampling, resegmentation, filtering,   discretisation, and feature extraction in a declarative manner.</li> <li>State Management: Tracks the state of the image and masks (morphological and intensity)   throughout the pipeline.</li> <li>Logging: Records execution details, parameters, and errors for reproducibility.</li> <li>Batch Processing: Can process multiple configurations on the same input data.</li> </ul>"},{"location":"api/pipeline/#pictologics.pipeline.EmptyROIMaskError","title":"<code>EmptyROIMaskError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when preprocessing yields an empty ROI mask.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>class EmptyROIMaskError(ValueError):\n    \"\"\"Raised when preprocessing yields an empty ROI mask.\"\"\"\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.PipelineState","title":"<code>PipelineState</code>  <code>dataclass</code>","text":"<p>Holds the current state of the image and masks during pipeline execution.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>@dataclass\nclass PipelineState:\n    \"\"\"\n    Holds the current state of the image and masks during pipeline execution.\n    \"\"\"\n\n    image: Image  # May be discretised after discretise step\n    raw_image: Image  # Always the non-discretised image (for intensity/morphology)\n    morph_mask: Image\n    intensity_mask: Image\n    is_discretised: bool = False\n    n_bins: Optional[int] = None\n    bin_width: Optional[float] = None\n    discretisation_method: Optional[str] = None\n    discretisation_min: Optional[float] = None\n    discretisation_max: Optional[float] = None\n    mask_was_generated: bool = False\n    is_filtered: bool = False\n    filter_type: Optional[str] = None\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline","title":"<code>RadiomicsPipeline</code>","text":"<p>A flexible, configurable pipeline for radiomic feature extraction. Allows defining multiple processing configurations (sequences of steps) to be run on data.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>class RadiomicsPipeline:\n    \"\"\"\n    A flexible, configurable pipeline for radiomic feature extraction.\n    Allows defining multiple processing configurations (sequences of steps) to be run on data.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize pipeline with empty config registry and load predefined configs.\"\"\"\n        self._configs: dict[str, list[dict[str, Any]]] = {}\n        self._log: list[dict[str, Any]] = []\n        self._load_predefined_configs()\n\n    def _load_predefined_configs(self) -&gt; None:\n        \"\"\"\n        Load predefined, commonly used pipeline configurations from templates.\n        \"\"\"\n        try:\n            standard_configs = get_standard_templates()\n            for name, steps in standard_configs.items():\n                # Convert YAML lists to tuples where needed (e.g., new_spacing)\n                converted_steps = self._convert_yaml_steps(steps)\n                self._configs[name] = converted_steps\n        except Exception as e:\n            _logger.warning(f\"Failed to load standard templates: {e}\")\n            # Fallback to empty configs - user can add their own\n            pass\n\n    def _convert_yaml_steps(self, steps: list[dict[str, Any]]) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Convert YAML-loaded steps to internal format.\n\n        YAML loads lists, but some parameters expect tuples (e.g., new_spacing).\n        \"\"\"\n        converted = []\n        for step in steps:\n            new_step = {\"step\": step[\"step\"]}\n            if \"params\" in step:\n                params = copy.deepcopy(step[\"params\"])\n                # Convert new_spacing list to tuple\n                if \"new_spacing\" in params and isinstance(params[\"new_spacing\"], list):\n                    params[\"new_spacing\"] = tuple(params[\"new_spacing\"])\n                new_step[\"params\"] = params\n            converted.append(new_step)\n        return converted\n\n    def get_all_standard_config_names(self) -&gt; list[str]:\n        \"\"\"\n        Returns the list of all standard configuration names.\n\n        Returns names from loaded templates that start with 'standard_'.\n        \"\"\"\n        return sorted([name for name in self._configs.keys() if name.startswith(\"standard_\")])\n\n    def add_config(self, name: str, steps: list[dict[str, Any]]) -&gt; \"RadiomicsPipeline\":\n        \"\"\"\n        Add a processing configuration.\n\n        Args:\n            name: Unique name for this configuration.\n            steps: List of steps. Each step is a dict with 'step' (name) and 'params' (dict).\n                   Supported steps:\n                   - 'resample': params: new_spacing (required), interpolation (optional)\n                                     - 'resegment': params: range_min, range_max\n                                     - 'filter_outliers': params: sigma\n                                     - 'binarize_mask': params: threshold (float, default 0.5),\n                                         mask_values (int | list[int] | tuple[int, int]), apply_to ('morph'|'intensity'|'both')\n                                     - 'keep_largest_component': params: None\n                   - 'round_intensities': params: None\n                   - 'discretise': params: method, n_bins/bin_width, etc.\n                   - 'extract_features': params: families (list), etc.\n\n        Note:\n            - Texture features require a prior 'discretise' step.\n            - IVH features are configured via 'ivh_params' dict.\n        \"\"\"\n        if not isinstance(steps, list):\n            raise ValueError(\"Configuration must be a list of steps\")\n\n        for step in steps:\n            if not isinstance(step, dict):\n                raise ValueError(\"Each step must be a dictionary\")\n            if \"step\" not in step:\n                raise ValueError(\"Each step must have a 'step' key\")\n\n        self._configs[name] = steps\n        return self\n\n    def run(\n        self,\n        image: str | Image,\n        mask: str | Image | None = None,\n        subject_id: Optional[str] = None,\n        config_names: Optional[list[str]] = None,\n    ) -&gt; dict[str, pd.Series]:\n        \"\"\"\n        Run configurations on the provided image and mask.\n\n        Args:\n            image: Path to image or Image object.\n            mask: Optional path to mask or Image object.\n                If omitted (or passed as `None` / empty string), the pipeline will\n                treat the **entire image** as the ROI by generating a full (all-ones)\n                mask matching the input image geometry.\n            subject_id: Optional identifier for the subject.\n            config_names: List of specific configuration names to run.\n                          If None, runs all registered configurations.\n                          Supports \"all_standard\" to run all 6 standard configs.\n\n        Returns:\n            Dictionary mapping config names to pandas Series of features.\n\n        Example:\n            Run standard pipeline components:\n\n            ```python\n            from pictologics.pipeline import RadiomicsPipeline\n\n            # Initialize\n            pipeline = RadiomicsPipeline()\n\n            # Run on image and mask\n            results = pipeline.run(\n                image=\"data/image.nii.gz\",\n                mask=\"data/mask.nii.gz\",\n                subject_id=\"subject_001\",\n                config_names=[\"standard_fbn_32\"]\n            )\n\n            # Access results\n            print(results[\"standard_fbn_32\"].head())\n            ```\n        \"\"\"\n        # 1. Load Data\n        if isinstance(image, str):\n            orig_img = load_image(image)\n            img_source = image\n        else:\n            orig_img = image\n            img_source = \"InMemory\"\n\n        mask_was_generated = False\n        if mask is None or (isinstance(mask, str) and mask.strip() == \"\"):\n            orig_mask = create_full_mask(orig_img)\n            mask_source = \"GeneratedFullMask\"\n            mask_was_generated = True\n        elif isinstance(mask, str):\n            orig_mask = load_image(mask)\n            mask_source = mask\n        else:\n            orig_mask = mask\n            mask_source = \"InMemory\"\n\n        all_results = {}\n\n        # Determine which configs to run\n        if config_names is None:\n            target_configs = list(self._configs.keys())\n        else:\n            target_configs = []\n            for name in config_names:\n                if name == \"all_standard\":\n                    target_configs.extend(self.get_all_standard_config_names())\n                elif name in self._configs:\n                    target_configs.append(name)\n                else:\n                    raise ValueError(f\"Configuration '{name}' not found.\")\n\n        # Run each configuration\n        for config_name in target_configs:\n            steps = self._configs[config_name]\n\n            # Initialize State\n            # We start with fresh copies for each config\n            state = PipelineState(\n                image=orig_img,\n                raw_image=orig_img,  # Track non-discretised image\n                morph_mask=orig_mask,\n                intensity_mask=Image(\n                    array=orig_mask.array.copy(),\n                    spacing=orig_mask.spacing,\n                    origin=orig_mask.origin,\n                    direction=orig_mask.direction,\n                    modality=orig_mask.modality,\n                ),\n                mask_was_generated=mask_was_generated,\n            )\n\n            self._ensure_nonempty_roi(state, context=\"initialization\")\n\n            config_log: dict[str, Any] = {\n                \"timestamp\": datetime.datetime.now().isoformat(),\n                \"subject_id\": subject_id,\n                \"config_name\": config_name,\n                \"image_source\": img_source,\n                \"mask_source\": mask_source,\n                \"steps_executed\": [],\n            }\n\n            config_features = {}\n\n            try:\n                for step_def in steps:\n                    step_name = step_def[\"step\"]\n                    params = step_def.get(\"params\", {})\n\n                    # Execute Step\n                    if step_name == \"extract_features\":\n                        features = self._extract_features(state, params)\n                        config_features.update(features)\n                    else:\n                        self._execute_preprocessing_step(state, step_name, params)\n\n                    # Log\n                    config_log[\"steps_executed\"].append(\n                        {\"step\": step_name, \"params\": params, \"status\": \"completed\"}\n                    )\n\n            except Exception as e:\n                config_log[\"error\"] = str(e)\n                config_log[\"failed_step\"] = step_def\n                print(f\"Error in config '{config_name}', step '{step_def}': {e}\")\n\n                # For empty ROI, fail fast (do not silently return empty/partial features).\n                if isinstance(e, EmptyROIMaskError):\n                    self._log.append(config_log)\n                    raise\n\n            self._log.append(config_log)\n\n            # Create Series\n            series = pd.Series(config_features)\n            if subject_id:\n                series[\"subject_id\"] = subject_id\n            all_results[config_name] = series\n\n        return all_results\n\n    def clear_log(self) -&gt; None:\n        \"\"\"Clear the in-memory processing log.\"\"\"\n        self._log.clear()\n\n    def _ensure_nonempty_roi(self, state: PipelineState, context: str) -&gt; None:\n        \"\"\"Raise a clear error if the ROI is empty.\n\n        The pipeline uses `mask_values=1` semantics throughout (see `apply_mask`).\n        \"\"\"\n        has_intensity_roi = bool(np.any(state.intensity_mask.array == 1))\n        has_morph_roi = bool(np.any(state.morph_mask.array == 1))\n\n        if not has_intensity_roi or not has_morph_roi:\n            raise EmptyROIMaskError(\n                \"ROI is empty after preprocessing \"\n                f\"({context}). Ensure your mask contains at least one voxel with value 1, \"\n                \"or relax resegmentation/outlier filtering thresholds.\"\n            )\n\n    def _execute_preprocessing_step(\n        self, state: PipelineState, step_name: str, params: dict[str, Any]\n    ) -&gt; None:\n        \"\"\"\n        Execute a single preprocessing step and update the state in-place.\n        \"\"\"\n        if step_name == \"resample\":\n            # Params\n            if \"new_spacing\" not in params:\n                raise ValueError(\"Resample step requires 'new_spacing' parameter.\")\n\n            spacing = params[\"new_spacing\"]\n            interp_img = params.get(\"interpolation\", \"linear\")\n            interp_mask = params.get(\"mask_interpolation\", \"nearest\")\n            mask_thresh = params.get(\"mask_threshold\", 0.5)\n            round_intensities_flag = params.get(\"round_intensities\", False)\n\n            # Update Image and raw_image\n            state.image = resample_image(\n                state.image,\n                spacing,\n                interpolation=interp_img,\n                round_intensities=round_intensities_flag,\n            )\n            state.raw_image = (\n                state.image\n            )  # Keep raw_image in sync before discretisation\n\n            # Update Masks\n            thresh_arg = mask_thresh if interp_mask != \"nearest\" else None\n            state.morph_mask = resample_image(\n                state.morph_mask,\n                spacing,\n                interpolation=interp_mask,\n                mask_threshold=thresh_arg,\n            )\n            state.intensity_mask = resample_image(\n                state.intensity_mask,\n                spacing,\n                interpolation=interp_mask,\n                mask_threshold=thresh_arg,\n            )\n\n            self._ensure_nonempty_roi(state, context=\"resample\")\n\n        elif step_name == \"resegment\":\n            range_min = params.get(\"range_min\")\n            range_max = params.get(\"range_max\")\n            state.intensity_mask = resegment_mask(\n                state.image, state.intensity_mask, range_min, range_max\n            )\n\n            # If the mask was auto-generated (mask omitted), treat resegmentation as ROI definition\n            # for both intensity and morphology features.\n            if state.mask_was_generated:\n                state.morph_mask = resegment_mask(\n                    state.image, state.morph_mask, range_min, range_max\n                )\n\n            self._ensure_nonempty_roi(state, context=\"resegment\")\n\n        elif step_name == \"filter_outliers\":\n            sigma = params.get(\"sigma\", 3.0)\n            state.intensity_mask = filter_outliers(\n                state.image, state.intensity_mask, sigma\n            )\n\n            if state.mask_was_generated:\n                state.morph_mask = filter_outliers(state.image, state.morph_mask, sigma)\n\n            self._ensure_nonempty_roi(state, context=\"filter_outliers\")\n\n        elif step_name == \"round_intensities\":\n            state.image = round_intensities(state.image)\n            state.raw_image = (\n                state.image\n            )  # Keep raw_image in sync before discretisation\n\n        elif step_name == \"keep_largest_component\":\n            # apply_to: \"morph\", \"intensity\", or \"both\" (default)\n            apply_to = params.get(\"apply_to\", \"both\")\n            if apply_to in (\"morph\", \"both\"):\n                state.morph_mask = keep_largest_component(state.morph_mask)\n            if apply_to in (\"intensity\", \"both\"):\n                state.intensity_mask = keep_largest_component(state.intensity_mask)\n\n            self._ensure_nonempty_roi(state, context=\"keep_largest_component\")\n\n        elif step_name == \"binarize_mask\":\n            apply_to = params.get(\"apply_to\", \"both\")\n            threshold = params.get(\"threshold\", 0.5)\n            mask_values = params.get(\"mask_values\")\n\n            def _binarize(image: Image) -&gt; Image:\n                if mask_values is not None:\n                    if isinstance(mask_values, tuple) and len(mask_values) == 2:\n                        lo, hi = mask_values\n                        mask_arr = (image.array &gt;= lo) &amp; (image.array &lt;= hi)\n                    else:\n                        values = mask_values\n                        if isinstance(values, int):\n                            values = [values]\n                        mask_arr = np.isin(image.array, values)\n                else:\n                    if threshold is None:\n                        raise ValueError(\n                            \"binarize_mask requires 'threshold' unless mask_values is provided\"\n                        )\n                    mask_arr = image.array &gt;= float(threshold)\n\n                return Image(\n                    array=mask_arr.astype(np.uint8),\n                    spacing=image.spacing,\n                    origin=image.origin,\n                    direction=image.direction,\n                    modality=image.modality,\n                )\n\n            if apply_to in (\"morph\", \"both\"):\n                state.morph_mask = _binarize(state.morph_mask)\n            if apply_to in (\"intensity\", \"both\"):\n                state.intensity_mask = _binarize(state.intensity_mask)\n\n            self._ensure_nonempty_roi(state, context=\"binarize_mask\")\n\n        elif step_name == \"discretise\":\n            self._ensure_nonempty_roi(state, context=\"discretise\")\n            method = params.get(\"method\", \"FBN\")\n\n            # Avoid passing 'method' twice\n            disc_params = params.copy()\n            if \"method\" in disc_params:\n                del disc_params[\"method\"]\n\n            state.image = cast(\n                Image,\n                discretise_image(\n                    state.image,\n                    method=method,\n                    roi_mask=state.intensity_mask,\n                    **disc_params,\n                ),\n            )\n\n            state.is_discretised = True\n            state.discretisation_method = method\n            state.n_bins = params.get(\"n_bins\")\n            state.bin_width = params.get(\"bin_width\")\n\n            # If FBS, n_bins is dynamic. We can estimate it from the result.\n            if method == \"FBS\":\n                masked_vals = apply_mask(state.image, state.intensity_mask)\n                if len(masked_vals) &gt; 0:\n                    state.n_bins = int(np.max(masked_vals))\n                else:\n                    raise EmptyROIMaskError(\n                        \"ROI is empty after preprocessing (discretise). \"\n                        \"Cannot infer FBS bin count from an empty ROI.\"\n                    )\n\n        elif step_name == \"filter\":\n            # Apply image filter\n            filter_type = params.get(\"type\")\n            if not filter_type:\n                raise ValueError(\"Filter step requires 'type' parameter.\")\n\n            # Get boundary condition (default: mirror per IBSI 2)\n            boundary_str = params.get(\"boundary\", \"mirror\")\n            boundary_map = {\n                \"mirror\": BoundaryCondition.MIRROR,\n                \"nearest\": BoundaryCondition.NEAREST,\n                \"constant\": BoundaryCondition.ZERO,\n                \"wrap\": BoundaryCondition.PERIODIC,\n                \"zero\": BoundaryCondition.ZERO,\n                \"periodic\": BoundaryCondition.PERIODIC,\n            }\n            boundary = boundary_map.get(boundary_str, BoundaryCondition.MIRROR)\n\n            # Extract filter-specific params (exclude type and boundary)\n            filter_params = {\n                k: v for k, v in params.items() if k not in (\"type\", \"boundary\")\n            }\n\n            # Apply filter based on type\n            filtered_array: npt.NDArray[np.floating[Any]]\n\n            if filter_type == \"mean\":\n                filter_params[\"boundary\"] = boundary\n                filtered_array = mean_filter(state.image.array, **filter_params)\n\n            elif filter_type == \"log\":\n                filter_params[\"boundary\"] = boundary\n                # Use image spacing if not provided\n                if \"spacing_mm\" not in filter_params:\n                    filter_params[\"spacing_mm\"] = state.image.spacing\n                filtered_array = laplacian_of_gaussian(\n                    state.image.array, **filter_params\n                )\n\n            elif filter_type == \"laws\":\n                filter_params[\"boundary\"] = boundary\n                # 'kernel' param maps to first positional arg\n                kernel = filter_params.pop(\"kernel\", \"L5E5E5\")\n                filtered_array = laws_filter(state.image.array, kernel, **filter_params)\n\n            elif filter_type == \"gabor\":\n                filter_params[\"boundary\"] = boundary\n                if \"spacing_mm\" not in filter_params:\n                    filter_params[\"spacing_mm\"] = state.image.spacing\n                filtered_array = gabor_filter(state.image.array, **filter_params)\n\n            elif filter_type == \"wavelet\":\n                filter_params[\"boundary\"] = boundary\n                filtered_array = wavelet_transform(state.image.array, **filter_params)\n\n            elif filter_type == \"simoncelli\":\n                # Simoncelli doesn't use boundary param\n                filtered_array = simoncelli_wavelet(state.image.array, **filter_params)\n\n            elif filter_type == \"riesz\":\n                # Riesz transform variants\n                variant = filter_params.pop(\"variant\", \"base\")\n                if variant == \"log\":\n                    if \"spacing_mm\" not in filter_params:\n                        filter_params[\"spacing_mm\"] = state.image.spacing\n                    filtered_array = riesz_log(state.image.array, **filter_params)\n                elif variant == \"simoncelli\":\n                    filtered_array = riesz_simoncelli(\n                        state.image.array, **filter_params\n                    )\n                else:\n                    filtered_array = riesz_transform(state.image.array, **filter_params)\n\n            else:\n                raise ValueError(\n                    f\"Unknown filter type: {filter_type}. \"\n                    \"Supported: mean, log, laws, gabor, wavelet, simoncelli, riesz\"\n                )\n\n            # Update state with filtered image\n            state.image = Image(\n                array=filtered_array,\n                spacing=state.image.spacing,\n                origin=state.image.origin,\n                direction=state.image.direction,\n                modality=state.image.modality,\n            )\n            state.raw_image = state.image  # Update raw_image post-filter\n            state.is_filtered = True\n            state.filter_type = filter_type\n\n        else:\n            raise ValueError(f\"Unknown preprocessing step: {step_name}\")\n\n    def _extract_features(\n        self, state: PipelineState, params: dict[str, Any]\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Extract features based on current state.\n        \"\"\"\n        results = {}\n        families = params.get(\n            \"families\", [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"]\n        )\n\n        # Optional kwargs pass-through (advanced usage)\n        spatial_intensity_params = params.get(\"spatial_intensity_params\", {})\n        local_intensity_params = params.get(\"local_intensity_params\", {})\n        ivh_params = params.get(\"ivh_params\", {})\n        texture_matrix_params = params.get(\"texture_matrix_params\", {})\n\n        if spatial_intensity_params is None:\n            spatial_intensity_params = {}\n        if local_intensity_params is None:\n            local_intensity_params = {}\n        if ivh_params is None:\n            ivh_params = {}\n        if texture_matrix_params is None:\n            texture_matrix_params = {}\n\n        if not isinstance(spatial_intensity_params, dict):\n            raise ValueError(\"spatial_intensity_params must be a dict\")\n        if not isinstance(local_intensity_params, dict):\n            raise ValueError(\"local_intensity_params must be a dict\")\n        if not isinstance(ivh_params, dict):\n            raise ValueError(\"ivh_params must be a dict\")\n        if not isinstance(texture_matrix_params, dict):\n            raise ValueError(\"texture_matrix_params must be a dict\")\n\n        # Morphology - uses raw_image (non-discretised) for intensity-based features\n        if \"morphology\" in families:\n            results.update(\n                calculate_morphology_features(\n                    state.morph_mask,\n                    state.raw_image,\n                    intensity_mask=state.intensity_mask,\n                )\n            )\n\n        # Intensity - uses raw_image (non-discretised)\n        if \"intensity\" in families:\n            masked_values = apply_mask(state.raw_image, state.intensity_mask)\n            results.update(calculate_intensity_features(masked_values))\n\n            include_spatial = bool(params.get(\"include_spatial_intensity\", False))\n            include_local = bool(params.get(\"include_local_intensity\", False))\n\n            if include_spatial:\n                results.update(\n                    calculate_spatial_intensity_features(\n                        state.raw_image,\n                        state.intensity_mask,\n                        **spatial_intensity_params,\n                    )\n                )\n            if include_local:\n                results.update(\n                    calculate_local_intensity_features(\n                        state.raw_image, state.intensity_mask, **local_intensity_params\n                    )\n                )\n\n        # Optional explicit families (no-op unless requested)\n        if \"spatial_intensity\" in families and \"intensity\" not in families:\n            results.update(\n                calculate_spatial_intensity_features(\n                    state.raw_image, state.intensity_mask, **spatial_intensity_params\n                )\n            )\n\n        if \"local_intensity\" in families and \"intensity\" not in families:\n            results.update(\n                calculate_local_intensity_features(\n                    state.raw_image, state.intensity_mask, **local_intensity_params\n                )\n            )\n\n        # Histogram / IVH\n        if \"histogram\" in families:\n            # Usually on discretised image\n            if not state.is_discretised:\n                warnings.warn(\n                    \"Histogram features requested but image is not discretised. \"\n                    \"Features may be unreliable or fail if integer bins are expected.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n\n            masked_values = apply_mask(state.image, state.intensity_mask)\n            results.update(calculate_intensity_histogram_features(masked_values))\n\n        if \"ivh\" in families:\n            # IVH computation supports three modes:\n            # 1. ivh_use_continuous=True: Use raw (pre-discretised) intensity values\n            # 2. ivh_discretisation={...}: Apply temporary discretisation just for IVH\n            # 3. Default: Use the pipeline's discretised image (if discretised)\n\n            ivh_use_continuous = params.get(\"ivh_use_continuous\", False)\n            ivh_discretisation = params.get(\"ivh_discretisation\", None)\n\n            # Track discretisation params for IVH calculation\n            ivh_disc_bin_width: Optional[float] = None\n            ivh_disc_min_val: Optional[float] = None\n\n            if ivh_use_continuous:\n                # Use raw intensity values (non-discretised)\n                # This is used for continuous IVH (e.g., IBSI Config D)\n                ivh_values = apply_mask(state.raw_image, state.intensity_mask)\n            elif ivh_discretisation:\n                # Apply temporary discretisation for IVH only\n                # This allows different binning for IVH vs texture features\n                # Uses raw_image as the base to discretise from raw values\n                ivh_disc_params = ivh_discretisation.copy()\n                ivh_method = ivh_disc_params.pop(\"method\", \"FBS\")\n                # Save bin_width and min_val for passing to calculate_ivh_features\n                ivh_disc_bin_width = ivh_disc_params.get(\"bin_width\")\n                ivh_disc_min_val = ivh_disc_params.get(\"min_val\")\n                temp_ivh_disc = discretise_image(\n                    state.raw_image,\n                    method=ivh_method,\n                    roi_mask=state.intensity_mask,\n                    **ivh_disc_params,\n                )\n                ivh_values = apply_mask(temp_ivh_disc, state.intensity_mask)\n            else:\n                # Default: use the current image (which may be discretised)\n                ivh_values = apply_mask(state.image, state.intensity_mask)\n\n            # IVH accepts several optional arguments; support both explicit top-level\n            # keys and an \"ivh_params\" dict for full control.\n            ivh_kwargs: dict[str, Any] = {}\n\n            # If ivh_discretisation was used, pass its bin_width and min_val\n            if ivh_disc_bin_width is not None:\n                ivh_kwargs[\"bin_width\"] = ivh_disc_bin_width\n            if ivh_disc_min_val is not None:\n                ivh_kwargs[\"min_val\"] = ivh_disc_min_val\n\n            # Dict-based params (preferred) - these override discretisation defaults\n            if \"bin_width\" in ivh_params:\n                ivh_kwargs[\"bin_width\"] = ivh_params.get(\"bin_width\")\n            if \"min_val\" in ivh_params:\n                ivh_kwargs[\"min_val\"] = ivh_params.get(\"min_val\")\n            if \"max_val\" in ivh_params:\n                ivh_kwargs[\"max_val\"] = ivh_params.get(\"max_val\")\n            if \"target_range_min\" in ivh_params:\n                ivh_kwargs[\"target_range_min\"] = ivh_params.get(\"target_range_min\")\n            if \"target_range_max\" in ivh_params:\n                ivh_kwargs[\"target_range_max\"] = ivh_params.get(\"target_range_max\")\n\n            # If not provided, and we are discretised (and not using continuous mode),\n            # default bin_width to 1.0 (bin indices)\n            if (\n                not ivh_use_continuous\n                and state.is_discretised\n                and ivh_kwargs.get(\"bin_width\") is None\n                and not ivh_discretisation\n            ):\n                ivh_kwargs[\"bin_width\"] = 1.0\n\n            # Only pass non-None arguments\n            ivh_kwargs = {k: v for k, v in ivh_kwargs.items() if v is not None}\n\n            results.update(calculate_ivh_features(ivh_values, **ivh_kwargs))\n\n        # Texture\n        if \"texture\" in families:\n            if not state.is_discretised:\n                raise ValueError(\n                    \"Texture features requested but image is not discretised. \"\n                    \"You must include a 'discretise' step before extracting texture features.\"\n                )\n\n            disc_image = state.image\n            n_bins = state.n_bins if state.n_bins else 32  # Fallback\n\n            # Calculate Matrices\n            # Use morphological mask for distance map (GLDZM)\n            # Advanced: allow overriding matrix computation parameters via texture_matrix_params.\n            matrix_kwargs: dict[str, Any] = {}\n            if \"ngldm_alpha\" in texture_matrix_params:\n                matrix_kwargs[\"ngldm_alpha\"] = texture_matrix_params.get(\"ngldm_alpha\")\n            matrix_kwargs = {k: v for k, v in matrix_kwargs.items() if v is not None}\n\n            texture_matrices = calculate_all_texture_matrices(\n                disc_image.array,\n                state.intensity_mask.array,\n                n_bins,\n                distance_mask=state.morph_mask.array,\n                **matrix_kwargs,\n            )\n\n            results.update(\n                calculate_glcm_features(\n                    disc_image.array,\n                    state.intensity_mask.array,\n                    n_bins,\n                    glcm_matrix=texture_matrices[\"glcm\"],\n                )\n            )\n            results.update(\n                calculate_glrlm_features(\n                    disc_image.array,\n                    state.intensity_mask.array,\n                    n_bins,\n                    glrlm_matrix=texture_matrices[\"glrlm\"],\n                )\n            )\n            results.update(\n                calculate_glszm_features(\n                    disc_image.array,\n                    state.intensity_mask.array,\n                    n_bins,\n                    glszm_matrix=texture_matrices[\"glszm\"],\n                )\n            )\n            results.update(\n                calculate_gldzm_features(\n                    disc_image.array,\n                    state.intensity_mask.array,\n                    n_bins,\n                    gldzm_matrix=texture_matrices[\"gldzm\"],\n                    distance_mask=state.morph_mask.array,\n                )\n            )\n            results.update(\n                calculate_ngtdm_features(\n                    disc_image.array,\n                    state.intensity_mask.array,\n                    n_bins,\n                    ngtdm_matrices=(\n                        texture_matrices[\"ngtdm_s\"],\n                        texture_matrices[\"ngtdm_n\"],\n                    ),\n                )\n            )\n            results.update(\n                calculate_ngldm_features(\n                    disc_image.array,\n                    state.intensity_mask.array,\n                    n_bins,\n                    ngldm_matrix=texture_matrices[\"ngldm\"],\n                )\n            )\n\n        return results\n\n    def save_log(self, output_path: str) -&gt; None:\n        \"\"\"\n        Save the processing log to a JSON file.\n        \"\"\"\n        if not output_path.endswith(\".json\"):\n            output_path += \".json\"\n\n        with open(output_path, \"w\") as f:\n            json.dump(self._log, f, indent=4, default=str)\n\n    # -------------------------------------------------------------------------\n    # Configuration Serialization Methods\n    # -------------------------------------------------------------------------\n\n    def list_configs(self) -&gt; list[str]:\n        \"\"\"\n        List all registered configuration names.\n\n        Returns:\n            List of configuration names.\n        \"\"\"\n        return list(self._configs.keys())\n\n    def get_config(self, name: str) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Get a copy of a configuration by name.\n\n        Args:\n            name: Configuration name.\n\n        Returns:\n            Deep copy of the configuration steps.\n\n        Raises:\n            KeyError: If configuration not found.\n        \"\"\"\n        if name not in self._configs:\n            raise KeyError(f\"Configuration '{name}' not found\")\n        return copy.deepcopy(self._configs[name])\n\n    def remove_config(self, name: str) -&gt; \"RadiomicsPipeline\":\n        \"\"\"\n        Remove a configuration by name.\n\n        Args:\n            name: Configuration name to remove.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            KeyError: If configuration not found.\n        \"\"\"\n        if name not in self._configs:\n            raise KeyError(f\"Configuration '{name}' not found\")\n        del self._configs[name]\n        return self\n\n    def to_dict(\n        self,\n        config_names: Optional[list[str]] = None,\n        include_metadata: bool = True,\n    ) -&gt; dict[str, Any]:\n        \"\"\"\n        Export configurations to a dictionary.\n\n        Args:\n            config_names: Specific configs to export. If None, exports all.\n            include_metadata: Whether to include schema version and metadata.\n\n        Returns:\n            Dictionary with configs and optional metadata.\n        \"\"\"\n        if config_names is None:\n            configs_to_export = self._configs\n        else:\n            configs_to_export = {\n                name: self._configs[name]\n                for name in config_names\n                if name in self._configs\n            }\n\n        # Convert tuples to lists for serialization\n        serializable_configs: dict[str, Any] = {}\n        for name, steps in configs_to_export.items():\n            serializable_configs[name] = {\n                \"steps\": self._make_serializable(steps)\n            }\n\n        if include_metadata:\n            return {\n                \"schema_version\": CONFIG_SCHEMA_VERSION,\n                \"exported_at\": datetime.datetime.now().isoformat(),\n                \"configs\": serializable_configs,\n            }\n        else:\n            return {\"configs\": serializable_configs}\n\n    def _make_serializable(self, obj: Any) -&gt; Any:\n        \"\"\"Convert tuples and other non-serializable types to serializable forms.\"\"\"\n        if isinstance(obj, tuple):\n            return list(obj)\n        elif isinstance(obj, dict):\n            return {k: self._make_serializable(v) for k, v in obj.items()}\n        elif isinstance(obj, list):\n            return [self._make_serializable(item) for item in obj]\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, (np.integer, np.floating)):\n            return obj.item()\n        return obj\n\n    def to_json(\n        self,\n        config_names: Optional[list[str]] = None,\n        indent: int = 2,\n    ) -&gt; str:\n        \"\"\"\n        Export configurations to a JSON string.\n\n        Args:\n            config_names: Specific configs to export. If None, exports all.\n            indent: JSON indentation level.\n\n        Returns:\n            JSON string representation.\n        \"\"\"\n        data = self.to_dict(config_names=config_names)\n        return json.dumps(data, indent=indent, default=str)\n\n    def to_yaml(\n        self,\n        config_names: Optional[list[str]] = None,\n    ) -&gt; str:\n        \"\"\"\n        Export configurations to a YAML string.\n\n        Args:\n            config_names: Specific configs to export. If None, exports all.\n\n        Returns:\n            YAML string representation.\n        \"\"\"\n        data = self.to_dict(config_names=config_names)\n        result: str = yaml.dump(data, default_flow_style=False, sort_keys=False)\n        return result\n\n    def save_configs(\n        self,\n        output_path: str | Path,\n        config_names: Optional[list[str]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Save configurations to a file (JSON or YAML based on extension).\n\n        Args:\n            output_path: Path to output file. Extension determines format.\n            config_names: Specific configs to export. If None, exports all.\n\n        Raises:\n            ValueError: If file extension is not .json, .yaml, or .yml.\n        \"\"\"\n        path = Path(output_path)\n        suffix = path.suffix.lower()\n\n        if suffix == \".json\":\n            content = self.to_json(config_names=config_names)\n        elif suffix in (\".yaml\", \".yml\"):\n            content = self.to_yaml(config_names=config_names)\n        else:\n            raise ValueError(f\"Unsupported file extension: {suffix}. Use .json, .yaml, or .yml\")\n\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(content, encoding=\"utf-8\")\n\n    @classmethod\n    def from_dict(\n        cls,\n        data: dict[str, Any],\n        validate: bool = False,\n    ) -&gt; \"RadiomicsPipeline\":\n        \"\"\"\n        Create a new pipeline instance from a configuration dictionary.\n\n        Args:\n            data: Configuration dictionary with 'configs' key.\n            validate: Whether to validate parameters (logs warnings for issues).\n\n        Returns:\n            New RadiomicsPipeline instance with loaded configs.\n        \"\"\"\n        pipeline = cls()\n\n        # Handle schema version migration if needed\n        schema_version = data.get(\"schema_version\", \"1.0\")\n        migrated_data = cls._migrate_config(data, schema_version)\n\n        configs = migrated_data.get(\"configs\", {})\n        for name, config_data in configs.items():\n            if isinstance(config_data, dict) and \"steps\" in config_data:\n                steps = config_data[\"steps\"]\n            elif isinstance(config_data, list):\n                steps = config_data\n            else:\n                _logger.warning(f\"Invalid config format for '{name}', skipping\")\n                continue\n\n            # Convert YAML lists to tuples where needed\n            converted_steps = pipeline._convert_yaml_steps(steps)\n\n            if validate:\n                cls._validate_config(name, converted_steps)\n\n            pipeline._configs[name] = converted_steps\n\n        return pipeline\n\n    @classmethod\n    def from_json(\n        cls,\n        json_string: str,\n        validate: bool = False,\n    ) -&gt; \"RadiomicsPipeline\":\n        \"\"\"\n        Create a new pipeline instance from a JSON string.\n\n        Args:\n            json_string: JSON configuration string.\n            validate: Whether to validate parameters.\n\n        Returns:\n            New RadiomicsPipeline instance.\n        \"\"\"\n        data = json.loads(json_string)\n        return cls.from_dict(data, validate=validate)\n\n    @classmethod\n    def from_yaml(\n        cls,\n        yaml_string: str,\n        validate: bool = False,\n    ) -&gt; \"RadiomicsPipeline\":\n        \"\"\"\n        Create a new pipeline instance from a YAML string.\n\n        Args:\n            yaml_string: YAML configuration string.\n            validate: Whether to validate parameters.\n\n        Returns:\n            New RadiomicsPipeline instance.\n        \"\"\"\n        data = yaml.safe_load(yaml_string)\n        return cls.from_dict(data, validate=validate)\n\n    @classmethod\n    def load_configs(\n        cls,\n        file_path: str | Path,\n        validate: bool = False,\n    ) -&gt; \"RadiomicsPipeline\":\n        \"\"\"\n        Load configurations from a file (JSON or YAML).\n\n        Args:\n            file_path: Path to configuration file.\n            validate: Whether to validate parameters.\n\n        Returns:\n            New RadiomicsPipeline instance.\n\n        Raises:\n            FileNotFoundError: If file doesn't exist.\n            ValueError: If file extension is unsupported.\n        \"\"\"\n        path = Path(file_path)\n        if not path.exists():\n            raise FileNotFoundError(f\"Configuration file not found: {path}\")\n\n        suffix = path.suffix.lower()\n        content = path.read_text(encoding=\"utf-8\")\n\n        if suffix == \".json\":\n            return cls.from_json(content, validate=validate)\n        elif suffix in (\".yaml\", \".yml\"):\n            return cls.from_yaml(content, validate=validate)\n        else:\n            raise ValueError(f\"Unsupported file extension: {suffix}. Use .json, .yaml, or .yml\")\n\n    def merge_configs(\n        self,\n        other: \"RadiomicsPipeline\",\n        overwrite: bool = False,\n    ) -&gt; \"RadiomicsPipeline\":\n        \"\"\"\n        Merge configurations from another pipeline instance.\n\n        Args:\n            other: Another RadiomicsPipeline to merge from.\n            overwrite: Whether to overwrite existing configs with same name.\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        for name, steps in other._configs.items():\n            if name in self._configs and not overwrite:\n                _logger.warning(f\"Config '{name}' already exists, skipping (use overwrite=True)\")\n                continue\n            self._configs[name] = copy.deepcopy(steps)\n        return self\n\n    # -------------------------------------------------------------------------\n    # Schema Migration\n    # -------------------------------------------------------------------------\n\n    @staticmethod\n    def _migrate_config(data: dict[str, Any], from_version: str) -&gt; dict[str, Any]:\n        \"\"\"\n        Migrate configuration from an older schema version to current.\n\n        Args:\n            data: Configuration data to migrate.\n            from_version: Source schema version.\n\n        Returns:\n            Migrated configuration data.\n        \"\"\"\n        if from_version == CONFIG_SCHEMA_VERSION:\n            return data\n\n        # Future migrations would go here\n        # if from_version == \"1.0\" and CONFIG_SCHEMA_VERSION == \"1.1\":\n        #     # Apply 1.0 -&gt; 1.1 migrations\n        #     pass\n\n        _logger.info(f\"Migrated config from v{from_version} to v{CONFIG_SCHEMA_VERSION}\")\n        return data\n\n    # -------------------------------------------------------------------------\n    # Validation\n    # -------------------------------------------------------------------------\n\n    # Known step types and their valid parameters\n    _VALID_STEPS: dict[str, set[str]] = {\n        \"resample\": {\"new_spacing\", \"interpolation\"},\n        \"resegment\": {\"range_min\", \"range_max\"},\n        \"filter_outliers\": {\"sigma\"},\n        \"binarize_mask\": {\"threshold\", \"mask_values\", \"apply_to\"},\n        \"keep_largest_component\": set(),\n        \"round_intensities\": set(),\n        \"discretise\": {\"method\", \"n_bins\", \"bin_width\", \"min_value\", \"max_value\"},\n        \"filter\": {\n            \"filter_type\", \"sigma\", \"cutoff\", \"compute_response_map\", \"boundary_condition\",\n            \"orientation\", \"kernal_size\", \"output_type\", \"alpha\", \"order\", \"levels\",\n            \"wavelet_name\", \"rotation_invariant\", \"pool_method\", \"response_type\",\n        },\n        \"extract_features\": {\n            \"families\", \"include_spatial_intensity\", \"include_local_intensity\",\n            \"texture_matrix_params\", \"ivh_params\",\n        },\n    }\n\n    @classmethod\n    def _validate_config(cls, name: str, steps: list[dict[str, Any]]) -&gt; bool:\n        \"\"\"\n        Validate a configuration, logging warnings for issues.\n\n        Args:\n            name: Configuration name (for logging).\n            steps: List of step dictionaries.\n\n        Returns:\n            True if valid, False if issues found (warnings are logged).\n        \"\"\"\n        is_valid = True\n\n        if not isinstance(steps, list):\n            _logger.warning(f\"Config '{name}': steps must be a list\")\n            return False\n\n        for i, step in enumerate(steps):\n            if not isinstance(step, dict):\n                _logger.warning(f\"Config '{name}' step {i}: must be a dictionary\")\n                is_valid = False\n                continue\n\n            step_type = step.get(\"step\")\n            if not step_type:\n                _logger.warning(f\"Config '{name}' step {i}: missing 'step' key\")\n                is_valid = False\n                continue\n\n            if step_type not in cls._VALID_STEPS:\n                _logger.warning(f\"Config '{name}' step {i}: unknown step type '{step_type}'\")\n                is_valid = False\n                continue\n\n            # Check for unknown parameters\n            params = step.get(\"params\", {})\n            if params:\n                valid_params = cls._VALID_STEPS[step_type]\n                for param_name in params.keys():\n                    if param_name not in valid_params:\n                        _logger.warning(\n                            f\"Config '{name}' step {i} ({step_type}): \"\n                            f\"unknown parameter '{param_name}'\"\n                        )\n\n        return is_valid\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.__init__","title":"<code>__init__()</code>","text":"<p>Initialize pipeline with empty config registry and load predefined configs.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize pipeline with empty config registry and load predefined configs.\"\"\"\n    self._configs: dict[str, list[dict[str, Any]]] = {}\n    self._log: list[dict[str, Any]] = []\n    self._load_predefined_configs()\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.add_config","title":"<code>add_config(name, steps)</code>","text":"<p>Add a processing configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name for this configuration.</p> required <code>steps</code> <code>list[dict[str, Any]]</code> <p>List of steps. Each step is a dict with 'step' (name) and 'params' (dict).    Supported steps:    - 'resample': params: new_spacing (required), interpolation (optional)                      - 'resegment': params: range_min, range_max                      - 'filter_outliers': params: sigma                      - 'binarize_mask': params: threshold (float, default 0.5),                          mask_values (int | list[int] | tuple[int, int]), apply_to ('morph'|'intensity'|'both')                      - 'keep_largest_component': params: None    - 'round_intensities': params: None    - 'discretise': params: method, n_bins/bin_width, etc.    - 'extract_features': params: families (list), etc.</p> required Note <ul> <li>Texture features require a prior 'discretise' step.</li> <li>IVH features are configured via 'ivh_params' dict.</li> </ul> Source code in <code>pictologics/pipeline.py</code> <pre><code>def add_config(self, name: str, steps: list[dict[str, Any]]) -&gt; \"RadiomicsPipeline\":\n    \"\"\"\n    Add a processing configuration.\n\n    Args:\n        name: Unique name for this configuration.\n        steps: List of steps. Each step is a dict with 'step' (name) and 'params' (dict).\n               Supported steps:\n               - 'resample': params: new_spacing (required), interpolation (optional)\n                                 - 'resegment': params: range_min, range_max\n                                 - 'filter_outliers': params: sigma\n                                 - 'binarize_mask': params: threshold (float, default 0.5),\n                                     mask_values (int | list[int] | tuple[int, int]), apply_to ('morph'|'intensity'|'both')\n                                 - 'keep_largest_component': params: None\n               - 'round_intensities': params: None\n               - 'discretise': params: method, n_bins/bin_width, etc.\n               - 'extract_features': params: families (list), etc.\n\n    Note:\n        - Texture features require a prior 'discretise' step.\n        - IVH features are configured via 'ivh_params' dict.\n    \"\"\"\n    if not isinstance(steps, list):\n        raise ValueError(\"Configuration must be a list of steps\")\n\n    for step in steps:\n        if not isinstance(step, dict):\n            raise ValueError(\"Each step must be a dictionary\")\n        if \"step\" not in step:\n            raise ValueError(\"Each step must have a 'step' key\")\n\n    self._configs[name] = steps\n    return self\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.clear_log","title":"<code>clear_log()</code>","text":"<p>Clear the in-memory processing log.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def clear_log(self) -&gt; None:\n    \"\"\"Clear the in-memory processing log.\"\"\"\n    self._log.clear()\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.from_dict","title":"<code>from_dict(data, validate=False)</code>  <code>classmethod</code>","text":"<p>Create a new pipeline instance from a configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Configuration dictionary with 'configs' key.</p> required <code>validate</code> <code>bool</code> <p>Whether to validate parameters (logs warnings for issues).</p> <code>False</code> <p>Returns:</p> Type Description <code>'RadiomicsPipeline'</code> <p>New RadiomicsPipeline instance with loaded configs.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls,\n    data: dict[str, Any],\n    validate: bool = False,\n) -&gt; \"RadiomicsPipeline\":\n    \"\"\"\n    Create a new pipeline instance from a configuration dictionary.\n\n    Args:\n        data: Configuration dictionary with 'configs' key.\n        validate: Whether to validate parameters (logs warnings for issues).\n\n    Returns:\n        New RadiomicsPipeline instance with loaded configs.\n    \"\"\"\n    pipeline = cls()\n\n    # Handle schema version migration if needed\n    schema_version = data.get(\"schema_version\", \"1.0\")\n    migrated_data = cls._migrate_config(data, schema_version)\n\n    configs = migrated_data.get(\"configs\", {})\n    for name, config_data in configs.items():\n        if isinstance(config_data, dict) and \"steps\" in config_data:\n            steps = config_data[\"steps\"]\n        elif isinstance(config_data, list):\n            steps = config_data\n        else:\n            _logger.warning(f\"Invalid config format for '{name}', skipping\")\n            continue\n\n        # Convert YAML lists to tuples where needed\n        converted_steps = pipeline._convert_yaml_steps(steps)\n\n        if validate:\n            cls._validate_config(name, converted_steps)\n\n        pipeline._configs[name] = converted_steps\n\n    return pipeline\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.from_json","title":"<code>from_json(json_string, validate=False)</code>  <code>classmethod</code>","text":"<p>Create a new pipeline instance from a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>json_string</code> <code>str</code> <p>JSON configuration string.</p> required <code>validate</code> <code>bool</code> <p>Whether to validate parameters.</p> <code>False</code> <p>Returns:</p> Type Description <code>'RadiomicsPipeline'</code> <p>New RadiomicsPipeline instance.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    json_string: str,\n    validate: bool = False,\n) -&gt; \"RadiomicsPipeline\":\n    \"\"\"\n    Create a new pipeline instance from a JSON string.\n\n    Args:\n        json_string: JSON configuration string.\n        validate: Whether to validate parameters.\n\n    Returns:\n        New RadiomicsPipeline instance.\n    \"\"\"\n    data = json.loads(json_string)\n    return cls.from_dict(data, validate=validate)\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.from_yaml","title":"<code>from_yaml(yaml_string, validate=False)</code>  <code>classmethod</code>","text":"<p>Create a new pipeline instance from a YAML string.</p> <p>Parameters:</p> Name Type Description Default <code>yaml_string</code> <code>str</code> <p>YAML configuration string.</p> required <code>validate</code> <code>bool</code> <p>Whether to validate parameters.</p> <code>False</code> <p>Returns:</p> Type Description <code>'RadiomicsPipeline'</code> <p>New RadiomicsPipeline instance.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>@classmethod\ndef from_yaml(\n    cls,\n    yaml_string: str,\n    validate: bool = False,\n) -&gt; \"RadiomicsPipeline\":\n    \"\"\"\n    Create a new pipeline instance from a YAML string.\n\n    Args:\n        yaml_string: YAML configuration string.\n        validate: Whether to validate parameters.\n\n    Returns:\n        New RadiomicsPipeline instance.\n    \"\"\"\n    data = yaml.safe_load(yaml_string)\n    return cls.from_dict(data, validate=validate)\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.get_all_standard_config_names","title":"<code>get_all_standard_config_names()</code>","text":"<p>Returns the list of all standard configuration names.</p> <p>Returns names from loaded templates that start with 'standard_'.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def get_all_standard_config_names(self) -&gt; list[str]:\n    \"\"\"\n    Returns the list of all standard configuration names.\n\n    Returns names from loaded templates that start with 'standard_'.\n    \"\"\"\n    return sorted([name for name in self._configs.keys() if name.startswith(\"standard_\")])\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.get_config","title":"<code>get_config(name)</code>","text":"<p>Get a copy of a configuration by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Configuration name.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>Deep copy of the configuration steps.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If configuration not found.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def get_config(self, name: str) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Get a copy of a configuration by name.\n\n    Args:\n        name: Configuration name.\n\n    Returns:\n        Deep copy of the configuration steps.\n\n    Raises:\n        KeyError: If configuration not found.\n    \"\"\"\n    if name not in self._configs:\n        raise KeyError(f\"Configuration '{name}' not found\")\n    return copy.deepcopy(self._configs[name])\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.list_configs","title":"<code>list_configs()</code>","text":"<p>List all registered configuration names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of configuration names.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def list_configs(self) -&gt; list[str]:\n    \"\"\"\n    List all registered configuration names.\n\n    Returns:\n        List of configuration names.\n    \"\"\"\n    return list(self._configs.keys())\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.load_configs","title":"<code>load_configs(file_path, validate=False)</code>  <code>classmethod</code>","text":"<p>Load configurations from a file (JSON or YAML).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to configuration file.</p> required <code>validate</code> <code>bool</code> <p>Whether to validate parameters.</p> <code>False</code> <p>Returns:</p> Type Description <code>'RadiomicsPipeline'</code> <p>New RadiomicsPipeline instance.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file doesn't exist.</p> <code>ValueError</code> <p>If file extension is unsupported.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>@classmethod\ndef load_configs(\n    cls,\n    file_path: str | Path,\n    validate: bool = False,\n) -&gt; \"RadiomicsPipeline\":\n    \"\"\"\n    Load configurations from a file (JSON or YAML).\n\n    Args:\n        file_path: Path to configuration file.\n        validate: Whether to validate parameters.\n\n    Returns:\n        New RadiomicsPipeline instance.\n\n    Raises:\n        FileNotFoundError: If file doesn't exist.\n        ValueError: If file extension is unsupported.\n    \"\"\"\n    path = Path(file_path)\n    if not path.exists():\n        raise FileNotFoundError(f\"Configuration file not found: {path}\")\n\n    suffix = path.suffix.lower()\n    content = path.read_text(encoding=\"utf-8\")\n\n    if suffix == \".json\":\n        return cls.from_json(content, validate=validate)\n    elif suffix in (\".yaml\", \".yml\"):\n        return cls.from_yaml(content, validate=validate)\n    else:\n        raise ValueError(f\"Unsupported file extension: {suffix}. Use .json, .yaml, or .yml\")\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.merge_configs","title":"<code>merge_configs(other, overwrite=False)</code>","text":"<p>Merge configurations from another pipeline instance.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>'RadiomicsPipeline'</code> <p>Another RadiomicsPipeline to merge from.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing configs with same name.</p> <code>False</code> <p>Returns:</p> Type Description <code>'RadiomicsPipeline'</code> <p>Self for method chaining.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def merge_configs(\n    self,\n    other: \"RadiomicsPipeline\",\n    overwrite: bool = False,\n) -&gt; \"RadiomicsPipeline\":\n    \"\"\"\n    Merge configurations from another pipeline instance.\n\n    Args:\n        other: Another RadiomicsPipeline to merge from.\n        overwrite: Whether to overwrite existing configs with same name.\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    for name, steps in other._configs.items():\n        if name in self._configs and not overwrite:\n            _logger.warning(f\"Config '{name}' already exists, skipping (use overwrite=True)\")\n            continue\n        self._configs[name] = copy.deepcopy(steps)\n    return self\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.remove_config","title":"<code>remove_config(name)</code>","text":"<p>Remove a configuration by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Configuration name to remove.</p> required <p>Returns:</p> Type Description <code>'RadiomicsPipeline'</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If configuration not found.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def remove_config(self, name: str) -&gt; \"RadiomicsPipeline\":\n    \"\"\"\n    Remove a configuration by name.\n\n    Args:\n        name: Configuration name to remove.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        KeyError: If configuration not found.\n    \"\"\"\n    if name not in self._configs:\n        raise KeyError(f\"Configuration '{name}' not found\")\n    del self._configs[name]\n    return self\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.run","title":"<code>run(image, mask=None, subject_id=None, config_names=None)</code>","text":"<p>Run configurations on the provided image and mask.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str | Image</code> <p>Path to image or Image object.</p> required <code>mask</code> <code>str | Image | None</code> <p>Optional path to mask or Image object. If omitted (or passed as <code>None</code> / empty string), the pipeline will treat the entire image as the ROI by generating a full (all-ones) mask matching the input image geometry.</p> <code>None</code> <code>subject_id</code> <code>Optional[str]</code> <p>Optional identifier for the subject.</p> <code>None</code> <code>config_names</code> <code>Optional[list[str]]</code> <p>List of specific configuration names to run.           If None, runs all registered configurations.           Supports \"all_standard\" to run all 6 standard configs.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Series]</code> <p>Dictionary mapping config names to pandas Series of features.</p> Example <p>Run standard pipeline components:</p> <pre><code>from pictologics.pipeline import RadiomicsPipeline\n\n# Initialize\npipeline = RadiomicsPipeline()\n\n# Run on image and mask\nresults = pipeline.run(\n    image=\"data/image.nii.gz\",\n    mask=\"data/mask.nii.gz\",\n    subject_id=\"subject_001\",\n    config_names=[\"standard_fbn_32\"]\n)\n\n# Access results\nprint(results[\"standard_fbn_32\"].head())\n</code></pre> Source code in <code>pictologics/pipeline.py</code> <pre><code>def run(\n    self,\n    image: str | Image,\n    mask: str | Image | None = None,\n    subject_id: Optional[str] = None,\n    config_names: Optional[list[str]] = None,\n) -&gt; dict[str, pd.Series]:\n    \"\"\"\n    Run configurations on the provided image and mask.\n\n    Args:\n        image: Path to image or Image object.\n        mask: Optional path to mask or Image object.\n            If omitted (or passed as `None` / empty string), the pipeline will\n            treat the **entire image** as the ROI by generating a full (all-ones)\n            mask matching the input image geometry.\n        subject_id: Optional identifier for the subject.\n        config_names: List of specific configuration names to run.\n                      If None, runs all registered configurations.\n                      Supports \"all_standard\" to run all 6 standard configs.\n\n    Returns:\n        Dictionary mapping config names to pandas Series of features.\n\n    Example:\n        Run standard pipeline components:\n\n        ```python\n        from pictologics.pipeline import RadiomicsPipeline\n\n        # Initialize\n        pipeline = RadiomicsPipeline()\n\n        # Run on image and mask\n        results = pipeline.run(\n            image=\"data/image.nii.gz\",\n            mask=\"data/mask.nii.gz\",\n            subject_id=\"subject_001\",\n            config_names=[\"standard_fbn_32\"]\n        )\n\n        # Access results\n        print(results[\"standard_fbn_32\"].head())\n        ```\n    \"\"\"\n    # 1. Load Data\n    if isinstance(image, str):\n        orig_img = load_image(image)\n        img_source = image\n    else:\n        orig_img = image\n        img_source = \"InMemory\"\n\n    mask_was_generated = False\n    if mask is None or (isinstance(mask, str) and mask.strip() == \"\"):\n        orig_mask = create_full_mask(orig_img)\n        mask_source = \"GeneratedFullMask\"\n        mask_was_generated = True\n    elif isinstance(mask, str):\n        orig_mask = load_image(mask)\n        mask_source = mask\n    else:\n        orig_mask = mask\n        mask_source = \"InMemory\"\n\n    all_results = {}\n\n    # Determine which configs to run\n    if config_names is None:\n        target_configs = list(self._configs.keys())\n    else:\n        target_configs = []\n        for name in config_names:\n            if name == \"all_standard\":\n                target_configs.extend(self.get_all_standard_config_names())\n            elif name in self._configs:\n                target_configs.append(name)\n            else:\n                raise ValueError(f\"Configuration '{name}' not found.\")\n\n    # Run each configuration\n    for config_name in target_configs:\n        steps = self._configs[config_name]\n\n        # Initialize State\n        # We start with fresh copies for each config\n        state = PipelineState(\n            image=orig_img,\n            raw_image=orig_img,  # Track non-discretised image\n            morph_mask=orig_mask,\n            intensity_mask=Image(\n                array=orig_mask.array.copy(),\n                spacing=orig_mask.spacing,\n                origin=orig_mask.origin,\n                direction=orig_mask.direction,\n                modality=orig_mask.modality,\n            ),\n            mask_was_generated=mask_was_generated,\n        )\n\n        self._ensure_nonempty_roi(state, context=\"initialization\")\n\n        config_log: dict[str, Any] = {\n            \"timestamp\": datetime.datetime.now().isoformat(),\n            \"subject_id\": subject_id,\n            \"config_name\": config_name,\n            \"image_source\": img_source,\n            \"mask_source\": mask_source,\n            \"steps_executed\": [],\n        }\n\n        config_features = {}\n\n        try:\n            for step_def in steps:\n                step_name = step_def[\"step\"]\n                params = step_def.get(\"params\", {})\n\n                # Execute Step\n                if step_name == \"extract_features\":\n                    features = self._extract_features(state, params)\n                    config_features.update(features)\n                else:\n                    self._execute_preprocessing_step(state, step_name, params)\n\n                # Log\n                config_log[\"steps_executed\"].append(\n                    {\"step\": step_name, \"params\": params, \"status\": \"completed\"}\n                )\n\n        except Exception as e:\n            config_log[\"error\"] = str(e)\n            config_log[\"failed_step\"] = step_def\n            print(f\"Error in config '{config_name}', step '{step_def}': {e}\")\n\n            # For empty ROI, fail fast (do not silently return empty/partial features).\n            if isinstance(e, EmptyROIMaskError):\n                self._log.append(config_log)\n                raise\n\n        self._log.append(config_log)\n\n        # Create Series\n        series = pd.Series(config_features)\n        if subject_id:\n            series[\"subject_id\"] = subject_id\n        all_results[config_name] = series\n\n    return all_results\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.save_configs","title":"<code>save_configs(output_path, config_names=None)</code>","text":"<p>Save configurations to a file (JSON or YAML based on extension).</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str | Path</code> <p>Path to output file. Extension determines format.</p> required <code>config_names</code> <code>Optional[list[str]]</code> <p>Specific configs to export. If None, exports all.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If file extension is not .json, .yaml, or .yml.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def save_configs(\n    self,\n    output_path: str | Path,\n    config_names: Optional[list[str]] = None,\n) -&gt; None:\n    \"\"\"\n    Save configurations to a file (JSON or YAML based on extension).\n\n    Args:\n        output_path: Path to output file. Extension determines format.\n        config_names: Specific configs to export. If None, exports all.\n\n    Raises:\n        ValueError: If file extension is not .json, .yaml, or .yml.\n    \"\"\"\n    path = Path(output_path)\n    suffix = path.suffix.lower()\n\n    if suffix == \".json\":\n        content = self.to_json(config_names=config_names)\n    elif suffix in (\".yaml\", \".yml\"):\n        content = self.to_yaml(config_names=config_names)\n    else:\n        raise ValueError(f\"Unsupported file extension: {suffix}. Use .json, .yaml, or .yml\")\n\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(content, encoding=\"utf-8\")\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.save_log","title":"<code>save_log(output_path)</code>","text":"<p>Save the processing log to a JSON file.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def save_log(self, output_path: str) -&gt; None:\n    \"\"\"\n    Save the processing log to a JSON file.\n    \"\"\"\n    if not output_path.endswith(\".json\"):\n        output_path += \".json\"\n\n    with open(output_path, \"w\") as f:\n        json.dump(self._log, f, indent=4, default=str)\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.to_dict","title":"<code>to_dict(config_names=None, include_metadata=True)</code>","text":"<p>Export configurations to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>config_names</code> <code>Optional[list[str]]</code> <p>Specific configs to export. If None, exports all.</p> <code>None</code> <code>include_metadata</code> <code>bool</code> <p>Whether to include schema version and metadata.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with configs and optional metadata.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def to_dict(\n    self,\n    config_names: Optional[list[str]] = None,\n    include_metadata: bool = True,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Export configurations to a dictionary.\n\n    Args:\n        config_names: Specific configs to export. If None, exports all.\n        include_metadata: Whether to include schema version and metadata.\n\n    Returns:\n        Dictionary with configs and optional metadata.\n    \"\"\"\n    if config_names is None:\n        configs_to_export = self._configs\n    else:\n        configs_to_export = {\n            name: self._configs[name]\n            for name in config_names\n            if name in self._configs\n        }\n\n    # Convert tuples to lists for serialization\n    serializable_configs: dict[str, Any] = {}\n    for name, steps in configs_to_export.items():\n        serializable_configs[name] = {\n            \"steps\": self._make_serializable(steps)\n        }\n\n    if include_metadata:\n        return {\n            \"schema_version\": CONFIG_SCHEMA_VERSION,\n            \"exported_at\": datetime.datetime.now().isoformat(),\n            \"configs\": serializable_configs,\n        }\n    else:\n        return {\"configs\": serializable_configs}\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.to_json","title":"<code>to_json(config_names=None, indent=2)</code>","text":"<p>Export configurations to a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>config_names</code> <code>Optional[list[str]]</code> <p>Specific configs to export. If None, exports all.</p> <code>None</code> <code>indent</code> <code>int</code> <p>JSON indentation level.</p> <code>2</code> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representation.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def to_json(\n    self,\n    config_names: Optional[list[str]] = None,\n    indent: int = 2,\n) -&gt; str:\n    \"\"\"\n    Export configurations to a JSON string.\n\n    Args:\n        config_names: Specific configs to export. If None, exports all.\n        indent: JSON indentation level.\n\n    Returns:\n        JSON string representation.\n    \"\"\"\n    data = self.to_dict(config_names=config_names)\n    return json.dumps(data, indent=indent, default=str)\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.to_yaml","title":"<code>to_yaml(config_names=None)</code>","text":"<p>Export configurations to a YAML string.</p> <p>Parameters:</p> Name Type Description Default <code>config_names</code> <code>Optional[list[str]]</code> <p>Specific configs to export. If None, exports all.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>YAML string representation.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def to_yaml(\n    self,\n    config_names: Optional[list[str]] = None,\n) -&gt; str:\n    \"\"\"\n    Export configurations to a YAML string.\n\n    Args:\n        config_names: Specific configs to export. If None, exports all.\n\n    Returns:\n        YAML string representation.\n    \"\"\"\n    data = self.to_dict(config_names=config_names)\n    result: str = yaml.dump(data, default_flow_style=False, sort_keys=False)\n    return result\n</code></pre>"},{"location":"api/preprocessing/","title":"Preprocessing API","text":""},{"location":"api/preprocessing/#pictologics.preprocessing","title":"<code>pictologics.preprocessing</code>","text":""},{"location":"api/preprocessing/#pictologics.preprocessing--image-preprocessing-module","title":"Image Preprocessing Module","text":"<p>This module provides a collection of preprocessing functions essential for radiomics analysis. These functions are designed to be IBSI-compliant where applicable.</p>"},{"location":"api/preprocessing/#pictologics.preprocessing--key-features","title":"Key Features:","text":"<ul> <li>Resampling: Voxel resampling using 'Align grid centers' (IBSI compliant).</li> <li>Discretisation: Fixed Bin Number (FBN) and Fixed Bin Size (FBS) algorithms.</li> <li>Filtering: Outlier filtering (mean +/- sigma).</li> <li>Mask Operations: Resegmentation (thresholding), ROI extraction, Largest Connected Component.</li> <li>Utilities: Rounding intensities, applying masks.</li> </ul>"},{"location":"api/preprocessing/#pictologics.preprocessing.apply_mask","title":"<code>apply_mask(image, mask, mask_values=1)</code>","text":"<p>Apply mask to image and return flattened array of voxel values.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image | NDArray[floating[Any]]</code> <p>Image object or numpy array.</p> required <code>mask</code> <code>Image | NDArray[floating[Any]]</code> <p>Image object (mask) or numpy array.</p> required <code>mask_values</code> <code>int | list[int] | None</code> <p>Value(s) in the mask to consider as ROI. Default is 1.          Can be a single integer or a list of integers.</p> <code>1</code> <p>Returns:</p> Type Description <code>NDArray[floating[Any]]</code> <p>1D numpy array of values within the mask.</p> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def apply_mask(\n    image: Image | npt.NDArray[np.floating[Any]],\n    mask: Image | npt.NDArray[np.floating[Any]],\n    mask_values: int | list[int] | None = 1,\n) -&gt; npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Apply mask to image and return flattened array of voxel values.\n\n    Args:\n        image: Image object or numpy array.\n        mask: Image object (mask) or numpy array.\n        mask_values: Value(s) in the mask to consider as ROI. Default is 1.\n                     Can be a single integer or a list of integers.\n\n    Returns:\n        1D numpy array of values within the mask.\n    \"\"\"\n    # Handle inputs\n    img_arr = image.array if isinstance(image, Image) else image\n    mask_arr = mask.array if isinstance(mask, Image) else mask\n\n    # Ensure shapes match\n    if img_arr.shape != mask_arr.shape:\n        raise ValueError(\n            f\"Image shape {img_arr.shape} and mask shape {mask_arr.shape} do not match\"\n        )\n\n    # Handle mask values\n    if mask_values is None:\n        mask_values = [1]\n    elif isinstance(mask_values, int):\n        mask_values = [mask_values]\n\n    # Create boolean mask\n    roi_mask = np.isin(mask_arr, mask_values)\n\n    if not np.any(roi_mask):\n        return np.array([])\n\n    # Apply mask\n    return img_arr[roi_mask]\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.discretise_image","title":"<code>discretise_image(image, method, roi_mask=None, n_bins=None, bin_width=None, min_val=None, max_val=None, cutoffs=None)</code>","text":"<p>Discretise image intensities.</p> <p>Supports IBSI-compliant Fixed Bin Number (FBN) and Fixed Bin Size (FBS).</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image | NDArray[floating[Any]]</code> <p>Input Image object or numpy array.</p> required <code>method</code> <code>str</code> <p>'FBN' (Fixed Bin Number), 'FBS' (Fixed Bin Size), or 'FIXED_CUTOFFS'.</p> required <code>roi_mask</code> <code>Image | NDArray[floating[Any]] | None</code> <p>Optional mask to define the ROI for determining min/max values.</p> <code>None</code> <code>n_bins</code> <code>Optional[int]</code> <p>Number of bins (required for FBN).</p> <code>None</code> <code>bin_width</code> <code>Optional[float]</code> <p>Bin width (required for FBS).</p> <code>None</code> <code>min_val</code> <code>Optional[float]</code> <p>Minimum value for discretisation.      For FBS, defaults to ROI minimum (or global minimum).      For FBN, defaults to ROI minimum.</p> <code>None</code> <code>max_val</code> <code>Optional[float]</code> <p>Maximum value for discretisation (FBN only).      Defaults to ROI maximum.</p> <code>None</code> <code>cutoffs</code> <code>Optional[list[float]]</code> <p>List of cutoffs (required for FIXED_CUTOFFS).</p> <code>None</code> <p>Returns:</p> Type Description <code>Image | NDArray[floating[Any]]</code> <p>Discretised Image object or numpy array (depending on input).</p> <code>Image | NDArray[floating[Any]]</code> <p>Values are 1-based indices.</p> Example <p>Discretise image into 32 fixed bins (FBN):</p> <pre><code>from pictologics.preprocessing import discretise_image\n\n# FBN with 32 bins\ndisc_image = discretise_image(\n    image,\n    method=\"FBN\",\n    n_bins=32\n)\n</code></pre> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def discretise_image(\n    image: Image | npt.NDArray[np.floating[Any]],\n    method: str,\n    roi_mask: Image | npt.NDArray[np.floating[Any]] | None = None,\n    n_bins: Optional[int] = None,\n    bin_width: Optional[float] = None,\n    min_val: Optional[float] = None,\n    max_val: Optional[float] = None,\n    cutoffs: Optional[list[float]] = None,\n) -&gt; Image | npt.NDArray[np.floating[Any]]:\n    \"\"\"\n    Discretise image intensities.\n\n    Supports IBSI-compliant Fixed Bin Number (FBN) and Fixed Bin Size (FBS).\n\n    Args:\n        image: Input Image object or numpy array.\n        method: 'FBN' (Fixed Bin Number), 'FBS' (Fixed Bin Size), or 'FIXED_CUTOFFS'.\n        roi_mask: Optional mask to define the ROI for determining min/max values.\n        n_bins: Number of bins (required for FBN).\n        bin_width: Bin width (required for FBS).\n        min_val: Minimum value for discretisation.\n                 For FBS, defaults to ROI minimum (or global minimum).\n                 For FBN, defaults to ROI minimum.\n        max_val: Maximum value for discretisation (FBN only).\n                 Defaults to ROI maximum.\n        cutoffs: List of cutoffs (required for FIXED_CUTOFFS).\n\n    Returns:\n        Discretised Image object or numpy array (depending on input).\n        Values are 1-based indices.\n\n    Example:\n        Discretise image into 32 fixed bins (FBN):\n\n        ```python\n        from pictologics.preprocessing import discretise_image\n\n        # FBN with 32 bins\n        disc_image = discretise_image(\n            image,\n            method=\"FBN\",\n            n_bins=32\n        )\n        ```\n    \"\"\"\n    # Handle input type\n    if isinstance(image, Image):\n        array = image.array\n        is_image_obj = True\n    else:\n        array = image\n        is_image_obj = False\n\n    # Determine ROI values for default min/max\n    if roi_mask is not None:\n        if isinstance(roi_mask, Image):\n            mask_arr = roi_mask.array\n        else:\n            mask_arr = roi_mask\n\n        if mask_arr.shape != array.shape:\n            raise ValueError(\n                f\"Shape mismatch: Image {array.shape} vs Mask {mask_arr.shape}\"\n            )\n\n        # Extract ROI values (ignoring NaNs)\n        roi_values = array[(mask_arr &gt; 0) &amp; (~np.isnan(array))]\n    else:\n        roi_values = array[~np.isnan(array)]\n\n    # Initialize result\n    discretised = np.zeros(array.shape, dtype=int)\n\n    # We process all non-NaN pixels in the image\n    valid_mask = ~np.isnan(array)\n    values = array[valid_mask]\n\n    if values.size == 0:\n        if is_image_obj:\n            # Create new Image with discretised array\n            return Image(\n                array=discretised,\n                spacing=image.spacing,  # type: ignore\n                origin=image.origin,  # type: ignore\n                direction=image.direction,  # type: ignore\n                modality=image.modality,  # type: ignore\n            )\n        return discretised\n\n    if method == \"FBN\":\n        if n_bins is None:\n            raise ValueError(\"n_bins required for FBN\")\n        if n_bins &lt;= 0:\n            raise ValueError(\"n_bins must be positive\")\n\n        # Determine min/max\n        current_min = min_val\n        if current_min is None:\n            current_min = np.min(roi_values) if roi_values.size &gt; 0 else np.min(values)\n\n        current_max = max_val\n        if current_max is None:\n            current_max = np.max(roi_values) if roi_values.size &gt; 0 else np.max(values)\n\n        if current_max &lt;= current_min:\n            # Edge case: flat region or invalid range\n            discretised[valid_mask] = 1\n        else:\n            # IBSI FBN: floor(N_g * (X - X_min) / (X_max - X_min)) + 1\n            temp_discretised = (\n                np.floor(n_bins * (values - current_min) / (current_max - current_min))\n                + 1\n            )\n\n            # Handle max value case (it falls into N_g + 1 with this formula)\n            # Also clip outliers\n            temp_discretised[values &gt;= current_max] = n_bins\n            temp_discretised = np.clip(temp_discretised, 1, n_bins)\n\n            discretised[valid_mask] = temp_discretised.astype(int)\n\n    elif method == \"FBS\":\n        if bin_width is None:\n            raise ValueError(\"bin_width required for FBS\")\n        if bin_width &lt;= 0:\n            raise ValueError(\"bin_width must be positive\")\n\n        current_min = min_val\n        if current_min is None:\n            current_min = np.min(roi_values) if roi_values.size &gt; 0 else np.min(values)\n\n        # IBSI FBS: floor((X - X_min) / w_b) + 1\n        temp_discretised = np.floor((values - current_min) / bin_width) + 1\n\n        # Ensure minimum bin is 1\n        temp_discretised[temp_discretised &lt; 1] = 1\n        discretised[valid_mask] = temp_discretised.astype(int)\n\n    elif method == \"FIXED_CUTOFFS\":\n        if cutoffs is None:\n            raise ValueError(\"cutoffs required for FIXED_CUTOFFS\")\n\n        temp_discretised = np.digitize(values, bins=np.array(cutoffs))\n        discretised[valid_mask] = temp_discretised.astype(int)\n\n    else:\n        raise ValueError(f\"Unknown discretisation method: {method}\")\n\n    if is_image_obj:\n        return Image(\n            array=discretised,\n            spacing=image.spacing,  # type: ignore\n            origin=image.origin,  # type: ignore\n            direction=image.direction,  # type: ignore\n            modality=image.modality,  # type: ignore\n        )\n    return discretised\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.extract_roi","title":"<code>extract_roi(image, mask, mask_values=1)</code>","text":"<p>Extract ROI from image. Voxels outside the mask are set to NaN. IBSI 'ROI extraction'.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image object.</p> required <code>mask</code> <code>Image</code> <p>Image object (mask).</p> required <code>mask_values</code> <code>int | list[int] | None</code> <p>Value(s) in the mask to consider as ROI. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Image</code> <p>New Image object with non-ROI voxels set to NaN.</p> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def extract_roi(\n    image: Image,\n    mask: Image,\n    mask_values: int | list[int] | None = 1,\n) -&gt; Image:\n    \"\"\"\n    Extract ROI from image. Voxels outside the mask are set to NaN.\n    IBSI 'ROI extraction'.\n\n    Args:\n        image: Image object.\n        mask: Image object (mask).\n        mask_values: Value(s) in the mask to consider as ROI. Default is 1.\n\n    Returns:\n        New Image object with non-ROI voxels set to NaN.\n    \"\"\"\n    if image.array.shape != mask.array.shape:\n        raise ValueError(\"Image and mask must have the same shape.\")\n\n    # Handle mask values\n    if mask_values is None:\n        mask_values = [1]\n    elif isinstance(mask_values, int):\n        mask_values = [mask_values]\n\n    roi_mask = np.isin(mask.array, mask_values)\n\n    new_array = image.array.astype(float).copy()\n    new_array[~roi_mask] = np.nan\n\n    return Image(\n        array=new_array,\n        spacing=image.spacing,\n        origin=image.origin,\n        direction=image.direction,\n        modality=image.modality,\n    )\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.filter_outliers","title":"<code>filter_outliers(image, mask, sigma=3.0)</code>","text":"<p>Exclude outliers from the mask based on mean +/- sigma * std. IBSI 3.6.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image object.</p> required <code>mask</code> <code>Image</code> <p>Image object (mask).</p> required <code>sigma</code> <code>float</code> <p>Number of standard deviations.</p> <code>3.0</code> <p>Returns:</p> Type Description <code>Image</code> <p>New Image object (mask) with outliers removed.</p> Example <p>Remove outliers beyond 3 standard deviations from the mean:</p> <pre><code>from pictologics.preprocessing import filter_outliers\n\n# Remove outliers &gt; 3 sigma\nclean_mask = filter_outliers(\n    image,\n    mask,\n    sigma=3.0\n)\n</code></pre> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def filter_outliers(image: Image, mask: Image, sigma: float = 3.0) -&gt; Image:\n    \"\"\"\n    Exclude outliers from the mask based on mean +/- sigma * std.\n    IBSI 3.6.\n\n    Args:\n        image: Image object.\n        mask: Image object (mask).\n        sigma: Number of standard deviations.\n\n    Returns:\n        New Image object (mask) with outliers removed.\n\n    Example:\n        Remove outliers beyond 3 standard deviations from the mean:\n\n        ```python\n        from pictologics.preprocessing import filter_outliers\n\n        # Remove outliers &gt; 3 sigma\n        clean_mask = filter_outliers(\n            image,\n            mask,\n            sigma=3.0\n        )\n        ```\n    \"\"\"\n    # Extract values within the mask\n    values = apply_mask(image, mask)\n\n    if values.size == 0:\n        return mask\n\n    mean_val = np.mean(values)\n    # IBSI uses population std (no bias correction, ddof=0)\n    std_val = np.std(values, ddof=0)\n\n    lower_bound = mean_val - sigma * std_val\n    upper_bound = mean_val + sigma * std_val\n\n    # Create outlier mask\n    # Keep values within [lower, upper]\n    valid_mask = (image.array &gt;= lower_bound) &amp; (image.array &lt;= upper_bound)\n\n    # Update original mask\n    new_mask_array = mask.array.copy()\n\n    # Ensure boolean or integer type for bitwise operation\n    # Assuming mask.array is binary (0/1) or boolean\n    if new_mask_array.dtype == bool:\n        new_mask_array = new_mask_array &amp; valid_mask\n    else:\n        new_mask_array = (new_mask_array * valid_mask).astype(np.uint8)\n\n    return Image(\n        array=new_mask_array,\n        spacing=mask.spacing,\n        origin=mask.origin,\n        direction=mask.direction,\n        modality=mask.modality,\n    )\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.keep_largest_component","title":"<code>keep_largest_component(mask)</code>","text":"<p>Keep only the largest connected component in the mask.</p> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def keep_largest_component(mask: Image) -&gt; Image:\n    \"\"\"\n    Keep only the largest connected component in the mask.\n    \"\"\"\n    mask_array = mask.array\n    labeled_mask, num_features = label(mask_array)\n    if num_features &lt;= 1:\n        return mask\n\n    max_size = 0\n    max_label = 0\n    for i in range(1, num_features + 1):\n        size = np.sum(labeled_mask == i)\n        if size &gt; max_size:\n            max_size = size\n            max_label = i\n\n    new_array = (labeled_mask == max_label).astype(np.uint8)\n\n    return Image(\n        array=new_array,\n        spacing=mask.spacing,\n        origin=mask.origin,\n        direction=mask.direction,\n        modality=mask.modality,\n    )\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.resample_image","title":"<code>resample_image(image, new_spacing, interpolation='linear', boundary_mode='nearest', round_intensities=False, mask_threshold=None)</code>","text":"<p>Resample image to new voxel spacing using IBSI-compliant 'Align grid centers' method.</p> <p>Uses scipy.ndimage.affine_transform for memory efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input Image object.</p> required <code>new_spacing</code> <code>tuple[float, float, float]</code> <p>Target spacing (x, y, z). Must be positive.</p> required <code>interpolation</code> <code>str</code> <p>Interpolation method. 'nearest': Nearest neighbour (order 0). 'linear': Trilinear (order 1). 'cubic': Tricubic spline (order 3).</p> <code>'linear'</code> <code>boundary_mode</code> <code>str</code> <p>Padding mode for extrapolation. 'nearest' (default): Replicates edge values (aaaa|abcd|dddd). 'constant': Pads with constant value (0). 'reflect': Reflects at boundary. 'wrap': Wraps around.</p> <code>'nearest'</code> <code>round_intensities</code> <code>bool</code> <p>If True, round resulting intensities to nearest integer.</p> <code>False</code> <code>mask_threshold</code> <code>Optional[float]</code> <p>If provided, treat output as a binary mask.             Values &gt;= threshold become 1, others 0.             Commonly 0.5 for partial volume correction.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>Resampled Image object.</p> Example <p>Resample image to isotropic 1mm spacing using linear interpolation:</p> <pre><code>from pictologics.preprocessing import resample_image\n\n# Resample to 1x1x1 mm\nresampled_img = resample_image(\n    image,\n    new_spacing=(1.0, 1.0, 1.0),\n    interpolation=\"linear\"\n)\n</code></pre> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def resample_image(\n    image: Image,\n    new_spacing: tuple[float, float, float],\n    interpolation: str = \"linear\",\n    boundary_mode: str = \"nearest\",\n    round_intensities: bool = False,\n    mask_threshold: Optional[float] = None,\n) -&gt; Image:\n    \"\"\"\n    Resample image to new voxel spacing using IBSI-compliant 'Align grid centers' method.\n\n    Uses scipy.ndimage.affine_transform for memory efficiency.\n\n    Args:\n        image: Input Image object.\n        new_spacing: Target spacing (x, y, z). Must be positive.\n        interpolation: Interpolation method.\n            'nearest': Nearest neighbour (order 0).\n            'linear': Trilinear (order 1).\n            'cubic': Tricubic spline (order 3).\n        boundary_mode: Padding mode for extrapolation.\n            'nearest' (default): Replicates edge values (aaaa|abcd|dddd).\n            'constant': Pads with constant value (0).\n            'reflect': Reflects at boundary.\n            'wrap': Wraps around.\n        round_intensities: If True, round resulting intensities to nearest integer.\n        mask_threshold: If provided, treat output as a binary mask.\n                        Values &gt;= threshold become 1, others 0.\n                        Commonly 0.5 for partial volume correction.\n\n    Returns:\n        Resampled Image object.\n\n    Example:\n        Resample image to isotropic 1mm spacing using linear interpolation:\n\n        ```python\n        from pictologics.preprocessing import resample_image\n\n        # Resample to 1x1x1 mm\n        resampled_img = resample_image(\n            image,\n            new_spacing=(1.0, 1.0, 1.0),\n            interpolation=\"linear\"\n        )\n        ```\n    \"\"\"\n    if any(s &lt;= 0 for s in new_spacing):\n        raise ValueError(f\"New spacing must be positive, got {new_spacing}\")\n\n    # Map interpolation string to spline order\n    interpolation_map = {\n        \"nearest\": 0,\n        \"linear\": 1,\n        \"cubic\": 3,\n    }\n\n    if interpolation not in interpolation_map:\n        raise ValueError(\n            f\"Unknown interpolation method: {interpolation}. \"\n            f\"Supported: {list(interpolation_map.keys())}\"\n        )\n\n    order = interpolation_map[interpolation]\n\n    # Calculate new shape\n    # IBSI: nb = ceil(na * sa / sb)\n    original_spacing = np.array(image.spacing)\n    target_spacing = np.array(new_spacing)\n\n    # Scale factor for dimensions (how many new voxels per old voxel)\n    # dim_scale = s_old / s_new\n    dim_scale = original_spacing / target_spacing\n\n    new_shape = np.ceil(image.array.shape * dim_scale).astype(int)\n\n    # Calculate affine transform parameters\n    # We map Output Coordinate (x_out) -&gt; Input Coordinate (x_in)\n    # x_in = matrix * x_out + offset\n\n    # Scale factor for coordinates (step size in input space per step in output space)\n    # step_in = s_new / s_old\n    coord_scale = target_spacing / original_spacing\n    matrix = coord_scale  # Diagonal matrix elements\n\n    # Calculate offset for 'Align Grid Centers\n    center_orig = (np.array(image.array.shape) - 1) / 2.0\n    center_new = (new_shape - 1) / 2.0\n\n    offset = center_orig - matrix * center_new\n\n    resampled_array = affine_transform(\n        image.array,\n        matrix=matrix,\n        offset=offset,\n        output_shape=new_shape,\n        order=order,\n        mode=boundary_mode,\n    )\n\n    # Post-processing\n    if mask_threshold is not None:\n        # Binarize mask\n        resampled_array = (resampled_array &gt;= mask_threshold).astype(np.uint8)\n    elif round_intensities:\n        # Round intensities\n        resampled_array = np.round(resampled_array)\n\n    # Update origin to maintain center alignment\n    # O_new = O_old + 0.5 * ( (N_old-1)*S_old - (N_new-1)*S_new )\n    extent_orig = (np.array(image.array.shape) - 1) * original_spacing\n    extent_new = (new_shape - 1) * target_spacing\n    origin_shift = 0.5 * (extent_orig - extent_new)\n    new_origin = tuple(np.array(image.origin) + origin_shift)\n\n    return Image(\n        array=resampled_array,\n        spacing=new_spacing,\n        origin=new_origin,\n        direction=image.direction,\n        modality=image.modality,\n    )\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.resegment_mask","title":"<code>resegment_mask(image, mask, range_min=None, range_max=None)</code>","text":"<p>Update mask to exclude voxels where image intensity is outside the specified range. Used for IBSI re-segmentation (e.g. [-1000, 400] HU).</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image object.</p> required <code>mask</code> <code>Image</code> <p>Image object (mask).</p> required <code>range_min</code> <code>Optional[float]</code> <p>Minimum intensity value (inclusive). If None, no lower bound.</p> <code>None</code> <code>range_max</code> <code>Optional[float]</code> <p>Maximum intensity value (inclusive). If None, no upper bound.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>Updated Image object (mask) with re-segmentation applied.</p> Example <p>Resegment mask to keep only values between -1000 and 400 (e.g. HU range):</p> <pre><code>from pictologics.preprocessing import resegment_mask\n\n# Keep voxels in range [-1000, 400]\nnew_mask = resegment_mask(\n    image,\n    mask,\n    range_min=-1000,\n    range_max=400\n)\n</code></pre> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def resegment_mask(\n    image: Image,\n    mask: Image,\n    range_min: Optional[float] = None,\n    range_max: Optional[float] = None,\n) -&gt; Image:\n    \"\"\"\n    Update mask to exclude voxels where image intensity is outside the specified range.\n    Used for IBSI re-segmentation (e.g. [-1000, 400] HU).\n\n    Args:\n        image: Image object.\n        mask: Image object (mask).\n        range_min: Minimum intensity value (inclusive). If None, no lower bound.\n        range_max: Maximum intensity value (inclusive). If None, no upper bound.\n\n    Returns:\n        Updated Image object (mask) with re-segmentation applied.\n\n    Example:\n        Resegment mask to keep only values between -1000 and 400 (e.g. HU range):\n\n        ```python\n        from pictologics.preprocessing import resegment_mask\n\n        # Keep voxels in range [-1000, 400]\n        new_mask = resegment_mask(\n            image,\n            mask,\n            range_min=-1000,\n            range_max=400\n        )\n        ```\n    \"\"\"\n    if image.array.shape != mask.array.shape:\n        raise ValueError(\"Image and mask must have the same shape for re-segmentation.\")\n\n    new_mask_array = mask.array.copy()\n\n    # Identify outliers\n    outliers = np.zeros(image.array.shape, dtype=bool)\n\n    if range_min is not None:\n        outliers |= image.array &lt; range_min\n\n    if range_max is not None:\n        outliers |= image.array &gt; range_max\n\n    # Set mask to 0 where outliers exist\n    new_mask_array[outliers] = 0\n\n    return Image(\n        array=new_mask_array,\n        spacing=mask.spacing,\n        origin=mask.origin,\n        direction=mask.direction,\n        modality=mask.modality,\n    )\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.round_intensities","title":"<code>round_intensities(image)</code>","text":"<p>Round image intensities to the nearest integer.</p> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def round_intensities(image: Image) -&gt; Image:\n    \"\"\"\n    Round image intensities to the nearest integer.\n    \"\"\"\n    new_array = np.round(image.array)\n    return Image(\n        array=new_array,\n        spacing=image.spacing,\n        origin=image.origin,\n        direction=image.direction,\n        modality=image.modality,\n    )\n</code></pre>"},{"location":"api/results/","title":"Results API","text":""},{"location":"api/results/#pictologics.results","title":"<code>pictologics.results</code>","text":""},{"location":"api/results/#pictologics.results--results-module","title":"Results Module","text":"<p>This module provides utilities for formatting and saving radiomic feature extraction results. It supports multiple output formats (wide, long) and file formats (CSV, JSON).</p>"},{"location":"api/results/#pictologics.results--key-functions","title":"Key Functions:","text":"<ul> <li>format_results: Convert pipeline output to various formats (dict, pandas DataFrame, JSON).</li> <li>save_results: Save results to CSV or JSON files with automatic format detection.</li> </ul>"},{"location":"api/results/#pictologics.results.format_results","title":"<code>format_results(results, fmt='wide', meta=None, output_type='dict', config_col='config')</code>","text":"<p>Format the output of RadiomicsPipeline.run() into a structured format.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Series]</code> <p>Dictionary mapping configuration names to pandas Series of features      (the standard output of RadiomicsPipeline.run).</p> required <code>fmt</code> <code>str</code> <p>\"wide\" or \"long\".  - \"wide\": Flattens keys to '{config}__{feature}'. Returns 1 row (dict/df).  - \"long\": Tidy format with columns for config, feature_name, and value.</p> <code>'wide'</code> <code>meta</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to prepend to the result (e.g., subject ID).</p> <code>None</code> <code>output_type</code> <code>str</code> <p>Format of the returned object: \"dict\", \"pandas\", or \"json\".</p> <code>'dict'</code> <code>config_col</code> <code>str</code> <p>Name of the column holding the configuration name (only used if fmt=\"long\").</p> <code>'config'</code> <p>Returns:</p> Type Description <code>dict[str, Any] | DataFrame | str | list[dict[str, Any]]</code> <p>Formatted data in the specified output_type.</p> Example <p>Format results as a single pandas DataFrame row (wide format):</p> <pre><code>from pictologics.results import format_results\n\n# Assume 'results' is output from pipeline.run()\ndf = format_results(\n    results,\n    fmt=\"wide\",\n    meta={\"custom_id\": 123},\n    output_type=\"pandas\"\n)\n</code></pre> Source code in <code>pictologics/results.py</code> <pre><code>def format_results(\n    results: dict[str, pd.Series],\n    fmt: str = \"wide\",\n    meta: dict[str, Any] | None = None,\n    output_type: str = \"dict\",\n    config_col: str = \"config\",\n) -&gt; dict[str, Any] | pd.DataFrame | str | list[dict[str, Any]]:\n    \"\"\"\n    Format the output of RadiomicsPipeline.run() into a structured format.\n\n    Args:\n        results: Dictionary mapping configuration names to pandas Series of features\n                 (the standard output of RadiomicsPipeline.run).\n        fmt: \"wide\" or \"long\".\n             - \"wide\": Flattens keys to '{config}__{feature}'. Returns 1 row (dict/df).\n             - \"long\": Tidy format with columns for config, feature_name, and value.\n        meta: Optional dictionary of metadata to prepend to the result (e.g., subject ID).\n        output_type: Format of the returned object: \"dict\", \"pandas\", or \"json\".\n        config_col: Name of the column holding the configuration name (only used if fmt=\"long\").\n\n    Returns:\n        Formatted data in the specified output_type.\n\n    Example:\n        Format results as a single pandas DataFrame row (wide format):\n\n        ```python\n        from pictologics.results import format_results\n\n        # Assume 'results' is output from pipeline.run()\n        df = format_results(\n            results,\n            fmt=\"wide\",\n            meta={\"custom_id\": 123},\n            output_type=\"pandas\"\n        )\n        ```\n    \"\"\"\n    if meta is None:\n        meta = {}\n\n    if fmt == \"wide\":\n        # Wide format: { \"meta_key\": val, \"config__feature\": val }\n        formatted_data = meta.copy()\n        for config_name, series in results.items():\n            for feature_name, value in series.items():\n                col_name = f\"{config_name}__{feature_name}\"\n                formatted_data[col_name] = value\n\n        if output_type == \"dict\":\n            return formatted_data\n        elif output_type == \"pandas\":\n            return pd.DataFrame([formatted_data])\n        elif output_type == \"json\":\n            return json.dumps(formatted_data)\n        else:\n            raise ValueError(f\"Unknown output_type: {output_type}\")\n\n    elif fmt == \"long\":\n        # Long format: Rows of [meta_cols..., config, feature_name, value]\n        rows = []\n        for config_name, series in results.items():\n            for feature_name, value in series.items():\n                row = meta.copy()\n                row[config_col] = config_name\n                row[\"feature_name\"] = feature_name\n                row[\"value\"] = value\n                rows.append(row)\n\n        if not rows:\n            # Handle empty results case\n            # For pure python output, empty list is fine.\n            # For pandas, we need a dataframe with columns.\n            if output_type != \"pandas\":\n                if output_type == \"dict\":\n                    return []\n                elif output_type == \"json\":\n                    return \"[]\"\n\n            df = pd.DataFrame(\n                columns=list(meta.keys()) + [config_col, \"feature_name\", \"value\"]\n            )\n            return df  # Columns are already in order\n\n        # Reorder keys/columns\n        # Determine strict order\n        meta_keys = list(meta.keys())\n        standard_cols = [config_col, \"feature_name\", \"value\"]\n        # Ensure we don't duplicate keys\n        cols_order = meta_keys + [c for c in standard_cols if c not in meta_keys]\n\n        # If output is dict/json, reorder the dictionaries directly\n        if output_type in (\"dict\", \"json\"):\n            # Ensure each row has keys in the desired order\n            ordered_rows = []\n            for r in rows:\n                new_r = {k: r.get(k) for k in cols_order if k in r}\n\n                ordered_rows.append(new_r)\n\n            if output_type == \"dict\":\n                return ordered_rows\n            else:  # json\n                return json.dumps(ordered_rows)\n\n        # Output type is 'pandas'\n        df = pd.DataFrame(rows)\n        # Verify which columns actually exist in the dataframe\n        existing_cols = list(df.columns)\n        final_order = [c for c in cols_order if c in existing_cols] + [\n            c for c in existing_cols if c not in cols_order\n        ]\n        df = df.reindex(columns=final_order)\n\n        return df\n\n    else:\n        raise ValueError(f\"Unknown format: {fmt}. Use 'wide' or 'long'.\")\n</code></pre>"},{"location":"api/results/#pictologics.results.save_results","title":"<code>save_results(data, path, file_format=None)</code>","text":"<p>Save results to a file (CSV, JSON, etc.), automatically handling merging of lists.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any] | list[dict[str, Any]] | DataFrame | list[DataFrame] | str | list[str]</code> <p>The data to save. Supported types:   - Dict or List[Dict]   - DataFrame or List[DataFrame]   - JSON string or List[JSON strings]</p> required <code>path</code> <code>str | Path</code> <p>Output file path.</p> required <code>file_format</code> <code>str | None</code> <p>\"csv\" or \"json\". If None, inferred from file extension.</p> <code>None</code> Example <p>Save formatted results to JSON:</p> <pre><code>from pictologics.results import save_results\n\nsave_results(formatted_data, \"output/features.json\")\n</code></pre> Source code in <code>pictologics/results.py</code> <pre><code>def save_results(\n    data: (\n        dict[str, Any]\n        | list[dict[str, Any]]\n        | pd.DataFrame\n        | list[pd.DataFrame]\n        | str\n        | list[str]\n    ),\n    path: str | Path,\n    file_format: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Save results to a file (CSV, JSON, etc.), automatically handling merging of lists.\n\n    Args:\n        data: The data to save. Supported types:\n              - Dict or List[Dict]\n              - DataFrame or List[DataFrame]\n              - JSON string or List[JSON strings]\n        path: Output file path.\n        file_format: \"csv\" or \"json\". If None, inferred from file extension.\n\n    Example:\n        Save formatted results to JSON:\n\n        ```python\n        from pictologics.results import save_results\n\n        save_results(formatted_data, \"output/features.json\")\n        ```\n    \"\"\"\n    path = Path(path)\n    if file_format is None:\n        if path.suffix.lower() == \".csv\":\n            file_format = \"csv\"\n        elif path.suffix.lower() == \".json\":\n            file_format = \"json\"\n        else:\n            file_format = \"csv\"\n\n    # If format is JSON and data is already a dict or list of dicts, bypass pandas\n    # to avoid overhead and potential C-extension conflicts in coverage/threading.\n    if file_format == \"json\":\n        # Check if data is already in a compatible format\n        is_dict = isinstance(data, dict)\n        is_list_of_dicts = isinstance(data, list) and (\n            not data or isinstance(data[0], dict)\n        )\n\n        if is_dict:\n            with open(path, \"w\") as f:\n                json.dump([data], f, indent=2)\n            return\n\n        if is_list_of_dicts:\n            with open(path, \"w\") as f:\n                json.dump(data, f, indent=2)\n            return\n\n    # Normalize input to a single DataFrame\n    try:\n        final_df = _normalize_to_dataframe(data)\n    except Exception as e:\n        # If normalization fails but we want to debug, re-raise\n        raise e\n\n    # Export\n    if file_format == \"csv\":\n        final_df.to_csv(path, index=False)\n    elif file_format == \"json\":\n        # Use standard json library to avoid potential pandas C-extension issues during coverage\n        with open(path, \"w\") as f:\n            json.dump(final_df.to_dict(orient=\"records\"), f, indent=2)\n    else:\n        raise ValueError(f\"Unsupported export format: {file_format}\")\n</code></pre>"},{"location":"api/utilities/","title":"Utilities API","text":""},{"location":"api/utilities/#pictologics.utilities.dicom_database","title":"<code>pictologics.utilities.dicom_database</code>","text":""},{"location":"api/utilities/#pictologics.utilities.dicom_database--dicom-database-module","title":"DICOM Database Module","text":"<p>This module provides dataclass-based hierarchical organization of DICOM files with completeness validation and multi-level DataFrame exports.</p> <p>The implementation supports parallel processing for improved performance on large datasets, with stateless file processing and immutable intermediate results.</p>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomInstance","title":"<code>DicomInstance</code>  <code>dataclass</code>","text":"<p>Represents a single DICOM file/instance.</p> <p>Attributes:</p> Name Type Description <code>sop_instance_uid</code> <code>str</code> <p>Unique identifier for this instance.</p> <code>file_path</code> <code>Path</code> <p>Absolute path to the DICOM file.</p> <code>instance_number</code> <code>Optional[int]</code> <p>Instance number within the series.</p> <code>image_position_patient</code> <code>Optional[tuple[float, float, float]]</code> <p>(x, y, z) position in patient coordinates.</p> <code>image_orientation_patient</code> <code>Optional[tuple[float, ...]]</code> <p>Direction cosines for row and column.</p> <code>slice_location</code> <code>Optional[float]</code> <p>Slice location value from DICOM header.</p> <code>acquisition_datetime</code> <code>Optional[str]</code> <p>Combined acquisition date and time.</p> <code>projection_score</code> <code>Optional[float]</code> <p>Calculated projection onto slice normal for sorting.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional extracted metadata tags.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>@dataclass\nclass DicomInstance:\n    \"\"\"Represents a single DICOM file/instance.\n\n    Attributes:\n        sop_instance_uid: Unique identifier for this instance.\n        file_path: Absolute path to the DICOM file.\n        instance_number: Instance number within the series.\n        image_position_patient: (x, y, z) position in patient coordinates.\n        image_orientation_patient: Direction cosines for row and column.\n        slice_location: Slice location value from DICOM header.\n        acquisition_datetime: Combined acquisition date and time.\n        projection_score: Calculated projection onto slice normal for sorting.\n        metadata: Additional extracted metadata tags.\n    \"\"\"\n\n    sop_instance_uid: str\n    file_path: Path\n    instance_number: Optional[int] = None\n    image_position_patient: Optional[tuple[float, float, float]] = None\n    image_orientation_patient: Optional[tuple[float, ...]] = None\n    slice_location: Optional[float] = None\n    acquisition_datetime: Optional[str] = None\n    projection_score: Optional[float] = None\n    metadata: dict[str, Any] = field(default_factory=dict)\n    tags: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomSeries","title":"<code>DicomSeries</code>  <code>dataclass</code>","text":"<p>Represents a DICOM series containing multiple instances.</p> <p>Attributes:</p> Name Type Description <code>series_instance_uid</code> <code>str</code> <p>Unique identifier for this series.</p> <code>series_number</code> <code>Optional[int]</code> <p>Series number within the study.</p> <code>series_description</code> <code>Optional[str]</code> <p>Description of the series.</p> <code>modality</code> <code>Optional[str]</code> <p>Imaging modality (CT, MR, etc.).</p> <code>frame_of_reference_uid</code> <code>Optional[str]</code> <p>Frame of reference UID.</p> <code>instances</code> <code>list[DicomInstance]</code> <p>List of DicomInstance objects in this series.</p> <code>common_metadata</code> <code>dict[str, Any]</code> <p>Metadata tags identical across all instances.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>@dataclass\nclass DicomSeries:\n    \"\"\"Represents a DICOM series containing multiple instances.\n\n    Attributes:\n        series_instance_uid: Unique identifier for this series.\n        series_number: Series number within the study.\n        series_description: Description of the series.\n        modality: Imaging modality (CT, MR, etc.).\n        frame_of_reference_uid: Frame of reference UID.\n        instances: List of DicomInstance objects in this series.\n        common_metadata: Metadata tags identical across all instances.\n    \"\"\"\n\n    series_instance_uid: str\n    series_number: Optional[int] = None\n    series_description: Optional[str] = None\n    modality: Optional[str] = None\n    frame_of_reference_uid: Optional[str] = None\n    instances: list[DicomInstance] = field(default_factory=list)\n    common_metadata: dict[str, Any] = field(default_factory=dict)\n\n    def get_sorted_instances(self) -&gt; list[DicomInstance]:\n        \"\"\"Return instances sorted by spatial position (projection score).\n\n        Uses the same methodology as pictologics.loader for spatial sorting.\n        Falls back to instance number if projection scores are not available.\n        \"\"\"\n        if all(inst.projection_score is not None for inst in self.instances):\n            return sorted(self.instances, key=lambda x: x.projection_score or 0)\n        return sorted(\n            self.instances, key=lambda x: x.instance_number if x.instance_number else 0\n        )\n\n    def check_completeness(self, spacing_tolerance: float = 0.1) -&gt; dict[str, Any]:\n        \"\"\"Check if the series has all expected slices.\n\n        Uses geometric validation based on ImagePositionPatient projection\n        to detect missing slices and gaps.\n\n        Args:\n            spacing_tolerance: Tolerance for gap detection (default 10%).\n\n        Returns:\n            Dictionary with completeness information.\n        \"\"\"\n        result: dict[str, Any] = {\n            \"series_uid\": self.series_instance_uid,\n            \"total_instances\": len(self.instances),\n            \"expected_instances\": len(self.instances),\n            \"is_complete\": True,\n            \"has_gaps\": False,\n            \"gap_indices\": [],\n            \"gap_positions\": [],\n            \"spacing_mm\": None,\n            \"spacing_std\": None,\n            \"spacing_uniform\": True,\n            \"first_slice_position\": None,\n            \"last_slice_position\": None,\n            \"frame_of_reference_uid\": self.frame_of_reference_uid,\n            \"warnings\": [],\n        }\n\n        if len(self.instances) &lt; 2:\n            result[\"warnings\"].append(\"Series has fewer than 2 instances\")\n            return result\n\n        # Get sorted instances by projection score\n        sorted_instances = self.get_sorted_instances()\n\n        # Check if we have projection scores for geometric validation\n        projection_scores = [\n            inst.projection_score\n            for inst in sorted_instances\n            if inst.projection_score is not None\n        ]\n\n        if len(projection_scores) &lt; 2:\n            result[\"warnings\"].append(\n                \"Insufficient spatial data for geometric completeness check\"\n            )\n            # Fall back to instance number validation\n            instance_numbers = [\n                inst.instance_number\n                for inst in sorted_instances\n                if inst.instance_number is not None\n            ]\n            if len(instance_numbers) &gt;= 2:\n                instance_numbers_sorted = sorted(instance_numbers)\n                expected_range = set(\n                    range(instance_numbers_sorted[0], instance_numbers_sorted[-1] + 1)\n                )\n                missing = expected_range - set(instance_numbers)\n                if missing:\n                    result[\"is_complete\"] = False\n                    result[\"has_gaps\"] = True\n                    result[\"gap_indices\"] = sorted(missing)\n                    result[\"expected_instances\"] = len(expected_range)\n            return result\n\n        # Calculate spacings between consecutive slices\n        spacings = np.diff(projection_scores)\n        median_spacing = float(np.median(np.abs(spacings)))\n        spacing_std = float(np.std(np.abs(spacings)))\n\n        result[\"spacing_mm\"] = median_spacing\n        result[\"spacing_std\"] = spacing_std\n        result[\"first_slice_position\"] = projection_scores[0]\n        result[\"last_slice_position\"] = projection_scores[-1]\n\n        # Check for uniform spacing\n        if median_spacing &gt; 0:\n            spacing_cv = spacing_std / median_spacing  # Coefficient of variation\n            result[\"spacing_uniform\"] = spacing_cv &lt; spacing_tolerance\n\n        # Detect gaps (spacing significantly larger than expected)\n        gap_threshold = median_spacing * (1 + spacing_tolerance)\n        gap_indices = []\n        gap_positions = []\n\n        for i, spacing in enumerate(np.abs(spacings)):\n            if spacing &gt; gap_threshold * 1.5:  # Gap detected\n                gap_indices.append(i + 1)\n                gap_positions.append(projection_scores[i])\n\n        if gap_indices:\n            result[\"has_gaps\"] = True\n            result[\"gap_indices\"] = gap_indices\n            result[\"gap_positions\"] = gap_positions\n\n            # Estimate expected instances based on position range and median spacing\n            position_range = abs(projection_scores[-1] - projection_scores[0])\n            if median_spacing &gt; 0:\n                expected = int(round(position_range / median_spacing)) + 1\n                result[\"expected_instances\"] = expected\n                result[\"is_complete\"] = False\n\n        return result\n\n    def get_instance_uids(self) -&gt; list[str]:\n        \"\"\"Get list of all instance SOPInstanceUIDs.\"\"\"\n        return [inst.sop_instance_uid for inst in self.instances]\n\n    def get_file_paths(self) -&gt; list[str]:\n        \"\"\"Get list of all instance file paths as strings.\"\"\"\n        return [str(inst.file_path) for inst in self.instances]\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomSeries.check_completeness","title":"<code>check_completeness(spacing_tolerance=0.1)</code>","text":"<p>Check if the series has all expected slices.</p> <p>Uses geometric validation based on ImagePositionPatient projection to detect missing slices and gaps.</p> <p>Parameters:</p> Name Type Description Default <code>spacing_tolerance</code> <code>float</code> <p>Tolerance for gap detection (default 10%).</p> <code>0.1</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with completeness information.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def check_completeness(self, spacing_tolerance: float = 0.1) -&gt; dict[str, Any]:\n    \"\"\"Check if the series has all expected slices.\n\n    Uses geometric validation based on ImagePositionPatient projection\n    to detect missing slices and gaps.\n\n    Args:\n        spacing_tolerance: Tolerance for gap detection (default 10%).\n\n    Returns:\n        Dictionary with completeness information.\n    \"\"\"\n    result: dict[str, Any] = {\n        \"series_uid\": self.series_instance_uid,\n        \"total_instances\": len(self.instances),\n        \"expected_instances\": len(self.instances),\n        \"is_complete\": True,\n        \"has_gaps\": False,\n        \"gap_indices\": [],\n        \"gap_positions\": [],\n        \"spacing_mm\": None,\n        \"spacing_std\": None,\n        \"spacing_uniform\": True,\n        \"first_slice_position\": None,\n        \"last_slice_position\": None,\n        \"frame_of_reference_uid\": self.frame_of_reference_uid,\n        \"warnings\": [],\n    }\n\n    if len(self.instances) &lt; 2:\n        result[\"warnings\"].append(\"Series has fewer than 2 instances\")\n        return result\n\n    # Get sorted instances by projection score\n    sorted_instances = self.get_sorted_instances()\n\n    # Check if we have projection scores for geometric validation\n    projection_scores = [\n        inst.projection_score\n        for inst in sorted_instances\n        if inst.projection_score is not None\n    ]\n\n    if len(projection_scores) &lt; 2:\n        result[\"warnings\"].append(\n            \"Insufficient spatial data for geometric completeness check\"\n        )\n        # Fall back to instance number validation\n        instance_numbers = [\n            inst.instance_number\n            for inst in sorted_instances\n            if inst.instance_number is not None\n        ]\n        if len(instance_numbers) &gt;= 2:\n            instance_numbers_sorted = sorted(instance_numbers)\n            expected_range = set(\n                range(instance_numbers_sorted[0], instance_numbers_sorted[-1] + 1)\n            )\n            missing = expected_range - set(instance_numbers)\n            if missing:\n                result[\"is_complete\"] = False\n                result[\"has_gaps\"] = True\n                result[\"gap_indices\"] = sorted(missing)\n                result[\"expected_instances\"] = len(expected_range)\n        return result\n\n    # Calculate spacings between consecutive slices\n    spacings = np.diff(projection_scores)\n    median_spacing = float(np.median(np.abs(spacings)))\n    spacing_std = float(np.std(np.abs(spacings)))\n\n    result[\"spacing_mm\"] = median_spacing\n    result[\"spacing_std\"] = spacing_std\n    result[\"first_slice_position\"] = projection_scores[0]\n    result[\"last_slice_position\"] = projection_scores[-1]\n\n    # Check for uniform spacing\n    if median_spacing &gt; 0:\n        spacing_cv = spacing_std / median_spacing  # Coefficient of variation\n        result[\"spacing_uniform\"] = spacing_cv &lt; spacing_tolerance\n\n    # Detect gaps (spacing significantly larger than expected)\n    gap_threshold = median_spacing * (1 + spacing_tolerance)\n    gap_indices = []\n    gap_positions = []\n\n    for i, spacing in enumerate(np.abs(spacings)):\n        if spacing &gt; gap_threshold * 1.5:  # Gap detected\n            gap_indices.append(i + 1)\n            gap_positions.append(projection_scores[i])\n\n    if gap_indices:\n        result[\"has_gaps\"] = True\n        result[\"gap_indices\"] = gap_indices\n        result[\"gap_positions\"] = gap_positions\n\n        # Estimate expected instances based on position range and median spacing\n        position_range = abs(projection_scores[-1] - projection_scores[0])\n        if median_spacing &gt; 0:\n            expected = int(round(position_range / median_spacing)) + 1\n            result[\"expected_instances\"] = expected\n            result[\"is_complete\"] = False\n\n    return result\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomSeries.get_file_paths","title":"<code>get_file_paths()</code>","text":"<p>Get list of all instance file paths as strings.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def get_file_paths(self) -&gt; list[str]:\n    \"\"\"Get list of all instance file paths as strings.\"\"\"\n    return [str(inst.file_path) for inst in self.instances]\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomSeries.get_instance_uids","title":"<code>get_instance_uids()</code>","text":"<p>Get list of all instance SOPInstanceUIDs.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def get_instance_uids(self) -&gt; list[str]:\n    \"\"\"Get list of all instance SOPInstanceUIDs.\"\"\"\n    return [inst.sop_instance_uid for inst in self.instances]\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomSeries.get_sorted_instances","title":"<code>get_sorted_instances()</code>","text":"<p>Return instances sorted by spatial position (projection score).</p> <p>Uses the same methodology as pictologics.loader for spatial sorting. Falls back to instance number if projection scores are not available.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def get_sorted_instances(self) -&gt; list[DicomInstance]:\n    \"\"\"Return instances sorted by spatial position (projection score).\n\n    Uses the same methodology as pictologics.loader for spatial sorting.\n    Falls back to instance number if projection scores are not available.\n    \"\"\"\n    if all(inst.projection_score is not None for inst in self.instances):\n        return sorted(self.instances, key=lambda x: x.projection_score or 0)\n    return sorted(\n        self.instances, key=lambda x: x.instance_number if x.instance_number else 0\n    )\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomStudy","title":"<code>DicomStudy</code>  <code>dataclass</code>","text":"<p>Represents a DICOM study containing multiple series.</p> <p>Attributes:</p> Name Type Description <code>study_instance_uid</code> <code>str</code> <p>Unique identifier for this study.</p> <code>study_date</code> <code>Optional[str]</code> <p>Date of the study.</p> <code>study_time</code> <code>Optional[str]</code> <p>Time of the study.</p> <code>study_description</code> <code>Optional[str]</code> <p>Description of the study.</p> <code>series</code> <code>list[DicomSeries]</code> <p>List of DicomSeries objects in this study.</p> <code>common_metadata</code> <code>dict[str, Any]</code> <p>Metadata tags identical across all series.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>@dataclass\nclass DicomStudy:\n    \"\"\"Represents a DICOM study containing multiple series.\n\n    Attributes:\n        study_instance_uid: Unique identifier for this study.\n        study_date: Date of the study.\n        study_time: Time of the study.\n        study_description: Description of the study.\n        series: List of DicomSeries objects in this study.\n        common_metadata: Metadata tags identical across all series.\n    \"\"\"\n\n    study_instance_uid: str\n    study_date: Optional[str] = None\n    study_time: Optional[str] = None\n    study_description: Optional[str] = None\n    series: list[DicomSeries] = field(default_factory=list)\n    common_metadata: dict[str, Any] = field(default_factory=dict)\n\n    def get_instance_uids(self) -&gt; list[str]:\n        \"\"\"Get list of all instance SOPInstanceUIDs in this study.\"\"\"\n        uids = []\n        for s in self.series:\n            uids.extend(s.get_instance_uids())\n        return uids\n\n    def get_file_paths(self) -&gt; list[str]:\n        \"\"\"Get list of all instance file paths in this study.\"\"\"\n        paths = []\n        for s in self.series:\n            paths.extend(s.get_file_paths())\n        return paths\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomStudy.get_file_paths","title":"<code>get_file_paths()</code>","text":"<p>Get list of all instance file paths in this study.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def get_file_paths(self) -&gt; list[str]:\n    \"\"\"Get list of all instance file paths in this study.\"\"\"\n    paths = []\n    for s in self.series:\n        paths.extend(s.get_file_paths())\n    return paths\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomStudy.get_instance_uids","title":"<code>get_instance_uids()</code>","text":"<p>Get list of all instance SOPInstanceUIDs in this study.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def get_instance_uids(self) -&gt; list[str]:\n    \"\"\"Get list of all instance SOPInstanceUIDs in this study.\"\"\"\n    uids = []\n    for s in self.series:\n        uids.extend(s.get_instance_uids())\n    return uids\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomPatient","title":"<code>DicomPatient</code>  <code>dataclass</code>","text":"<p>Represents a DICOM patient containing multiple studies.</p> <p>Attributes:</p> Name Type Description <code>patient_id</code> <code>str</code> <p>Patient identifier.</p> <code>patients_name</code> <code>Optional[str]</code> <p>Patient's name.</p> <code>patients_birth_date</code> <code>Optional[str]</code> <p>Patient's birth date.</p> <code>patients_sex</code> <code>Optional[str]</code> <p>Patient's sex.</p> <code>studies</code> <code>list[DicomStudy]</code> <p>List of DicomStudy objects for this patient.</p> <code>common_metadata</code> <code>dict[str, Any]</code> <p>Metadata tags identical across all studies.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>@dataclass\nclass DicomPatient:\n    \"\"\"Represents a DICOM patient containing multiple studies.\n\n    Attributes:\n        patient_id: Patient identifier.\n        patients_name: Patient's name.\n        patients_birth_date: Patient's birth date.\n        patients_sex: Patient's sex.\n        studies: List of DicomStudy objects for this patient.\n        common_metadata: Metadata tags identical across all studies.\n    \"\"\"\n\n    patient_id: str\n    patients_name: Optional[str] = None\n    patients_birth_date: Optional[str] = None\n    patients_sex: Optional[str] = None\n    studies: list[DicomStudy] = field(default_factory=list)\n    common_metadata: dict[str, Any] = field(default_factory=dict)\n\n    def get_instance_uids(self) -&gt; list[str]:\n        \"\"\"Get list of all instance SOPInstanceUIDs for this patient.\"\"\"\n        uids = []\n        for study in self.studies:\n            uids.extend(study.get_instance_uids())\n        return uids\n\n    def get_file_paths(self) -&gt; list[str]:\n        \"\"\"Get list of all instance file paths for this patient.\"\"\"\n        paths = []\n        for study in self.studies:\n            paths.extend(study.get_file_paths())\n        return paths\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomPatient.get_file_paths","title":"<code>get_file_paths()</code>","text":"<p>Get list of all instance file paths for this patient.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def get_file_paths(self) -&gt; list[str]:\n    \"\"\"Get list of all instance file paths for this patient.\"\"\"\n    paths = []\n    for study in self.studies:\n        paths.extend(study.get_file_paths())\n    return paths\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomPatient.get_instance_uids","title":"<code>get_instance_uids()</code>","text":"<p>Get list of all instance SOPInstanceUIDs for this patient.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def get_instance_uids(self) -&gt; list[str]:\n    \"\"\"Get list of all instance SOPInstanceUIDs for this patient.\"\"\"\n    uids = []\n    for study in self.studies:\n        uids.extend(study.get_instance_uids())\n    return uids\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomDatabase","title":"<code>DicomDatabase</code>  <code>dataclass</code>","text":"<p>Top-level database containing all patients.</p> <p>This class provides the main interface for building a DICOM database from folders and exporting to various formats.</p> <p>Attributes:</p> Name Type Description <code>patients</code> <code>list[DicomPatient]</code> <p>List of DicomPatient objects.</p> <code>spacing_tolerance</code> <code>float</code> <p>Tolerance for gap detection in completeness checks.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>@dataclass\nclass DicomDatabase:\n    \"\"\"Top-level database containing all patients.\n\n    This class provides the main interface for building a DICOM database\n    from folders and exporting to various formats.\n\n    Attributes:\n        patients: List of DicomPatient objects.\n        spacing_tolerance: Tolerance for gap detection in completeness checks.\n    \"\"\"\n\n    patients: list[DicomPatient] = field(default_factory=list)\n    spacing_tolerance: float = 0.1\n\n    @classmethod\n    def from_folders(\n        cls,\n        paths: list[str | Path],\n        recursive: bool = True,\n        spacing_tolerance: float = 0.1,\n        show_progress: bool = True,\n        extract_private_tags: bool = True,\n        num_workers: Optional[int] = None,\n        split_multiseries: bool = True,\n    ) -&gt; \"DicomDatabase\":\n        \"\"\"Build a database from folder paths.\n\n        Args:\n            paths: List of folder paths to scan.\n            recursive: Whether to scan subdirectories.\n            spacing_tolerance: Tolerance for gap detection (default 10%).\n            show_progress: Whether to display progress bars.\n            extract_private_tags: Whether to extract vendor-specific private tags.\n            num_workers: Number of parallel workers. None=auto (cpu_count-1),\n                        1=sequential (no multiprocessing).\n            split_multiseries: Whether to split multi-phase series (e.g. cardiac)\n                              into separate series based on tags or spatial duplicates.\n\n        Returns:\n            DicomDatabase instance populated with all discovered DICOM files.\n\n        Example:\n            Build database from multiple folders:\n\n            ```python\n            from pictologics.utilities.dicom_database import DicomDatabase\n\n            db = DicomDatabase.from_folders(\n                paths=[\"data/patient1\", \"data/patient2\"],\n                recursive=True,\n                num_workers=4\n            )\n            print(f\"Found {len(db.patients)} patients\")\n            ```\n        \"\"\"\n        # Convert paths to Path objects\n        path_objs = [Path(p) for p in paths]\n\n        # Determine number of workers\n        workers = _get_num_workers(num_workers)\n\n        # Step 1: Discover all DICOM files\n        dicom_files = _scan_dicom_files(path_objs, recursive, show_progress, workers)\n\n        if not dicom_files:\n            return cls(patients=[], spacing_tolerance=spacing_tolerance)\n\n        # Step 2: Extract metadata from each file (parallel if workers &gt; 1)\n        file_metadata = _extract_all_metadata(\n            dicom_files, show_progress, extract_private_tags, workers\n        )\n\n        # Step 3: Build hierarchy from flat metadata list\n        patients = _build_hierarchy(file_metadata, spacing_tolerance, split_multiseries)\n\n        # Step 4: Sort the hierarchy for consistent output\n        patients = _sort_hierarchy(patients)\n\n        return cls(patients=patients, spacing_tolerance=spacing_tolerance)\n\n    # ========================================================================\n    # DataFrame Export Methods\n    # ========================================================================\n\n    def get_patients_df(self, include_instance_lists: bool = False) -&gt; pd.DataFrame:\n        \"\"\"Export patient-level summary DataFrame.\n\n        Args:\n            include_instance_lists: Whether to include InstanceSOPUIDs and\n                InstanceFilePaths columns. Defaults to False to reduce memory.\n\n        Returns:\n            DataFrame with patient information and aggregated statistics.\n        \"\"\"\n        rows = []\n        for patient in self.patients:\n            row: Dict[str, Any] = {\n                \"PatientID\": patient.patient_id,\n                \"PatientsName\": patient.patients_name,\n                \"PatientsBirthDate\": patient.patients_birth_date,\n                \"PatientsSex\": patient.patients_sex,\n                \"NumStudies\": len(patient.studies),\n                \"NumSeries\": sum(len(study.series) for study in patient.studies),\n                \"NumInstances\": sum(\n                    len(series.instances)\n                    for study in patient.studies\n                    for series in study.series\n                ),\n            }\n            if include_instance_lists:\n                row[\"InstanceSOPUIDs\"] = patient.get_instance_uids()\n                row[\"InstanceFilePaths\"] = patient.get_file_paths()\n\n            # Add study date range\n            study_dates = [\n                study.study_date for study in patient.studies if study.study_date\n            ]\n            if study_dates:\n                row[\"EarliestStudyDate\"] = min(study_dates)\n                row[\"LatestStudyDate\"] = max(study_dates)\n            else:\n                row[\"EarliestStudyDate\"] = None\n                row[\"LatestStudyDate\"] = None\n\n            # Add common metadata from patient level\n            for key, value in patient.common_metadata.items():\n                if key not in row:\n                    row[key] = value\n\n            rows.append(row)\n\n        return pd.DataFrame(rows)\n\n    def get_studies_df(\n        self,\n        patient_id: Optional[str] = None,\n        include_instance_lists: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Export study-level summary DataFrame.\n\n        Args:\n            patient_id: Optional filter by patient ID.\n            include_instance_lists: Whether to include InstanceSOPUIDs and\n                InstanceFilePaths columns. Defaults to False to reduce memory.\n\n        Returns:\n            DataFrame with study information.\n        \"\"\"\n        rows = []\n        for patient in self.patients:\n            if patient_id and patient.patient_id != patient_id:\n                continue\n\n            for study in patient.studies:\n                row: Dict[str, Any] = {\n                    # Patient info\n                    \"PatientID\": patient.patient_id,\n                    \"PatientsName\": patient.patients_name,\n                    \"PatientsBirthDate\": patient.patients_birth_date,\n                    \"PatientsSex\": patient.patients_sex,\n                    # Study info\n                    \"StudyInstanceUID\": study.study_instance_uid,\n                    \"StudyDate\": study.study_date,\n                    \"StudyTime\": study.study_time,\n                    \"StudyDescription\": study.study_description,\n                    \"NumSeries\": len(study.series),\n                    \"NumInstances\": sum(\n                        len(series.instances) for series in study.series\n                    ),\n                }\n                if include_instance_lists:\n                    row[\"InstanceSOPUIDs\"] = study.get_instance_uids()\n                    row[\"InstanceFilePaths\"] = study.get_file_paths()\n\n                # Collect modalities present\n                modalities = list(set(s.modality for s in study.series if s.modality))\n                row[\"ModalitiesPresent\"] = modalities\n\n                # Add common metadata\n                for key, value in patient.common_metadata.items():\n                    if key not in row:\n                        row[key] = value\n                for key, value in study.common_metadata.items():\n                    if key not in row:\n                        row[key] = value\n\n                rows.append(row)\n\n        return pd.DataFrame(rows)\n\n    def get_series_df(\n        self,\n        patient_id: Optional[str] = None,\n        study_uid: Optional[str] = None,\n        include_instance_lists: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Export series-level summary DataFrame with completeness info.\n\n        Args:\n            patient_id: Optional filter by patient ID.\n            study_uid: Optional filter by study UID.\n            include_instance_lists: Whether to include InstanceSOPUIDs and\n                InstanceFilePaths columns. Defaults to False to reduce memory.\n\n        Returns:\n            DataFrame with series information including completeness validation.\n        \"\"\"\n        rows = []\n        for patient in self.patients:\n            if patient_id and patient.patient_id != patient_id:\n                continue\n\n            for study in patient.studies:\n                if study_uid and study.study_instance_uid != study_uid:\n                    continue\n\n                for series in study.series:\n                    completeness = series.check_completeness(self.spacing_tolerance)\n\n                    row = {\n                        # Patient info\n                        \"PatientID\": patient.patient_id,\n                        \"PatientsName\": patient.patients_name,\n                        # Study info\n                        \"StudyInstanceUID\": study.study_instance_uid,\n                        \"StudyDate\": study.study_date,\n                        \"StudyDescription\": study.study_description,\n                        # Series info\n                        \"SeriesInstanceUID\": series.series_instance_uid,\n                        \"SeriesNumber\": series.series_number,\n                        \"SeriesDescription\": series.series_description,\n                        \"Modality\": series.modality,\n                        \"FrameOfReferenceUID\": series.frame_of_reference_uid,\n                        # Completeness\n                        \"NumInstances\": completeness[\"total_instances\"],\n                        \"ExpectedInstances\": completeness[\"expected_instances\"],\n                        \"IsComplete\": completeness[\"is_complete\"],\n                        \"HasGaps\": completeness[\"has_gaps\"],\n                        \"GapIndices\": completeness[\"gap_indices\"],\n                        \"SpacingMM\": completeness[\"spacing_mm\"],\n                        \"SpacingUniform\": completeness[\"spacing_uniform\"],\n                        \"FirstSlicePosition\": completeness[\"first_slice_position\"],\n                        \"LastSlicePosition\": completeness[\"last_slice_position\"],\n                        \"CompletenessWarnings\": completeness[\"warnings\"],\n                    }\n                    if include_instance_lists:\n                        row[\"InstanceSOPUIDs\"] = series.get_instance_uids()\n                        row[\"InstanceFilePaths\"] = series.get_file_paths()\n\n                    # Add common metadata from all levels\n                    for key, value in patient.common_metadata.items():\n                        if key not in row:\n                            row[key] = value\n                    for key, value in study.common_metadata.items():\n                        if key not in row:\n                            row[key] = value\n                    for key, value in series.common_metadata.items():\n                        if key not in row:\n                            row[key] = value\n\n                    rows.append(row)\n\n        return pd.DataFrame(rows)\n\n    def get_instances_df(\n        self,\n        patient_id: Optional[str] = None,\n        study_uid: Optional[str] = None,\n        series_uid: Optional[str] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Export instance-level detail DataFrame.\n\n        Args:\n            patient_id: Optional filter by patient ID.\n            study_uid: Optional filter by study UID.\n            series_uid: Optional filter by series UID.\n\n        Returns:\n            DataFrame with complete instance information.\n        \"\"\"\n        rows = []\n        for patient in self.patients:\n            if patient_id and patient.patient_id != patient_id:\n                continue\n\n            for study in patient.studies:\n                if study_uid and study.study_instance_uid != study_uid:\n                    continue\n\n                for series in study.series:\n                    if series_uid and series.series_instance_uid != series_uid:\n                        continue\n\n                    for instance in series.instances:\n                        row: dict[str, Any] = {\n                            # Hierarchy IDs\n                            \"PatientID\": patient.patient_id,\n                            \"StudyInstanceUID\": study.study_instance_uid,\n                            \"SeriesInstanceUID\": series.series_instance_uid,\n                            \"SOPInstanceUID\": instance.sop_instance_uid,\n                            # Instance info\n                            \"FilePath\": str(instance.file_path),\n                            \"InstanceNumber\": instance.instance_number,\n                            \"SliceLocation\": instance.slice_location,\n                            \"ProjectionScore\": instance.projection_score,\n                            \"AcquisitionDateTime\": instance.acquisition_datetime,\n                        }\n\n                        # Image position\n                        if instance.image_position_patient:\n                            row[\"ImagePositionPatient_X\"] = (\n                                instance.image_position_patient[0]\n                            )\n                            row[\"ImagePositionPatient_Y\"] = (\n                                instance.image_position_patient[1]\n                            )\n                            row[\"ImagePositionPatient_Z\"] = (\n                                instance.image_position_patient[2]\n                            )\n                        else:\n                            row[\"ImagePositionPatient_X\"] = None\n                            row[\"ImagePositionPatient_Y\"] = None\n                            row[\"ImagePositionPatient_Z\"] = None\n\n                        # Image orientation\n                        row[\"ImageOrientationPatient\"] = (\n                            instance.image_orientation_patient\n                        )\n\n                        # Add parent-level metadata\n                        row[\"PatientsName\"] = patient.patients_name\n                        row[\"StudyDate\"] = study.study_date\n                        row[\"StudyDescription\"] = study.study_description\n                        row[\"SeriesNumber\"] = series.series_number\n                        row[\"SeriesDescription\"] = series.series_description\n                        row[\"Modality\"] = series.modality\n\n                        # Add instance-specific metadata\n                        for key, value in instance.metadata.items():\n                            if key not in row:\n                                row[key] = value\n\n                        rows.append(row)\n\n        return pd.DataFrame(rows)\n\n    # ========================================================================\n    # Export Methods\n    # ========================================================================\n\n    def export_csv(\n        self,\n        base_path: str,\n        levels: Optional[list[str]] = None,\n        include_instance_lists: bool = False,\n    ) -&gt; dict[str, str]:\n        \"\"\"Export DataFrames to separate CSV files.\n\n        Args:\n            base_path: Base path for output files (without extension).\n            levels: List of levels to export ('patients', 'studies', 'series',\n                   'instances'). Defaults to all levels.\n            include_instance_lists: Whether to include InstanceSOPUIDs and\n                InstanceFilePaths columns. Defaults to False to reduce file size.\n\n        Returns:\n            Dictionary mapping level names to created file paths.\n\n        Example:\n            Export database to CSV files:\n\n            ```python\n            files = db.export_csv(\n                base_path=\"output/dicom_db\",\n                levels=[\"patients\", \"studies\", \"series\"]\n            )\n            # Creates output/dicom_db_patients.csv, output/dicom_db_studies.csv, etc.\n            ```\n        \"\"\"\n        if levels is None:\n            levels = [\"patients\", \"studies\", \"series\", \"instances\"]\n\n        created_files = {}\n\n        for level in levels:\n            if level == \"patients\":\n                df = self.get_patients_df(include_instance_lists=include_instance_lists)\n            elif level == \"studies\":\n                df = self.get_studies_df(include_instance_lists=include_instance_lists)\n            elif level == \"series\":\n                df = self.get_series_df(include_instance_lists=include_instance_lists)\n            elif level == \"instances\":\n                df = self.get_instances_df()\n            else:\n                continue\n\n            file_path = f\"{base_path}_{level}.csv\"\n            # Convert list columns to JSON strings for CSV compatibility\n            for col in df.columns:\n                if df[col].apply(lambda x: isinstance(x, list)).any():\n                    df[col] = df[col].apply(\n                        lambda x: json.dumps(x) if isinstance(x, list) else x\n                    )\n            df.to_csv(file_path, index=False)\n            created_files[level] = file_path\n\n        return created_files\n\n    def export_json(\n        self,\n        json_path: str,\n        include_instance_lists: bool = True,\n    ) -&gt; str:\n        \"\"\"Export full hierarchy to JSON.\n\n        Args:\n            json_path: Path for the output JSON file.\n            include_instance_lists: Whether to include per-instance file paths\n                in the JSON output. Defaults to True for full export.\n\n        Returns:\n            Path to the created file.\n\n        Example:\n            Export full database hierarchy to JSON:\n\n            ```python\n            json_path = db.export_json(\"output/db.json\")\n            ```\n        \"\"\"\n        data: dict[str, list[Any]] = {\"patients\": []}\n\n        for patient in self.patients:\n            patient_dict: dict[str, Any] = {\n                \"patient_id\": patient.patient_id,\n                \"patients_name\": patient.patients_name,\n                \"patients_birth_date\": patient.patients_birth_date,\n                \"patients_sex\": patient.patients_sex,\n                \"common_metadata\": patient.common_metadata,\n                \"studies\": [],\n            }\n            if include_instance_lists:\n                patient_dict[\"instance_uids\"] = patient.get_instance_uids()\n                patient_dict[\"file_paths\"] = patient.get_file_paths()\n\n            for study in patient.studies:\n                study_dict: dict[str, Any] = {\n                    \"study_instance_uid\": study.study_instance_uid,\n                    \"study_date\": study.study_date,\n                    \"study_time\": study.study_time,\n                    \"study_description\": study.study_description,\n                    \"common_metadata\": study.common_metadata,\n                    \"series\": [],\n                }\n\n                for series in study.series:\n                    completeness = series.check_completeness(self.spacing_tolerance)\n                    series_dict: dict[str, Any] = {\n                        \"series_instance_uid\": series.series_instance_uid,\n                        \"series_number\": series.series_number,\n                        \"series_description\": series.series_description,\n                        \"modality\": series.modality,\n                        \"frame_of_reference_uid\": series.frame_of_reference_uid,\n                        \"common_metadata\": series.common_metadata,\n                        \"completeness\": completeness,\n                        \"instances\": [],\n                    }\n\n                    for instance in series.instances:\n                        instance_dict: dict[str, Any] = {\n                            \"sop_instance_uid\": instance.sop_instance_uid,\n                            \"instance_number\": instance.instance_number,\n                            \"image_position_patient\": instance.image_position_patient,\n                            \"slice_location\": instance.slice_location,\n                            \"projection_score\": instance.projection_score,\n                            \"metadata\": instance.metadata,\n                        }\n                        if include_instance_lists:\n                            instance_dict[\"file_path\"] = str(instance.file_path)\n                        series_dict[\"instances\"].append(instance_dict)\n\n                    study_dict[\"series\"].append(series_dict)\n\n                patient_dict[\"studies\"].append(study_dict)\n\n            data[\"patients\"].append(patient_dict)\n\n        with open(json_path, \"w\") as f:\n            json.dump(data, f, indent=2, default=str)\n\n        return json_path\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomDatabase.export_csv","title":"<code>export_csv(base_path, levels=None, include_instance_lists=False)</code>","text":"<p>Export DataFrames to separate CSV files.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>Base path for output files (without extension).</p> required <code>levels</code> <code>Optional[list[str]]</code> <p>List of levels to export ('patients', 'studies', 'series',    'instances'). Defaults to all levels.</p> <code>None</code> <code>include_instance_lists</code> <code>bool</code> <p>Whether to include InstanceSOPUIDs and InstanceFilePaths columns. Defaults to False to reduce file size.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary mapping level names to created file paths.</p> Example <p>Export database to CSV files:</p> <pre><code>files = db.export_csv(\n    base_path=\"output/dicom_db\",\n    levels=[\"patients\", \"studies\", \"series\"]\n)\n# Creates output/dicom_db_patients.csv, output/dicom_db_studies.csv, etc.\n</code></pre> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def export_csv(\n    self,\n    base_path: str,\n    levels: Optional[list[str]] = None,\n    include_instance_lists: bool = False,\n) -&gt; dict[str, str]:\n    \"\"\"Export DataFrames to separate CSV files.\n\n    Args:\n        base_path: Base path for output files (without extension).\n        levels: List of levels to export ('patients', 'studies', 'series',\n               'instances'). Defaults to all levels.\n        include_instance_lists: Whether to include InstanceSOPUIDs and\n            InstanceFilePaths columns. Defaults to False to reduce file size.\n\n    Returns:\n        Dictionary mapping level names to created file paths.\n\n    Example:\n        Export database to CSV files:\n\n        ```python\n        files = db.export_csv(\n            base_path=\"output/dicom_db\",\n            levels=[\"patients\", \"studies\", \"series\"]\n        )\n        # Creates output/dicom_db_patients.csv, output/dicom_db_studies.csv, etc.\n        ```\n    \"\"\"\n    if levels is None:\n        levels = [\"patients\", \"studies\", \"series\", \"instances\"]\n\n    created_files = {}\n\n    for level in levels:\n        if level == \"patients\":\n            df = self.get_patients_df(include_instance_lists=include_instance_lists)\n        elif level == \"studies\":\n            df = self.get_studies_df(include_instance_lists=include_instance_lists)\n        elif level == \"series\":\n            df = self.get_series_df(include_instance_lists=include_instance_lists)\n        elif level == \"instances\":\n            df = self.get_instances_df()\n        else:\n            continue\n\n        file_path = f\"{base_path}_{level}.csv\"\n        # Convert list columns to JSON strings for CSV compatibility\n        for col in df.columns:\n            if df[col].apply(lambda x: isinstance(x, list)).any():\n                df[col] = df[col].apply(\n                    lambda x: json.dumps(x) if isinstance(x, list) else x\n                )\n        df.to_csv(file_path, index=False)\n        created_files[level] = file_path\n\n    return created_files\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomDatabase.export_json","title":"<code>export_json(json_path, include_instance_lists=True)</code>","text":"<p>Export full hierarchy to JSON.</p> <p>Parameters:</p> Name Type Description Default <code>json_path</code> <code>str</code> <p>Path for the output JSON file.</p> required <code>include_instance_lists</code> <code>bool</code> <p>Whether to include per-instance file paths in the JSON output. Defaults to True for full export.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the created file.</p> Example <p>Export full database hierarchy to JSON:</p> <pre><code>json_path = db.export_json(\"output/db.json\")\n</code></pre> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def export_json(\n    self,\n    json_path: str,\n    include_instance_lists: bool = True,\n) -&gt; str:\n    \"\"\"Export full hierarchy to JSON.\n\n    Args:\n        json_path: Path for the output JSON file.\n        include_instance_lists: Whether to include per-instance file paths\n            in the JSON output. Defaults to True for full export.\n\n    Returns:\n        Path to the created file.\n\n    Example:\n        Export full database hierarchy to JSON:\n\n        ```python\n        json_path = db.export_json(\"output/db.json\")\n        ```\n    \"\"\"\n    data: dict[str, list[Any]] = {\"patients\": []}\n\n    for patient in self.patients:\n        patient_dict: dict[str, Any] = {\n            \"patient_id\": patient.patient_id,\n            \"patients_name\": patient.patients_name,\n            \"patients_birth_date\": patient.patients_birth_date,\n            \"patients_sex\": patient.patients_sex,\n            \"common_metadata\": patient.common_metadata,\n            \"studies\": [],\n        }\n        if include_instance_lists:\n            patient_dict[\"instance_uids\"] = patient.get_instance_uids()\n            patient_dict[\"file_paths\"] = patient.get_file_paths()\n\n        for study in patient.studies:\n            study_dict: dict[str, Any] = {\n                \"study_instance_uid\": study.study_instance_uid,\n                \"study_date\": study.study_date,\n                \"study_time\": study.study_time,\n                \"study_description\": study.study_description,\n                \"common_metadata\": study.common_metadata,\n                \"series\": [],\n            }\n\n            for series in study.series:\n                completeness = series.check_completeness(self.spacing_tolerance)\n                series_dict: dict[str, Any] = {\n                    \"series_instance_uid\": series.series_instance_uid,\n                    \"series_number\": series.series_number,\n                    \"series_description\": series.series_description,\n                    \"modality\": series.modality,\n                    \"frame_of_reference_uid\": series.frame_of_reference_uid,\n                    \"common_metadata\": series.common_metadata,\n                    \"completeness\": completeness,\n                    \"instances\": [],\n                }\n\n                for instance in series.instances:\n                    instance_dict: dict[str, Any] = {\n                        \"sop_instance_uid\": instance.sop_instance_uid,\n                        \"instance_number\": instance.instance_number,\n                        \"image_position_patient\": instance.image_position_patient,\n                        \"slice_location\": instance.slice_location,\n                        \"projection_score\": instance.projection_score,\n                        \"metadata\": instance.metadata,\n                    }\n                    if include_instance_lists:\n                        instance_dict[\"file_path\"] = str(instance.file_path)\n                    series_dict[\"instances\"].append(instance_dict)\n\n                study_dict[\"series\"].append(series_dict)\n\n            patient_dict[\"studies\"].append(study_dict)\n\n        data[\"patients\"].append(patient_dict)\n\n    with open(json_path, \"w\") as f:\n        json.dump(data, f, indent=2, default=str)\n\n    return json_path\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomDatabase.from_folders","title":"<code>from_folders(paths, recursive=True, spacing_tolerance=0.1, show_progress=True, extract_private_tags=True, num_workers=None, split_multiseries=True)</code>  <code>classmethod</code>","text":"<p>Build a database from folder paths.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str | Path]</code> <p>List of folder paths to scan.</p> required <code>recursive</code> <code>bool</code> <p>Whether to scan subdirectories.</p> <code>True</code> <code>spacing_tolerance</code> <code>float</code> <p>Tolerance for gap detection (default 10%).</p> <code>0.1</code> <code>show_progress</code> <code>bool</code> <p>Whether to display progress bars.</p> <code>True</code> <code>extract_private_tags</code> <code>bool</code> <p>Whether to extract vendor-specific private tags.</p> <code>True</code> <code>num_workers</code> <code>Optional[int]</code> <p>Number of parallel workers. None=auto (cpu_count-1),         1=sequential (no multiprocessing).</p> <code>None</code> <code>split_multiseries</code> <code>bool</code> <p>Whether to split multi-phase series (e.g. cardiac)               into separate series based on tags or spatial duplicates.</p> <code>True</code> <p>Returns:</p> Type Description <code>'DicomDatabase'</code> <p>DicomDatabase instance populated with all discovered DICOM files.</p> Example <p>Build database from multiple folders:</p> <pre><code>from pictologics.utilities.dicom_database import DicomDatabase\n\ndb = DicomDatabase.from_folders(\n    paths=[\"data/patient1\", \"data/patient2\"],\n    recursive=True,\n    num_workers=4\n)\nprint(f\"Found {len(db.patients)} patients\")\n</code></pre> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>@classmethod\ndef from_folders(\n    cls,\n    paths: list[str | Path],\n    recursive: bool = True,\n    spacing_tolerance: float = 0.1,\n    show_progress: bool = True,\n    extract_private_tags: bool = True,\n    num_workers: Optional[int] = None,\n    split_multiseries: bool = True,\n) -&gt; \"DicomDatabase\":\n    \"\"\"Build a database from folder paths.\n\n    Args:\n        paths: List of folder paths to scan.\n        recursive: Whether to scan subdirectories.\n        spacing_tolerance: Tolerance for gap detection (default 10%).\n        show_progress: Whether to display progress bars.\n        extract_private_tags: Whether to extract vendor-specific private tags.\n        num_workers: Number of parallel workers. None=auto (cpu_count-1),\n                    1=sequential (no multiprocessing).\n        split_multiseries: Whether to split multi-phase series (e.g. cardiac)\n                          into separate series based on tags or spatial duplicates.\n\n    Returns:\n        DicomDatabase instance populated with all discovered DICOM files.\n\n    Example:\n        Build database from multiple folders:\n\n        ```python\n        from pictologics.utilities.dicom_database import DicomDatabase\n\n        db = DicomDatabase.from_folders(\n            paths=[\"data/patient1\", \"data/patient2\"],\n            recursive=True,\n            num_workers=4\n        )\n        print(f\"Found {len(db.patients)} patients\")\n        ```\n    \"\"\"\n    # Convert paths to Path objects\n    path_objs = [Path(p) for p in paths]\n\n    # Determine number of workers\n    workers = _get_num_workers(num_workers)\n\n    # Step 1: Discover all DICOM files\n    dicom_files = _scan_dicom_files(path_objs, recursive, show_progress, workers)\n\n    if not dicom_files:\n        return cls(patients=[], spacing_tolerance=spacing_tolerance)\n\n    # Step 2: Extract metadata from each file (parallel if workers &gt; 1)\n    file_metadata = _extract_all_metadata(\n        dicom_files, show_progress, extract_private_tags, workers\n    )\n\n    # Step 3: Build hierarchy from flat metadata list\n    patients = _build_hierarchy(file_metadata, spacing_tolerance, split_multiseries)\n\n    # Step 4: Sort the hierarchy for consistent output\n    patients = _sort_hierarchy(patients)\n\n    return cls(patients=patients, spacing_tolerance=spacing_tolerance)\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomDatabase.get_instances_df","title":"<code>get_instances_df(patient_id=None, study_uid=None, series_uid=None)</code>","text":"<p>Export instance-level detail DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>patient_id</code> <code>Optional[str]</code> <p>Optional filter by patient ID.</p> <code>None</code> <code>study_uid</code> <code>Optional[str]</code> <p>Optional filter by study UID.</p> <code>None</code> <code>series_uid</code> <code>Optional[str]</code> <p>Optional filter by series UID.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with complete instance information.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def get_instances_df(\n    self,\n    patient_id: Optional[str] = None,\n    study_uid: Optional[str] = None,\n    series_uid: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Export instance-level detail DataFrame.\n\n    Args:\n        patient_id: Optional filter by patient ID.\n        study_uid: Optional filter by study UID.\n        series_uid: Optional filter by series UID.\n\n    Returns:\n        DataFrame with complete instance information.\n    \"\"\"\n    rows = []\n    for patient in self.patients:\n        if patient_id and patient.patient_id != patient_id:\n            continue\n\n        for study in patient.studies:\n            if study_uid and study.study_instance_uid != study_uid:\n                continue\n\n            for series in study.series:\n                if series_uid and series.series_instance_uid != series_uid:\n                    continue\n\n                for instance in series.instances:\n                    row: dict[str, Any] = {\n                        # Hierarchy IDs\n                        \"PatientID\": patient.patient_id,\n                        \"StudyInstanceUID\": study.study_instance_uid,\n                        \"SeriesInstanceUID\": series.series_instance_uid,\n                        \"SOPInstanceUID\": instance.sop_instance_uid,\n                        # Instance info\n                        \"FilePath\": str(instance.file_path),\n                        \"InstanceNumber\": instance.instance_number,\n                        \"SliceLocation\": instance.slice_location,\n                        \"ProjectionScore\": instance.projection_score,\n                        \"AcquisitionDateTime\": instance.acquisition_datetime,\n                    }\n\n                    # Image position\n                    if instance.image_position_patient:\n                        row[\"ImagePositionPatient_X\"] = (\n                            instance.image_position_patient[0]\n                        )\n                        row[\"ImagePositionPatient_Y\"] = (\n                            instance.image_position_patient[1]\n                        )\n                        row[\"ImagePositionPatient_Z\"] = (\n                            instance.image_position_patient[2]\n                        )\n                    else:\n                        row[\"ImagePositionPatient_X\"] = None\n                        row[\"ImagePositionPatient_Y\"] = None\n                        row[\"ImagePositionPatient_Z\"] = None\n\n                    # Image orientation\n                    row[\"ImageOrientationPatient\"] = (\n                        instance.image_orientation_patient\n                    )\n\n                    # Add parent-level metadata\n                    row[\"PatientsName\"] = patient.patients_name\n                    row[\"StudyDate\"] = study.study_date\n                    row[\"StudyDescription\"] = study.study_description\n                    row[\"SeriesNumber\"] = series.series_number\n                    row[\"SeriesDescription\"] = series.series_description\n                    row[\"Modality\"] = series.modality\n\n                    # Add instance-specific metadata\n                    for key, value in instance.metadata.items():\n                        if key not in row:\n                            row[key] = value\n\n                    rows.append(row)\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomDatabase.get_patients_df","title":"<code>get_patients_df(include_instance_lists=False)</code>","text":"<p>Export patient-level summary DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>include_instance_lists</code> <code>bool</code> <p>Whether to include InstanceSOPUIDs and InstanceFilePaths columns. Defaults to False to reduce memory.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with patient information and aggregated statistics.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def get_patients_df(self, include_instance_lists: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Export patient-level summary DataFrame.\n\n    Args:\n        include_instance_lists: Whether to include InstanceSOPUIDs and\n            InstanceFilePaths columns. Defaults to False to reduce memory.\n\n    Returns:\n        DataFrame with patient information and aggregated statistics.\n    \"\"\"\n    rows = []\n    for patient in self.patients:\n        row: Dict[str, Any] = {\n            \"PatientID\": patient.patient_id,\n            \"PatientsName\": patient.patients_name,\n            \"PatientsBirthDate\": patient.patients_birth_date,\n            \"PatientsSex\": patient.patients_sex,\n            \"NumStudies\": len(patient.studies),\n            \"NumSeries\": sum(len(study.series) for study in patient.studies),\n            \"NumInstances\": sum(\n                len(series.instances)\n                for study in patient.studies\n                for series in study.series\n            ),\n        }\n        if include_instance_lists:\n            row[\"InstanceSOPUIDs\"] = patient.get_instance_uids()\n            row[\"InstanceFilePaths\"] = patient.get_file_paths()\n\n        # Add study date range\n        study_dates = [\n            study.study_date for study in patient.studies if study.study_date\n        ]\n        if study_dates:\n            row[\"EarliestStudyDate\"] = min(study_dates)\n            row[\"LatestStudyDate\"] = max(study_dates)\n        else:\n            row[\"EarliestStudyDate\"] = None\n            row[\"LatestStudyDate\"] = None\n\n        # Add common metadata from patient level\n        for key, value in patient.common_metadata.items():\n            if key not in row:\n                row[key] = value\n\n        rows.append(row)\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomDatabase.get_series_df","title":"<code>get_series_df(patient_id=None, study_uid=None, include_instance_lists=False)</code>","text":"<p>Export series-level summary DataFrame with completeness info.</p> <p>Parameters:</p> Name Type Description Default <code>patient_id</code> <code>Optional[str]</code> <p>Optional filter by patient ID.</p> <code>None</code> <code>study_uid</code> <code>Optional[str]</code> <p>Optional filter by study UID.</p> <code>None</code> <code>include_instance_lists</code> <code>bool</code> <p>Whether to include InstanceSOPUIDs and InstanceFilePaths columns. Defaults to False to reduce memory.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with series information including completeness validation.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def get_series_df(\n    self,\n    patient_id: Optional[str] = None,\n    study_uid: Optional[str] = None,\n    include_instance_lists: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Export series-level summary DataFrame with completeness info.\n\n    Args:\n        patient_id: Optional filter by patient ID.\n        study_uid: Optional filter by study UID.\n        include_instance_lists: Whether to include InstanceSOPUIDs and\n            InstanceFilePaths columns. Defaults to False to reduce memory.\n\n    Returns:\n        DataFrame with series information including completeness validation.\n    \"\"\"\n    rows = []\n    for patient in self.patients:\n        if patient_id and patient.patient_id != patient_id:\n            continue\n\n        for study in patient.studies:\n            if study_uid and study.study_instance_uid != study_uid:\n                continue\n\n            for series in study.series:\n                completeness = series.check_completeness(self.spacing_tolerance)\n\n                row = {\n                    # Patient info\n                    \"PatientID\": patient.patient_id,\n                    \"PatientsName\": patient.patients_name,\n                    # Study info\n                    \"StudyInstanceUID\": study.study_instance_uid,\n                    \"StudyDate\": study.study_date,\n                    \"StudyDescription\": study.study_description,\n                    # Series info\n                    \"SeriesInstanceUID\": series.series_instance_uid,\n                    \"SeriesNumber\": series.series_number,\n                    \"SeriesDescription\": series.series_description,\n                    \"Modality\": series.modality,\n                    \"FrameOfReferenceUID\": series.frame_of_reference_uid,\n                    # Completeness\n                    \"NumInstances\": completeness[\"total_instances\"],\n                    \"ExpectedInstances\": completeness[\"expected_instances\"],\n                    \"IsComplete\": completeness[\"is_complete\"],\n                    \"HasGaps\": completeness[\"has_gaps\"],\n                    \"GapIndices\": completeness[\"gap_indices\"],\n                    \"SpacingMM\": completeness[\"spacing_mm\"],\n                    \"SpacingUniform\": completeness[\"spacing_uniform\"],\n                    \"FirstSlicePosition\": completeness[\"first_slice_position\"],\n                    \"LastSlicePosition\": completeness[\"last_slice_position\"],\n                    \"CompletenessWarnings\": completeness[\"warnings\"],\n                }\n                if include_instance_lists:\n                    row[\"InstanceSOPUIDs\"] = series.get_instance_uids()\n                    row[\"InstanceFilePaths\"] = series.get_file_paths()\n\n                # Add common metadata from all levels\n                for key, value in patient.common_metadata.items():\n                    if key not in row:\n                        row[key] = value\n                for key, value in study.common_metadata.items():\n                    if key not in row:\n                        row[key] = value\n                for key, value in series.common_metadata.items():\n                    if key not in row:\n                        row[key] = value\n\n                rows.append(row)\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_database.DicomDatabase.get_studies_df","title":"<code>get_studies_df(patient_id=None, include_instance_lists=False)</code>","text":"<p>Export study-level summary DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>patient_id</code> <code>Optional[str]</code> <p>Optional filter by patient ID.</p> <code>None</code> <code>include_instance_lists</code> <code>bool</code> <p>Whether to include InstanceSOPUIDs and InstanceFilePaths columns. Defaults to False to reduce memory.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with study information.</p> Source code in <code>pictologics/utilities/dicom_database.py</code> <pre><code>def get_studies_df(\n    self,\n    patient_id: Optional[str] = None,\n    include_instance_lists: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Export study-level summary DataFrame.\n\n    Args:\n        patient_id: Optional filter by patient ID.\n        include_instance_lists: Whether to include InstanceSOPUIDs and\n            InstanceFilePaths columns. Defaults to False to reduce memory.\n\n    Returns:\n        DataFrame with study information.\n    \"\"\"\n    rows = []\n    for patient in self.patients:\n        if patient_id and patient.patient_id != patient_id:\n            continue\n\n        for study in patient.studies:\n            row: Dict[str, Any] = {\n                # Patient info\n                \"PatientID\": patient.patient_id,\n                \"PatientsName\": patient.patients_name,\n                \"PatientsBirthDate\": patient.patients_birth_date,\n                \"PatientsSex\": patient.patients_sex,\n                # Study info\n                \"StudyInstanceUID\": study.study_instance_uid,\n                \"StudyDate\": study.study_date,\n                \"StudyTime\": study.study_time,\n                \"StudyDescription\": study.study_description,\n                \"NumSeries\": len(study.series),\n                \"NumInstances\": sum(\n                    len(series.instances) for series in study.series\n                ),\n            }\n            if include_instance_lists:\n                row[\"InstanceSOPUIDs\"] = study.get_instance_uids()\n                row[\"InstanceFilePaths\"] = study.get_file_paths()\n\n            # Collect modalities present\n            modalities = list(set(s.modality for s in study.series if s.modality))\n            row[\"ModalitiesPresent\"] = modalities\n\n            # Add common metadata\n            for key, value in patient.common_metadata.items():\n                if key not in row:\n                    row[key] = value\n            for key, value in study.common_metadata.items():\n                if key not in row:\n                    row[key] = value\n\n            rows.append(row)\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_utils","title":"<code>pictologics.utilities.dicom_utils</code>","text":"<p>DICOM Utility Functions.</p> <p>This module provides shared utility functions for working with DICOM files, including multi-phase series detection and splitting logic used by both the DicomDatabase and the image loader.</p>"},{"location":"api/utilities/#pictologics.utilities.dicom_utils.DicomPhaseInfo","title":"<code>DicomPhaseInfo</code>  <code>dataclass</code>","text":"<p>Information about a detected phase in a DICOM series.</p> <p>Attributes:</p> Name Type Description <code>index</code> <code>int</code> <p>Zero-based index of this phase.</p> <code>num_slices</code> <code>int</code> <p>Number of slices/instances in this phase.</p> <code>file_paths</code> <code>list[Path]</code> <p>List of file paths belonging to this phase.</p> <code>label</code> <code>Optional[str]</code> <p>Human-readable label (e.g., \"Phase 0%\", \"Echo 1\").</p> <code>split_tag</code> <code>Optional[str]</code> <p>The DICOM tag used to detect this phase, or \"spatial\" if detected via duplicate positions.</p> <code>split_value</code> <code>Optional[Any]</code> <p>The value of the split tag for this phase.</p> Source code in <code>pictologics/utilities/dicom_utils.py</code> <pre><code>@dataclass\nclass DicomPhaseInfo:\n    \"\"\"Information about a detected phase in a DICOM series.\n\n    Attributes:\n        index: Zero-based index of this phase.\n        num_slices: Number of slices/instances in this phase.\n        file_paths: List of file paths belonging to this phase.\n        label: Human-readable label (e.g., \"Phase 0%\", \"Echo 1\").\n        split_tag: The DICOM tag used to detect this phase, or \"spatial\"\n            if detected via duplicate positions.\n        split_value: The value of the split tag for this phase.\n    \"\"\"\n\n    index: int\n    num_slices: int\n    file_paths: list[Path]\n    label: Optional[str] = None\n    split_tag: Optional[str] = None\n    split_value: Optional[Any] = None\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_utils.get_dicom_phases","title":"<code>get_dicom_phases(path, recursive=False)</code>","text":"<p>Discover phases in a DICOM series directory.</p> <p>Scans a directory for DICOM files and detects if the series contains multiple phases (e.g., cardiac phases, temporal positions, echo numbers). This is useful before calling <code>load_image()</code> with a specific <code>dataset_index</code>.</p> <p>Multi-phase detection uses the same logic as :class:<code>DicomDatabase</code> to ensure consistent behavior across the library.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to directory containing DICOM files.</p> required <code>recursive</code> <code>bool</code> <p>If True, recursively searches subdirectories. Default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[DicomPhaseInfo]</code> <p>List of :class:<code>DicomPhaseInfo</code> objects describing each detected phase.</p> <code>list[DicomPhaseInfo]</code> <p>For single-phase series, returns a list with one element.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the path does not exist.</p> <code>ValueError</code> <p>If no DICOM files are found.</p> Example <p>Discover phases before loading:</p> <pre><code>from pictologics.utilities import get_dicom_phases\nfrom pictologics import load_image\n\n# Discover phases in a cardiac CT directory\nphases = get_dicom_phases(\"cardiac_ct/\")\nprint(f\"Found {len(phases)} phases:\")\nfor phase in phases:\n    print(f\"  Phase {phase.index}: {phase.num_slices} slices - {phase.label}\")\n\n# Load the 5th phase (40%)\nimg = load_image(\"cardiac_ct/\", dataset_index=4)\n\n# Check if series is multi-phase\nif len(phases) &gt; 1:\n    print(\"Multi-phase series detected!\")\nelse:\n    print(\"Single-phase series\")\n</code></pre> See Also <ul> <li>:func:<code>load_image</code>: Main image loading function with <code>dataset_index</code> support.</li> <li>:class:<code>DicomDatabase</code>: Full DICOM database parsing with automatic phase splitting.</li> </ul> Source code in <code>pictologics/utilities/dicom_utils.py</code> <pre><code>def get_dicom_phases(\n    path: str,\n    recursive: bool = False,\n) -&gt; list[DicomPhaseInfo]:\n    \"\"\"Discover phases in a DICOM series directory.\n\n    Scans a directory for DICOM files and detects if the series contains\n    multiple phases (e.g., cardiac phases, temporal positions, echo numbers).\n    This is useful before calling ``load_image()`` with a specific ``dataset_index``.\n\n    Multi-phase detection uses the same logic as :class:`DicomDatabase` to ensure\n    consistent behavior across the library.\n\n    Args:\n        path: Path to directory containing DICOM files.\n        recursive: If True, recursively searches subdirectories. Default False.\n\n    Returns:\n        List of :class:`DicomPhaseInfo` objects describing each detected phase.\n        For single-phase series, returns a list with one element.\n\n    Raises:\n        FileNotFoundError: If the path does not exist.\n        ValueError: If no DICOM files are found.\n\n    Example:\n        Discover phases before loading:\n\n        ```python\n        from pictologics.utilities import get_dicom_phases\n        from pictologics import load_image\n\n        # Discover phases in a cardiac CT directory\n        phases = get_dicom_phases(\"cardiac_ct/\")\n        print(f\"Found {len(phases)} phases:\")\n        for phase in phases:\n            print(f\"  Phase {phase.index}: {phase.num_slices} slices - {phase.label}\")\n\n        # Load the 5th phase (40%)\n        img = load_image(\"cardiac_ct/\", dataset_index=4)\n\n        # Check if series is multi-phase\n        if len(phases) &gt; 1:\n            print(\"Multi-phase series detected!\")\n        else:\n            print(\"Single-phase series\")\n        ```\n\n    See Also:\n        - :func:`load_image`: Main image loading function with ``dataset_index`` support.\n        - :class:`DicomDatabase`: Full DICOM database parsing with automatic phase splitting.\n    \"\"\"\n    path_obj = Path(path)\n    if not path_obj.exists():\n        raise FileNotFoundError(f\"Path does not exist: {path}\")\n\n    # Collect DICOM files\n    if path_obj.is_file():\n        if pydicom.misc.is_dicom(path_obj):\n            dicom_files = [path_obj]\n        else:\n            raise ValueError(f\"File is not a DICOM file: {path}\")\n    else:\n        if recursive:\n            candidates = list(path_obj.rglob(\"*\"))\n        else:\n            candidates = list(path_obj.iterdir())\n\n        dicom_files = [\n            f for f in candidates if f.is_file() and pydicom.misc.is_dicom(f)\n        ]\n\n    if not dicom_files:\n        raise ValueError(f\"No DICOM files found in: {path}\")\n\n    # Extract metadata for phase detection\n    file_metadata: list[dict[str, Any]] = []\n    for f in dicom_files:\n        try:\n            dcm = pydicom.dcmread(f, stop_before_pixels=True)\n            meta: dict[str, Any] = {\n                \"file_path\": f,\n                \"InstanceNumber\": getattr(dcm, \"InstanceNumber\", None),\n            }\n\n            # Extract position\n            try:\n                ipp = dcm.ImagePositionPatient\n                meta[\"ImagePositionPatient\"] = (\n                    float(ipp[0]),\n                    float(ipp[1]),\n                    float(ipp[2]),\n                )\n            except (AttributeError, IndexError, TypeError):\n                meta[\"ImagePositionPatient\"] = None\n\n            # Extract multi-phase tags\n            for tag in MULTI_PHASE_TAGS:\n                val = getattr(dcm, tag, None)\n                if val is not None:\n                    meta[tag] = val\n\n            file_metadata.append(meta)\n        except Exception:\n            continue\n\n    if not file_metadata:\n        raise ValueError(f\"Could not read any DICOM files from: {path}\")\n\n    # Split into phases\n    phases = split_dicom_phases(file_metadata)\n\n    # Determine which tag was used for splitting\n    split_tag = None\n    if len(phases) &gt; 1:\n        # Check which tag has different values across phases\n        for tag in MULTI_PHASE_TAGS:\n            first_val = phases[0][0].get(tag) if phases[0] else None\n            if first_val is not None:\n                # Check if other phases have different values\n                for phase in phases[1:]:\n                    if phase and phase[0].get(tag) != first_val:\n                        split_tag = tag\n                        break\n            if split_tag:\n                break\n        if not split_tag:\n            split_tag = \"spatial\"  # Fallback was used\n\n    # Build DicomPhaseInfo objects\n    result: list[DicomPhaseInfo] = []\n    for i, phase_meta in enumerate(phases):\n        file_paths = [m[\"file_path\"] for m in phase_meta]\n        split_value = phase_meta[0].get(split_tag) if split_tag and phase_meta else None\n\n        # Generate label\n        if split_tag == \"NominalPercentageOfCardiacPhase\":\n            label = f\"Phase {split_value}%\" if split_value is not None else f\"Phase {i}\"\n        elif split_tag == \"TemporalPositionIdentifier\":\n            label = (\n                f\"Temporal {split_value}\" if split_value is not None else f\"Time {i}\"\n            )\n        elif split_tag == \"EchoNumber\":\n            label = f\"Echo {split_value}\" if split_value is not None else f\"Echo {i}\"\n        elif split_tag == \"AcquisitionNumber\":\n            label = (\n                f\"Acquisition {split_value}\" if split_value is not None else f\"Acq {i}\"\n            )\n        elif split_tag == \"TriggerTime\":\n            label = (\n                f\"Trigger {split_value}ms\"\n                if split_value is not None\n                else f\"Trigger {i}\"\n            )\n        elif split_tag == \"spatial\":\n            label = f\"Volume {i + 1}\"\n        else:\n            label = f\"Dataset {i}\"\n\n        result.append(\n            DicomPhaseInfo(\n                index=i,\n                num_slices=len(file_paths),\n                file_paths=file_paths,\n                label=label,\n                split_tag=split_tag,\n                split_value=split_value,\n            )\n        )\n\n    return result\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.dicom_utils.split_dicom_phases","title":"<code>split_dicom_phases(file_metadata)</code>","text":"<p>Split DICOM file metadata into multiple phases/groups.</p> <p>This function detects multi-phase DICOM series (e.g., cardiac phases, multi-echo, dynamic contrast) and splits them into separate groups.</p> <p>The detection strategy is: 1. Check for distinctive DICOM tags (CardiacPhase, TemporalPosition, etc.)    If a tag has &gt;1 unique value, use it to group files. 2. Fallback: Check for duplicate spatial positions (ImagePositionPatient).    If duplicates exist, group by order of appearance.</p> <p>Parameters:</p> Name Type Description Default <code>file_metadata</code> <code>list[dict[str, Any]]</code> <p>List of dictionaries containing at minimum: - 'file_path': Path to the DICOM file - 'ImagePositionPatient': Optional tuple of (x, y, z) - Any of the MULTI_PHASE_TAGS (optional)</p> required <p>Returns:</p> Type Description <code>list[list[dict[str, Any]]]</code> <p>List of lists, where each inner list contains metadata dicts</p> <code>list[list[dict[str, Any]]]</code> <p>for one phase. Single-phase series return [[all_metadata]].</p> Example <p>Split DICOM metadata into separate phases:</p> <pre><code>from pictologics.utilities.dicom_utils import split_dicom_phases\nfrom pathlib import Path\n\n# Assume metadata list already collected\nmetadata = [\n    {'file_path': Path('slice1.dcm'), 'CardiacPhase': 0},\n    {'file_path': Path('slice2.dcm'), 'CardiacPhase': 10},\n    # ... more files\n]\nphases = split_dicom_phases(metadata)\nprint(f\"Found {len(phases)} phases\")\n</code></pre> Source code in <code>pictologics/utilities/dicom_utils.py</code> <pre><code>def split_dicom_phases(\n    file_metadata: list[dict[str, Any]],\n) -&gt; list[list[dict[str, Any]]]:\n    \"\"\"Split DICOM file metadata into multiple phases/groups.\n\n    This function detects multi-phase DICOM series (e.g., cardiac phases,\n    multi-echo, dynamic contrast) and splits them into separate groups.\n\n    The detection strategy is:\n    1. Check for distinctive DICOM tags (CardiacPhase, TemporalPosition, etc.)\n       If a tag has &gt;1 unique value, use it to group files.\n    2. Fallback: Check for duplicate spatial positions (ImagePositionPatient).\n       If duplicates exist, group by order of appearance.\n\n    Args:\n        file_metadata: List of dictionaries containing at minimum:\n            - 'file_path': Path to the DICOM file\n            - 'ImagePositionPatient': Optional tuple of (x, y, z)\n            - Any of the MULTI_PHASE_TAGS (optional)\n\n    Returns:\n        List of lists, where each inner list contains metadata dicts\n        for one phase. Single-phase series return [[all_metadata]].\n\n    Example:\n        Split DICOM metadata into separate phases:\n\n        ```python\n        from pictologics.utilities.dicom_utils import split_dicom_phases\n        from pathlib import Path\n\n        # Assume metadata list already collected\n        metadata = [\n            {'file_path': Path('slice1.dcm'), 'CardiacPhase': 0},\n            {'file_path': Path('slice2.dcm'), 'CardiacPhase': 10},\n            # ... more files\n        ]\n        phases = split_dicom_phases(metadata)\n        print(f\"Found {len(phases)} phases\")\n        ```\n    \"\"\"\n    if len(file_metadata) &lt; 2:\n        return [file_metadata]\n\n    # 1. Try splitting by multi-phase tags\n    for tag in MULTI_PHASE_TAGS:\n        values: dict[Any, list[dict[str, Any]]] = {}\n        for meta in file_metadata:\n            val = meta.get(tag)\n            if val is not None:\n                values.setdefault(val, []).append(meta)\n\n        # If we have multiple groups and covered all files\n        if len(values) &gt; 1:\n            total_grouped = sum(len(g) for g in values.values())\n            if total_grouped == len(file_metadata):\n                # Sort groups by tag value\n                sorted_keys = sorted(values.keys())\n                return [values[k] for k in sorted_keys]\n\n    # 2. Fallback: Spatial duplication check\n    pos_map: dict[tuple[float, float, float], list[dict[str, Any]]] = {}\n    for meta in file_metadata:\n        pos = meta.get(\"ImagePositionPatient\")\n        if pos:\n            pos_tuple = tuple(pos) if isinstance(pos, (list, tuple)) else pos\n            pos_map.setdefault(pos_tuple, []).append(meta)\n\n    # Check if we have duplicates (any position has &gt;1 instance)\n    if any(len(g) &gt; 1 for g in pos_map.values()):\n        num_phases = max(len(g) for g in pos_map.values())\n        phase_groups: list[list[dict[str, Any]]] = [[] for _ in range(num_phases)]\n\n        # Sort by instance number for consistency\n        sorted_metadata = sorted(\n            file_metadata,\n            key=lambda x: x.get(\"InstanceNumber\", 0) or 0,\n        )\n\n        # Re-map with sorted metadata\n        pos_map_sorted: dict[tuple[float, float, float], list[dict[str, Any]]] = {}\n        for meta in sorted_metadata:\n            pos = meta.get(\"ImagePositionPatient\")\n            if pos:\n                pos_tuple = tuple(pos) if isinstance(pos, (list, tuple)) else pos\n                pos_map_sorted.setdefault(pos_tuple, []).append(meta)\n            else:\n                phase_groups[0].append(meta)\n\n        # Distribute duplicates across phases\n        for _, metas in pos_map_sorted.items():\n            for i, meta in enumerate(metas):\n                if i &lt; num_phases:\n                    phase_groups[i].append(meta)\n                else:\n                    phase_groups[-1].append(meta)  # pragma: no cover\n\n        # Filter empty groups\n        return [g for g in phase_groups if g]\n\n    return [file_metadata]\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser","title":"<code>pictologics.utilities.sr_parser</code>","text":""},{"location":"api/utilities/#pictologics.utilities.sr_parser--dicom-structured-report-sr-parser","title":"DICOM Structured Report (SR) Parser","text":"<p>This module provides functionality for parsing DICOM Structured Reports (SR) and extracting measurement data into structured formats (DataFrames, CSV, JSON).</p> <p>Supports TID1500 (Measurement Report) and other common SR templates. Uses highdicom for robust SR parsing and content extraction.</p>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRMeasurement","title":"<code>SRMeasurement</code>  <code>dataclass</code>","text":"<p>Represents a single measurement from an SR document.</p> <p>This dataclass captures individual measurement values extracted from DICOM Structured Reports, including the measurement name, value, units, and associated context.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Measurement concept name (e.g., \"Agatston Score\", \"Volume\").</p> <code>value</code> <code>float</code> <p>Numerical measurement value.</p> <code>unit</code> <code>str</code> <p>Unit of measurement (e.g., \"mm\", \"HU\", \"1\" for unitless).</p> <code>finding_type</code> <code>Optional[str]</code> <p>Type of finding this measurement relates to (optional).</p> <code>finding_site</code> <code>Optional[str]</code> <p>Anatomical site of finding (optional).</p> <code>derivation</code> <code>Optional[str]</code> <p>How the measurement was derived (optional).</p> <code>tracking_id</code> <code>Optional[str]</code> <p>Optional tracking identifier for longitudinal studies.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional extracted attributes not captured above.</p> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>@dataclass\nclass SRMeasurement:\n    \"\"\"Represents a single measurement from an SR document.\n\n    This dataclass captures individual measurement values extracted from\n    DICOM Structured Reports, including the measurement name, value,\n    units, and associated context.\n\n    Attributes:\n        name: Measurement concept name (e.g., \"Agatston Score\", \"Volume\").\n        value: Numerical measurement value.\n        unit: Unit of measurement (e.g., \"mm\", \"HU\", \"1\" for unitless).\n        finding_type: Type of finding this measurement relates to (optional).\n        finding_site: Anatomical site of finding (optional).\n        derivation: How the measurement was derived (optional).\n        tracking_id: Optional tracking identifier for longitudinal studies.\n        metadata: Additional extracted attributes not captured above.\n    \"\"\"\n\n    name: str\n    value: float\n    unit: str\n    finding_type: Optional[str] = None\n    finding_site: Optional[str] = None\n    derivation: Optional[str] = None\n    tracking_id: Optional[str] = None\n    metadata: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRMeasurementGroup","title":"<code>SRMeasurementGroup</code>  <code>dataclass</code>","text":"<p>Represents a group of related measurements.</p> <p>SR documents often organize measurements into groups based on anatomical site, finding type, or other criteria. This dataclass captures such groupings.</p> <p>Attributes:</p> Name Type Description <code>group_id</code> <code>Optional[str]</code> <p>Identifier for this measurement group (optional).</p> <code>finding_type</code> <code>Optional[str]</code> <p>Type of finding for this group (optional).</p> <code>finding_site</code> <code>Optional[str]</code> <p>Anatomical site for this group (optional).</p> <code>measurements</code> <code>list[SRMeasurement]</code> <p>List of SRMeasurement objects in this group.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional group-level attributes.</p> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>@dataclass\nclass SRMeasurementGroup:\n    \"\"\"Represents a group of related measurements.\n\n    SR documents often organize measurements into groups based on\n    anatomical site, finding type, or other criteria. This dataclass\n    captures such groupings.\n\n    Attributes:\n        group_id: Identifier for this measurement group (optional).\n        finding_type: Type of finding for this group (optional).\n        finding_site: Anatomical site for this group (optional).\n        measurements: List of SRMeasurement objects in this group.\n        metadata: Additional group-level attributes.\n    \"\"\"\n\n    group_id: Optional[str] = None\n    finding_type: Optional[str] = None\n    finding_site: Optional[str] = None\n    measurements: list[SRMeasurement] = field(default_factory=list)\n    metadata: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRDocument","title":"<code>SRDocument</code>  <code>dataclass</code>","text":"<p>Represents a parsed DICOM Structured Report.</p> <p>This class provides the main interface for accessing SR content, following the same pattern as DicomDatabase. It can be constructed from a file using the <code>from_file()</code> class method.</p> <p>Attributes:</p> Name Type Description <code>file_path</code> <code>Path</code> <p>Path to the source SR file.</p> <code>sop_instance_uid</code> <code>str</code> <p>Unique identifier for this SR instance.</p> <code>template_id</code> <code>Optional[str]</code> <p>SR template identifier (e.g., \"1500\" for TID1500).</p> <code>document_title</code> <code>Optional[str]</code> <p>Title of the SR document.</p> <code>measurement_groups</code> <code>list[SRMeasurementGroup]</code> <p>List of SRMeasurementGroup objects.</p> <code>patient_id</code> <code>Optional[str]</code> <p>Patient identifier.</p> <code>study_instance_uid</code> <code>Optional[str]</code> <p>Study UID.</p> <code>series_instance_uid</code> <code>Optional[str]</code> <p>Series UID.</p> <code>content_datetime</code> <code>Optional[str]</code> <p>When the SR was created.</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional document-level attributes.</p> Example <p>Load and parse an SR document:</p> <pre><code>from pictologics.utilities.sr_parser import SRDocument\n\nsr = SRDocument.from_file(\"measurements.dcm\")\nprint(f\"Template: {sr.template_id}\")\nprint(f\"Groups: {len(sr.measurement_groups)}\")\n\n# Export measurements to DataFrame\ndf = sr.get_measurements_df()\nprint(df[[\"measurement_name\", \"value\", \"unit\"]])\n\n# Export to CSV\nsr.export_csv(\"measurements.csv\")\n</code></pre> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>@dataclass\nclass SRDocument:\n    \"\"\"Represents a parsed DICOM Structured Report.\n\n    This class provides the main interface for accessing SR content,\n    following the same pattern as DicomDatabase. It can be constructed\n    from a file using the `from_file()` class method.\n\n    Attributes:\n        file_path: Path to the source SR file.\n        sop_instance_uid: Unique identifier for this SR instance.\n        template_id: SR template identifier (e.g., \"1500\" for TID1500).\n        document_title: Title of the SR document.\n        measurement_groups: List of SRMeasurementGroup objects.\n        patient_id: Patient identifier.\n        study_instance_uid: Study UID.\n        series_instance_uid: Series UID.\n        content_datetime: When the SR was created.\n        metadata: Additional document-level attributes.\n\n    Example:\n        Load and parse an SR document:\n\n        ```python\n        from pictologics.utilities.sr_parser import SRDocument\n\n        sr = SRDocument.from_file(\"measurements.dcm\")\n        print(f\"Template: {sr.template_id}\")\n        print(f\"Groups: {len(sr.measurement_groups)}\")\n\n        # Export measurements to DataFrame\n        df = sr.get_measurements_df()\n        print(df[[\"measurement_name\", \"value\", \"unit\"]])\n\n        # Export to CSV\n        sr.export_csv(\"measurements.csv\")\n        ```\n    \"\"\"\n\n    file_path: Path\n    sop_instance_uid: str\n    template_id: Optional[str] = None\n    document_title: Optional[str] = None\n    measurement_groups: list[SRMeasurementGroup] = field(default_factory=list)\n    patient_id: Optional[str] = None\n    study_instance_uid: Optional[str] = None\n    series_instance_uid: Optional[str] = None\n    content_datetime: Optional[str] = None\n    metadata: dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_file(\n        cls,\n        path: str | Path,\n        extract_private_tags: bool = False,\n    ) -&gt; \"SRDocument\":\n        \"\"\"Load and parse an SR document from file.\n\n        This method reads a DICOM Structured Report file and extracts\n        all measurement content into the hierarchical dataclass structure.\n        Follows the same pattern as DicomDatabase.from_folders().\n\n        Args:\n            path: Path to DICOM SR file.\n            extract_private_tags: Whether to extract vendor-specific tags\n                into the metadata dictionaries. Defaults to False.\n\n        Returns:\n            SRDocument instance with parsed content.\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n            ValueError: If the file is not a valid DICOM SR object.\n        \"\"\"\n        import pydicom\n\n        path_obj = Path(path)\n        if not path_obj.exists():\n            raise FileNotFoundError(f\"SR file not found: {path}\")\n\n        # Load the DICOM file\n        try:\n            dcm = pydicom.dcmread(str(path_obj))\n        except Exception as e:\n            raise ValueError(f\"Failed to read DICOM file: {e}\") from e\n\n        # Check if it's an SR document\n        sr_sop_classes = [\n            \"1.2.840.10008.5.1.4.1.1.88.11\",  # Basic Text SR\n            \"1.2.840.10008.5.1.4.1.1.88.22\",  # Enhanced SR\n            \"1.2.840.10008.5.1.4.1.1.88.33\",  # Comprehensive SR\n            \"1.2.840.10008.5.1.4.1.1.88.34\",  # Comprehensive 3D SR\n            \"1.2.840.10008.5.1.4.1.1.88.35\",  # Extensible SR\n            \"1.2.840.10008.5.1.4.1.1.88.40\",  # Procedure Log\n        ]\n        sop_class = str(getattr(dcm, \"SOPClassUID\", \"\"))\n        if sop_class not in sr_sop_classes:\n            raise ValueError(\n                f\"File is not a DICOM SR document. SOPClassUID: {sop_class}\"\n            )\n\n        # Extract basic document info\n        sop_instance_uid = str(getattr(dcm, \"SOPInstanceUID\", \"\"))\n        patient_id = str(getattr(dcm, \"PatientID\", \"\")) or None\n        study_uid = str(getattr(dcm, \"StudyInstanceUID\", \"\")) or None\n        series_uid = str(getattr(dcm, \"SeriesInstanceUID\", \"\")) or None\n\n        # Content datetime\n        content_date = getattr(dcm, \"ContentDate\", None)\n        content_time = getattr(dcm, \"ContentTime\", None)\n        if content_date:\n            content_datetime = str(content_date)\n            if content_time:\n                content_datetime += f\"T{content_time}\"\n        else:\n            content_datetime = None\n\n        # Extract document title from ConceptNameCodeSequence\n        doc_title = None\n        if hasattr(dcm, \"ConceptNameCodeSequence\") and dcm.ConceptNameCodeSequence:\n            concept = dcm.ConceptNameCodeSequence[0]\n            doc_title = str(getattr(concept, \"CodeMeaning\", \"\")) or None\n\n        # Extract template ID if present\n        template_id = None\n        if hasattr(dcm, \"ContentTemplateSequence\") and dcm.ContentTemplateSequence:\n            template = dcm.ContentTemplateSequence[0]\n            template_id = str(getattr(template, \"TemplateIdentifier\", \"\")) or None\n\n        # Parse content sequence for measurements\n        measurement_groups = _parse_content_sequence(dcm, extract_private_tags)\n\n        # Build metadata dict\n        metadata: dict[str, Any] = {}\n        if extract_private_tags:\n            # Extract any private tags\n            for elem in dcm:\n                if elem.tag.is_private:\n                    try:\n                        metadata[elem.keyword or str(elem.tag)] = str(elem.value)\n                    except Exception:\n                        pass\n\n        return cls(\n            file_path=path_obj,\n            sop_instance_uid=sop_instance_uid,\n            template_id=template_id,\n            document_title=doc_title,\n            measurement_groups=measurement_groups,\n            patient_id=patient_id,\n            study_instance_uid=study_uid,\n            series_instance_uid=series_uid,\n            content_datetime=content_datetime,\n            metadata=metadata,\n        )\n\n    def get_measurements_df(self) -&gt; pd.DataFrame:\n        \"\"\"Export all measurements as a DataFrame.\n\n        Returns a flat DataFrame with all measurements from all groups,\n        including group context for each measurement.\n\n        Returns:\n            DataFrame with columns:\n            - group_id: Identifier of the measurement group\n            - finding_type: Type of finding\n            - finding_site: Anatomical site\n            - measurement_name: Name of the measurement\n            - value: Numerical value\n            - unit: Unit of measurement\n            - derivation: How it was derived\n            - tracking_id: Tracking identifier\n        \"\"\"\n        rows = []\n        for group in self.measurement_groups:\n            for meas in group.measurements:\n                rows.append(\n                    {\n                        \"group_id\": group.group_id,\n                        \"finding_type\": group.finding_type or meas.finding_type,\n                        \"finding_site\": group.finding_site or meas.finding_site,\n                        \"measurement_name\": meas.name,\n                        \"value\": meas.value,\n                        \"unit\": meas.unit,\n                        \"derivation\": meas.derivation,\n                        \"tracking_id\": meas.tracking_id,\n                    }\n                )\n\n        return pd.DataFrame(rows)\n\n    def get_summary(self) -&gt; dict[str, Any]:\n        \"\"\"Get document summary without full parsing.\n\n        Returns:\n            Dictionary with summary information including:\n            - sop_instance_uid\n            - template_id\n            - document_title\n            - num_groups\n            - num_measurements\n            - patient_id\n            - study_instance_uid\n        \"\"\"\n        total_measurements = sum(len(g.measurements) for g in self.measurement_groups)\n        return {\n            \"sop_instance_uid\": self.sop_instance_uid,\n            \"template_id\": self.template_id,\n            \"document_title\": self.document_title,\n            \"num_groups\": len(self.measurement_groups),\n            \"num_measurements\": total_measurements,\n            \"patient_id\": self.patient_id,\n            \"study_instance_uid\": self.study_instance_uid,\n            \"content_datetime\": self.content_datetime,\n        }\n\n    def export_csv(self, path: str | Path) -&gt; Path:\n        \"\"\"Export measurements to CSV file.\n\n        Args:\n            path: Output path for the CSV file.\n\n        Returns:\n            Path to the created CSV file.\n        \"\"\"\n        path_obj = Path(path)\n        df = self.get_measurements_df()\n        df.to_csv(path_obj, index=False)\n        return path_obj\n\n    def export_json(self, path: str | Path) -&gt; Path:\n        \"\"\"Export full SR content to JSON.\n\n        Exports the complete document structure including all groups,\n        measurements, and metadata.\n\n        Args:\n            path: Output path for the JSON file.\n\n        Returns:\n            Path to the created JSON file.\n        \"\"\"\n        import json\n\n        path_obj = Path(path)\n\n        # Build JSON structure\n        data: dict[str, Any] = {\n            \"sop_instance_uid\": self.sop_instance_uid,\n            \"template_id\": self.template_id,\n            \"document_title\": self.document_title,\n            \"patient_id\": self.patient_id,\n            \"study_instance_uid\": self.study_instance_uid,\n            \"series_instance_uid\": self.series_instance_uid,\n            \"content_datetime\": self.content_datetime,\n            \"metadata\": self.metadata,\n            \"measurement_groups\": [],\n        }\n\n        for group in self.measurement_groups:\n            group_data: dict[str, Any] = {\n                \"group_id\": group.group_id,\n                \"finding_type\": group.finding_type,\n                \"finding_site\": group.finding_site,\n                \"metadata\": group.metadata,\n                \"measurements\": [],\n            }\n            for meas in group.measurements:\n                meas_data = {\n                    \"name\": meas.name,\n                    \"value\": meas.value,\n                    \"unit\": meas.unit,\n                    \"finding_type\": meas.finding_type,\n                    \"finding_site\": meas.finding_site,\n                    \"derivation\": meas.derivation,\n                    \"tracking_id\": meas.tracking_id,\n                    \"metadata\": meas.metadata,\n                }\n                group_data[\"measurements\"].append(meas_data)\n            data[\"measurement_groups\"].append(group_data)\n\n        with open(path_obj, \"w\") as f:\n            json.dump(data, f, indent=2)\n\n        return path_obj\n\n    # ========================================================================\n    # Batch Processing (from_folders)\n    # ========================================================================\n\n    @classmethod\n    def from_folders(\n        cls,\n        paths: list[str | Path],\n        recursive: bool = True,\n        show_progress: bool = True,\n        num_workers: Optional[int] = None,\n        output_dir: Optional[str | Path] = None,\n        export_csv: bool = True,\n        export_json: bool = True,\n        extract_private_tags: bool = False,\n    ) -&gt; \"SRBatch\":\n        \"\"\"Batch process SR files from folders.\n\n        Scans directories for DICOM SR files, parses each one, and optionally\n        exports individual CSV/JSON files plus a combined output and log.\n\n        This method follows the same pattern as DicomDatabase.from_folders().\n\n        Args:\n            paths: List of folder paths to scan for SR files.\n            recursive: Whether to scan subdirectories (default: True).\n            show_progress: Whether to display progress bars (default: True).\n            num_workers: Number of parallel workers. None=auto (cpu_count-1),\n                        1=sequential (no multiprocessing).\n            output_dir: If specified, exports each SR to this directory.\n            export_csv: Export individual CSV files (default: True).\n            export_json: Export individual JSON files (default: True).\n            extract_private_tags: Whether to extract private tags (default: False).\n\n        Returns:\n            SRBatch containing all parsed documents and processing log.\n\n        Example:\n            Process all SR files in a folder:\n\n            ```python\n            from pictologics.utilities.sr_parser import SRDocument\n\n            # Process folder\n            batch = SRDocument.from_folders([\"sr_data/\"])\n            print(f\"Found {len(batch.documents)} SR files\")\n            df = batch.get_combined_measurements_df()\n\n            # Process with exports\n            batch = SRDocument.from_folders(\n                [\"sr_data/\"],\n                output_dir=\"sr_exports/\",\n                export_csv=True,\n                export_json=True\n            )\n            batch.export_log(\"sr_exports/processing_log.csv\")\n            ```\n        \"\"\"\n        import os\n        from concurrent.futures import ProcessPoolExecutor\n\n        from tqdm import tqdm\n\n        # Convert paths to Path objects\n        path_objs = [Path(p) for p in paths]\n\n        # Determine number of workers\n        if num_workers is None:\n            cpu_count = os.cpu_count()\n            num_workers = max(1, (cpu_count - 1) if cpu_count else 1)\n\n        # Step 1: Discover all SR files\n        sr_files: list[Path] = []\n        for path_obj in path_objs:\n            if not path_obj.exists():\n                continue\n            if path_obj.is_file():\n                if is_dicom_sr(path_obj):\n                    sr_files.append(path_obj)\n            else:\n                # Directory\n                iterator = path_obj.rglob(\"*\") if recursive else path_obj.iterdir()\n                for f in iterator:\n                    if f.is_file() and is_dicom_sr(f):\n                        sr_files.append(f)\n\n        if not sr_files:\n            return SRBatch(documents=[], processing_log=[], output_dir=None)\n\n        # Create output directory if specified\n        out_path = Path(output_dir) if output_dir else None\n        if out_path:\n            out_path.mkdir(parents=True, exist_ok=True)\n\n        # Step 2: Process each SR file\n        processing_log: list[dict[str, Any]] = []\n        documents: list[\"SRDocument\"] = []\n\n        # Prepare worker arguments\n        worker_args = [\n            (f, extract_private_tags, out_path, export_csv, export_json)\n            for f in sr_files\n        ]\n\n        if num_workers == 1:\n            # Sequential processing\n            iterator = tqdm(\n                worker_args, desc=\"Processing SR files\", disable=not show_progress\n            )\n            for args in iterator:\n                result = _process_sr_file_worker(args)\n                processing_log.append(result[\"log\"])\n                if result[\"document\"] is not None:\n                    documents.append(result[\"document\"])\n        else:\n            # Parallel processing\n            with ProcessPoolExecutor(max_workers=num_workers) as executor:\n                results = list(\n                    tqdm(\n                        executor.map(_process_sr_file_worker, worker_args),\n                        total=len(worker_args),\n                        desc=\"Processing SR files\",\n                        disable=not show_progress,\n                    )\n                )\n            for result in results:\n                processing_log.append(result[\"log\"])\n                if result[\"document\"] is not None:\n                    documents.append(result[\"document\"])  # pragma: no cover\n\n        return SRBatch(\n            documents=documents,\n            processing_log=processing_log,\n            output_dir=out_path,\n        )\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRDocument.export_csv","title":"<code>export_csv(path)</code>","text":"<p>Export measurements to CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Output path for the CSV file.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the created CSV file.</p> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>def export_csv(self, path: str | Path) -&gt; Path:\n    \"\"\"Export measurements to CSV file.\n\n    Args:\n        path: Output path for the CSV file.\n\n    Returns:\n        Path to the created CSV file.\n    \"\"\"\n    path_obj = Path(path)\n    df = self.get_measurements_df()\n    df.to_csv(path_obj, index=False)\n    return path_obj\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRDocument.export_json","title":"<code>export_json(path)</code>","text":"<p>Export full SR content to JSON.</p> <p>Exports the complete document structure including all groups, measurements, and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Output path for the JSON file.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the created JSON file.</p> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>def export_json(self, path: str | Path) -&gt; Path:\n    \"\"\"Export full SR content to JSON.\n\n    Exports the complete document structure including all groups,\n    measurements, and metadata.\n\n    Args:\n        path: Output path for the JSON file.\n\n    Returns:\n        Path to the created JSON file.\n    \"\"\"\n    import json\n\n    path_obj = Path(path)\n\n    # Build JSON structure\n    data: dict[str, Any] = {\n        \"sop_instance_uid\": self.sop_instance_uid,\n        \"template_id\": self.template_id,\n        \"document_title\": self.document_title,\n        \"patient_id\": self.patient_id,\n        \"study_instance_uid\": self.study_instance_uid,\n        \"series_instance_uid\": self.series_instance_uid,\n        \"content_datetime\": self.content_datetime,\n        \"metadata\": self.metadata,\n        \"measurement_groups\": [],\n    }\n\n    for group in self.measurement_groups:\n        group_data: dict[str, Any] = {\n            \"group_id\": group.group_id,\n            \"finding_type\": group.finding_type,\n            \"finding_site\": group.finding_site,\n            \"metadata\": group.metadata,\n            \"measurements\": [],\n        }\n        for meas in group.measurements:\n            meas_data = {\n                \"name\": meas.name,\n                \"value\": meas.value,\n                \"unit\": meas.unit,\n                \"finding_type\": meas.finding_type,\n                \"finding_site\": meas.finding_site,\n                \"derivation\": meas.derivation,\n                \"tracking_id\": meas.tracking_id,\n                \"metadata\": meas.metadata,\n            }\n            group_data[\"measurements\"].append(meas_data)\n        data[\"measurement_groups\"].append(group_data)\n\n    with open(path_obj, \"w\") as f:\n        json.dump(data, f, indent=2)\n\n    return path_obj\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRDocument.from_file","title":"<code>from_file(path, extract_private_tags=False)</code>  <code>classmethod</code>","text":"<p>Load and parse an SR document from file.</p> <p>This method reads a DICOM Structured Report file and extracts all measurement content into the hierarchical dataclass structure. Follows the same pattern as DicomDatabase.from_folders().</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to DICOM SR file.</p> required <code>extract_private_tags</code> <code>bool</code> <p>Whether to extract vendor-specific tags into the metadata dictionaries. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>'SRDocument'</code> <p>SRDocument instance with parsed content.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> <code>ValueError</code> <p>If the file is not a valid DICOM SR object.</p> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>@classmethod\ndef from_file(\n    cls,\n    path: str | Path,\n    extract_private_tags: bool = False,\n) -&gt; \"SRDocument\":\n    \"\"\"Load and parse an SR document from file.\n\n    This method reads a DICOM Structured Report file and extracts\n    all measurement content into the hierarchical dataclass structure.\n    Follows the same pattern as DicomDatabase.from_folders().\n\n    Args:\n        path: Path to DICOM SR file.\n        extract_private_tags: Whether to extract vendor-specific tags\n            into the metadata dictionaries. Defaults to False.\n\n    Returns:\n        SRDocument instance with parsed content.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: If the file is not a valid DICOM SR object.\n    \"\"\"\n    import pydicom\n\n    path_obj = Path(path)\n    if not path_obj.exists():\n        raise FileNotFoundError(f\"SR file not found: {path}\")\n\n    # Load the DICOM file\n    try:\n        dcm = pydicom.dcmread(str(path_obj))\n    except Exception as e:\n        raise ValueError(f\"Failed to read DICOM file: {e}\") from e\n\n    # Check if it's an SR document\n    sr_sop_classes = [\n        \"1.2.840.10008.5.1.4.1.1.88.11\",  # Basic Text SR\n        \"1.2.840.10008.5.1.4.1.1.88.22\",  # Enhanced SR\n        \"1.2.840.10008.5.1.4.1.1.88.33\",  # Comprehensive SR\n        \"1.2.840.10008.5.1.4.1.1.88.34\",  # Comprehensive 3D SR\n        \"1.2.840.10008.5.1.4.1.1.88.35\",  # Extensible SR\n        \"1.2.840.10008.5.1.4.1.1.88.40\",  # Procedure Log\n    ]\n    sop_class = str(getattr(dcm, \"SOPClassUID\", \"\"))\n    if sop_class not in sr_sop_classes:\n        raise ValueError(\n            f\"File is not a DICOM SR document. SOPClassUID: {sop_class}\"\n        )\n\n    # Extract basic document info\n    sop_instance_uid = str(getattr(dcm, \"SOPInstanceUID\", \"\"))\n    patient_id = str(getattr(dcm, \"PatientID\", \"\")) or None\n    study_uid = str(getattr(dcm, \"StudyInstanceUID\", \"\")) or None\n    series_uid = str(getattr(dcm, \"SeriesInstanceUID\", \"\")) or None\n\n    # Content datetime\n    content_date = getattr(dcm, \"ContentDate\", None)\n    content_time = getattr(dcm, \"ContentTime\", None)\n    if content_date:\n        content_datetime = str(content_date)\n        if content_time:\n            content_datetime += f\"T{content_time}\"\n    else:\n        content_datetime = None\n\n    # Extract document title from ConceptNameCodeSequence\n    doc_title = None\n    if hasattr(dcm, \"ConceptNameCodeSequence\") and dcm.ConceptNameCodeSequence:\n        concept = dcm.ConceptNameCodeSequence[0]\n        doc_title = str(getattr(concept, \"CodeMeaning\", \"\")) or None\n\n    # Extract template ID if present\n    template_id = None\n    if hasattr(dcm, \"ContentTemplateSequence\") and dcm.ContentTemplateSequence:\n        template = dcm.ContentTemplateSequence[0]\n        template_id = str(getattr(template, \"TemplateIdentifier\", \"\")) or None\n\n    # Parse content sequence for measurements\n    measurement_groups = _parse_content_sequence(dcm, extract_private_tags)\n\n    # Build metadata dict\n    metadata: dict[str, Any] = {}\n    if extract_private_tags:\n        # Extract any private tags\n        for elem in dcm:\n            if elem.tag.is_private:\n                try:\n                    metadata[elem.keyword or str(elem.tag)] = str(elem.value)\n                except Exception:\n                    pass\n\n    return cls(\n        file_path=path_obj,\n        sop_instance_uid=sop_instance_uid,\n        template_id=template_id,\n        document_title=doc_title,\n        measurement_groups=measurement_groups,\n        patient_id=patient_id,\n        study_instance_uid=study_uid,\n        series_instance_uid=series_uid,\n        content_datetime=content_datetime,\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRDocument.from_folders","title":"<code>from_folders(paths, recursive=True, show_progress=True, num_workers=None, output_dir=None, export_csv=True, export_json=True, extract_private_tags=False)</code>  <code>classmethod</code>","text":"<p>Batch process SR files from folders.</p> <p>Scans directories for DICOM SR files, parses each one, and optionally exports individual CSV/JSON files plus a combined output and log.</p> <p>This method follows the same pattern as DicomDatabase.from_folders().</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str | Path]</code> <p>List of folder paths to scan for SR files.</p> required <code>recursive</code> <code>bool</code> <p>Whether to scan subdirectories (default: True).</p> <code>True</code> <code>show_progress</code> <code>bool</code> <p>Whether to display progress bars (default: True).</p> <code>True</code> <code>num_workers</code> <code>Optional[int]</code> <p>Number of parallel workers. None=auto (cpu_count-1),         1=sequential (no multiprocessing).</p> <code>None</code> <code>output_dir</code> <code>Optional[str | Path]</code> <p>If specified, exports each SR to this directory.</p> <code>None</code> <code>export_csv</code> <code>bool</code> <p>Export individual CSV files (default: True).</p> <code>True</code> <code>export_json</code> <code>bool</code> <p>Export individual JSON files (default: True).</p> <code>True</code> <code>extract_private_tags</code> <code>bool</code> <p>Whether to extract private tags (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>'SRBatch'</code> <p>SRBatch containing all parsed documents and processing log.</p> Example <p>Process all SR files in a folder:</p> <pre><code>from pictologics.utilities.sr_parser import SRDocument\n\n# Process folder\nbatch = SRDocument.from_folders([\"sr_data/\"])\nprint(f\"Found {len(batch.documents)} SR files\")\ndf = batch.get_combined_measurements_df()\n\n# Process with exports\nbatch = SRDocument.from_folders(\n    [\"sr_data/\"],\n    output_dir=\"sr_exports/\",\n    export_csv=True,\n    export_json=True\n)\nbatch.export_log(\"sr_exports/processing_log.csv\")\n</code></pre> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>@classmethod\ndef from_folders(\n    cls,\n    paths: list[str | Path],\n    recursive: bool = True,\n    show_progress: bool = True,\n    num_workers: Optional[int] = None,\n    output_dir: Optional[str | Path] = None,\n    export_csv: bool = True,\n    export_json: bool = True,\n    extract_private_tags: bool = False,\n) -&gt; \"SRBatch\":\n    \"\"\"Batch process SR files from folders.\n\n    Scans directories for DICOM SR files, parses each one, and optionally\n    exports individual CSV/JSON files plus a combined output and log.\n\n    This method follows the same pattern as DicomDatabase.from_folders().\n\n    Args:\n        paths: List of folder paths to scan for SR files.\n        recursive: Whether to scan subdirectories (default: True).\n        show_progress: Whether to display progress bars (default: True).\n        num_workers: Number of parallel workers. None=auto (cpu_count-1),\n                    1=sequential (no multiprocessing).\n        output_dir: If specified, exports each SR to this directory.\n        export_csv: Export individual CSV files (default: True).\n        export_json: Export individual JSON files (default: True).\n        extract_private_tags: Whether to extract private tags (default: False).\n\n    Returns:\n        SRBatch containing all parsed documents and processing log.\n\n    Example:\n        Process all SR files in a folder:\n\n        ```python\n        from pictologics.utilities.sr_parser import SRDocument\n\n        # Process folder\n        batch = SRDocument.from_folders([\"sr_data/\"])\n        print(f\"Found {len(batch.documents)} SR files\")\n        df = batch.get_combined_measurements_df()\n\n        # Process with exports\n        batch = SRDocument.from_folders(\n            [\"sr_data/\"],\n            output_dir=\"sr_exports/\",\n            export_csv=True,\n            export_json=True\n        )\n        batch.export_log(\"sr_exports/processing_log.csv\")\n        ```\n    \"\"\"\n    import os\n    from concurrent.futures import ProcessPoolExecutor\n\n    from tqdm import tqdm\n\n    # Convert paths to Path objects\n    path_objs = [Path(p) for p in paths]\n\n    # Determine number of workers\n    if num_workers is None:\n        cpu_count = os.cpu_count()\n        num_workers = max(1, (cpu_count - 1) if cpu_count else 1)\n\n    # Step 1: Discover all SR files\n    sr_files: list[Path] = []\n    for path_obj in path_objs:\n        if not path_obj.exists():\n            continue\n        if path_obj.is_file():\n            if is_dicom_sr(path_obj):\n                sr_files.append(path_obj)\n        else:\n            # Directory\n            iterator = path_obj.rglob(\"*\") if recursive else path_obj.iterdir()\n            for f in iterator:\n                if f.is_file() and is_dicom_sr(f):\n                    sr_files.append(f)\n\n    if not sr_files:\n        return SRBatch(documents=[], processing_log=[], output_dir=None)\n\n    # Create output directory if specified\n    out_path = Path(output_dir) if output_dir else None\n    if out_path:\n        out_path.mkdir(parents=True, exist_ok=True)\n\n    # Step 2: Process each SR file\n    processing_log: list[dict[str, Any]] = []\n    documents: list[\"SRDocument\"] = []\n\n    # Prepare worker arguments\n    worker_args = [\n        (f, extract_private_tags, out_path, export_csv, export_json)\n        for f in sr_files\n    ]\n\n    if num_workers == 1:\n        # Sequential processing\n        iterator = tqdm(\n            worker_args, desc=\"Processing SR files\", disable=not show_progress\n        )\n        for args in iterator:\n            result = _process_sr_file_worker(args)\n            processing_log.append(result[\"log\"])\n            if result[\"document\"] is not None:\n                documents.append(result[\"document\"])\n    else:\n        # Parallel processing\n        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n            results = list(\n                tqdm(\n                    executor.map(_process_sr_file_worker, worker_args),\n                    total=len(worker_args),\n                    desc=\"Processing SR files\",\n                    disable=not show_progress,\n                )\n            )\n        for result in results:\n            processing_log.append(result[\"log\"])\n            if result[\"document\"] is not None:\n                documents.append(result[\"document\"])  # pragma: no cover\n\n    return SRBatch(\n        documents=documents,\n        processing_log=processing_log,\n        output_dir=out_path,\n    )\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRDocument.get_measurements_df","title":"<code>get_measurements_df()</code>","text":"<p>Export all measurements as a DataFrame.</p> <p>Returns a flat DataFrame with all measurements from all groups, including group context for each measurement.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns:</p> <code>DataFrame</code> <ul> <li>group_id: Identifier of the measurement group</li> </ul> <code>DataFrame</code> <ul> <li>finding_type: Type of finding</li> </ul> <code>DataFrame</code> <ul> <li>finding_site: Anatomical site</li> </ul> <code>DataFrame</code> <ul> <li>measurement_name: Name of the measurement</li> </ul> <code>DataFrame</code> <ul> <li>value: Numerical value</li> </ul> <code>DataFrame</code> <ul> <li>unit: Unit of measurement</li> </ul> <code>DataFrame</code> <ul> <li>derivation: How it was derived</li> </ul> <code>DataFrame</code> <ul> <li>tracking_id: Tracking identifier</li> </ul> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>def get_measurements_df(self) -&gt; pd.DataFrame:\n    \"\"\"Export all measurements as a DataFrame.\n\n    Returns a flat DataFrame with all measurements from all groups,\n    including group context for each measurement.\n\n    Returns:\n        DataFrame with columns:\n        - group_id: Identifier of the measurement group\n        - finding_type: Type of finding\n        - finding_site: Anatomical site\n        - measurement_name: Name of the measurement\n        - value: Numerical value\n        - unit: Unit of measurement\n        - derivation: How it was derived\n        - tracking_id: Tracking identifier\n    \"\"\"\n    rows = []\n    for group in self.measurement_groups:\n        for meas in group.measurements:\n            rows.append(\n                {\n                    \"group_id\": group.group_id,\n                    \"finding_type\": group.finding_type or meas.finding_type,\n                    \"finding_site\": group.finding_site or meas.finding_site,\n                    \"measurement_name\": meas.name,\n                    \"value\": meas.value,\n                    \"unit\": meas.unit,\n                    \"derivation\": meas.derivation,\n                    \"tracking_id\": meas.tracking_id,\n                }\n            )\n\n    return pd.DataFrame(rows)\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRDocument.get_summary","title":"<code>get_summary()</code>","text":"<p>Get document summary without full parsing.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with summary information including:</p> <code>dict[str, Any]</code> <ul> <li>sop_instance_uid</li> </ul> <code>dict[str, Any]</code> <ul> <li>template_id</li> </ul> <code>dict[str, Any]</code> <ul> <li>document_title</li> </ul> <code>dict[str, Any]</code> <ul> <li>num_groups</li> </ul> <code>dict[str, Any]</code> <ul> <li>num_measurements</li> </ul> <code>dict[str, Any]</code> <ul> <li>patient_id</li> </ul> <code>dict[str, Any]</code> <ul> <li>study_instance_uid</li> </ul> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>def get_summary(self) -&gt; dict[str, Any]:\n    \"\"\"Get document summary without full parsing.\n\n    Returns:\n        Dictionary with summary information including:\n        - sop_instance_uid\n        - template_id\n        - document_title\n        - num_groups\n        - num_measurements\n        - patient_id\n        - study_instance_uid\n    \"\"\"\n    total_measurements = sum(len(g.measurements) for g in self.measurement_groups)\n    return {\n        \"sop_instance_uid\": self.sop_instance_uid,\n        \"template_id\": self.template_id,\n        \"document_title\": self.document_title,\n        \"num_groups\": len(self.measurement_groups),\n        \"num_measurements\": total_measurements,\n        \"patient_id\": self.patient_id,\n        \"study_instance_uid\": self.study_instance_uid,\n        \"content_datetime\": self.content_datetime,\n    }\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRBatch","title":"<code>SRBatch</code>  <code>dataclass</code>","text":"<p>Collection of parsed SR documents from batch processing.</p> <p>This class holds the results of batch SR processing via SRDocument.from_folders(). It provides access to all parsed documents and methods for combined exports.</p> <p>Attributes:</p> Name Type Description <code>documents</code> <code>list[SRDocument]</code> <p>List of successfully parsed SRDocument objects.</p> <code>processing_log</code> <code>list[dict[str, Any]]</code> <p>Log entries for each processed file (success/error).</p> <code>output_dir</code> <code>Optional[Path]</code> <p>Directory where individual exports were written.</p> Example <pre><code>from pictologics.utilities.sr_parser import SRDocument\n\nbatch = SRDocument.from_folders([\"sr_data/\"], output_dir=\"exports/\")\nprint(f\"Processed {len(batch.documents)} SR files\")\ndf = batch.get_combined_measurements_df()\nbatch.export_log(\"exports/processing_log.csv\")\n</code></pre> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>@dataclass\nclass SRBatch:\n    \"\"\"Collection of parsed SR documents from batch processing.\n\n    This class holds the results of batch SR processing via\n    SRDocument.from_folders(). It provides access to all parsed documents\n    and methods for combined exports.\n\n    Attributes:\n        documents: List of successfully parsed SRDocument objects.\n        processing_log: Log entries for each processed file (success/error).\n        output_dir: Directory where individual exports were written.\n\n    Example:\n        ```python\n        from pictologics.utilities.sr_parser import SRDocument\n\n        batch = SRDocument.from_folders([\"sr_data/\"], output_dir=\"exports/\")\n        print(f\"Processed {len(batch.documents)} SR files\")\n        df = batch.get_combined_measurements_df()\n        batch.export_log(\"exports/processing_log.csv\")\n        ```\n    \"\"\"\n\n    documents: list[SRDocument] = field(default_factory=list)\n    processing_log: list[dict[str, Any]] = field(default_factory=list)\n    output_dir: Optional[Path] = None\n\n    def get_combined_measurements_df(self) -&gt; pd.DataFrame:\n        \"\"\"Combine measurements from all documents into a single DataFrame.\n\n        Each measurement row includes the source document's SOP Instance UID,\n        patient ID, and study UID for traceability.\n\n        Returns:\n            DataFrame with all measurements from all documents.\n        \"\"\"\n        all_rows: list[dict[str, Any]] = []\n\n        for doc in self.documents:\n            for group in doc.measurement_groups:\n                for meas in group.measurements:\n                    all_rows.append(\n                        {\n                            \"sop_instance_uid\": doc.sop_instance_uid,\n                            \"patient_id\": doc.patient_id,\n                            \"study_instance_uid\": doc.study_instance_uid,\n                            \"group_finding_type\": group.finding_type,\n                            \"group_finding_site\": group.finding_site,\n                            \"measurement_name\": meas.name,\n                            \"value\": meas.value,\n                            \"unit\": meas.unit,\n                            \"finding_type\": meas.finding_type,\n                            \"finding_site\": meas.finding_site,\n                            \"derivation\": meas.derivation,\n                            \"tracking_id\": meas.tracking_id,\n                        }\n                    )\n\n        return pd.DataFrame(all_rows)\n\n    def export_combined_csv(self, path: str | Path) -&gt; Path:\n        \"\"\"Export combined measurements to a single CSV file.\n\n        Args:\n            path: Output path for the combined CSV.\n\n        Returns:\n            Path to the created CSV file.\n        \"\"\"\n        path_obj = Path(path)\n        df = self.get_combined_measurements_df()\n        df.to_csv(path_obj, index=False)\n        return path_obj\n\n    def export_log(self, path: str | Path) -&gt; Path:\n        \"\"\"Export processing log to CSV.\n\n        The log contains one row per processed file with status,\n        output paths, and any error messages.\n\n        Args:\n            path: Output path for the log CSV.\n\n        Returns:\n            Path to the created CSV file.\n        \"\"\"\n        path_obj = Path(path)\n        df = pd.DataFrame(self.processing_log)\n        df.to_csv(path_obj, index=False)\n        return path_obj\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRBatch.export_combined_csv","title":"<code>export_combined_csv(path)</code>","text":"<p>Export combined measurements to a single CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Output path for the combined CSV.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the created CSV file.</p> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>def export_combined_csv(self, path: str | Path) -&gt; Path:\n    \"\"\"Export combined measurements to a single CSV file.\n\n    Args:\n        path: Output path for the combined CSV.\n\n    Returns:\n        Path to the created CSV file.\n    \"\"\"\n    path_obj = Path(path)\n    df = self.get_combined_measurements_df()\n    df.to_csv(path_obj, index=False)\n    return path_obj\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRBatch.export_log","title":"<code>export_log(path)</code>","text":"<p>Export processing log to CSV.</p> <p>The log contains one row per processed file with status, output paths, and any error messages.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Output path for the log CSV.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path to the created CSV file.</p> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>def export_log(self, path: str | Path) -&gt; Path:\n    \"\"\"Export processing log to CSV.\n\n    The log contains one row per processed file with status,\n    output paths, and any error messages.\n\n    Args:\n        path: Output path for the log CSV.\n\n    Returns:\n        Path to the created CSV file.\n    \"\"\"\n    path_obj = Path(path)\n    df = pd.DataFrame(self.processing_log)\n    df.to_csv(path_obj, index=False)\n    return path_obj\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.SRBatch.get_combined_measurements_df","title":"<code>get_combined_measurements_df()</code>","text":"<p>Combine measurements from all documents into a single DataFrame.</p> <p>Each measurement row includes the source document's SOP Instance UID, patient ID, and study UID for traceability.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with all measurements from all documents.</p> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>def get_combined_measurements_df(self) -&gt; pd.DataFrame:\n    \"\"\"Combine measurements from all documents into a single DataFrame.\n\n    Each measurement row includes the source document's SOP Instance UID,\n    patient ID, and study UID for traceability.\n\n    Returns:\n        DataFrame with all measurements from all documents.\n    \"\"\"\n    all_rows: list[dict[str, Any]] = []\n\n    for doc in self.documents:\n        for group in doc.measurement_groups:\n            for meas in group.measurements:\n                all_rows.append(\n                    {\n                        \"sop_instance_uid\": doc.sop_instance_uid,\n                        \"patient_id\": doc.patient_id,\n                        \"study_instance_uid\": doc.study_instance_uid,\n                        \"group_finding_type\": group.finding_type,\n                        \"group_finding_site\": group.finding_site,\n                        \"measurement_name\": meas.name,\n                        \"value\": meas.value,\n                        \"unit\": meas.unit,\n                        \"finding_type\": meas.finding_type,\n                        \"finding_site\": meas.finding_site,\n                        \"derivation\": meas.derivation,\n                        \"tracking_id\": meas.tracking_id,\n                    }\n                )\n\n    return pd.DataFrame(all_rows)\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.sr_parser.is_dicom_sr","title":"<code>is_dicom_sr(path)</code>","text":"<p>Check if a DICOM file is a Structured Report.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the potential DICOM file.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the file is a DICOM SR object, False otherwise.</p> Source code in <code>pictologics/utilities/sr_parser.py</code> <pre><code>def is_dicom_sr(path: str | Path) -&gt; bool:\n    \"\"\"Check if a DICOM file is a Structured Report.\n\n    Args:\n        path: Path to the potential DICOM file.\n\n    Returns:\n        True if the file is a DICOM SR object, False otherwise.\n    \"\"\"\n    import pydicom\n\n    try:\n        dcm = pydicom.dcmread(str(path), stop_before_pixels=True)\n        sop_class = str(getattr(dcm, \"SOPClassUID\", \"\"))\n\n        sr_sop_classes = [\n            \"1.2.840.10008.5.1.4.1.1.88.11\",  # Basic Text SR\n            \"1.2.840.10008.5.1.4.1.1.88.22\",  # Enhanced SR\n            \"1.2.840.10008.5.1.4.1.1.88.33\",  # Comprehensive SR\n            \"1.2.840.10008.5.1.4.1.1.88.34\",  # Comprehensive 3D SR\n            \"1.2.840.10008.5.1.4.1.1.88.35\",  # Extensible SR\n            \"1.2.840.10008.5.1.4.1.1.88.40\",  # Procedure Log\n        ]\n\n        return sop_class in sr_sop_classes\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.visualization","title":"<code>pictologics.utilities.visualization</code>","text":""},{"location":"api/utilities/#pictologics.utilities.visualization--visualization-module","title":"Visualization Module","text":"<p>This module provides utilities for visualizing medical images and segmentation masks. It supports interactive slice scrolling and batch export of images.</p>"},{"location":"api/utilities/#pictologics.utilities.visualization--key-features","title":"Key Features","text":"<ul> <li>Interactive slice viewer with matplotlib</li> <li>Flexible display modes: image-only, mask-only, or overlay</li> <li>Multi-label mask support (up to 20+ labels with distinct colors)</li> <li>Window/Level normalization for CT/MR viewing</li> <li>Configurable output formats (PNG, JPEG, TIFF)</li> <li>Flexible slice selection for batch export</li> </ul>"},{"location":"api/utilities/#pictologics.utilities.visualization--display-modes","title":"Display Modes","text":"<p>The visualization functions support three display modes based on which inputs are provided:</p> <ol> <li> <p>Image + Mask (Overlay Mode):    Both <code>image</code> and <code>mask</code> are provided. The mask is overlaid on the grayscale image    with the specified transparency (alpha) and colormap.</p> </li> <li> <p>Image Only:    Only <code>image</code> is provided (<code>mask=None</code>). The image is displayed as grayscale,    optionally with window/level normalization applied.</p> </li> <li> <p>Mask Only:    Only <code>mask</code> is provided (<code>image=None</code>). The mask can be displayed either:</p> </li> <li>As a colormap visualization (<code>mask_as_colormap=True</code>, default): Each unique      label value gets a distinct color from the specified colormap.</li> <li>As grayscale (<code>mask_as_colormap=False</code>): Values are normalized to 0-255.</li> </ol>"},{"location":"api/utilities/#pictologics.utilities.visualization--windowlevel-normalization","title":"Window/Level Normalization","text":"<p>For medical imaging (CT, MR), window/level controls are essential for proper visualization. When <code>window_center</code> and <code>window_width</code> are specified:</p> <ul> <li>window_center (Level): The center value of the display window (default: 200 HU for soft tissue)</li> <li>window_width (Width): The range of values displayed (default: 600 HU)</li> </ul> <p>Values outside [center - width/2, center + width/2] are clipped to black/white.</p> <p>Common presets: - Soft tissue: Center=40, Width=400 - Bone: Center=400, Width=1800 - Lung: Center=-600, Width=1500 - Brain: Center=40, Width=80</p>"},{"location":"api/utilities/#pictologics.utilities.visualization.visualize_slices","title":"<code>visualize_slices(image=None, mask=None, alpha=0.25, colormap='tab20', axis=2, initial_slice=None, window_title='Slice Viewer', window_center=None, window_width=None, mask_as_colormap=True)</code>","text":"<p>Display interactive slice viewer with scrolling.</p> <p>This function supports three display modes:</p> <ol> <li> <p>Image + Mask (Overlay Mode): Both <code>image</code> and <code>mask</code> are provided.    The mask is overlaid on the grayscale image with transparency.</p> </li> <li> <p>Image Only: Only <code>image</code> is provided. Displays grayscale slices,    optionally with window/level normalization.</p> </li> <li> <p>Mask Only: Only <code>mask</code> is provided. Displays mask visualization    using either a colormap or grayscale display.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Optional[Image]</code> <p>Optional Pictologics Image object containing the image data.</p> <code>None</code> <code>mask</code> <code>Optional[Image]</code> <p>Optional Pictologics Image object containing the mask data.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Transparency of mask overlay (0-1). Only used in overlay mode.</p> <code>0.25</code> <code>colormap</code> <code>str</code> <p>Colormap for mask labels. Options: - \"tab10\": 10 distinct colors - \"tab20\": 20 distinct colors (default) - \"Set1\": 9 bold colors - \"Set2\": 8 pastel colors - \"Paired\": 12 paired colors</p> <code>'tab20'</code> <code>axis</code> <code>int</code> <p>Axis along which to slice (0=sagittal, 1=coronal, 2=axial).</p> <code>2</code> <code>initial_slice</code> <code>Optional[int]</code> <p>Initial slice to display (default: middle).</p> <code>None</code> <code>window_title</code> <code>str</code> <p>Title for the viewer window.</p> <code>'Slice Viewer'</code> <code>window_center</code> <code>Optional[float]</code> <p>Window center (level) for normalization. Default: None (min-max).</p> <code>None</code> <code>window_width</code> <code>Optional[float]</code> <p>Window width for normalization. Default: None (min-max).</p> <code>None</code> <code>mask_as_colormap</code> <code>bool</code> <p>If True and mask-only mode, display with colormap. If False, display as grayscale.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither image nor mask is provided, or if shapes don't match when both are provided.</p> Example <p>Visualise slices interactively:</p> <pre><code>from pictologics import load_image\nfrom pictologics.utilities import visualize_slices\n\n# View image with mask overlay\nimg = load_image(\"scan.nii.gz\")\nmask = load_image(\"segmentation.nii.gz\")\nvisualize_slices(image=img, mask=mask)\n\n# View image only\nvisualize_slices(image=img, window_center=40, window_width=400)\n\n# View mask only with colormap\nvisualize_slices(mask=mask)\n</code></pre> Source code in <code>pictologics/utilities/visualization.py</code> <pre><code>def visualize_slices(\n    image: Optional[Image] = None,\n    mask: Optional[Image] = None,\n    alpha: float = 0.25,\n    colormap: str = \"tab20\",\n    axis: int = 2,\n    initial_slice: Optional[int] = None,\n    window_title: str = \"Slice Viewer\",\n    window_center: Optional[float] = None,\n    window_width: Optional[float] = None,\n    mask_as_colormap: bool = True,\n) -&gt; None:\n    \"\"\"\n    Display interactive slice viewer with scrolling.\n\n    This function supports three display modes:\n\n    1. **Image + Mask (Overlay Mode)**: Both `image` and `mask` are provided.\n       The mask is overlaid on the grayscale image with transparency.\n\n    2. **Image Only**: Only `image` is provided. Displays grayscale slices,\n       optionally with window/level normalization.\n\n    3. **Mask Only**: Only `mask` is provided. Displays mask visualization\n       using either a colormap or grayscale display.\n\n    Args:\n        image: Optional Pictologics Image object containing the image data.\n        mask: Optional Pictologics Image object containing the mask data.\n        alpha: Transparency of mask overlay (0-1). Only used in overlay mode.\n        colormap: Colormap for mask labels. Options:\n            - \"tab10\": 10 distinct colors\n            - \"tab20\": 20 distinct colors (default)\n            - \"Set1\": 9 bold colors\n            - \"Set2\": 8 pastel colors\n            - \"Paired\": 12 paired colors\n        axis: Axis along which to slice (0=sagittal, 1=coronal, 2=axial).\n        initial_slice: Initial slice to display (default: middle).\n        window_title: Title for the viewer window.\n        window_center: Window center (level) for normalization. Default: None (min-max).\n        window_width: Window width for normalization. Default: None (min-max).\n        mask_as_colormap: If True and mask-only mode, display with colormap.\n            If False, display as grayscale.\n\n    Raises:\n        ValueError: If neither image nor mask is provided, or if shapes don't match\n            when both are provided.\n\n    Example:\n        Visualise slices interactively:\n\n        ```python\n        from pictologics import load_image\n        from pictologics.utilities import visualize_slices\n\n        # View image with mask overlay\n        img = load_image(\"scan.nii.gz\")\n        mask = load_image(\"segmentation.nii.gz\")\n        visualize_slices(image=img, mask=mask)\n\n        # View image only\n        visualize_slices(image=img, window_center=40, window_width=400)\n\n        # View mask only with colormap\n        visualize_slices(mask=mask)\n        ```\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from matplotlib.widgets import Slider\n\n    if image is None and mask is None:\n        raise ValueError(\"At least one of image or mask must be provided.\")\n\n    # Validate shapes if both provided\n    if image is not None and mask is not None:\n        if image.array.shape != mask.array.shape:\n            raise ValueError(\n                f\"Image shape {image.array.shape} does not match \"\n                f\"mask shape {mask.array.shape}\"\n            )\n\n    # Get reference array for shape\n    ref_array = _get_reference_array(image, mask)\n\n    # Get number of slices\n    num_slices = ref_array.shape[axis]\n\n    # Set initial slice\n    if initial_slice is None:\n        initial_slice = num_slices // 2\n\n    # Create figure and axes\n    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    plt.subplots_adjust(bottom=0.15)\n\n    # Get slice data\n    def get_slice(\n        idx: int,\n    ) -&gt; tuple[\n        Optional[npt.NDArray[np.floating[Any]]], Optional[npt.NDArray[np.floating[Any]]]\n    ]:\n        img_slice = None\n        mask_slice = None\n\n        if image is not None:\n            if axis == 0:\n                img_slice = image.array[idx, :, :]\n            elif axis == 1:\n                img_slice = image.array[:, idx, :]\n            else:\n                img_slice = image.array[:, :, idx]\n\n        if mask is not None:\n            if axis == 0:\n                mask_slice = mask.array[idx, :, :]\n            elif axis == 1:\n                mask_slice = mask.array[:, idx, :]\n            else:\n                mask_slice = mask.array[:, :, idx]\n\n        return img_slice, mask_slice\n\n    img_slice, mask_slice = get_slice(initial_slice)\n    rgba = _create_display_rgba(\n        img_slice,\n        mask_slice,\n        alpha,\n        colormap,\n        window_center,\n        window_width,\n        mask_as_colormap,\n    )\n\n    # Display\n    im = ax.imshow(rgba, aspect=\"equal\")\n    ax.set_title(f\"Slice {initial_slice}/{num_slices - 1}\")\n    ax.axis(\"off\")\n\n    # Add slider\n    ax_slider = plt.axes((0.15, 0.05, 0.7, 0.03))\n    slider = Slider(\n        ax=ax_slider,\n        label=\"Slice\",\n        valmin=0,\n        valmax=num_slices - 1,\n        valinit=initial_slice,\n        valstep=1,\n    )\n\n    def update(val: float) -&gt; None:\n        idx = int(val)\n        img_slice, mask_slice = get_slice(idx)\n        rgba = _create_display_rgba(\n            img_slice,\n            mask_slice,\n            alpha,\n            colormap,\n            window_center,\n            window_width,\n            mask_as_colormap,\n        )\n        im.set_data(rgba)\n        ax.set_title(f\"Slice {idx}/{num_slices - 1}\")\n        fig.canvas.draw_idle()\n\n    slider.on_changed(update)\n\n    # Add scroll wheel support\n    def on_scroll(event) -&gt; None:  # type: ignore[no-untyped-def]\n        if event.button == \"up\":\n            new_val = min(slider.val + 1, num_slices - 1)\n        else:\n            new_val = max(slider.val - 1, 0)\n        slider.set_val(new_val)\n\n    fig.canvas.mpl_connect(\"scroll_event\", on_scroll)\n\n    fig.suptitle(window_title)\n    plt.show()\n</code></pre>"},{"location":"api/utilities/#pictologics.utilities.visualization.save_slices","title":"<code>save_slices(output_dir, image=None, mask=None, slice_selection='10%', format='png', dpi=300, alpha=0.25, colormap='tab20', axis=2, filename_prefix='slice', window_center=None, window_width=None, mask_as_colormap=True)</code>","text":"<p>Save image slices to files.</p> <p>This function supports three display modes:</p> <ol> <li> <p>Image + Mask (Overlay Mode): Both <code>image</code> and <code>mask</code> are provided.    The mask is overlaid on the grayscale image with transparency.</p> </li> <li> <p>Image Only: Only <code>image</code> is provided. Saves grayscale slices,    optionally with window/level normalization.</p> </li> <li> <p>Mask Only: Only <code>mask</code> is provided. Saves mask visualization    using either a colormap or grayscale display.</p> </li> </ol> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Directory to save output images.</p> required <code>image</code> <code>Optional[Image]</code> <p>Optional Pictologics Image object containing the image data.</p> <code>None</code> <code>mask</code> <code>Optional[Image]</code> <p>Optional Pictologics Image object containing the mask data.</p> <code>None</code> <code>slice_selection</code> <code>Union[str, int, list[int]]</code> <p>Slice selection specification: - \"every_N\" or \"N\": Every Nth slice - \"N%\": Slices at each N% interval (e.g., \"10%\" = ~10 images) - int: Single slice index - list[int]: Specific slice indices</p> <code>'10%'</code> <code>format</code> <code>str</code> <p>Output format (\"png\", \"jpeg\", \"tiff\").</p> <code>'png'</code> <code>dpi</code> <code>int</code> <p>Output resolution in dots per inch.</p> <code>300</code> <code>alpha</code> <code>float</code> <p>Transparency of mask overlay (0-1). Only used in overlay mode.</p> <code>0.25</code> <code>colormap</code> <code>str</code> <p>Colormap for mask labels. Options: - \"tab10\": 10 distinct colors - \"tab20\": 20 distinct colors (default) - \"Set1\": 9 bold colors - \"Set2\": 8 pastel colors - \"Paired\": 12 paired colors</p> <code>'tab20'</code> <code>axis</code> <code>int</code> <p>Axis along which to slice (0=sagittal, 1=coronal, 2=axial).</p> <code>2</code> <code>filename_prefix</code> <code>str</code> <p>Prefix for output filenames.</p> <code>'slice'</code> <code>window_center</code> <code>Optional[float]</code> <p>Window center (level) for normalization. Default: None (min-max).</p> <code>None</code> <code>window_width</code> <code>Optional[float]</code> <p>Window width for normalization. Default: None (min-max).</p> <code>None</code> <code>mask_as_colormap</code> <code>bool</code> <p>If True and mask-only mode, display with colormap. If False, display as grayscale.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of paths to saved files.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither image nor mask is provided, or if shapes don't match when both are provided.</p> Example <p>Save image slices with and without mask overlay:</p> <pre><code>from pictologics import load_image\nfrom pictologics.utilities import save_slices\n\n# Save image with mask overlay\nimg = load_image(\"scan.nii.gz\")\nmask = load_image(\"segmentation.nii.gz\")\nfiles = save_slices(\"output/\", image=img, mask=mask, slice_selection=\"10%\")\n\n# Save image only (no mask)\nfiles = save_slices(\"output/\", image=img, slice_selection=\"10%\")\n\n# Save mask only with colormap\nfiles = save_slices(\"output/\", mask=mask, slice_selection=\"10%\")\n</code></pre> Source code in <code>pictologics/utilities/visualization.py</code> <pre><code>def save_slices(\n    output_dir: str,\n    image: Optional[Image] = None,\n    mask: Optional[Image] = None,\n    slice_selection: Union[str, int, list[int]] = \"10%\",\n    format: str = \"png\",\n    dpi: int = 300,\n    alpha: float = 0.25,\n    colormap: str = \"tab20\",\n    axis: int = 2,\n    filename_prefix: str = \"slice\",\n    window_center: Optional[float] = None,\n    window_width: Optional[float] = None,\n    mask_as_colormap: bool = True,\n) -&gt; list[str]:\n    \"\"\"\n    Save image slices to files.\n\n    This function supports three display modes:\n\n    1. **Image + Mask (Overlay Mode)**: Both `image` and `mask` are provided.\n       The mask is overlaid on the grayscale image with transparency.\n\n    2. **Image Only**: Only `image` is provided. Saves grayscale slices,\n       optionally with window/level normalization.\n\n    3. **Mask Only**: Only `mask` is provided. Saves mask visualization\n       using either a colormap or grayscale display.\n\n    Args:\n        output_dir: Directory to save output images.\n        image: Optional Pictologics Image object containing the image data.\n        mask: Optional Pictologics Image object containing the mask data.\n        slice_selection: Slice selection specification:\n            - \"every_N\" or \"N\": Every Nth slice\n            - \"N%\": Slices at each N% interval (e.g., \"10%\" = ~10 images)\n            - int: Single slice index\n            - list[int]: Specific slice indices\n        format: Output format (\"png\", \"jpeg\", \"tiff\").\n        dpi: Output resolution in dots per inch.\n        alpha: Transparency of mask overlay (0-1). Only used in overlay mode.\n        colormap: Colormap for mask labels. Options:\n            - \"tab10\": 10 distinct colors\n            - \"tab20\": 20 distinct colors (default)\n            - \"Set1\": 9 bold colors\n            - \"Set2\": 8 pastel colors\n            - \"Paired\": 12 paired colors\n        axis: Axis along which to slice (0=sagittal, 1=coronal, 2=axial).\n        filename_prefix: Prefix for output filenames.\n        window_center: Window center (level) for normalization. Default: None (min-max).\n        window_width: Window width for normalization. Default: None (min-max).\n        mask_as_colormap: If True and mask-only mode, display with colormap.\n            If False, display as grayscale.\n\n    Returns:\n        List of paths to saved files.\n\n    Raises:\n        ValueError: If neither image nor mask is provided, or if shapes don't match\n            when both are provided.\n\n    Example:\n        Save image slices with and without mask overlay:\n\n        ```python\n        from pictologics import load_image\n        from pictologics.utilities import save_slices\n\n        # Save image with mask overlay\n        img = load_image(\"scan.nii.gz\")\n        mask = load_image(\"segmentation.nii.gz\")\n        files = save_slices(\"output/\", image=img, mask=mask, slice_selection=\"10%\")\n\n        # Save image only (no mask)\n        files = save_slices(\"output/\", image=img, slice_selection=\"10%\")\n\n        # Save mask only with colormap\n        files = save_slices(\"output/\", mask=mask, slice_selection=\"10%\")\n        ```\n    \"\"\"\n    if image is None and mask is None:\n        raise ValueError(\"At least one of image or mask must be provided.\")\n\n    # Validate shapes if both provided\n    if image is not None and mask is not None:\n        if image.array.shape != mask.array.shape:\n            raise ValueError(\n                f\"Image shape {image.array.shape} does not match \"\n                f\"mask shape {mask.array.shape}\"\n            )\n\n    # Get reference array for shape\n    ref_array = _get_reference_array(image, mask)\n\n    # Create output directory\n    out_path = Path(output_dir)\n    out_path.mkdir(parents=True, exist_ok=True)\n\n    # Get number of slices along axis\n    num_slices = ref_array.shape[axis]\n\n    # Parse slice selection\n    slice_indices = _parse_slice_selection(slice_selection, num_slices)\n\n    # Validate format\n    format = format.lower()\n    if format == \"jpg\":\n        format = \"jpeg\"\n    if format not in (\"png\", \"jpeg\", \"tiff\"):\n        format = \"png\"\n\n    # Calculate pixel size based on DPI\n    scale_factor = dpi / 72.0\n\n    saved_files = []\n\n    for idx in slice_indices:\n        # Extract slices\n        img_slice = None\n        mask_slice = None\n\n        if image is not None:\n            if axis == 0:\n                img_slice = image.array[idx, :, :]\n            elif axis == 1:\n                img_slice = image.array[:, idx, :]\n            else:\n                img_slice = image.array[:, :, idx]\n\n        if mask is not None:\n            if axis == 0:\n                mask_slice = mask.array[idx, :, :]\n            elif axis == 1:\n                mask_slice = mask.array[:, idx, :]\n            else:\n                mask_slice = mask.array[:, :, idx]\n\n        # Create display RGBA\n        rgba = _create_display_rgba(\n            img_slice,\n            mask_slice,\n            alpha,\n            colormap,\n            window_center,\n            window_width,\n            mask_as_colormap,\n        )\n\n        # Scale if needed for DPI\n        if scale_factor != 1.0:\n            h, w = rgba.shape[:2]\n            new_h = int(h * scale_factor)\n            new_w = int(w * scale_factor)\n            pil_img = PILImage.fromarray(rgba)\n            pil_img = pil_img.resize((new_w, new_h), PILImage.Resampling.LANCZOS)\n        else:\n            pil_img = PILImage.fromarray(rgba)\n\n        # Convert to RGB for JPEG (no alpha support)\n        if format == \"jpeg\":\n            pil_img = pil_img.convert(\"RGB\")\n\n        # Save\n        ext = {\"png\": \".png\", \"jpeg\": \".jpg\", \"tiff\": \".tiff\"}[format]\n        filename = f\"{filename_prefix}_{idx:04d}{ext}\"\n        filepath = out_path / filename\n        pil_img.save(filepath, dpi=(dpi, dpi))\n        saved_files.append(str(filepath))\n\n    return saved_files\n</code></pre>"},{"location":"api/features/intensity/","title":"Intensity Features API","text":""},{"location":"api/features/intensity/#pictologics.features.intensity","title":"<code>pictologics.features.intensity</code>","text":""},{"location":"api/features/intensity/#pictologics.features.intensity--intensity-feature-extraction-module","title":"Intensity Feature Extraction Module","text":"<p>This module provides functions for calculating First Order Statistics (Intensity) features from medical images. It implements the Image Biomarker Standardisation Initiative (IBSI) compliant algorithms.</p>"},{"location":"api/features/intensity/#pictologics.features.intensity--key-features","title":"Key Features:","text":"<ul> <li>First Order Statistics: Mean, Variance, Skewness, Kurtosis, Percentiles, etc.</li> <li>Intensity Histogram: Features based on discretised intensity histograms.</li> <li>Intensity-Volume Histogram (IVH): Volume fractions and intensity fractions, AUC.</li> <li>Spatial Intensity: Moran's I and Geary's C (spatial autocorrelation) [Optimized].</li> <li>Local Intensity: Local and Global Intensity Peaks.</li> </ul>"},{"location":"api/features/intensity/#pictologics.features.intensity--optimization","title":"Optimization:","text":"<p>Uses <code>numba</code> for JIT compilation, with parallel execution for computationally intensive spatial feature calculations.</p>"},{"location":"api/features/intensity/#pictologics.features.intensity.calculate_intensity_features","title":"<code>calculate_intensity_features(values)</code>","text":"<p>Calculate intensity-based features (First Order Statistics) as defined in IBSI 4.1.</p> <p>Computes 18 statistical features from the intensity values within the ROI: mean, variance, skewness, kurtosis, median, min/max, percentiles (10th, 90th), interquartile range, range, MAD variants, coefficient of variation, energy, RMS.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>NDArray[floating[Any]]</code> <p>1D array of intensity values from the ROI (after mask application).</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary mapping feature names (with IBSI codes) to computed values.</p> <code>dict[str, float]</code> <p>Empty dict if input is empty.</p> Example <p>Calculate features from an ROI:</p> <pre><code>from pictologics.features.intensity import calculate_intensity_features\nfrom pictologics.preprocessing import apply_mask\n\n# Get values within ROI\nroi_values = apply_mask(image, mask)\n\n# Calculate features\nfeatures = calculate_intensity_features(roi_values)\nprint(features[\"mean_intensity_Q4LE\"])\n</code></pre> Source code in <code>pictologics/features/intensity.py</code> <pre><code>def calculate_intensity_features(\n    values: npt.NDArray[np.floating[Any]],\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate intensity-based features (First Order Statistics) as defined in IBSI 4.1.\n\n    Computes 18 statistical features from the intensity values within the ROI:\n    mean, variance, skewness, kurtosis, median, min/max, percentiles (10th, 90th),\n    interquartile range, range, MAD variants, coefficient of variation, energy, RMS.\n\n    Args:\n        values: 1D array of intensity values from the ROI (after mask application).\n\n    Returns:\n        Dictionary mapping feature names (with IBSI codes) to computed values.\n        Empty dict if input is empty.\n\n    Example:\n        Calculate features from an ROI:\n\n        ```python\n        from pictologics.features.intensity import calculate_intensity_features\n        from pictologics.preprocessing import apply_mask\n\n        # Get values within ROI\n        roi_values = apply_mask(image, mask)\n\n        # Calculate features\n        features = calculate_intensity_features(roi_values)\n        print(features[\"mean_intensity_Q4LE\"])\n        ```\n    \"\"\"\n    if len(values) == 0:\n        return {}\n\n    features: dict[str, float] = {}\n\n    # 4.1.1 Mean intensity (Q4LE)\n    mean_val = np.mean(values)\n    features[\"mean_intensity_Q4LE\"] = float(mean_val)\n\n    # 4.1.2 Intensity variance (ECT3)\n    var_val = float(np.var(values, ddof=0))\n    features[\"intensity_variance_ECT3\"] = float(var_val)\n\n    # 4.1.3 Intensity skewness (KE2A)\n    if var_val == 0.0:\n        features[\"intensity_skewness_KE2A\"] = np.nan\n        features[\"intensity_kurtosis_IPH6\"] = np.nan\n    else:\n        m2, m3, m4 = _central_moments_2_3_4(values, float(mean_val))\n        denom = m2**1.5\n        if denom != 0.0:\n            features[\"intensity_skewness_KE2A\"] = float(m3 / denom)\n            features[\"intensity_kurtosis_IPH6\"] = float((m4 / (m2 * m2)) - 3.0)\n\n    # 4.1.5 Median intensity (Y12H)\n    median_val = np.median(values)\n    features[\"median_intensity_Y12H\"] = float(median_val)\n\n    # 4.1.6 Minimum intensity (1GSF)\n    min_val = np.min(values)\n    features[\"minimum_intensity_1GSF\"] = float(min_val)\n\n    p10, p25, p75, p90 = np.percentile(values, [10, 25, 75, 90])\n    features[\"10th_intensity_percentile_QG58\"] = float(p10)\n    features[\"90th_intensity_percentile_8DWT\"] = float(p90)\n\n    # 4.1.9 Maximum intensity (84IY)\n    max_val = np.max(values)\n    features[\"maximum_intensity_84IY\"] = float(max_val)\n\n    # 4.1.10 Intensity interquartile range (SALO)\n    features[\"intensity_interquartile_range_SALO\"] = float(p75 - p25)\n\n    # 4.1.11 Intensity range (2OJQ)\n    features[\"intensity_range_2OJQ\"] = float(max_val - min_val)\n\n    # 4.1.12 Mean absolute deviation (4FUA)\n    features[\"intensity_mean_absolute_deviation_4FUA\"] = float(\n        _mean_abs_dev(values, float(mean_val))\n    )\n\n    # 4.1.13 Robust mean absolute deviation (1128)\n    features[\"intensity_robust_mean_absolute_deviation_1128\"] = float(\n        _robust_mean_abs_dev(values, float(p10), float(p90))\n    )\n\n    # 4.1.14 Median absolute deviation (N72L)\n    features[\"intensity_median_absolute_deviation_N72L\"] = float(\n        _mean_abs_dev(values, float(median_val))\n    )\n\n    # 4.1.15 Coefficient of variation (7TET)\n    if mean_val != 0:\n        features[\"intensity_coefficient_of_variation_7TET\"] = float(\n            np.sqrt(var_val) / mean_val\n        )\n    else:\n        features[\"intensity_coefficient_of_variation_7TET\"] = np.nan\n\n    # 4.1.16 Quartile coefficient of dispersion (9S40)\n    if (p75 + p25) != 0:\n        features[\"intensity_quartile_coefficient_of_dispersion_9S40\"] = float(\n            (p75 - p25) / (p75 + p25)\n        )\n    else:\n        features[\"intensity_quartile_coefficient_of_dispersion_9S40\"] = np.nan\n\n    # 4.1.17 Energy (N8CA)\n    energy = float(np.dot(values, values))\n    features[\"intensity_energy_N8CA\"] = energy\n\n    # 4.1.18 Root mean square (5ZWQ)\n    features[\"root_mean_square_intensity_5ZWQ\"] = float(np.sqrt(energy / len(values)))\n\n    return features\n</code></pre>"},{"location":"api/features/intensity/#pictologics.features.intensity.calculate_intensity_histogram_features","title":"<code>calculate_intensity_histogram_features(discretised_values)</code>","text":"<p>Calculate intensity histogram features as defined in IBSI 4.2.</p> <p>Computes features from the discretised intensity histogram including mean, variance, skewness, kurtosis, mode, entropy, uniformity, and gradient features.</p> <p>Parameters:</p> Name Type Description Default <code>discretised_values</code> <code>NDArray[floating[Any]]</code> <p>1D array of discretised intensity values (after binning).</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary mapping feature names (with IBSI codes) to computed values.</p> <code>dict[str, float]</code> <p>Empty dict if input is empty.</p> Source code in <code>pictologics/features/intensity.py</code> <pre><code>def calculate_intensity_histogram_features(\n    discretised_values: npt.NDArray[np.floating[Any]],\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate intensity histogram features as defined in IBSI 4.2.\n\n    Computes features from the discretised intensity histogram including\n    mean, variance, skewness, kurtosis, mode, entropy, uniformity, and gradient features.\n\n    Args:\n        discretised_values: 1D array of discretised intensity values (after binning).\n\n    Returns:\n        Dictionary mapping feature names (with IBSI codes) to computed values.\n        Empty dict if input is empty.\n    \"\"\"\n    if len(discretised_values) == 0:\n        return {}\n\n    features: dict[str, float] = {}\n\n    disc = np.asarray(discretised_values)\n    n = disc.size\n\n    # Support negative values by shifting for bincount compatibility\n    min_val_i = int(np.min(disc))\n    max_val_i = int(np.max(disc))\n\n    shifted = disc.astype(np.int64) - min_val_i\n    counts_full = np.bincount(shifted, minlength=(max_val_i - min_val_i + 1))\n    total = float(n)\n    p = counts_full[counts_full &gt; 0].astype(np.float64) / total\n\n    # 4.2.1 Mean discretised intensity (X6K6)\n    mean_disc = float(np.mean(disc))\n    features[\"mean_discretised_intensity_X6K6\"] = float(mean_disc)\n\n    # 4.2.2 Discretised intensity variance (CH89)\n    var_disc = float(np.var(disc, ddof=0))\n    features[\"discretised_intensity_variance_CH89\"] = float(var_disc)\n\n    # 4.2.3 Discretised intensity skewness (88K1)\n    if var_disc == 0.0:\n        features[\"discretised_intensity_skewness_88K1\"] = np.nan\n        features[\"discretised_intensity_kurtosis_C3I7\"] = np.nan\n    else:\n        m2, m3, m4 = _central_moments_2_3_4(disc, float(mean_disc))\n        denom = m2**1.5\n        if denom != 0.0:\n            features[\"discretised_intensity_skewness_88K1\"] = float(m3 / denom)\n            features[\"discretised_intensity_kurtosis_C3I7\"] = float(\n                (m4 / (m2 * m2)) - 3.0\n            )\n\n    p10, p25, median_val, p75, p90 = np.percentile(disc, [10, 25, 50, 75, 90])\n\n    features[\"median_discretised_intensity_WIFQ\"] = float(median_val)\n    features[\"minimum_discretised_intensity_1PR8\"] = float(min_val_i)\n    features[\"10th_discretised_intensity_percentile_1PR\"] = float(p10)\n    features[\"90th_discretised_intensity_percentile_GPMT\"] = float(p90)\n    features[\"maximum_discretised_intensity_3NCY\"] = float(max_val_i)\n\n    mode_index = int(np.argmax(counts_full))\n    features[\"intensity_histogram_mode_AMMC\"] = float(min_val_i + mode_index)\n\n    features[\"discretised_intensity_interquartile_range_WR0O\"] = float(p75 - p25)\n\n    features[\"discretised_intensity_range_5Z3W\"] = float(\n        features[\"maximum_discretised_intensity_3NCY\"]\n        - features[\"minimum_discretised_intensity_1PR8\"]\n    )\n\n    features[\"intensity_histogram_mean_absolute_deviation_D2ZX\"] = float(\n        _mean_abs_dev(disc, float(mean_disc))\n    )\n\n    features[\"intensity_histogram_robust_mean_absolute_deviation_WRZB\"] = float(\n        _robust_mean_abs_dev(disc, float(p10), float(p90))\n    )\n\n    features[\"intensity_histogram_median_absolute_deviation_4RNL\"] = float(\n        _mean_abs_dev(disc, float(median_val))\n    )\n\n    if mean_disc != 0:\n        features[\"intensity_histogram_coefficient_of_variation_CWYJ\"] = float(\n            np.sqrt(var_disc) / mean_disc\n        )\n    else:\n        features[\"intensity_histogram_coefficient_of_variation_CWYJ\"] = np.nan\n\n    if (p75 + p25) != 0:\n        features[\"intensity_histogram_quartile_coefficient_of_dispersion_SLWD\"] = float(\n            (p75 - p25) / (p75 + p25)\n        )\n    else:\n        features[\"intensity_histogram_quartile_coefficient_of_dispersion_SLWD\"] = np.nan\n\n    # Vectorized entropy/uniformity\n    features[\"discretised_intensity_entropy_TLU2\"] = float(-np.sum(p * np.log2(p)))\n    features[\"discretised_intensity_uniformity_BJ5W\"] = float(np.sum(p * p))\n\n    hist_counts = counts_full.astype(np.float64)\n    if len(hist_counts) &lt; 2:\n        features[\"maximum_histogram_gradient_12CE\"] = np.nan\n        features[\"maximum_histogram_gradient_intensity_8E6O\"] = np.nan\n        features[\"minimum_histogram_gradient_VQB3\"] = np.nan\n        features[\"minimum_histogram_gradient_intensity_RHQZ\"] = np.nan\n    else:\n        gradient = np.gradient(hist_counts)\n        features[\"maximum_histogram_gradient_12CE\"] = float(np.max(gradient))\n        max_grad_idx = int(np.argmax(gradient))\n        features[\"maximum_histogram_gradient_intensity_8E6O\"] = float(\n            min_val_i + max_grad_idx\n        )\n        features[\"minimum_histogram_gradient_VQB3\"] = float(np.min(gradient))\n        min_grad_idx = int(np.argmin(gradient))\n        features[\"minimum_histogram_gradient_intensity_RHQZ\"] = float(\n            min_val_i + min_grad_idx\n        )\n\n    return features\n</code></pre>"},{"location":"api/features/intensity/#pictologics.features.intensity.calculate_ivh_features","title":"<code>calculate_ivh_features(discretised_values, bin_width=None, min_val=None, max_val=None, target_range_min=None, target_range_max=None)</code>","text":"<p>Calculate Intensity-Volume Histogram (IVH) features as defined in IBSI 4.3.</p> <p>Computes volume fractions at intensity thresholds, intensity values at volume fractions, and Area Under the IVH Curve (AUC).</p> <p>Parameters:</p> Name Type Description Default <code>discretised_values</code> <code>NDArray[floating[Any]]</code> <p>1D array of discretised intensity values.</p> required <code>bin_width</code> <code>Optional[float]</code> <p>Optional bin width used in discretisation (for physical units).</p> <code>None</code> <code>min_val</code> <code>Optional[float]</code> <p>Optional minimum value used in discretisation.</p> <code>None</code> <code>max_val</code> <code>Optional[float]</code> <p>Optional maximum value used in discretisation.</p> <code>None</code> <code>target_range_min</code> <code>Optional[float]</code> <p>Optional target range minimum for fraction calculations.</p> <code>None</code> <code>target_range_max</code> <code>Optional[float]</code> <p>Optional target range maximum for fraction calculations.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary mapping feature names (with IBSI codes) to computed values.</p> <code>dict[str, float]</code> <p>Empty dict if input is empty.</p> Source code in <code>pictologics/features/intensity.py</code> <pre><code>def calculate_ivh_features(\n    discretised_values: npt.NDArray[np.floating[Any]],\n    bin_width: Optional[float] = None,\n    min_val: Optional[float] = None,\n    max_val: Optional[float] = None,\n    target_range_min: Optional[float] = None,\n    target_range_max: Optional[float] = None,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate Intensity-Volume Histogram (IVH) features as defined in IBSI 4.3.\n\n    Computes volume fractions at intensity thresholds, intensity values at volume\n    fractions, and Area Under the IVH Curve (AUC).\n\n    Args:\n        discretised_values: 1D array of discretised intensity values.\n        bin_width: Optional bin width used in discretisation (for physical units).\n        min_val: Optional minimum value used in discretisation.\n        max_val: Optional maximum value used in discretisation.\n        target_range_min: Optional target range minimum for fraction calculations.\n        target_range_max: Optional target range maximum for fraction calculations.\n\n    Returns:\n        Dictionary mapping feature names (with IBSI codes) to computed values.\n        Empty dict if input is empty.\n    \"\"\"\n    if len(discretised_values) == 0:\n        return {}\n\n    features: dict[str, float] = {}\n    N = len(discretised_values)\n\n    vals = np.asarray(discretised_values)\n    sorted_vals = np.sort(vals)\n\n    # -------------------------------------------------------------------------\n    # 1. Volume Fractions\n    # -------------------------------------------------------------------------\n    t_min = target_range_min if target_range_min is not None else min_val\n    t_max = target_range_max if target_range_max is not None else max_val\n\n    # Fallback to data min/max if still None\n    if t_min is None or t_max is None:\n        t_min_idx = float(sorted_vals[0])\n        t_max_idx = float(sorted_vals[-1])\n        val_range_idx = t_max_idx - t_min_idx\n\n        def get_volume_fraction_at_intensity_fraction_indices(frac: float) -&gt; float:\n            threshold_idx = t_min_idx + frac * val_range_idx\n            idx = int(np.searchsorted(sorted_vals, threshold_idx, side=\"left\"))\n            count = N - idx\n            return float(count / N)\n\n        features[\"volume_at_intensity_fraction_0.10_BC2M_10\"] = (\n            get_volume_fraction_at_intensity_fraction_indices(0.10)\n        )\n        features[\"volume_at_intensity_fraction_0.90_BC2M_90\"] = (\n            get_volume_fraction_at_intensity_fraction_indices(0.90)\n        )\n\n    else:\n        # We have physical units for the target range.\n        t_range = t_max - t_min\n\n        def get_volume_fraction_at_intensity_fraction_physical(frac: float) -&gt; float:\n            threshold_val = t_min + frac * t_range  # type: ignore\n            if bin_width is not None and min_val is not None:\n                # Convert to index based on FBS\n                mv = min_val\n                bw = bin_width\n                threshold_idx = np.floor((threshold_val - mv) / bw) + 1  # type: ignore\n            else:\n                threshold_idx = threshold_val\n\n            idx = int(np.searchsorted(sorted_vals, threshold_idx, side=\"left\"))\n            count = N - idx\n            return float(count / N)\n\n        features[\"volume_at_intensity_fraction_0.10_BC2M_10\"] = (\n            get_volume_fraction_at_intensity_fraction_physical(0.10)\n        )\n        features[\"volume_at_intensity_fraction_0.90_BC2M_90\"] = (\n            get_volume_fraction_at_intensity_fraction_physical(0.90)\n        )\n\n    features[\n        \"volume_fraction_difference_between_intensity_0.10_and_0.90_fractions_DDTU\"\n    ] = float(\n        features[\"volume_at_intensity_fraction_0.10_BC2M_10\"]\n        - features[\"volume_at_intensity_fraction_0.90_BC2M_90\"]\n    )\n\n    # -------------------------------------------------------------------------\n    # 2. Intensity Fractions\n    # -------------------------------------------------------------------------\n    def get_intensity_at_volume_fraction(vol_frac: float) -&gt; float:\n        # Fast path for standard integer bins (step=1)\n        if (\n            bin_width is not None\n            and min_val is None\n            and bin_width &gt; 0\n            and float(bin_width) == 1.0\n            and np.issubdtype(sorted_vals.dtype, np.integer)\n        ):\n            target_count = int(np.floor(vol_frac * N))\n            if target_count &lt;= 0:\n                return float(sorted_vals[-1])\n\n            # Smallest integer threshold t such that count(vals &gt;= t) &lt;= target_count.\n            # Let k = N - target_count. We need searchsorted(t) &gt;= k.\n            k = N - target_count\n            v = int(sorted_vals[k - 1])\n            t = v + 1\n            vmax = int(sorted_vals[-1])\n            if t &gt; vmax:\n                t = vmax\n            return float(t)\n\n        # Determine candidates\n        if bin_width is not None:\n            g_min = min_val if min_val is not None else np.min(discretised_values)\n            if max_val is not None:\n                g_max = max_val\n            elif min_val is not None:\n                g_max = min_val + np.max(discretised_values) * bin_width\n            else:\n                g_max = np.max(discretised_values)\n\n            if bin_width &gt; 0:\n                num_steps = int(np.round((g_max - g_min) / bin_width))\n                if min_val is not None:\n                    # Candidates are bin centers\n                    idx = np.arange(num_steps, dtype=np.float64)\n                    candidates = g_min + (idx + 0.5) * bin_width\n                else:\n                    idx = np.arange(num_steps + 1, dtype=np.float64)\n                    candidates = g_min + idx * bin_width\n            else:\n                candidates = sorted_vals.astype(np.float64)\n        else:\n            candidates = sorted_vals\n\n        target_count = int(np.floor(vol_frac * N))\n\n        # Binary search\n        low = 0\n        high = len(candidates) - 1\n        ans_idx = -1\n\n        while low &lt;= high:\n            mid = (low + high) // 2\n            val = candidates[mid]\n\n            # Convert physical value to index if in discrete mode\n            if bin_width is not None and min_val is not None and bin_width &gt; 0:\n                check_val = np.floor((val - min_val) / bin_width) + 1\n            else:\n                check_val = val\n\n            idx = np.searchsorted(sorted_vals, check_val, side=\"left\")\n            count = N - idx\n\n            if count &lt;= target_count:\n                ans_idx = mid\n                high = mid - 1\n            else:\n                low = mid + 1\n\n        if ans_idx != -1:\n            return float(candidates[ans_idx])\n        else:\n            return float(candidates[-1])\n\n    features[\"intensity_at_volume_fraction_0.10_GBPN_10\"] = (\n        get_intensity_at_volume_fraction(0.10)\n    )\n    features[\"intensity_at_volume_fraction_0.90_GBPN_90\"] = (\n        get_intensity_at_volume_fraction(0.90)\n    )\n\n    features[\n        \"intensity_fraction_difference_between_volume_0.10_and_0.90_fractions_CNV2\"\n    ] = float(\n        features[\"intensity_at_volume_fraction_0.10_GBPN_10\"]\n        - features[\"intensity_at_volume_fraction_0.90_GBPN_90\"]\n    )\n\n    # -------------------------------------------------------------------------\n    # 3. Area Under the IVH Curve (AUC)\n    # -------------------------------------------------------------------------\n    # IVH Curve: Volume Fraction (phi) vs Intensity (I)\n    # We construct the curve points from the unique values in the data.\n    unique_vals = np.unique(sorted_vals)\n    if len(unique_vals) == 1:\n        # If there is only one discretised intensity, AUC is 0 by definition.\n        features[\"area_under_the_ivh_curve_9CMM\"] = 0.0\n    else:\n        # P(X &gt;= i)\n        # For each unique value, calculate fraction &gt;= value\n\n        # If we have physical mapping, map unique_vals to physical intensities\n        if bin_width is not None and min_val is not None:\n            # Map index to physical center: min_val + (idx - 0.5) * w ?\n            # Standard FBS mapping: index k corresponds to [min + (k-1)w, min + kw)\n            # center = min_val + (k - 1 + 0.5) * w\n            # k is the value in unique_vals\n            intensities_arr = (\n                min_val + (unique_vals.astype(np.float64) - 0.5) * bin_width\n            )\n        else:\n            intensities_arr = unique_vals.astype(np.float64)\n\n        # Calculate volume fractions\n        # searchsorted returns first index where val fits.\n        # Since sorted_vals is sorted, all elements &gt;= val are from searchsorted(val) onwards.\n        indices = np.searchsorted(sorted_vals, unique_vals, side=\"left\")\n        counts = N - indices\n        fractions = counts.astype(np.float64) / float(N)\n\n        # Riemann Sum (Trapezoidal)\n        # Integrate fraction(I) over I.\n        auc = 0.0\n        for k in range(1, len(intensities_arr)):\n            i_curr = intensities_arr[k]\n            i_prev = intensities_arr[k - 1]\n            phi_curr = fractions[k]\n            phi_prev = fractions[k - 1]\n\n            # Trapezoid area\n            width = i_curr - i_prev\n            avg_height = (phi_curr + phi_prev) * 0.5\n            auc += width * avg_height\n\n        features[\"area_under_the_ivh_curve_9CMM\"] = float(auc)\n\n    return features\n</code></pre>"},{"location":"api/features/intensity/#pictologics.features.intensity.calculate_local_intensity_features","title":"<code>calculate_local_intensity_features(image, mask, *, enabled=True)</code>","text":"<p>Calculate local intensity features: Local and Global Intensity Peak (IBSI 4.5).</p> <p>Computes intensity peaks using a 1 cm\u00b3 spherical neighborhood (radius ~6.2mm). Global peak is the maximum local mean, local peak is the local mean at max intensity.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image object containing intensity data.</p> required <code>mask</code> <code>Image</code> <p>Image object containing the ROI mask.</p> required <code>enabled</code> <code>bool</code> <p>If False, returns empty dict immediately (for performance).</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with 'global_intensity_peak_0F91' and 'local_intensity_peak_VJGA'.</p> Source code in <code>pictologics/features/intensity.py</code> <pre><code>def calculate_local_intensity_features(\n    image: Image,\n    mask: Image,\n    *,\n    enabled: bool = True,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate local intensity features: Local and Global Intensity Peak (IBSI 4.5).\n\n    Computes intensity peaks using a 1 cm\u00b3 spherical neighborhood (radius ~6.2mm).\n    Global peak is the maximum local mean, local peak is the local mean at max intensity.\n\n    Args:\n        image: Image object containing intensity data.\n        mask: Image object containing the ROI mask.\n        enabled: If False, returns empty dict immediately (for performance).\n\n    Returns:\n        Dictionary with 'global_intensity_peak_0F91' and 'local_intensity_peak_VJGA'.\n    \"\"\"\n    if not enabled:\n        return {}\n\n    features: dict[str, float] = {}\n\n    mask_array = mask.array\n    data = image.array\n    spacing_tuple = (\n        float(image.spacing[0]),\n        float(image.spacing[1]),\n        float(image.spacing[2]),\n    )\n\n    # Radius for 1 cm^3 sphere\n    radius_mm = 6.2035\n\n    # Get ROI indices\n    x_idx, y_idx, z_idx = np.where(mask_array &gt; 0)\n    if len(x_idx) == 0:\n        return features\n\n    mask_indices = np.ascontiguousarray(\n        np.stack([x_idx, y_idx, z_idx], axis=1).astype(np.int32)\n    )\n    offsets = _sphere_offsets_for_radius(spacing_tuple, radius_mm)\n\n    # Calculate local means only for ROI voxels\n    roi_means = _calculate_local_mean_numba(data, mask_indices, offsets)\n\n    # Compute both peaks without allocating ROI intensity arrays.\n    global_peak, local_peak = _calculate_local_peaks_numba(\n        data, mask_indices, roi_means\n    )\n    features[\"global_intensity_peak_0F91\"] = float(global_peak)\n    features[\"local_intensity_peak_VJGA\"] = float(local_peak)\n\n    return features\n</code></pre>"},{"location":"api/features/intensity/#pictologics.features.intensity.calculate_spatial_intensity_features","title":"<code>calculate_spatial_intensity_features(image, mask, *, enabled=True)</code>","text":"<p>Calculate spatial intensity features: Moran's I and Geary's C (IBSI 4.4).</p> <p>These features measure spatial autocorrelation of intensity values within the ROI. Computationally intensive (O(N\u00b2) where N = number of ROI voxels).</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image object containing intensity data.</p> required <code>mask</code> <code>Image</code> <p>Image object containing the ROI mask.</p> required <code>enabled</code> <code>bool</code> <p>If False, returns empty dict immediately (for performance).</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with 'morans_i_index_N365' and 'gearys_c_measure_NPT7'.</p> <code>dict[str, float]</code> <p>Returns NaN values if ROI has fewer than 2 voxels or constant intensity.</p> Source code in <code>pictologics/features/intensity.py</code> <pre><code>def calculate_spatial_intensity_features(\n    image: Image,\n    mask: Image,\n    *,\n    enabled: bool = True,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate spatial intensity features: Moran's I and Geary's C (IBSI 4.4).\n\n    These features measure spatial autocorrelation of intensity values within the ROI.\n    Computationally intensive (O(N\u00b2) where N = number of ROI voxels).\n\n    Args:\n        image: Image object containing intensity data.\n        mask: Image object containing the ROI mask.\n        enabled: If False, returns empty dict immediately (for performance).\n\n    Returns:\n        Dictionary with 'morans_i_index_N365' and 'gearys_c_measure_NPT7'.\n        Returns NaN values if ROI has fewer than 2 voxels or constant intensity.\n    \"\"\"\n    if not enabled:\n        return {}\n\n    features: dict[str, float] = {}\n\n    mask_array = mask.array\n    data = image.array\n    sx, sy, sz = (\n        float(image.spacing[0]),\n        float(image.spacing[1]),\n        float(image.spacing[2]),\n    )\n\n    # Get ROI indices (X, Y, Z)\n    x_idx, y_idx, z_idx = np.where(mask_array &gt; 0)\n\n    if len(x_idx) &lt; 2:\n        features[\"morans_i_index_N365\"] = np.nan\n        features[\"gearys_c_measure_NPT7\"] = np.nan\n        return features\n\n    xi = np.ascontiguousarray(x_idx.astype(np.int32))\n    yi = np.ascontiguousarray(y_idx.astype(np.int32))\n    zi = np.ascontiguousarray(z_idx.astype(np.int32))\n\n    intensities = np.ascontiguousarray(data[mask_array &gt; 0].astype(np.float64))\n\n    N = len(intensities)\n    mean_int = np.mean(intensities)\n\n    # Calculate terms using Parallelized Numba Function\n    numer_moran, numer_geary_1, numer_geary_2, sum_weights = (\n        _calculate_spatial_features_numba(\n            xi, yi, zi, intensities, float(mean_int), sx, sy, sz\n        )\n    )\n\n    # Moran's I - N365\n    denom = _sum_sq_centered(intensities, float(mean_int))\n\n    if denom != 0 and sum_weights != 0:\n        moran_i = (N / sum_weights) * (numer_moran / denom)\n        features[\"morans_i_index_N365\"] = float(moran_i)\n    else:\n        features[\"morans_i_index_N365\"] = np.nan\n\n    # Geary's C - NPT7\n    if denom != 0 and sum_weights != 0:\n        numer = 2 * numer_geary_1 - 2 * numer_geary_2\n        geary_c = ((N - 1) / (2 * sum_weights)) * (numer / denom)\n        features[\"gearys_c_measure_NPT7\"] = float(geary_c)\n    else:\n        features[\"gearys_c_measure_NPT7\"] = np.nan\n\n    return features\n</code></pre>"},{"location":"api/features/morphology/","title":"Morphology Features API","text":""},{"location":"api/features/morphology/#pictologics.features.morphology","title":"<code>pictologics.features.morphology</code>","text":""},{"location":"api/features/morphology/#pictologics.features.morphology--morphology-feature-extraction-module","title":"Morphology Feature Extraction Module","text":"<p>This module provides functions for calculating Morphological (Shape and Size) features from medical images. It implements the Image Biomarker Standardisation Initiative (IBSI) compliant algorithms.</p>"},{"location":"api/features/morphology/#pictologics.features.morphology--key-features","title":"Key Features:","text":"<ul> <li>Voxel-based: Volume (voxel counting).</li> <li>Mesh-based: Surface Area, Volume (mesh), Compactness, Sphericity.</li> <li>PCA-based: Major/Minor/Least Axis Length, Elongation, Flatness.</li> <li>Convex Hull: Volume, Area, Max 3D Diameter.</li> <li>Bounding Box: Oriented (OMBB) and Axis-Aligned (AABB) Bounding Boxes.</li> <li>Minimum Volume Enclosing Ellipsoid (MVEE): Volume, Area.</li> <li>Intensity-Weighted: Center of Mass Shift, Integrated Intensity.</li> </ul>"},{"location":"api/features/morphology/#pictologics.features.morphology--optimization","title":"Optimization:","text":"<p>Uses <code>numba</code> for optimizing the Khachiyan algorithm for MVEE calculation.</p> Example <p>Calculate morphology features from a mask:</p> <pre><code>import numpy as np\nfrom pictologics.loader import Image\nfrom pictologics.features.morphology import calculate_morphology_features\n\n# Create dummy mask\nmask_arr = np.zeros((50, 50, 50), dtype=np.uint8)\nmask_arr[10:40, 10:40, 10:40] = 1\nmask = Image(mask_arr, spacing=(1.0, 1.0, 1.0), origin=(0,0,0))\n\n# Calculate features\nfeatures = calculate_morphology_features(mask)\nprint(features[\"volume_voxel_counting_YEKZ\"])\n</code></pre>"},{"location":"api/features/morphology/#pictologics.features.morphology.calculate_morphology_features","title":"<code>calculate_morphology_features(mask, image=None, intensity_mask=None)</code>","text":"<p>Calculate morphological features from the ROI mask. Includes both voxel-based and mesh-based features (IBSI compliant).</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Image</code> <p>Image object containing the binary mask (Morphological Mask).</p> required <code>image</code> <code>Optional[Image]</code> <p>Optional Image object containing intensity data (required for some features).</p> <code>None</code> <code>intensity_mask</code> <code>Optional[Image]</code> <p>Optional Image object containing the intensity mask (e.g. after outlier filtering).             If provided, used for intensity-weighted features (99N0, KLMA).             If None, defaults to <code>mask</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary of calculated features.</p> Source code in <code>pictologics/features/morphology.py</code> <pre><code>def calculate_morphology_features(\n    mask: Image, image: Optional[Image] = None, intensity_mask: Optional[Image] = None\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate morphological features from the ROI mask.\n    Includes both voxel-based and mesh-based features (IBSI compliant).\n\n    Args:\n        mask: Image object containing the binary mask (Morphological Mask).\n        image: Optional Image object containing intensity data (required for some features).\n        intensity_mask: Optional Image object containing the intensity mask (e.g. after outlier filtering).\n                        If provided, used for intensity-weighted features (99N0, KLMA).\n                        If None, defaults to `mask`.\n\n    Returns:\n        Dictionary of calculated features.\n    \"\"\"\n    features: dict[str, float] = {}\n    i_mask = intensity_mask if intensity_mask is not None else mask\n\n    # 1. Voxel Based Features\n    voxel_volume = np.prod(mask.spacing)\n    n_voxels = np.sum(mask.array)\n    features[\"volume_voxel_counting_YEKZ\"] = float(n_voxels * voxel_volume)\n\n    # 2. Mesh Based Features\n    mesh_feats, verts, faces = _get_mesh_features(mask)\n    features.update(mesh_feats)\n\n    if verts is None or faces is None:\n        return features\n\n    mesh_volume = features.get(\"volume_RNU0\", 0.0)\n    surface_area = features.get(\"surface_area_C0JK\", 0.0)\n\n    # 3. Shape Features\n    features.update(_get_shape_features(surface_area, mesh_volume))\n\n    # 4. PCA Based Features\n    pca_feats, evals, evecs = _get_pca_features(mask, mesh_volume, surface_area)\n    features.update(pca_feats)\n\n    # 5. Convex Hull Features\n    hull_feats, hull = _get_convex_hull_features(verts, mesh_volume, surface_area)\n    features.update(hull_feats)\n\n    # 6. Bounding Box Features\n    features.update(_get_bounding_box_features(verts, evecs, mesh_volume, surface_area))\n\n    # 7. MVEE Features\n    features.update(_get_mvee_features(hull, verts, mesh_volume, surface_area))\n\n    # 8. Intensity Based Features\n    if image is not None:\n        features.update(\n            _get_intensity_morphology_features(mask, image, i_mask, mesh_volume)\n        )\n\n    return features\n</code></pre>"},{"location":"api/features/texture/","title":"Texture Features API","text":""},{"location":"api/features/texture/#pictologics.features.texture","title":"<code>pictologics.features.texture</code>","text":""},{"location":"api/features/texture/#pictologics.features.texture--texture-feature-extraction-module","title":"Texture Feature Extraction Module","text":"<p>This module provides a comprehensive suite of functions for calculating 3D texture features from medical images. It implements the Image Biomarker Standardisation Initiative (IBSI) compliant algorithms for various texture matrices.</p>"},{"location":"api/features/texture/#pictologics.features.texture--key-concepts","title":"Key Concepts:","text":"<p>Texture analysis quantifies the spatial arrangement of grey levels in an image. It assumes that the texture (e.g., \"smooth\", \"coarse\", \"regular\") is contained in the spatial relationship between the grey levels of the voxels.</p>"},{"location":"api/features/texture/#pictologics.features.texture--implemented-matrices","title":"Implemented Matrices:","text":"<ol> <li> <p>GLCM (Grey Level Co-occurrence Matrix):     Counts how often pairs of grey levels occur at a specific distance and direction.     Captures: Contrast, homogeneity, correlation.</p> </li> <li> <p>GLRLM (Grey Level Run Length Matrix):     Counts the lengths of consecutive runs of the same grey level.     Captures: Coarseness, directionality.</p> </li> <li> <p>GLSZM (Grey Level Size Zone Matrix):     Counts the size of zones (connected components) of the same grey level.     Captures: Regional homogeneity, size distribution of texture elements.</p> </li> <li> <p>GLDZM (Grey Level Distance Zone Matrix):     Counts zones based on their distance from the ROI border.     Captures: Spatial distribution relative to the boundary.</p> </li> <li> <p>NGTDM (Neighbourhood Grey Tone Difference Matrix):     Quantifies the difference between a voxel and its neighbours.     Captures: Human perception of texture (coarseness, contrast, busyness).</p> </li> <li> <p>NGLDM (Neighbourhood Grey Level Dependence Matrix):     Captures the dependence of grey levels on their neighbours.     Captures: Dependence, spatial relationships.</p> </li> </ol>"},{"location":"api/features/texture/#pictologics.features.texture--optimization","title":"Optimization:","text":"<p>This module uses <code>numba</code> for Just-In-Time (JIT) compilation to achieve high performance. The core calculations are parallelized and optimized for memory usage. - Single-pass calculation: Multiple matrices are computed in a single pass over the image   to minimize memory access overhead. - Flattened DFS: Zone-based features (GLSZM, GLDZM) use a memory-efficient Depth-First Search   with flattened stack indices.</p>"},{"location":"api/features/texture/#pictologics.features.texture--usage","title":"Usage:","text":"<p>The main entry point is <code>calculate_all_texture_matrices</code>, which computes all raw matrices. Then, specific feature calculation functions (e.g., <code>calculate_glcm_features</code>) can be called using these matrices.</p> Example <p>Calculate texture features:</p> <pre><code>import numpy as np\nfrom pictologics.features.texture import (\n    calculate_all_texture_matrices,\n    calculate_glcm_features\n)\n\n# Create dummy data\ndata = np.random.randint(1, 33, (50, 50, 50))\nmask = np.ones((50, 50, 50))\n\n# Calculate matrices\nmatrices = calculate_all_texture_matrices(data, mask, n_bins=32)\n\n# Extract features\nglcm_feats = calculate_glcm_features(\n    data,\n    mask,\n    n_bins=32,\n    glcm_matrix=matrices['glcm']\n)\nprint(glcm_feats['contrast_ACUI'])\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_all_texture_features","title":"<code>calculate_all_texture_features(disc_array, mask_array, n_bins, distance_mask_array=None, ngldm_alpha=0)</code>","text":"<p>Calculate all texture features (GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM).</p> <p>This is a convenience wrapper that computes all texture matrices and then extracts all available features.</p> <p>Parameters:</p> Name Type Description Default <code>disc_array</code> <code>NDArray[floating[Any]]</code> <p>Discretised image array.</p> required <code>mask_array</code> <code>NDArray[floating[Any]]</code> <p>Mask array (ROI).</p> required <code>n_bins</code> <code>int</code> <p>Number of bins.</p> required <code>distance_mask_array</code> <code>Optional[NDArray[floating[Any]]]</code> <p>Optional mask for GLDZM distance calculation.                  If None, mask_array is used.</p> <code>None</code> <code>ngldm_alpha</code> <code>int</code> <p>The coarseness parameter \u03b1 for NGLDM. Two grey levels are considered dependent if their absolute difference is \u2264 \u03b1. Default is 0 (IBSI standard).</p> <code>0</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary of all texture features.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_all_texture_features(\n    disc_array: npt.NDArray[np.floating[Any]],\n    mask_array: npt.NDArray[np.floating[Any]],\n    n_bins: int,\n    distance_mask_array: Optional[npt.NDArray[np.floating[Any]]] = None,\n    ngldm_alpha: int = 0,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate all texture features (GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM).\n\n    This is a convenience wrapper that computes all texture matrices and then\n    extracts all available features.\n\n    Args:\n        disc_array: Discretised image array.\n        mask_array: Mask array (ROI).\n        n_bins: Number of bins.\n        distance_mask_array: Optional mask for GLDZM distance calculation.\n                             If None, mask_array is used.\n        ngldm_alpha: The coarseness parameter \u03b1 for NGLDM. Two grey levels are considered\n            dependent if their absolute difference is \u2264 \u03b1. Default is 0 (IBSI standard).\n\n    Returns:\n        Dictionary of all texture features.\n    \"\"\"\n    results = {}\n\n    # Calculate all matrices once\n    texture_matrices = calculate_all_texture_matrices(\n        disc_array,\n        mask_array,\n        n_bins,\n        distance_mask=distance_mask_array,\n        ngldm_alpha=ngldm_alpha,\n    )\n\n    # GLCM\n    results.update(\n        calculate_glcm_features(\n            disc_array, mask_array, n_bins, glcm_matrix=texture_matrices[\"glcm\"]\n        )\n    )\n\n    # GLRLM\n    results.update(\n        calculate_glrlm_features(\n            disc_array, mask_array, n_bins, glrlm_matrix=texture_matrices[\"glrlm\"]\n        )\n    )\n\n    # GLSZM\n    results.update(\n        calculate_glszm_features(\n            disc_array, mask_array, n_bins, glszm_matrix=texture_matrices[\"glszm\"]\n        )\n    )\n\n    # GLDZM\n    results.update(\n        calculate_gldzm_features(\n            disc_array,\n            mask_array,\n            n_bins,\n            gldzm_matrix=texture_matrices[\"gldzm\"],\n            distance_mask=(\n                distance_mask_array if distance_mask_array is not None else mask_array\n            ),\n        )\n    )\n\n    # NGTDM\n    results.update(\n        calculate_ngtdm_features(\n            disc_array,\n            mask_array,\n            n_bins,\n            ngtdm_matrices=(texture_matrices[\"ngtdm_s\"], texture_matrices[\"ngtdm_n\"]),\n        )\n    )\n    # NGLDM\n    results.update(\n        calculate_ngldm_features(\n            disc_array, mask_array, n_bins, ngldm_matrix=texture_matrices[\"ngldm\"]\n        )\n    )\n\n    return results\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_all_texture_matrices","title":"<code>calculate_all_texture_matrices(data, mask, n_bins, distance_mask=None, ngldm_alpha=0)</code>","text":"<p>Calculate all texture matrices (GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM) in an optimized single pass.</p> <p>This function serves as the computational backbone for texture analysis. It computes the raw matrices required to extract specific texture features. By aggregating these calculations, it minimizes the number of passes over the image data, significantly improving performance.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[floating[Any]]</code> <p>The 3D image array containing discretised grey levels. Values should be integers in the range [1, n_bins].</p> required <code>mask</code> <code>NDArray[floating[Any]]</code> <p>The 3D binary mask array defining the Region of Interest (ROI). Must have the same shape as <code>data</code>. Non-zero values indicate the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels used for discretization (e.g., 16, 32, 64). This determines the size of the resulting matrices.</p> required <code>distance_mask</code> <code>Optional[NDArray[floating[Any]]]</code> <p>Optional mask used to calculate the distance map for GLDZM. If None, <code>mask</code> is used. This allows calculating distances based on the morphological mask while analyzing intensities from the intensity mask (e.g., after outlier filtering).</p> <code>None</code> <code>ngldm_alpha</code> <code>int</code> <p>The coarseness parameter \u03b1 for NGLDM calculation. Two grey levels are considered dependent if their absolute difference is \u2264 \u03b1. Default is 0 (exact match), which is the IBSI standard. Use \u03b1=1 for tolerance of \u00b11 grey level difference.</p> <code>0</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: A dictionary containing the calculated texture matrices: - 'glcm' (npt.NDArray[np.floating[Any]]): Grey Level Co-occurrence Matrix. Shape: (n_dirs, n_bins, n_bins). - 'glrlm' (npt.NDArray[np.floating[Any]]): Grey Level Run Length Matrix. Shape: (n_dirs, n_bins, max_run_length). - 'ngtdm_s' (npt.NDArray[np.floating[Any]]): NGTDM Sum of absolute differences. Shape: (n_bins,). - 'ngtdm_n' (npt.NDArray[np.floating[Any]]): NGTDM Number of valid voxels. Shape: (n_bins,). - 'ngldm' (npt.NDArray[np.floating[Any]]): Neighbouring Grey Level Dependence Matrix. Shape: (n_bins, n_dependence). - 'glszm' (npt.NDArray[np.floating[Any]]): Grey Level Size Zone Matrix. Shape: (n_bins, max_zone_size). - 'gldzm' (npt.NDArray[np.floating[Any]]): Grey Level Distance Zone Matrix. Shape: (n_bins, max_distance).</p> Example <p>Calculate all texture matrices:</p> <pre><code>import numpy as np\nfrom pictologics.features.texture import calculate_all_texture_matrices\n\n# Create dummy data\ndata = np.random.randint(1, 33, (50, 50, 50))\nmask = np.ones((50, 50, 50))\n\n# Calculate matrices\nmatrices = calculate_all_texture_matrices(data, mask, n_bins=32)\nprint(matrices['glcm'].shape)\n# (13, 32, 32)\n</code></pre> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_all_texture_matrices(\n    data: npt.NDArray[np.floating[Any]],\n    mask: npt.NDArray[np.floating[Any]],\n    n_bins: int,\n    distance_mask: Optional[npt.NDArray[np.floating[Any]]] = None,\n    ngldm_alpha: int = 0,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Calculate all texture matrices (GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM) in an optimized single pass.\n\n    This function serves as the computational backbone for texture analysis. It computes the raw\n    matrices required to extract specific texture features. By aggregating these calculations,\n    it minimizes the number of passes over the image data, significantly improving performance.\n\n    Args:\n        data (npt.NDArray[np.floating[Any]]): The 3D image array containing discretised grey levels.\n            Values should be integers in the range [1, n_bins].\n        mask (npt.NDArray[np.floating[Any]]): The 3D binary mask array defining the Region of Interest (ROI).\n            Must have the same shape as `data`. Non-zero values indicate the ROI.\n        n_bins (int): The number of grey levels used for discretization (e.g., 16, 32, 64).\n            This determines the size of the resulting matrices.\n        distance_mask (Optional[npt.NDArray[np.floating[Any]]]): Optional mask used to calculate the distance map for GLDZM.\n            If None, `mask` is used. This allows calculating distances based on the morphological mask\n            while analyzing intensities from the intensity mask (e.g., after outlier filtering).\n        ngldm_alpha (int): The coarseness parameter \u03b1 for NGLDM calculation. Two grey levels are\n            considered dependent if their absolute difference is \u2264 \u03b1. Default is 0 (exact match),\n            which is the IBSI standard. Use \u03b1=1 for tolerance of \u00b11 grey level difference.\n\n    Returns:\n        dict[str, Any]: A dictionary containing the calculated texture matrices:\n            - 'glcm' (npt.NDArray[np.floating[Any]]): Grey Level Co-occurrence Matrix. Shape: (n_dirs, n_bins, n_bins).\n            - 'glrlm' (npt.NDArray[np.floating[Any]]): Grey Level Run Length Matrix. Shape: (n_dirs, n_bins, max_run_length).\n            - 'ngtdm_s' (npt.NDArray[np.floating[Any]]): NGTDM Sum of absolute differences. Shape: (n_bins,).\n            - 'ngtdm_n' (npt.NDArray[np.floating[Any]]): NGTDM Number of valid voxels. Shape: (n_bins,).\n            - 'ngldm' (npt.NDArray[np.floating[Any]]): Neighbouring Grey Level Dependence Matrix. Shape: (n_bins, n_dependence).\n            - 'glszm' (npt.NDArray[np.floating[Any]]): Grey Level Size Zone Matrix. Shape: (n_bins, max_zone_size).\n            - 'gldzm' (npt.NDArray[np.floating[Any]]): Grey Level Distance Zone Matrix. Shape: (n_bins, max_distance).\n\n    Example:\n        Calculate all texture matrices:\n\n        ```python\n        import numpy as np\n        from pictologics.features.texture import calculate_all_texture_matrices\n\n        # Create dummy data\n        data = np.random.randint(1, 33, (50, 50, 50))\n        mask = np.ones((50, 50, 50))\n\n        # Calculate matrices\n        matrices = calculate_all_texture_matrices(data, mask, n_bins=32)\n        print(matrices['glcm'].shape)\n        # (13, 32, 32)\n        ```\n    \"\"\"\n    # Fast exit for empty ROI\n    if not bool(np.any(mask != 0)):\n        return {\n            \"glcm\": np.zeros((13, n_bins, n_bins), dtype=np.uint64),\n            \"glrlm\": np.zeros((13, n_bins, 1), dtype=np.uint64),\n            \"ngtdm_s\": np.zeros((n_bins,), dtype=np.float64),\n            \"ngtdm_n\": np.zeros((n_bins,), dtype=np.float64),\n            \"ngldm\": np.zeros((n_bins, 27), dtype=np.uint64),\n            \"glszm\": np.zeros((n_bins, 1), dtype=np.uint32),\n            \"gldzm\": np.zeros((n_bins, 1), dtype=np.uint32),\n        }\n\n    # Crop to ROI bounding box (union with distance_mask when provided) to reduce memory traffic.\n    data_c, mask_c, distmask_c = _maybe_crop_to_bbox(data, mask, distance_mask)\n\n    # Use a compact mask representation for kernels.\n    if mask_c.dtype == np.uint8:\n        mask_u8 = mask_c\n    else:\n        mask_u8 = (mask_c != 0).astype(np.uint8)\n    # 1. Local Features (GLCM, GLRLM, NGTDM, NGLDM)\n    # Pre-cast data to smallest possible int type (0-based)\n    # Input data is 1-based, so we subtract 1.\n    if n_bins &lt;= 256:\n        data_int = (data_c - 1).astype(np.uint8)\n    else:\n        data_int = (data_c - 1).astype(np.int32)\n\n    try:\n        n_threads = int(numba.config.NUMBA_NUM_THREADS)\n    except (ValueError, TypeError):\n        n_threads = 1  # Fallback\n\n    glcm, glrlm, ngtdm_s, ngtdm_n, ngldm = _calculate_local_features_numba(\n        data_int,\n        mask_u8,\n        n_bins,\n        calc_glcm=True,\n        calc_glrlm=True,\n        calc_ngtdm=True,\n        calc_ngldm=True,\n        offsets_26=OFFSETS_26,\n        directions_13=DIRECTIONS_13,\n        ngldm_alpha=ngldm_alpha,\n        n_threads=n_threads,\n    )\n\n    # 2. Zone Features (GLSZM, GLDZM)\n    # Pre-calculate distance map for GLDZM\n    # Use distance_mask if provided, else mask\n    d_mask = distmask_c if distmask_c is not None else mask_u8\n\n    # Pad the mask with 0s to ensure the image border is treated as an edge.\n    mask_bool = d_mask &gt; 0\n    mask_padded = np.pad(mask_bool, 1, mode=\"constant\", constant_values=0)\n    dist_map_padded = distance_transform_cdt(mask_padded, metric=\"taxicab\").astype(\n        np.int32\n    )\n    dist_map = dist_map_padded[1:-1, 1:-1, 1:-1]\n\n    glszm, gldzm = calculate_zone_features(\n        data_c,\n        mask_u8,\n        dist_map,\n        n_bins,\n        calc_glszm=True,\n        calc_gldzm=True,\n    )\n\n    return {\n        \"glcm\": glcm,\n        \"glrlm\": glrlm,\n        \"ngtdm_s\": ngtdm_s,\n        \"ngtdm_n\": ngtdm_n,\n        \"ngldm\": ngldm,\n        \"glszm\": glszm,\n        \"gldzm\": gldzm,\n    }\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_glcm_features","title":"<code>calculate_glcm_features(data, mask, n_bins, glcm_matrix=None)</code>","text":"<pre><code>Calculate Grey Level Co-occurrence Matrix (GLCM) features.\n\nThe GLCM describes the second-order statistical distribution of grey levels in the ROI.\nIt counts how often pairs of grey levels occur at a specific distance and direction.\nThis implementation computes features based on the 3D merged GLCM (averaged over all 13 directions),\nmaking the features rotationally invariant.\n\n**IBSI Reference**: Section 3.6 (Grey Level Co-occurrence Based Features).\n\n**Mathematical Definition**:\nLet $P(i,j)$ be the co-occurrence matrix, where $i$ and $j$ are grey levels.\nThe matrix is normalized such that $\\sum_{i,j} P(i,j) = 1$.\n\n**Calculated Features**:\n*   Joint Maximum (GYBY)\n*   Joint Average (60VM)\n*   Joint Variance (UR99)\n*   Joint Entropy (TU9B)\n*   Difference Average (TF7R)\n*   Difference Variance (D3YU)\n*   Difference Entropy (NTRS)\n*   Sum Average (ZGXS)\n*   Sum Variance (OEEB)\n*   Sum Entropy (P6QZ)\n*   Angular Second Moment (8ZQL)\n*   Contrast (ACUI)\n*   Dissimilarity (8S9J)\n*   Inverse Difference (IB1Z)\n*   Normalised Inverse Difference (NDRX)\n*   Inverse Difference Moment (WF0Z)\n*   Normalised Inverse Difference Moment (1QCO)\n*   Inverse Variance (E8JP)\n*   Correlation (NI2N)\n*   Autocorrelation (QWB0)\n*   Cluster Tendency (DG8W)\n*   Cluster Shade (7NFM)\n*   Cluster Prominence (AE86)\n*   Information Correlation 1 (R8DG)\n*   Information Correlation 2 (JN9H)\n\nArgs:\n    data (npt.NDArray[np.floating[Any]]): The 3D image array containing discretised grey levels.\n    mask (npt.NDArray[np.floating[Any]]): The 3D binary mask array defining the ROI.\n    n_bins (int): The number of grey levels.\n    glcm_matrix (Optional[npt.NDArray[np.floating[Any]]]): Pre-calculated GLCM matrix. If provided, `data` and `mask`\n        are ignored for matrix calculation, but `data` is still used for `Ng` estimation if needed.\n        If None, the matrix is calculated from scratch.\n\nReturns:\n    dict[str, float]: A dictionary of calculated GLCM features, keyed by their name and IBSI code.\n        Example keys: 'joint_maximum_GYBY', 'contrast_ACUI', 'correlation_NI2N'.\n\nExample:\n    ```python\n    import numpy as np\n</code></pre> <p>from numpy import typing as npt         # ... assuming data and mask defined ...         features = calculate_glcm_features(data, mask, n_bins=32)         print(features['contrast_ACUI'])         ```         12.5</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_glcm_features(\n    data: npt.NDArray[np.floating[Any]],\n    mask: npt.NDArray[np.floating[Any]],\n    n_bins: int,\n    glcm_matrix: Optional[npt.NDArray[np.floating[Any]]] = None,\n) -&gt; dict[str, float]:\n    r\"\"\"\n        Calculate Grey Level Co-occurrence Matrix (GLCM) features.\n\n        The GLCM describes the second-order statistical distribution of grey levels in the ROI.\n        It counts how often pairs of grey levels occur at a specific distance and direction.\n        This implementation computes features based on the 3D merged GLCM (averaged over all 13 directions),\n        making the features rotationally invariant.\n\n        **IBSI Reference**: Section 3.6 (Grey Level Co-occurrence Based Features).\n\n        **Mathematical Definition**:\n        Let $P(i,j)$ be the co-occurrence matrix, where $i$ and $j$ are grey levels.\n        The matrix is normalized such that $\\sum_{i,j} P(i,j) = 1$.\n\n        **Calculated Features**:\n        *   Joint Maximum (GYBY)\n        *   Joint Average (60VM)\n        *   Joint Variance (UR99)\n        *   Joint Entropy (TU9B)\n        *   Difference Average (TF7R)\n        *   Difference Variance (D3YU)\n        *   Difference Entropy (NTRS)\n        *   Sum Average (ZGXS)\n        *   Sum Variance (OEEB)\n        *   Sum Entropy (P6QZ)\n        *   Angular Second Moment (8ZQL)\n        *   Contrast (ACUI)\n        *   Dissimilarity (8S9J)\n        *   Inverse Difference (IB1Z)\n        *   Normalised Inverse Difference (NDRX)\n        *   Inverse Difference Moment (WF0Z)\n        *   Normalised Inverse Difference Moment (1QCO)\n        *   Inverse Variance (E8JP)\n        *   Correlation (NI2N)\n        *   Autocorrelation (QWB0)\n        *   Cluster Tendency (DG8W)\n        *   Cluster Shade (7NFM)\n        *   Cluster Prominence (AE86)\n        *   Information Correlation 1 (R8DG)\n        *   Information Correlation 2 (JN9H)\n\n        Args:\n            data (npt.NDArray[np.floating[Any]]): The 3D image array containing discretised grey levels.\n            mask (npt.NDArray[np.floating[Any]]): The 3D binary mask array defining the ROI.\n            n_bins (int): The number of grey levels.\n            glcm_matrix (Optional[npt.NDArray[np.floating[Any]]]): Pre-calculated GLCM matrix. If provided, `data` and `mask`\n                are ignored for matrix calculation, but `data` is still used for `Ng` estimation if needed.\n                If None, the matrix is calculated from scratch.\n\n        Returns:\n            dict[str, float]: A dictionary of calculated GLCM features, keyed by their name and IBSI code.\n                Example keys: 'joint_maximum_GYBY', 'contrast_ACUI', 'correlation_NI2N'.\n\n        Example:\n            ```python\n            import numpy as np\n    from numpy import typing as npt\n            # ... assuming data and mask defined ...\n            features = calculate_glcm_features(data, mask, n_bins=32)\n            print(features['contrast_ACUI'])\n            ```\n            12.5\n    \"\"\"\n    if glcm_matrix is None:\n        data_c, mask_c, _ = _maybe_crop_to_bbox(data, mask, None)\n        if mask_c.dtype == np.uint8:\n            mask_u8 = mask_c\n        else:\n            mask_u8 = (mask_c != 0).astype(np.uint8)\n\n        if n_bins &lt;= 256:\n            data_int = (data_c - 1).astype(np.uint8)\n        else:\n            data_int = (data_c - 1).astype(np.int32)\n\n        # Determine n_threads for JIT call\n        try:\n            n_threads = int(numba.config.NUMBA_NUM_THREADS)\n        except (ValueError, TypeError):\n            n_threads = 1  # Fallback\n\n        # Call combined kernel to calculate only GLCM\n        glcm, _, _, _, _ = _calculate_local_features_numba(\n            data_int,\n            mask_u8,\n            n_bins,\n            calc_glcm=True,\n            calc_glrlm=False,\n            calc_ngtdm=False,\n            calc_ngldm=False,\n            offsets_26=OFFSETS_26,\n            directions_13=DIRECTIONS_13,\n            ngldm_alpha=0,\n            n_threads=n_threads,\n        )\n    else:\n        glcm = glcm_matrix\n\n    # Merge (Sum) -&gt; IAZD\n    glcm_sum = np.sum(glcm, axis=0)\n    glcm_sym = glcm_sum + glcm_sum.T\n\n    # Normalize\n    total_sum = np.sum(glcm_sym)\n    if total_sum == 0:\n        return {}\n\n    P = glcm_sym / total_sum\n\n    # Indices (0-based from np.indices, convert to 1-based for IBSI)\n    i_idx, j_idx = np.indices((n_bins, n_bins))\n    I = i_idx + 1  # noqa: E741\n    J = j_idx + 1\n\n    features = {}\n\n    # Joint Maximum - GYBY\n    features[\"joint_maximum_GYBY\"] = np.max(P)\n\n    # Joint Average - 60VM\n    features[\"joint_average_60VM\"] = np.sum(I * P)\n\n    # Joint Variance - UR99\n    mu = features[\"joint_average_60VM\"]\n    features[\"joint_variance_UR99\"] = np.sum(((I - mu) ** 2) * P)\n\n    # Joint Entropy - TU9B\n    mask_p = P &gt; 0\n    features[\"joint_entropy_TU9B\"] = -np.sum(P[mask_p] * np.log2(P[mask_p]))\n\n    # Difference Average - TF7R\n    k_diff = np.abs(I - J)\n    features[\"difference_average_TF7R\"] = np.sum(k_diff * P)\n\n    # Optimized using bincount\n    k_diff_flat = k_diff.ravel().astype(np.int32)\n    P_flat = P.ravel()\n    p_diff = np.bincount(k_diff_flat, weights=P_flat)\n\n    mu_diff = features[\"difference_average_TF7R\"]\n    k_vals = np.arange(n_bins)\n    features[\"difference_variance_D3YU\"] = np.sum(((k_vals - mu_diff) ** 2) * p_diff)\n\n    # Difference Entropy - NTRS\n    mask_pd = p_diff &gt; 0\n    features[\"difference_entropy_NTRS\"] = -np.sum(\n        p_diff[mask_pd] * np.log2(p_diff[mask_pd])\n    )\n\n    # Sum Average - ZGXS\n    k_sum_grid = I + J\n\n    # Optimized using bincount\n    k_sum_flat = k_sum_grid.ravel().astype(np.int32)\n    # P_flat is already defined in Difference Variance block\n    p_sum_full = np.bincount(k_sum_flat, weights=P_flat)\n\n    # Slice from 2.\n    p_sum = p_sum_full[2:]\n\n    k_vals_sum = np.arange(2, 2 * n_bins + 1)\n    features[\"sum_average_ZGXS\"] = np.sum(k_vals_sum * p_sum)\n\n    # Sum Variance - OEEB\n    mu_sum = features[\"sum_average_ZGXS\"]\n    features[\"sum_variance_OEEB\"] = np.sum(((k_vals_sum - mu_sum) ** 2) * p_sum)\n\n    # Sum Entropy - P6QZ\n    mask_ps = p_sum &gt; 0\n    features[\"sum_entropy_P6QZ\"] = -np.sum(p_sum[mask_ps] * np.log2(p_sum[mask_ps]))\n\n    # Angular Second Moment (Energy) - 8ZQL\n    features[\"angular_second_moment_8ZQL\"] = np.sum(P**2)\n\n    # Contrast - ACUI\n    features[\"contrast_ACUI\"] = np.sum(((I - J) ** 2) * P)\n\n    # Dissimilarity - 8S9J\n    features[\"dissimilarity_8S9J\"] = np.sum(np.abs(I - J) * P)\n\n    # Inverse Difference - IB1Z\n    features[\"inverse_difference_IB1Z\"] = np.sum(P / (1 + np.abs(I - J)))\n\n    roi_data = data[mask &gt; 0]\n    if len(roi_data) &gt; 0:\n        Ng_eff = int(np.max(roi_data) - np.min(roi_data) + 1)\n    else:\n        Ng_eff = 1  # Fallback\n\n    # Normalised Inverse Difference - NDRX\n    features[\"normalised_inverse_difference_NDRX\"] = np.sum(\n        P / (1 + np.abs(I - J) / Ng_eff)\n    )\n\n    # Inverse Difference Moment - WF0Z\n    features[\"inverse_difference_moment_WF0Z\"] = np.sum(P / (1 + (I - J) ** 2))\n\n    # Normalised Inverse Difference Moment - 1QCO\n    features[\"normalised_inverse_difference_moment_1QCO\"] = np.sum(\n        P / (1 + ((I - J) ** 2) / (Ng_eff**2))\n    )\n\n    # Inverse Variance - E8JP\n    mask_neq = I != J\n    features[\"inverse_variance_E8JP\"] = np.sum(\n        P[mask_neq] / ((I[mask_neq] - J[mask_neq]) ** 2)\n    )\n\n    # Correlation - NI2N\n    term1 = np.sum((I - mu) * (J - mu) * P)\n    if features[\"joint_variance_UR99\"] != 0:\n        features[\"correlation_NI2N\"] = term1 / features[\"joint_variance_UR99\"]\n    else:\n        features[\"correlation_NI2N\"] = (\n            1.0  # Or NaN? IBSI doesn't specify for 0 variance.\n        )\n\n    # Autocorrelation - QWB0\n    features[\"autocorrelation_QWB0\"] = np.sum(I * J * P)\n\n    # Cluster Tendency - DG8W\n    features[\"cluster_tendency_DG8W\"] = np.sum(((I + J - 2 * mu) ** 2) * P)\n\n    # Cluster Shade - 7NFM\n    features[\"cluster_shade_7NFM\"] = np.sum(((I + J - 2 * mu) ** 3) * P)\n\n    # Cluster Prominence - AE86\n    features[\"cluster_prominence_AE86\"] = np.sum(((I + J - 2 * mu) ** 4) * P)\n\n    # Information Correlation 1 - R8DG\n    HXY = features[\"joint_entropy_TU9B\"]\n    p_x = np.sum(P, axis=1)\n    mask_px = p_x &gt; 0\n    HX = -np.sum(p_x[mask_px] * np.log2(p_x[mask_px]))\n\n    HXY1 = -np.sum(P[mask_p] * np.log2(p_x[I[mask_p] - 1] * p_x[J[mask_p] - 1]))\n\n    if HX != 0:\n        features[\"information_correlation_1_R8DG\"] = (HXY - HXY1) / HX\n    else:\n        features[\"information_correlation_1_R8DG\"] = np.nan\n\n    # Information Correlation 2 - JN9H\n    P_prod = np.outer(p_x, p_x)\n    mask_prod = P_prod &gt; 0\n    HXY2 = -np.sum(P_prod[mask_prod] * np.log2(P_prod[mask_prod]))\n\n    features[\"information_correlation_2_JN9H\"] = np.sqrt(1 - np.exp(-2 * (HXY2 - HXY)))\n    return features\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_gldzm_features","title":"<code>calculate_gldzm_features(data, mask, n_bins, gldzm_matrix=None, distance_mask=None)</code>","text":"<p>Calculate Grey Level Distance Zone Matrix (GLDZM) features.</p> <p>The GLDZM counts the number of zones of linked voxels with the same grey level, categorized by the distance of the zone from the ROI border. This captures information about the spatial distribution of textures relative to the boundary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[floating[Any]]</code> <p>The 3D image array containing discretised grey levels.</p> required <code>mask</code> <code>NDArray[floating[Any]]</code> <p>The 3D binary mask array defining the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels.</p> required <code>gldzm_matrix</code> <code>Optional[NDArray[floating[Any]]]</code> <p>Pre-calculated GLDZM matrix.</p> <code>None</code> <code>distance_mask</code> <code>Optional[NDArray[floating[Any]]]</code> <p>Optional mask used to calculate the distance map. If None, <code>mask</code> is used. This allows calculating distances based on the morphological mask while analyzing intensities from the intensity mask (e.g., after outlier filtering).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: A dictionary of calculated GLDZM features. Example keys: 'small_distance_emphasis_0GBI', 'zone_distance_entropy_GBDU'.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_gldzm_features(\n    data: npt.NDArray[np.floating[Any]],\n    mask: npt.NDArray[np.floating[Any]],\n    n_bins: int,\n    gldzm_matrix: Optional[npt.NDArray[np.floating[Any]]] = None,\n    distance_mask: Optional[npt.NDArray[np.floating[Any]]] = None,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate Grey Level Distance Zone Matrix (GLDZM) features.\n\n    The GLDZM counts the number of zones of linked voxels with the same grey level,\n    categorized by the distance of the zone from the ROI border.\n    This captures information about the spatial distribution of textures relative to the boundary.\n\n    Args:\n        data (npt.NDArray[np.floating[Any]]): The 3D image array containing discretised grey levels.\n        mask (npt.NDArray[np.floating[Any]]): The 3D binary mask array defining the ROI.\n        n_bins (int): The number of grey levels.\n        gldzm_matrix (Optional[npt.NDArray[np.floating[Any]]]): Pre-calculated GLDZM matrix.\n        distance_mask (Optional[npt.NDArray[np.floating[Any]]]): Optional mask used to calculate the distance map.\n            If None, `mask` is used. This allows calculating distances based on the morphological mask\n            while analyzing intensities from the intensity mask (e.g., after outlier filtering).\n\n    Returns:\n        dict[str, float]: A dictionary of calculated GLDZM features.\n            Example keys: 'small_distance_emphasis_0GBI', 'zone_distance_entropy_GBDU'.\n    \"\"\"\n    if gldzm_matrix is None:\n        # Calculate distance map\n        # Use distance_mask if provided, else mask\n        d_mask = distance_mask if distance_mask is not None else mask\n\n        # Pad the mask with 0s to ensure the image border is treated as an edge.\n        mask_bool = d_mask &gt; 0\n        mask_padded = np.pad(mask_bool, 1, mode=\"constant\", constant_values=0)\n        dist_map_padded = distance_transform_cdt(mask_padded, metric=\"taxicab\").astype(\n            np.int32\n        )\n        dist_map = dist_map_padded[1:-1, 1:-1, 1:-1]\n\n        _, gldzm = calculate_zone_features(  # type: ignore[arg-type]\n            data,\n            mask,\n            dist_map,\n            n_bins,\n            calc_glszm=False,\n            calc_gldzm=True,\n        )\n    else:\n        gldzm = gldzm_matrix\n\n    N_zones = np.sum(gldzm)\n    if N_zones == 0:\n        return {}\n\n    P = gldzm / N_zones\n\n    n_g, n_d = gldzm.shape\n    i_idx, j_idx = np.indices((n_g, n_d))\n    I = i_idx + 1  # noqa: E741\n    J = j_idx + 1  # Distance\n\n    features = {}\n\n    # Small Distance Emphasis (SDE) - 0GBI\n    features[\"small_distance_emphasis_0GBI\"] = np.sum(P / (J**2))\n\n    # Large Distance Emphasis (LDE) - MB4I\n    features[\"large_distance_emphasis_MB4I\"] = np.sum(P * (J**2))\n\n    # Grey Level Non-Uniformity (GLNU) - VFT7\n    s_i = np.sum(gldzm, axis=1)\n    features[\"grey_level_non_uniformity_VFT7\"] = np.sum(s_i**2) / N_zones\n\n    # Normalised Grey Level Non-Uniformity (GLNN) - 7HP3\n    features[\"normalised_grey_level_non_uniformity_7HP3\"] = np.sum(s_i**2) / (\n        N_zones**2\n    )\n\n    # Zone Distance Non-Uniformity (ZDNU) - V294\n    s_j = np.sum(gldzm, axis=0)\n    features[\"zone_distance_non_uniformity_V294\"] = np.sum(s_j**2) / N_zones\n\n    # Normalised Zone Distance Non-Uniformity (ZDNN) - IATH\n    features[\"normalised_zone_distance_non_uniformity_IATH\"] = np.sum(s_j**2) / (\n        N_zones**2\n    )\n\n    # Zone Percentage (ZP) - VIWW\n    n_voxels = np.sum(mask)\n    features[\"zone_percentage_VIWW\"] = N_zones / n_voxels\n\n    # Grey Level Variance (GLV) - QK93\n    mu_i = np.sum(I * P)\n    features[\"grey_level_variance_QK93\"] = np.sum(((I - mu_i) ** 2) * P)\n\n    # Zone Distance Variance (ZDV) - 7WT1\n    mu_j = np.sum(J * P)\n    features[\"zone_distance_variance_7WT1\"] = np.sum(((J - mu_j) ** 2) * P)\n\n    # Zone Distance Entropy (ZDE) - GBDU\n    mask_p = P &gt; 0\n    features[\"zone_distance_entropy_GBDU\"] = -np.sum(P[mask_p] * np.log2(P[mask_p]))\n\n    # Low Grey Level Zone Emphasis (LGLZE) - S1RA\n    features[\"low_grey_level_zone_emphasis_S1RA\"] = np.sum(P / (I**2))\n\n    # High Grey Level Zone Emphasis (HGLZE) - K26C\n    features[\"high_grey_level_zone_emphasis_K26C\"] = np.sum(P * (I**2))\n\n    # Small Distance Low Grey Level Emphasis (SDLGLE) - RUVG\n    features[\"small_distance_low_grey_level_emphasis_RUVG\"] = np.sum(\n        P / ((I**2) * (J**2))\n    )\n\n    # Small Distance High Grey Level Emphasis (SDHGLE) - DKNJ\n    features[\"small_distance_high_grey_level_emphasis_DKNJ\"] = np.sum(\n        P * (I**2) / (J**2)\n    )\n\n    # Large Distance Low Grey Level Emphasis (LDLGLE) - A7WM\n    features[\"large_distance_low_grey_level_emphasis_A7WM\"] = np.sum(\n        P * (J**2) / (I**2)\n    )\n\n    # Large Distance High Grey Level Emphasis (LDHGLE) - KLTH\n    features[\"large_distance_high_grey_level_emphasis_KLTH\"] = np.sum(\n        P * (I**2) * (J**2)\n    )\n\n    return features\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_glrlm_features","title":"<code>calculate_glrlm_features(data, mask, n_bins, glrlm_matrix=None)</code>","text":"<p>Calculate Grey Level Run Length Matrix (GLRLM) features.</p> <p>The GLRLM quantifies grey level runs, which are defined as the length in number of pixels, of consecutive pixels that have the same grey level value. This implementation computes features based on the 3D merged GLRLM (averaged over all 13 directions).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[floating[Any]]</code> <p>The 3D image array containing discretised grey levels.</p> required <code>mask</code> <code>NDArray[floating[Any]]</code> <p>The 3D binary mask array defining the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels.</p> required <code>glrlm_matrix</code> <code>Optional[NDArray[floating[Any]]]</code> <p>Pre-calculated GLRLM matrix.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: A dictionary of calculated GLRLM features. Example keys: 'short_runs_emphasis_22OV', 'grey_level_non_uniformity_R5YN'.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_glrlm_features(\n    data: npt.NDArray[np.floating[Any]],\n    mask: npt.NDArray[np.floating[Any]],\n    n_bins: int,\n    glrlm_matrix: Optional[npt.NDArray[np.floating[Any]]] = None,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate Grey Level Run Length Matrix (GLRLM) features.\n\n    The GLRLM quantifies grey level runs, which are defined as the length in number of pixels,\n    of consecutive pixels that have the same grey level value.\n    This implementation computes features based on the 3D merged GLRLM (averaged over all 13 directions).\n\n    Args:\n        data (npt.NDArray[np.floating[Any]]): The 3D image array containing discretised grey levels.\n        mask (npt.NDArray[np.floating[Any]]): The 3D binary mask array defining the ROI.\n        n_bins (int): The number of grey levels.\n        glrlm_matrix (Optional[npt.NDArray[np.floating[Any]]]): Pre-calculated GLRLM matrix.\n\n    Returns:\n        dict[str, float]: A dictionary of calculated GLRLM features.\n            Example keys: 'short_runs_emphasis_22OV', 'grey_level_non_uniformity_R5YN'.\n    \"\"\"\n    if glrlm_matrix is None:\n        if n_bins &lt;= 256:\n            data_int = (data - 1).astype(np.uint8)\n        else:\n            data_int = (data - 1).astype(np.int32)  # pragma: no cover\n\n        # Determine n_threads for JIT call\n        try:\n            n_threads = int(numba.config.NUMBA_NUM_THREADS)\n        except (ValueError, TypeError):\n            n_threads = 1  # Fallback\n\n        # Call combined kernel\n        _, glrlm, _, _, _ = _calculate_local_features_numba(\n            data_int,\n            mask,\n            n_bins,\n            calc_glcm=False,\n            calc_glrlm=True,\n            calc_ngtdm=False,\n            calc_ngldm=False,\n            offsets_26=OFFSETS_26,\n            directions_13=DIRECTIONS_13,\n            ngldm_alpha=0,\n            n_threads=n_threads,\n        )\n    else:\n        glrlm = glrlm_matrix\n\n    # Merge (Sum) -&gt; IAZD\n    glrlm_sum = np.sum(glrlm, axis=0)\n\n    # Remove length 0 (column 0)\n    glrlm = glrlm_sum[:, 1:]\n\n    N_runs = np.sum(glrlm)\n    if N_runs == 0:\n        return {}\n\n    P = glrlm / N_runs\n\n    n_g, n_r = glrlm.shape\n    i_idx, j_idx = np.indices((n_g, n_r))\n    I = i_idx + 1  # noqa: E741\n    J = j_idx + 1\n\n    features = {}\n\n    # Short Run Emphasis (SRE) - 22OV\n    features[\"short_runs_emphasis_22OV\"] = np.sum(P / (J**2))\n\n    # Long Run Emphasis (LRE) - W4KF\n    features[\"long_runs_emphasis_W4KF\"] = np.sum(P * (J**2))\n\n    # Grey Level Non-Uniformity (GLNU) - R5YN\n    s_i = np.sum(glrlm, axis=1)\n    features[\"grey_level_non_uniformity_R5YN\"] = np.sum(s_i**2) / N_runs\n\n    # Normalised Grey Level Non-Uniformity (GLNN) - OVBL\n    features[\"normalised_grey_level_non_uniformity_OVBL\"] = np.sum(s_i**2) / (N_runs**2)\n\n    # Run Length Non-Uniformity (RLNU) - W92Y\n    s_j = np.sum(glrlm, axis=0)\n    features[\"run_length_non_uniformity_W92Y\"] = np.sum(s_j**2) / N_runs\n\n    # Normalised Run Length Non-Uniformity (RLNN) - IC23\n    features[\"normalised_run_length_non_uniformity_IC23\"] = np.sum(s_j**2) / (N_runs**2)\n\n    # Run Percentage (RP) - 9ZK5\n    n_voxels = np.sum(mask)\n    n_dirs = 13  # Fixed for 3D\n    features[\"run_percentage_9ZK5\"] = N_runs / (n_voxels * n_dirs)\n\n    # Grey Level Variance (GLV) - 8CE5\n    mu_i = np.sum(I * P)\n    features[\"grey_level_variance_8CE5\"] = np.sum(((I - mu_i) ** 2) * P)\n\n    # Run Length Variance (RLV) - SXLW\n    mu_j = np.sum(J * P)\n    features[\"run_length_variance_SXLW\"] = np.sum(((J - mu_j) ** 2) * P)\n\n    # Run Entropy (RE) - HJ9O\n    mask_p = P &gt; 0\n    features[\"run_entropy_HJ9O\"] = -np.sum(P[mask_p] * np.log2(P[mask_p]))\n\n    # Low Grey Level Run Emphasis (LGLRE) - V3SW\n    features[\"low_grey_level_run_emphasis_V3SW\"] = np.sum(P / (I**2))\n\n    # High Grey Level Run Emphasis (HGLRE) - G3QZ\n    features[\"high_grey_level_run_emphasis_G3QZ\"] = np.sum(P * (I**2))\n\n    # Short Run Low Grey Level Emphasis (SRLGLE) - HTZT\n    features[\"short_run_low_grey_level_emphasis_HTZT\"] = np.sum(P / ((I**2) * (J**2)))\n\n    # Short Run High Grey Level Emphasis (SRHGLE) - GD3A\n    features[\"short_run_high_grey_level_emphasis_GD3A\"] = np.sum(P * (I**2) / (J**2))\n\n    # Long Run Low Grey Level Emphasis (LRLGLE) - IVPO\n    features[\"long_run_low_grey_level_emphasis_IVPO\"] = np.sum(P * (J**2) / (I**2))\n\n    # Long Run High Grey Level Emphasis (LRHGLE) - 3KUM\n    features[\"long_run_high_grey_level_emphasis_3KUM\"] = np.sum(P * (I**2) * (J**2))\n\n    return features\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_glszm_features","title":"<code>calculate_glszm_features(data, mask, n_bins, glszm_matrix=None)</code>","text":"<p>Calculate Grey Level Size Zone Matrix (GLSZM) features.</p> <p>The GLSZM counts the number of zones (connected components) of linked voxels that share the same grey level intensity. A zone is defined as a group of connected voxels with the same grey level. This matrix is rotationally invariant by definition.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[floating[Any]]</code> <p>The 3D image array containing discretised grey levels.</p> required <code>mask</code> <code>NDArray[floating[Any]]</code> <p>The 3D binary mask array defining the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels.</p> required <code>glszm_matrix</code> <code>Optional[NDArray[floating[Any]]]</code> <p>Pre-calculated GLSZM matrix.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: A dictionary of calculated GLSZM features. Example keys: 'small_zone_emphasis_P001', 'zone_percentage_P30P'.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_glszm_features(\n    data: npt.NDArray[np.floating[Any]],\n    mask: npt.NDArray[np.floating[Any]],\n    n_bins: int,\n    glszm_matrix: Optional[npt.NDArray[np.floating[Any]]] = None,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate Grey Level Size Zone Matrix (GLSZM) features.\n\n    The GLSZM counts the number of zones (connected components) of linked voxels\n    that share the same grey level intensity. A zone is defined as a group of connected voxels\n    with the same grey level. This matrix is rotationally invariant by definition.\n\n    Args:\n        data (npt.NDArray[np.floating[Any]]): The 3D image array containing discretised grey levels.\n        mask (npt.NDArray[np.floating[Any]]): The 3D binary mask array defining the ROI.\n        n_bins (int): The number of grey levels.\n        glszm_matrix (Optional[npt.NDArray[np.floating[Any]]]): Pre-calculated GLSZM matrix.\n\n    Returns:\n        dict[str, float]: A dictionary of calculated GLSZM features.\n            Example keys: 'small_zone_emphasis_P001', 'zone_percentage_P30P'.\n    \"\"\"\n    if glszm_matrix is None:\n        data_c, mask_c, _ = _maybe_crop_to_bbox(data, mask, None)\n        if mask_c.dtype == np.uint8:\n            mask_u8 = mask_c\n        else:\n            mask_u8 = (mask_c != 0).astype(np.uint8)\n\n        # We need dist_map for the combined kernel signature, even if unused for GLSZM\n        dummy_dist = np.zeros_like(data_c, dtype=np.int32)\n\n        glszm, _ = calculate_zone_features(\n            data_c, mask_u8, dummy_dist, n_bins, calc_glszm=True, calc_gldzm=False  # type: ignore[arg-type]\n        )\n    else:\n        glszm = glszm_matrix\n\n    N_zones = np.sum(glszm)\n    if N_zones == 0:\n        return {}\n\n    P = glszm / N_zones\n\n    n_g, n_s = glszm.shape\n    i_idx, j_idx = np.indices((n_g, n_s))\n    I = i_idx + 1  # noqa: E741\n    J = j_idx + 1  # Zone size\n\n    features = {}\n\n    # Small Zone Emphasis (SZE) - P001\n    features[\"small_zone_emphasis_P001\"] = np.sum(P / (J**2))\n\n    # Large Zone Emphasis (LZE) - 48P8\n    features[\"large_zone_emphasis_48P8\"] = np.sum(P * (J**2))\n\n    # Grey Level Non-Uniformity (GLNU) - JNSA\n    s_i = np.sum(glszm, axis=1)\n    features[\"grey_level_non_uniformity_JNSA\"] = np.sum(s_i**2) / N_zones\n\n    # Normalised Grey Level Non-Uniformity (GLNN) - Y1RO\n    features[\"normalised_grey_level_non_uniformity_Y1RO\"] = np.sum(s_i**2) / (\n        N_zones**2\n    )\n\n    # Zone Size Non-Uniformity (ZSNU) - 4JP3\n    s_j = np.sum(glszm, axis=0)\n    features[\"zone_size_non_uniformity_4JP3\"] = np.sum(s_j**2) / N_zones\n\n    # Normalised Zone Size Non-Uniformity (ZSNN) - VB3A\n    features[\"normalised_zone_size_non_uniformity_VB3A\"] = np.sum(s_j**2) / (N_zones**2)\n\n    # Zone Percentage (ZP) - P30P\n    n_voxels = np.sum(mask)\n    features[\"zone_percentage_P30P\"] = N_zones / n_voxels\n\n    # Grey Level Variance (GLV) - BYLV\n    mu_i = np.sum(I * P)\n    features[\"grey_level_variance_BYLV\"] = np.sum(((I - mu_i) ** 2) * P)\n\n    # Zone Size Variance (ZSV) - 3NSA\n    mu_j = np.sum(J * P)\n    features[\"zone_size_variance_3NSA\"] = np.sum(((J - mu_j) ** 2) * P)\n\n    # Zone Size Entropy (ZSE) - GU8N\n    mask_p = P &gt; 0\n    features[\"zone_size_entropy_GU8N\"] = -np.sum(P[mask_p] * np.log2(P[mask_p]))\n\n    # Low Grey Level Zone Emphasis (LGLZE) - XMSY\n    features[\"low_grey_level_zone_emphasis_XMSY\"] = np.sum(P / (I**2))\n\n    # High Grey Level Zone Emphasis (HGLZE) - 5GN9\n    features[\"high_grey_level_zone_emphasis_5GN9\"] = np.sum(P * (I**2))\n\n    # Small Zone Low Grey Level Emphasis (SZLGLE) - 5RAI\n    features[\"small_zone_low_grey_level_emphasis_5RAI\"] = np.sum(P / ((I**2) * (J**2)))\n\n    # Small Zone High Grey Level Emphasis (SZHGLE) - HW1V\n    features[\"small_zone_high_grey_level_emphasis_HW1V\"] = np.sum(P * (I**2) / (J**2))\n\n    # Large Zone Low Grey Level Emphasis (LZLGLE) - YH51\n    features[\"large_zone_low_grey_level_emphasis_YH51\"] = np.sum(P * (J**2) / (I**2))\n\n    # Large Zone High Grey Level Emphasis (LZHGLE) - J17V\n    features[\"large_zone_high_grey_level_emphasis_J17V\"] = np.sum(P * (I**2) * (J**2))\n\n    return features\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_ngldm_features","title":"<code>calculate_ngldm_features(data, mask, n_bins, ngldm_matrix=None, ngldm_alpha=0)</code>","text":"<p>Calculate Neighbourhood Grey Level Dependence Matrix (NGLDM) features.</p> <p>The NGLDM captures the dependence of grey levels on their neighbours. A \"dependence\" is defined as a connected voxel having a similar grey level (within a tolerance \u03b1).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[floating[Any]]</code> <p>The 3D image array containing discretised grey levels.</p> required <code>mask</code> <code>NDArray[floating[Any]]</code> <p>The 3D binary mask array defining the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels.</p> required <code>ngldm_matrix</code> <code>Optional[NDArray[floating[Any]]]</code> <p>Pre-calculated NGLDM matrix.</p> <code>None</code> <code>ngldm_alpha</code> <code>int</code> <p>The coarseness parameter \u03b1. Two grey levels are considered dependent if their absolute difference is \u2264 \u03b1. Default is 0 (exact match, IBSI standard).</p> <code>0</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: A dictionary of calculated NGLDM features. Example keys: 'low_dependence_emphasis_SODN', 'dependence_count_entropy_FCBV'.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_ngldm_features(\n    data: npt.NDArray[np.floating[Any]],\n    mask: npt.NDArray[np.floating[Any]],\n    n_bins: int,\n    ngldm_matrix: Optional[npt.NDArray[np.floating[Any]]] = None,\n    ngldm_alpha: int = 0,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate Neighbourhood Grey Level Dependence Matrix (NGLDM) features.\n\n    The NGLDM captures the dependence of grey levels on their neighbours.\n    A \"dependence\" is defined as a connected voxel having a similar grey level (within a tolerance \u03b1).\n\n    Args:\n        data (npt.NDArray[np.floating[Any]]): The 3D image array containing discretised grey levels.\n        mask (npt.NDArray[np.floating[Any]]): The 3D binary mask array defining the ROI.\n        n_bins (int): The number of grey levels.\n        ngldm_matrix (Optional[npt.NDArray[np.floating[Any]]]): Pre-calculated NGLDM matrix.\n        ngldm_alpha (int): The coarseness parameter \u03b1. Two grey levels are considered dependent\n            if their absolute difference is \u2264 \u03b1. Default is 0 (exact match, IBSI standard).\n\n    Returns:\n        dict[str, float]: A dictionary of calculated NGLDM features.\n            Example keys: 'low_dependence_emphasis_SODN', 'dependence_count_entropy_FCBV'.\n    \"\"\"\n    if ngldm_matrix is None:\n        if n_bins &lt;= 256:\n            data_int = (data - 1).astype(np.uint8)\n        elif n_bins &lt;= 65536:\n            data_int = (data - 1).astype(np.uint16)\n        else:\n            data_int = (data - 1).astype(np.int32)  # pragma: no cover\n\n        # Determine n_threads for JIT call\n        try:\n            n_threads = int(numba.config.NUMBA_NUM_THREADS)\n        except (ValueError, TypeError):\n            n_threads = 1  # Fallback\n\n        _, _, _, _, ngldm = _calculate_local_features_numba(\n            data_int,\n            mask,\n            n_bins,\n            calc_glcm=False,\n            calc_glrlm=False,\n            calc_ngtdm=False,\n            calc_ngldm=True,\n            offsets_26=OFFSETS_26,\n            directions_13=DIRECTIONS_13,\n            ngldm_alpha=ngldm_alpha,\n            n_threads=n_threads,\n        )\n    else:\n        ngldm = ngldm_matrix\n\n    N_s = np.sum(ngldm)\n    if N_s == 0:\n        return {}\n\n    P = ngldm / N_s\n\n    n_g, n_d = ngldm.shape\n    i_idx, j_idx = np.indices((n_g, n_d))\n    I = i_idx + 1  # noqa: E741\n    J = j_idx + 1  # Dependence count\n\n    features = {}\n\n    # Low Dependence Emphasis (LDE) - SODN\n    features[\"low_dependence_emphasis_SODN\"] = np.sum(P / (J**2))\n\n    # High Dependence Emphasis (HDE) - IMOQ\n    features[\"high_dependence_emphasis_IMOQ\"] = np.sum(P * (J**2))\n\n    # Low Grey Level Count Emphasis (LGCE) - TL9H\n    features[\"low_grey_level_count_emphasis_TL9H\"] = np.sum(P / (I**2))\n\n    # High Grey Level Count Emphasis (HGCE) - OAE7\n    features[\"high_grey_level_count_emphasis_OAE7\"] = np.sum(P * (I**2))\n\n    # Low Dependence Low Grey Level Emphasis (LDLGE) - EQ3F\n    features[\"low_dependence_low_grey_level_emphasis_EQ3F\"] = np.sum(\n        P / ((I**2) * (J**2))\n    )\n\n    # Low Dependence High Grey Level Emphasis (LDHGE) - JA6D\n    features[\"low_dependence_high_grey_level_emphasis_JA6D\"] = np.sum(\n        P * (I**2) / (J**2)\n    )\n\n    # High Dependence Low Grey Level Emphasis (HDLGE) - NBZI\n    features[\"high_dependence_low_grey_level_emphasis_NBZI\"] = np.sum(\n        P * (J**2) / (I**2)\n    )\n\n    # High Dependence High Grey Level Emphasis (HDHGE) - 9QMG\n    features[\"high_dependence_high_grey_level_emphasis_9QMG\"] = np.sum(\n        P * (I**2) * (J**2)\n    )\n\n    # Grey Level Non-Uniformity - FP8K\n    s_i = np.sum(ngldm, axis=1)\n    features[\"grey_level_non_uniformity_FP8K\"] = np.sum(s_i**2) / N_s\n\n    # Normalised Grey Level Non-Uniformity - 5SPA\n    features[\"normalised_grey_level_non_uniformity_5SPA\"] = np.sum(s_i**2) / (N_s**2)\n\n    # Dependence Count Non-Uniformity - Z87G\n    s_j = np.sum(ngldm, axis=0)\n    features[\"dependence_count_non_uniformity_Z87G\"] = np.sum(s_j**2) / N_s\n\n    # Normalised Dependence Count Non-Uniformity - OKJI\n    features[\"normalised_dependence_count_non_uniformity_OKJI\"] = np.sum(s_j**2) / (\n        N_s**2\n    )\n\n    # Dependence Count Percentage - 6XV8\n    n_voxels = np.sum(mask)\n    features[\"dependence_count_percentage_6XV8\"] = N_s / n_voxels\n\n    # Grey Level Variance - 1PFV\n    mu_i = np.sum(I * P)\n    features[\"grey_level_variance_1PFV\"] = np.sum(((I - mu_i) ** 2) * P)\n\n    # Dependence Count Variance - DNX2\n    mu_j = np.sum(J * P)\n    features[\"dependence_count_variance_DNX2\"] = np.sum(((J - mu_j) ** 2) * P)\n\n    # Dependence Count Entropy - FCBV\n    mask_p = P &gt; 0\n    features[\"dependence_count_entropy_FCBV\"] = -np.sum(P[mask_p] * np.log2(P[mask_p]))\n\n    # Dependence Count Energy - CAS9\n    features[\"dependence_count_energy_CAS9\"] = np.sum(P**2)\n\n    return features\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_ngtdm_features","title":"<code>calculate_ngtdm_features(data, mask, n_bins, ngtdm_matrices=None)</code>","text":"<p>Calculate Neighbourhood Grey Tone Difference Matrix (NGTDM) features.</p> <p>The NGTDM quantifies the difference between a grey value and the average grey value of its neighbours. It captures the coarseness and contrast of the texture.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[floating[Any]]</code> <p>The 3D image array containing discretised grey levels.</p> required <code>mask</code> <code>NDArray[floating[Any]]</code> <p>The 3D binary mask array defining the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels.</p> required <code>ngtdm_matrices</code> <code>Optional[tuple[NDArray[floating[Any]], NDArray[floating[Any]]]]</code> <p>Pre-calculated NGTDM matrices (sum of absolute differences <code>s</code>, and count <code>n</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>dict[str, float]: A dictionary of calculated NGTDM features. Example keys: 'coarseness_QCDE', 'contrast_65HE', 'busyness_NQ30'.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_ngtdm_features(\n    data: npt.NDArray[np.floating[Any]],\n    mask: npt.NDArray[np.floating[Any]],\n    n_bins: int,\n    ngtdm_matrices: Optional[\n        tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]]]\n    ] = None,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Calculate Neighbourhood Grey Tone Difference Matrix (NGTDM) features.\n\n    The NGTDM quantifies the difference between a grey value and the average grey value\n    of its neighbours. It captures the coarseness and contrast of the texture.\n\n    Args:\n        data (npt.NDArray[np.floating[Any]]): The 3D image array containing discretised grey levels.\n        mask (npt.NDArray[np.floating[Any]]): The 3D binary mask array defining the ROI.\n        n_bins (int): The number of grey levels.\n        ngtdm_matrices (Optional[tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]]]]): Pre-calculated NGTDM matrices\n            (sum of absolute differences `s`, and count `n`).\n\n    Returns:\n        dict[str, float]: A dictionary of calculated NGTDM features.\n            Example keys: 'coarseness_QCDE', 'contrast_65HE', 'busyness_NQ30'.\n    \"\"\"\n    if ngtdm_matrices is None:\n        if n_bins &lt;= 256:\n            data_int = (data - 1).astype(np.uint8)\n        elif n_bins &lt;= 65536:\n            data_int = (data - 1).astype(np.uint16)\n        else:\n            data_int = (data - 1).astype(np.int32)  # pragma: no cover\n\n        # Determine n_threads for JIT call\n        try:\n            n_threads = int(numba.config.NUMBA_NUM_THREADS)\n        except (ValueError, TypeError):\n            n_threads = 1  # Fallback\n\n        _, _, s, n, _ = _calculate_local_features_numba(\n            data_int,\n            mask,\n            n_bins,\n            calc_glcm=False,\n            calc_glrlm=False,\n            calc_ngtdm=True,\n            calc_ngldm=False,\n            offsets_26=OFFSETS_26,\n            directions_13=DIRECTIONS_13,\n            ngldm_alpha=0,\n            n_threads=n_threads,\n        )\n    else:\n        s, n = ngtdm_matrices\n\n    # s[i] is sum of absolute differences for grey level i+1\n    # n[i] is number of voxels of grey level i+1 with valid neighborhood\n\n    N_vp = np.sum(n)\n    if N_vp == 0:\n        return {}\n\n    p = n / N_vp\n\n    # Indices\n    i_idx = np.arange(n_bins)\n    I = i_idx + 1  # noqa: E741\n\n    features = {}\n\n    # Filter for non-zero probabilities (required for Busyness, Complexity, Strength)\n    mask_p = p &gt; 0\n    p_nz = p[mask_p]\n    s_nz = s[mask_p]\n    I_nz = I[mask_p]\n\n    # Coarseness - QCDE\n    sum_ps = np.sum(p_nz * s_nz)\n    if sum_ps &gt; 1e-10:\n        features[\"coarseness_QCDE\"] = 1 / sum_ps\n    else:\n        features[\"coarseness_QCDE\"] = 1e6\n\n    # Contrast - 65HE\n    Ng_p = len(p_nz)\n\n    if Ng_p &gt; 1:\n        # Term 1: Dynamic range variance\n        Pi, Pj = np.meshgrid(p_nz, p_nz, indexing=\"ij\")\n        Ii, Ij = np.meshgrid(I_nz, I_nz, indexing=\"ij\")\n\n        term1_sum = np.sum(Pi * Pj * ((Ii - Ij) ** 2))\n        term1 = term1_sum / (Ng_p * (Ng_p - 1))\n\n        # Term 2: Intensity change\n        sum_s = np.sum(s)\n        term2 = sum_s / N_vp\n\n        features[\"contrast_65HE\"] = term1 * term2\n    else:\n        features[\"contrast_65HE\"] = 0.0\n\n    # Busyness - NQ30\n    IPi = I_nz * p_nz\n\n    # Grid\n    IPi_grid, IPj_grid = np.meshgrid(IPi, IPi, indexing=\"ij\")\n    denom_busyness = np.sum(np.abs(IPi_grid - IPj_grid))\n\n    if denom_busyness &gt; 1e-10:\n        features[\"busyness_NQ30\"] = sum_ps / denom_busyness\n    else:\n        features[\"busyness_NQ30\"] = 0.0\n\n    # Complexity - HDEZ\n    Pi, Pj = np.meshgrid(p_nz, p_nz, indexing=\"ij\")\n    Si, Sj = np.meshgrid(s_nz, s_nz, indexing=\"ij\")\n    Ii, Ij = np.meshgrid(I_nz, I_nz, indexing=\"ij\")\n\n    denom_comp = Pi + Pj\n    term_comp = np.abs(Ii - Ij) * (Pi * Si + Pj * Sj) / denom_comp\n\n    features[\"complexity_HDEZ\"] = (1 / N_vp) * np.sum(term_comp)\n\n    # Strength - 1X9X\n    sum_s = np.sum(s)\n\n    term_str = (Pi + Pj) * ((Ii - Ij) ** 2)\n    sum_term_str = np.sum(term_str)\n\n    if sum_s &gt; 1e-10:\n        features[\"strength_1X9X\"] = sum_term_str / sum_s\n    else:\n        features[\"strength_1X9X\"] = 0.0\n\n    return features\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_zone_features","title":"<code>calculate_zone_features(data, mask, dist_map, n_bins, calc_glszm=True, calc_gldzm=True)</code>","text":"<p>Wrapper for _calculate_zone_features_numba with buffer pooling.</p> <p>This function manages pre-allocated buffers to reduce memory allocation overhead for repeated calls (e.g., during batch processing).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>NDArray[floating[Any]]</code> <p>3D discretized image data.</p> required <code>mask</code> <code>NDArray[floating[Any]]</code> <p>3D mask array (not modified - copied internally by JIT function).</p> required <code>dist_map</code> <code>NDArray[floating[Any]]</code> <p>3D distance map for GLDZM.</p> required <code>n_bins</code> <code>int</code> <p>Number of grey level bins.</p> required <code>calc_glszm</code> <code>bool</code> <p>Whether to calculate GLSZM.</p> <code>True</code> <code>calc_gldzm</code> <code>bool</code> <p>Whether to calculate GLDZM.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[NDArray[floating[Any]], NDArray[floating[Any]]]</code> <p>Tuple of (glszm, gldzm) matrices.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_zone_features(\n    data: npt.NDArray[np.floating[Any]],\n    mask: npt.NDArray[np.floating[Any]],\n    dist_map: npt.NDArray[np.floating[Any]],\n    n_bins: int,\n    calc_glszm: bool = True,\n    calc_gldzm: bool = True,\n) -&gt; tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]]]:\n    \"\"\"\n    Wrapper for _calculate_zone_features_numba with buffer pooling.\n\n    This function manages pre-allocated buffers to reduce memory allocation\n    overhead for repeated calls (e.g., during batch processing).\n\n    Args:\n        data: 3D discretized image data.\n        mask: 3D mask array (not modified - copied internally by JIT function).\n        dist_map: 3D distance map for GLDZM.\n        n_bins: Number of grey level bins.\n        calc_glszm: Whether to calculate GLSZM.\n        calc_gldzm: Whether to calculate GLDZM.\n\n    Returns:\n        Tuple of (glszm, gldzm) matrices.\n    \"\"\"\n    # For zone features, the worst-case number of zones is bounded by ROI voxel count.\n    # Sizing buffers to full image volume is extremely costly for sparse ROIs.\n    max_zones = int(np.count_nonzero(mask))\n    if max_zones &lt; 1:\n        max_zones = 1\n\n    # Get pre-allocated buffers from pool\n    pool = _ZoneBufferPool.get_instance()\n    res_gl, res_size, res_dist, stack = pool.get_buffers(max_zones)\n\n    return cast(\n        tuple[npt.NDArray[np.floating[Any]], npt.NDArray[np.floating[Any]]],\n        _calculate_zone_features_numba(\n            data,\n            mask,\n            dist_map,\n            n_bins,\n            res_gl,\n            res_size,\n            res_dist,\n            stack,\n            calc_glszm,\n            calc_gldzm,\n        ),\n    )\n</code></pre>"},{"location":"user_guide/case_examples/","title":"Case examples","text":"<p>This section provides practical, end-to-end examples for common Pictologics workflows. The goal is to show how to combine loading, preprocessing, feature extraction, and result export patterns into scripts you can reuse. Many of these common processing techniques are in development to make the batch processing of cases even more easy. Therefore check back often to see any of these processing steps being implemented into the core package.</p> <p>Share Your Configurations</p> <p>All custom configurations used in these examples can be exported to YAML/JSON files for reproducibility. Use <code>pipeline.save_configs(\"my_configs.yaml\")</code> to save your configurations and share them with collaborators. See the Predefined Configurations guide for details.</p>"},{"location":"user_guide/case_examples/#case-1-batch-radiomics-from-a-folder-of-nifti-files-no-masks","title":"Case 1: Batch radiomics from a folder of NIfTI files (no masks)","text":""},{"location":"user_guide/case_examples/#scenario","title":"Scenario","text":"<p>You have a folder of NIfTI volumes where each file is a separate exported segmentation-like volume. There are no mask files.</p> <p>You want to:</p> <ul> <li>Process every <code>*.nii</code> / <code>*.nii.gz</code> file in a folder.</li> <li>Use the whole image as the initial ROI (because no mask is provided).</li> <li>Resample to 0.5\u00d70.5\u00d70.5 mm.</li> <li>Restrict the ROI to intensities in [-100, 3000] (CT HU example).</li> <li>Remove disjoint parts by keeping the largest connected component.</li> <li>Compute all radiomic feature families for four discretisation settings:</li> <li>FBN with <code>n_bins=8</code></li> <li>FBN with <code>n_bins=16</code></li> <li>FBS with <code>bin_width=8</code></li> <li>FBS with <code>bin_width=16</code></li> <li>Export a single wide CSV where:</li> <li>Each row is one input NIfTI file.</li> <li>Columns include metadata (e.g., filename) + all computed features.</li> <li>Save pipeline logs to a separate folder.</li> </ul>"},{"location":"user_guide/case_examples/#important-notes","title":"Important notes","text":"<p>Note</p> <ul> <li>Maskless pipeline runs: <code>RadiomicsPipeline.run(...)</code> accepts <code>mask=None</code>, <code>mask=\"\"</code>, or an omitted mask argument.   In that case, Pictologics generates a full (all-ones) ROI mask internally (whole-image ROI).</li> <li>Empty ROI is an error: If your preprocessing removes all voxels (e.g., a too-tight HU range), the pipeline raises   a clear error instead of silently returning empty outputs.</li> <li>Morphology on whole-image ROI: Shape features will describe the shape of the ROI mask.   With a maskless run, that ROI starts as the full image volume, then becomes whatever remains after resegmentation   and connected-component filtering. This is valid computationally, but make sure it matches your scientific intent.</li> <li>Sentinel value filtering: The <code>resegment</code> step with <code>range_min=-100</code> also filters out common DICOM sentinel   values (e.g., -1024, -2048) that represent missing/invalid data. This is the IBSI-recommended approach for   handling NA values in medical imaging data.</li> </ul>"},{"location":"user_guide/case_examples/#full-example-script","title":"Full example script","text":"<p>Full example script</p> <pre><code>from pathlib import Path\n\nfrom pictologics import RadiomicsPipeline\nfrom pictologics.results import format_results, save_results\n\n\ndef main():\n    # Configure paths\n    input_dir = Path(\"path/to/nifti_folder\")\n    output_csv = Path(\"results.csv\")\n    log_dir = Path(\"logs\")\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Define common preprocessing steps\n    base_steps = [\n        # Resample to 0.5mm isotropic. Round intensities to integers (useful for HU).\n        {\"step\": \"resample\", \"params\": {\"new_spacing\": (0.5, 0.5, 0.5), \"round_intensities\": True}},\n        # Exclude voxels outside standard HU range\n        {\"step\": \"resegment\", \"params\": {\"range_min\": -100, \"range_max\": 3000}},\n        # Keep only the largest connected component of the ROI mask\n        {\"step\": \"keep_largest_component\", \"params\": {\"apply_to\": \"morph\"}},\n    ]\n\n    # Shared feature extraction configuration\n    extract_all = {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": False,\n            \"include_local_intensity\": False,\n        },\n    }\n\n    # Initialize the pipeline\n    pipeline = RadiomicsPipeline()\n\n    # Add 4 configurations (2 FBN, 2 FBS)\n    for n_bins in (8, 16):\n        pipeline.add_config(\n            f\"case1_fbn_{n_bins}\",\n            base_steps + [\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": n_bins}},\n                extract_all,\n            ],\n        )\n\n    for bin_width in (8, 16):\n        pipeline.add_config(\n            f\"case1_fbs_{bin_width}\",\n            base_steps + [\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": bin_width}},\n                extract_all,\n            ],\n        )\n\n    # Prepare for batch processing\n    rows = []\n    nifti_paths = sorted(input_dir.glob(\"*.nii*\"))\n    if not nifti_paths:\n        raise ValueError(f\"No NIfTI files found in: {input_dir}\")\n\n    # Process each NIfTI file\n    for path in nifti_paths:\n        # Simple suffix stripping using Python 3.9+ removesuffix\n        subject_id = path.name.removesuffix(\".nii.gz\").removesuffix(\".nii\")\n        pipeline.clear_log()\n\n        # Run pipeline (mask omitted -&gt; whole-image ROI)\n        results = pipeline.run(\n            image=str(path),\n            subject_id=subject_id,\n            config_names=[\"case1_fbn_8\", \"case1_fbn_16\", \"case1_fbs_8\", \"case1_fbs_16\"],\n        )\n\n        # Format results as flat dictionary and add metadata\n        row = format_results(\n            results,\n            fmt=\"wide\",\n            meta={\"subject_id\": subject_id, \"file\": str(path)}\n        )\n        rows.append(row)\n\n        # Save per-case logs\n        pipeline.save_log(str(log_dir / f\"{subject_id}.json\"))\n\n    # Consolidated export of all results\n    save_results(rows, output_csv)\n    print(f\"Wrote {len(rows)} rows to {output_csv}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user_guide/case_examples/#output-format","title":"Output format","text":"<ul> <li>One row per file.</li> <li>Columns include:</li> <li><code>subject_id</code></li> <li><code>file</code></li> <li>Feature columns prefixed by configuration name (e.g., <code>case1_fbn_8__mean_intensity_Q4LE</code>).</li> </ul>"},{"location":"user_guide/case_examples/#case-2-batch-radiomics-from-dicom-case-folders-image-segmentation","title":"Case 2: Batch radiomics from DICOM case folders (Image + Segmentation)","text":""},{"location":"user_guide/case_examples/#scenario_1","title":"Scenario","text":"<p>You have a folder of cases. Each case is a separate folder and contains two subfolders:</p> <ul> <li><code>Image/</code>: the DICOM image series (CT/MR/etc.), stored at an arbitrary depth.</li> <li><code>Segmentation/</code>: the DICOM segmentation, also stored at an arbitrary depth.</li> </ul> <p>You want to:</p> <ul> <li>For each case, recursively discover the DICOM series folders.</li> <li>Load the image series and segmentation series.</li> <li>Convert the segmentation to a binary ROI mask by keeping all voxels where the segmentation value is <code>&gt; 0</code> (handled automatically during loading).</li> <li>Resample to 1\u00d71\u00d71 mm.</li> <li>Do not apply intensity filtering/resegmentation, and do not keep the largest connected component.</li> <li>Compute all radiomic feature families for two discretisations:</li> <li>FBS with <code>bin_width=256</code></li> <li>FBN with <code>n_bins=64</code></li> <li>Export a long-format CSV (tidy data, one row per feature).</li> <li>Save pipeline logs into a separate folder (one JSON per case).</li> <li>Show a progress bar during batch processing.</li> </ul>"},{"location":"user_guide/case_examples/#notes","title":"Notes","text":"<p>Note</p> <ul> <li>Progress bar dependency: This example uses <code>tqdm</code>.<ul> <li>If you are running this outside the library repo, install it with <code>pip install tqdm</code>.</li> <li>If you are adding it to your Poetry-managed project, use <code>poetry add tqdm</code>.</li> </ul> </li> <li>Segmentation DICOM at arbitrary depth: The helper <code>_find_dicom_series_root(...)</code> looks for the subfolder with     the most <code>.dcm</code> files and uses that as the series root.</li> <li>Multiple masks in one SEG: If your segmentation DICOM encodes multiple labels (e.g., values 1..N), the     <code>_binarize_segmentation_mask(...)</code> step turns it into a single ROI by keeping all voxels where the value is <code>&gt; 0</code>.</li> <li>Multi-phase DICOM series: If your image series contains multiple phases (e.g., cardiac CT with 10%, 20%... phases),     use <code>get_dicom_phases()</code> to discover available phases and <code>dataset_index</code> to select one:     <pre><code>from pictologics.utilities import get_dicom_phases\n\nphases = get_dicom_phases(str(image_root))\nprint(f\"Found {len(phases)} phases\")\n# Load a specific phase (default is 0)\nimage = load_image(str(image_root), recursive=True, dataset_index=0)\n</code></pre></li> </ul>"},{"location":"user_guide/case_examples/#full-example-script_1","title":"Full example script","text":"<p>Full example script</p> <pre><code>from pathlib import Path\nimport numpy as np\nfrom pictologics import Image, RadiomicsPipeline, load_image, load_and_merge_images\nfrom pictologics.results import format_results, save_results\n\n\ndef main():\n    # Configure paths\n    cases_dir = Path(\"path/to/cases_root\")\n    output_csv = Path(\"dicom_results_long.csv\")\n    log_dir = Path(\"dicom_logs\")\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Find all case directories\n    case_dirs = sorted([p for p in cases_dir.iterdir() if p.is_dir()])\n    if not case_dirs:\n        raise ValueError(f\"No case folders found in: {cases_dir}\")\n\n    # Shared feature extraction settings\n    extract_all = {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": False,\n            \"include_local_intensity\": False,\n        },\n    }\n\n    # Initialize the pipeline\n    pipeline = RadiomicsPipeline()\n\n    # Configuration A: Fixed Bin Size\n    pipeline.add_config(\n        \"case2_fbs_256\",\n        [\n            {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n            {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": 256.0}},\n            extract_all,\n        ],\n    )\n\n    # Configuration B: Fixed Bin Number\n    pipeline.add_config(\n        \"case2_fbn_64\",\n        [\n            {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n            {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 64}},\n            extract_all,\n        ],\n    )\n\n    # Use tqdm for a progress bar\n    from tqdm import tqdm\n\n    rows = []\n    for case_dir in tqdm(case_dirs, desc=\"Radiomics (DICOM cases)\", unit=\"case\"):\n        subject_id = case_dir.name\n        image_root = case_dir / \"Image\"\n        seg_root = case_dir / \"Segmentation\"\n\n        # load_image with recursive=True finds the best series folder automatically\n        image = load_image(str(image_root), recursive=True)\n\n        # Load the segmentation using load_and_merge_images with binarize=True\n        # recursive=True ensures we find the DICOM series inside the Segmentation folder\n        mask = load_and_merge_images(\n            [str(seg_root)], \n            binarize=True, \n            recursive=True\n        )\n\n        pipeline.clear_log()\n\n        # Execute extraction for both configurations\n        results = pipeline.run(\n            image=image,\n            mask=mask,\n            subject_id=subject_id,\n            config_names=[\"case2_fbs_256\", \"case2_fbn_64\"],\n        )\n\n        # Format results and store\n        row = format_results(\n            results,\n            fmt=\"long\",  # Tidy format: [subject_id, config, feature_name, value]\n            meta={\n                \"subject_id\": subject_id,\n                \"image_root\": str(image_root),\n                \"seg_root\": str(seg_root),\n            },\n        )\n        rows.append(row)\n\n        # Save per-case logs\n        pipeline.save_log(str(log_dir / f\"{subject_id}.json\"))\n\n    # Final export\n    save_results(rows, output_csv)\n    print(f\"Wrote {len(rows)} rows to {output_csv}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user_guide/case_examples/#case-3-batch-radiomics-from-a-flat-nifti-folder-multiple-masks-per-image","title":"Case 3: Batch radiomics from a flat NIfTI folder (multiple masks per image)","text":""},{"location":"user_guide/case_examples/#scenario_2","title":"Scenario","text":"<p>You have a single large folder containing both images and masks as NIfTI files.</p> <ul> <li>Image files end with <code>_IMG</code> (e.g., <code>CASE001_IMG.nii.gz</code>).</li> <li>Mask files end with <code>MASKn</code> where <code>n</code> is a number (e.g., <code>CASE001_MASK1.nii.gz</code>, <code>CASE001_MASK2.nii.gz</code>).</li> <li>There can be multiple segmentation masks per image.</li> </ul> <p>You want to:</p> <ul> <li>For each image, automatically find all its corresponding masks.</li> <li>Merge all masks into a single ROI using <code>load_and_merge_images(...)</code>.</li> <li>Do not apply any preprocessing (no resampling, no thresholding, no connected components).</li> <li>Compute radiomics for the six standard discretisations (FBN 8/16/32 and FBS 8/16/32).</li> <li>Export a long-format JSON file for easy ingestion into NoSQL databases or web apps.</li> <li>Save logs for each case into a separate folder.</li> <li>Show a progress bar during batch processing.</li> </ul> <p>Tip</p> <p>Flexible Image Loading The <code>load_and_merge_images</code> function uses <code>load_image</code> for each path in its input list. This means you can merge: - Multiple NIfTI files (as shown here). - DICOM series folders. - Single DICOM slice files. - Or a mix of these formats, provided they share the same spatial geometry.</p> <p>You can also pass <code>recursive=True</code> (to search subfolders) or <code>dataset_index=N</code> (for 4D files) to control how each mask is loaded.</p>"},{"location":"user_guide/case_examples/#full-example-script_2","title":"Full example script","text":"<p>Full example script</p> <pre><code>from pathlib import Path\n\nimport numpy as np\n\nfrom pictologics import Image, RadiomicsPipeline, load_and_merge_images, load_image\nfrom pictologics.results import format_results, save_results\n\n\ndef main():\n    # Configure paths\n    input_dir = Path(\"path/to/nifti_folder\")\n    output_file = Path(\"case3_results_long.json\")\n    log_dir = Path(\"case3_logs\")\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Use tqdm for a progress bar\n    from tqdm import tqdm\n\n    # Identify all images (files ending in _IMG)\n    image_paths = sorted(input_dir.glob(\"*_IMG.nii*\"))\n    if not image_paths:\n        raise ValueError(f\"No *_IMG NIfTI files found in: {input_dir}\")\n\n    # Initialize the pipeline\n    pipeline = RadiomicsPipeline()\n\n    # Shared feature extraction settings\n    extract_all = {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": False,\n            \"include_local_intensity\": False,\n        },\n    }\n\n    # Add 6 target configurations (no preprocessing requested)\n    for n_bins in (8, 16, 32):\n        pipeline.add_config(\n            f\"case3_fbn_{n_bins}\",\n            [\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": n_bins}},\n                extract_all,\n            ],\n        )\n\n    for bin_width in (8.0, 16.0, 32.0):\n        pipeline.add_config(\n            f\"case3_fbs_{int(bin_width)}\",\n            [\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": bin_width}},\n                extract_all,\n            ],\n        )\n\n    target_configs = [\n        \"case3_fbn_8\", \"case3_fbn_16\", \"case3_fbn_32\",\n        \"case3_fbs_8\", \"case3_fbs_16\", \"case3_fbs_32\",\n    ]\n\n    # Process each image and its associated masks\n    rows = []\n    for img_path in tqdm(image_paths, desc=\"Radiomics (NIfTI images)\", unit=\"image\"):\n        # Strip extensions and _IMG suffix to get case ID\n        subject_id = img_path.name.removesuffix(\".nii.gz\").removesuffix(\".nii\").removesuffix(\"_IMG\")\n\n        # Find all corresponding masks (e.g., CASE001_MASK1.nii.gz, etc.)\n        mask_paths = sorted(input_dir.glob(f\"{subject_id}_MASK*.nii*\"))\n        if not mask_paths:\n            raise ValueError(f\"No masks found for {subject_id}\")\n\n        image = load_image(str(img_path))\n\n        # Merge multiple masks into one and ensure binary semantics\n        mask = load_and_merge_images(\n            [str(p) for p in mask_paths],\n            reference_image=image,\n            binarize=True\n        )\n\n        pipeline.clear_log()\n\n        # Run extraction for all target configurations\n        results = pipeline.run(\n            image=image,\n            mask=mask,\n            subject_id=subject_id,\n            config_names=target_configs,\n        )\n\n        # Format results and store metadata\n        row = format_results(\n            results,\n            fmt=\"long\",\n            meta={\n                \"subject_id\": subject_id,\n                \"image\": str(img_path),\n                \"masks\": \";\".join(str(p) for p in mask_paths),\n            },\n        )\n        rows.append(row)\n\n        # Save per-case logs\n        pipeline.save_log(str(log_dir / f\"{subject_id}.json\"))\n\n    # Consolidated export (save_results handles list of DataFrames for long format automatically)\n    save_results(rows, output_file)\n    print(f\"Wrote {len(rows)} cases to {output_file}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user_guide/case_examples/#case-4-parallel-batch-radiomics-from-dicom-cases-merge-multiple-segmentation-folders","title":"Case 4: Parallel batch radiomics from DICOM cases (merge multiple segmentation folders)","text":""},{"location":"user_guide/case_examples/#scenario_3","title":"Scenario","text":"<p>You have a folder of cases. Each case is a separate folder and contains:</p> <ul> <li><code>Image/</code>: the DICOM image series (CT/MR/etc.), stored at an arbitrary depth.</li> <li><code>Segmentation/</code>: multiple subfolders, each containing a segmentation series at an arbitrary depth.</li> </ul> <p>You want to:</p> <ul> <li>For each case, recursively discover the DICOM image series folder.</li> <li>Discover all segmentation series folders under <code>Segmentation/</code> and load them.</li> <li>Convert each segmentation to a binary mask (values <code>&gt; 0</code> become 1).</li> <li>Merge all binary masks into a single ROI mask per case.</li> <li>Apply all preprocessing steps supported by the pipeline (resample, resegment, outlier filtering,   intensity rounding, and largest connected component).</li> <li>Compute radiomics for six discretisations (FBN 8/16/32 and FBS 8/16/32).</li> <li>Run cases in parallel on multiple CPU cores, with a user-controlled <code>n_jobs</code>.</li> <li>Export a single wide JSON file (one object per case) and save per-case logs.</li> </ul>"},{"location":"user_guide/case_examples/#notes_1","title":"Notes","text":"<p>Note</p> <ul> <li>Progress bar dependency: This example uses <code>tqdm</code>.<ul> <li>If you are running this outside the library repo, install it with <code>pip install tqdm</code>.</li> <li>If you are adding it to your Poetry-managed project, use <code>poetry add tqdm</code>.</li> </ul> </li> <li>Multiprocessing requirement: On Windows/macOS, keep the parallel execution inside   <code>if __name__ == \"__main__\":</code> (as shown) to avoid process-spawn issues.</li> <li>JIT warmup in parallel workers: Pictologics performs a Numba JIT warmup at package import.     With <code>ProcessPoolExecutor</code>, each worker is a separate Python process, so warmup happens once per worker process     (on its first import of <code>pictologics</code>) and then stays warm for all cases that worker processes.     It is not re-run for every case unless you explicitly call <code>warmup_jit()</code> inside your per-case function.     You can disable auto-warmup via <code>PICTOLOGICS_DISABLE_WARMUP=1</code> if you prefer to skip the upfront cost.</li> <li>Preprocessing parameters are dataset-dependent: The <code>resegment</code> range here uses the CT HU example   <code>[-100, 3000]</code>. Adjust or remove it for non-CT data.</li> </ul>"},{"location":"user_guide/case_examples/#full-example-script_3","title":"Full example script","text":"<p>Full example script</p> <pre><code>from concurrent.futures import ProcessPoolExecutor, as_completed\nfrom pathlib import Path\nimport numpy as np\nfrom pictologics import Image, RadiomicsPipeline, load_image, load_and_merge_images\nfrom pictologics.results import format_results, save_results\n\n\ndef collect_segmentation_series_roots(seg_root):\n    \"\"\"Collect all segmentation series subfolders.\"\"\"\n    if not seg_root.exists():\n        raise ValueError(f\"Folder does not exist: {seg_root}\")\n\n    subdirs = sorted([p for p in seg_root.iterdir() if p.is_dir()])\n    if not subdirs:\n        return [seg_root]\n\n    return subdirs\n\ndef build_case4_pipeline():\n    \"\"\"Define the pipeline with preprocessing and all feature families.\"\"\"\n    pipeline = RadiomicsPipeline()\n\n    # Define standard CT preprocessing\n    preprocess_steps = [\n        {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n        {\"step\": \"resegment\", \"params\": {\"range_min\": -100, \"range_max\": 3000}},\n        {\"step\": \"filter_outliers\", \"params\": {\"sigma\": 3.0}},\n        {\"step\": \"round_intensities\"},\n        {\"step\": \"keep_largest_component\", \"params\": {\"apply_to\": \"morph\"}},\n    ]\n\n    extract_all = {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": False,\n            \"include_local_intensity\": False,\n        },\n    }\n\n    # Add discretisation variants\n    for n_bins in (8, 16, 32):\n        pipeline.add_config(\n            f\"case4_fbn_{n_bins}\",\n            preprocess_steps + [{\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": n_bins}}, extract_all],\n        )\n\n    for bin_width in (8.0, 16.0, 32.0):\n        pipeline.add_config(\n            f\"case4_fbs_{int(bin_width)}\",\n            preprocess_steps + [{\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": bin_width}}, extract_all],\n        )\n\n    config_names = [f\"case4_fbn_{b}\" for b in (8, 16, 32)] + [f\"case4_fbs_{b}\" for b in (8, 16, 32)]\n    return pipeline, config_names\n\ndef process_case(case_dir, log_dir):\n    \"\"\"Worker function for single case processing.\"\"\"\n    case_path = Path(case_dir)\n    subject_id = case_path.name\n    image_root = case_path / \"Image\"\n    seg_root = case_path / \"Segmentation\"\n\n    # Load image recursively\n    image = load_image(str(image_root), recursive=True)\n\n    # Load and merge all found segmentations\n    seg_roots = collect_segmentation_series_roots(seg_root)\n\n    # load_and_merge_images handles multiple paths, geometry checking, and binarization\n    try:\n        mask = load_and_merge_images(\n            [str(p) for p in seg_roots], \n            reference_image=image, \n            binarize=True, \n            recursive=True\n        )\n    except ValueError as e:\n         raise ValueError(f\"Failed to load/merge masks for {subject_id}: {e}\")\n\n    # Setup and run pipeline\n    pipeline, config_names = build_case4_pipeline()\n    pipeline.clear_log()\n    results = pipeline.run(image=image, mask=mask, subject_id=subject_id, config_names=config_names)\n\n    # Save results and log\n    Path(log_dir).mkdir(parents=True, exist_ok=True)\n    pipeline.save_log(str(Path(log_dir) / f\"{subject_id}.json\"))\n\n    return format_results(\n        results,\n        fmt=\"wide\",\n        meta={\n            \"subject_id\": subject_id,\n            \"image_root\": str(image_root),\n            \"seg_roots\": \";\".join(str(p) for p in seg_roots),\n        },\n    )\n\ndef main():\n    # Configure paths\n    cases_dir = Path(\"path/to/cases_root\")\n    output_file = Path(\"case4_parallel_results.json\")\n    log_dir = Path(\"case4_logs\")\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Start parallel processing\n    n_jobs = 4\n    case_dirs = sorted([p for p in cases_dir.iterdir() if p.is_dir()])\n    if not case_dirs:\n        raise ValueError(f\"No case folders found in: {cases_dir}\")\n\n    from tqdm import tqdm\n    rows = []\n    errors = []\n\n    # Map each case to the worker function\n    with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n        futures = {\n            executor.submit(process_case, str(case_dir), str(log_dir)): case_dir\n            for case_dir in case_dirs\n        }\n\n        with tqdm(total=len(futures), desc=\"Radiomics (parallel cases)\", unit=\"case\") as pbar:\n            for fut in as_completed(futures):\n                case_dir = futures[fut]\n                try:\n                    rows.append(fut.result())\n                except Exception as e:\n                    errors.append((str(case_dir), repr(e)))\n                finally:\n                    pbar.update(1)\n\n    if errors:\n        msg = \"\\n\".join(f\"- {case}: {err}\" for case, err in errors)\n        raise RuntimeError(f\"One or more cases failed:\\n{msg}\")\n\n    # Final data export\n    save_results(rows, output_file)\n    print(f\"Wrote {len(rows)} cases to {output_file}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user_guide/case_examples/#case-5-batch-radiomics-from-dicom-seg-files-with-multiple-segments","title":"Case 5: Batch radiomics from DICOM SEG files with multiple segments","text":""},{"location":"user_guide/case_examples/#scenario_4","title":"Scenario","text":"<p>You have a folder of cases. Each case contains:</p> <ul> <li><code>Image/</code>: the DICOM image series (CT/MR/etc.)</li> <li><code>segmentation.dcm</code>: a DICOM SEG file with multiple labeled segments (e.g., liver, spleen, kidneys)</li> </ul> <p>You want to:</p> <ul> <li>Load the image and inspect the available segments in the SEG file.</li> <li>Process each segment separately to get per-organ radiomics.</li> <li>Apply standard preprocessing and compute features for each segment.</li> <li>Export results with segment labels in the output.</li> </ul>"},{"location":"user_guide/case_examples/#notes_2","title":"Notes","text":"<p>Note</p> <ul> <li><code>get_segment_info()</code> returns metadata about each segment (number, label, algorithm).</li> <li><code>load_seg()</code> with <code>combine_segments=False</code> returns a dict mapping segment numbers to <code>Image</code> objects.</li> <li>Alignment: Use <code>reference_image</code> to ensure the SEG mask matches the CT geometry.</li> </ul>"},{"location":"user_guide/case_examples/#full-example-script_4","title":"Full example script","text":"<p>Full example script</p> <pre><code>from pathlib import Path\nfrom pictologics import load_image, load_seg, RadiomicsPipeline\nfrom pictologics.loaders import get_segment_info\nfrom pictologics.results import format_results, save_results\n\n\ndef main():\n    # Configure paths\n    cases_dir = Path(\"path/to/cases_root\")\n    output_csv = Path(\"case5_per_segment_results.csv\")\n    log_dir = Path(\"case5_logs\")\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Find all case directories\n    case_dirs = sorted([p for p in cases_dir.iterdir() if p.is_dir()])\n    if not case_dirs:\n        raise ValueError(f\"No case folders found in: {cases_dir}\")\n\n    # Initialize pipeline with standard config\n    pipeline = RadiomicsPipeline()\n    pipeline.add_config(\n        \"case5_fbn_32\",\n        [\n            {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n            {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n            {\n                \"step\": \"extract_features\",\n                \"params\": {\n                    \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\"],\n                    \"include_spatial_intensity\": False,\n                    \"include_local_intensity\": False,\n                },\n            },\n        ],\n    )\n\n    from tqdm import tqdm\n    rows = []\n\n    for case_dir in tqdm(case_dirs, desc=\"Radiomics (per-segment)\", unit=\"case\"):\n        subject_id = case_dir.name\n        image_root = case_dir / \"Image\"\n        seg_file = case_dir / \"segmentation.dcm\"\n\n        # Load the reference image\n        image = load_image(str(image_root), recursive=True)\n\n        # Inspect available segments\n        segments = get_segment_info(str(seg_file))\n        print(f\"\\n{subject_id}: Found {len(segments)} segments\")\n        for seg in segments:\n            print(f\"  Segment {seg['segment_number']}: {seg['segment_label']}\")\n\n        # Load each segment separately, aligned to image geometry\n        segment_masks = load_seg(\n            str(seg_file),\n            combine_segments=False,  # Returns dict {seg_num: Image}\n            reference_image=image\n        )\n\n        # Process each segment\n        for seg_num, mask in segment_masks.items():\n            # Find segment label from metadata\n            seg_info = next(s for s in segments if s[\"segment_number\"] == seg_num)\n            seg_label = seg_info[\"segment_label\"]\n\n            pipeline.clear_log()\n            results = pipeline.run(\n                image=image,\n                mask=mask,\n                subject_id=f\"{subject_id}_{seg_label}\",\n                config_names=[\"case5_fbn_32\"],\n            )\n\n            row = format_results(\n                results,\n                fmt=\"wide\",\n                meta={\n                    \"subject_id\": subject_id,\n                    \"segment_number\": seg_num,\n                    \"segment_label\": seg_label,\n                },\n            )\n            rows.append(row)\n\n            # Save per-segment log\n            pipeline.save_log(str(log_dir / f\"{subject_id}_{seg_label}.json\"))\n\n    # Export all results\n    save_results(rows, output_csv)\n    print(f\"\\nWrote {len(rows)} rows to {output_csv}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user_guide/case_examples/#output-format_1","title":"Output format","text":"<ul> <li>One row per segment per case.</li> <li>Columns include:</li> <li><code>subject_id</code> - Case identifier</li> <li><code>segment_number</code> - Numeric segment ID from SEG file</li> <li><code>segment_label</code> - Human-readable segment name (e.g., \"Liver\", \"Spleen\")</li> <li>Feature columns prefixed by configuration name</li> </ul>"},{"location":"user_guide/case_examples/#case-6-filtered-radiomics-using-ibsi-2-filters","title":"Case 6: Filtered radiomics using IBSI 2 filters","text":""},{"location":"user_guide/case_examples/#scenario_5","title":"Scenario","text":"<p>You want to extract radiomic features from filtered response maps (IBSI 2 paradigm). This is useful for:</p> <ul> <li>Capturing texture at multiple scales (LoG with different \u03c3 values).</li> <li>Extracting directional texture patterns (Gabor filters).</li> <li>Multi-resolution analysis (wavelet decomposition).</li> </ul> <p>You want to:</p> <ul> <li>Apply multiple filters to each image.</li> <li>Extract intensity features from each filtered response map.</li> <li>Compare features across filter types and parameters.</li> </ul>"},{"location":"user_guide/case_examples/#key-concepts","title":"Key concepts","text":"<ul> <li>Filter step before feature extraction: Apply <code>filter</code> after preprocessing but before <code>extract_features</code>.</li> <li>Intensity features make sense: For filtered images, first-order statistics (mean, variance, skewness) capture texture properties.</li> <li>No discretisation for filtered images: IBSI 2 Phase 2 recommends intensity features from continuous filtered values.</li> </ul>"},{"location":"user_guide/case_examples/#full-example-script_5","title":"Full example script","text":"<p>Full example script</p> <pre><code>from pathlib import Path\nfrom pictologics import RadiomicsPipeline\nfrom pictologics.results import format_results, save_results\n\n\ndef main():\n    # Configure paths\n    image_path = Path(\"path/to/image.nii.gz\")\n    mask_path = Path(\"path/to/mask.nii.gz\")\n    output_csv = Path(\"filtered_radiomics.csv\")\n\n    # Initialize pipeline\n    pipeline = RadiomicsPipeline()\n\n    # IBSI 2 Phase 2 preprocessing (Config B)\n    preprocess_steps = [\n        {\"step\": \"resample\", \"params\": {\n            \"new_spacing\": (1.0, 1.0, 1.0), \n            \"interpolation\": \"cubic\"\n        }},\n        {\"step\": \"round_intensities\", \"params\": {}},\n        {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n    ]\n\n    # Feature extraction (intensity only for filtered images)\n    extract_intensity = {\n        \"step\": \"extract_features\",\n        \"params\": {\"families\": [\"intensity\"]},\n    }\n\n    # Collect config names dynamically\n    filter_configs = []\n\n    # --- Configuration 1: LoG at multiple scales ---\n    for sigma in [1.5, 3.0, 5.0]:\n        name = f\"log_sigma_{sigma}\"\n        pipeline.add_config(\n            name,\n            preprocess_steps + [\n                {\"step\": \"filter\", \"params\": {\n                    \"type\": \"log\",\n                    \"sigma_mm\": sigma,\n                    \"truncate\": 4.0,\n                }},\n                extract_intensity,\n            ],\n        )\n        filter_configs.append(name)\n\n    # --- Configuration 2: Gabor filter ---\n    name = \"gabor_5mm\"\n    pipeline.add_config(\n        name,\n        preprocess_steps + [\n            {\"step\": \"filter\", \"params\": {\n                \"type\": \"gabor\",\n                \"sigma_mm\": 5.0,\n                \"lambda_mm\": 2.0,\n                \"gamma\": 1.5,\n                \"rotation_invariant\": True,\n                \"pooling\": \"average\",\n            }},\n            extract_intensity,\n        ],\n    )\n    filter_configs.append(name)\n\n    # --- Configuration 3: Wavelet decomposition ---\n    for decomp in [\"LLH\", \"HHL\", \"HHH\"]:\n        name = f\"wavelet_{decomp}\"\n        pipeline.add_config(\n            name,\n            preprocess_steps + [\n                {\"step\": \"filter\", \"params\": {\n                    \"type\": \"wavelet\",\n                    \"wavelet\": \"db3\",\n                    \"level\": 1,\n                    \"decomposition\": decomp,\n                    \"rotation_invariant\": True,\n                    \"pooling\": \"average\",\n                }},\n                extract_intensity,\n            ],\n        )\n        filter_configs.append(name)\n\n    # --- Configuration 4: Laws texture energy ---\n    name = \"laws_L5E5E5\"\n    pipeline.add_config(\n        name,\n        preprocess_steps + [\n            {\"step\": \"filter\", \"params\": {\n                \"type\": \"laws\",\n                \"kernel\": \"L5E5E5\",\n                \"rotation_invariant\": True,\n                \"pooling\": \"max\",\n                \"compute_energy\": True,\n                \"energy_distance\": 7,\n            }},\n            extract_intensity,\n        ],\n    )\n    filter_configs.append(name)\n\n    # Run pipeline\n    results = pipeline.run(\n        image=str(image_path),\n        mask=str(mask_path),\n        subject_id=\"subject_001\",\n        config_names=filter_configs,\n    )\n\n    # Format and save\n    row = format_results(\n        results,\n        fmt=\"wide\",\n        meta={\"subject_id\": \"subject_001\"},\n    )\n    save_results([row], output_csv)\n    print(f\"Saved filtered radiomics to {output_csv}\")\n\n    # Display key features for each filter\n    print(\"\\nMean values from each filter:\")\n    for config in filter_configs:\n        mean_val = results[config].get(\"mean_intensity_Q4LE\", \"N/A\")\n        print(f\"  {config}: {mean_val:.4f}\" if isinstance(mean_val, float) else f\"  {config}: {mean_val}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user_guide/case_examples/#output-format_2","title":"Output format","text":"<ul> <li>One row per subject with features from each filter as separate columns.</li> <li>Column names use the pattern <code>{filter_config}__mean_intensity_Q4LE</code> (e.g., <code>log_sigma_1.5__mean_intensity_Q4LE</code>).</li> </ul>"},{"location":"user_guide/case_examples/#combining-with-standard-texture-features","title":"Combining with standard texture features","text":"<p>You can also run both filtered and standard texture configs together:</p> <pre><code># Standard texture config (unfiltered)\npipeline.add_config(\"standard_fbn_32\", [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"texture\"]}},\n])\n\n# Run all configs together\nall_configs = filter_configs + [\"standard_fbn_32\"]\nresults = pipeline.run(image, mask, config_names=all_configs)\n</code></pre>"},{"location":"user_guide/case_examples/#output-options","title":"Output Options","text":"<p>For details on result formatting (<code>wide</code> vs <code>long</code>, <code>output_type</code> options) and export functions, see the Feature Calculations - Working with Results.</p>"},{"location":"user_guide/data_loading/","title":"Data Loading","text":"<p>This guide covers all aspects of loading medical imaging data into Pictologics. Whether you're working with NIfTI files, DICOM series, multi-phase acquisitions, or segmentation masks, this page will help you get your data into the <code>Image</code> class for radiomics analysis.</p>"},{"location":"user_guide/data_loading/#the-image-class","title":"The Image Class","text":"<p>All data in Pictologics is represented by the <code>Image</code> dataclass, which provides a standardized container for 3D medical image data. All data are stored as 3D numpy arrays, with additional metadata to describe the geometry of the data. If 2D data is provided, it is converted to a 3D numpy array with a singleton dimension.</p> <pre><code>from pictologics import Image\n\n# Image attributes:\n# - array: numpy.ndarray (3D, in X, Y, Z order)\n# - spacing: tuple[float, float, float] (voxel dimensions in mm)\n# - origin: tuple[float, float, float] (world coordinates of first voxel)\n# - direction: Optional[numpy.ndarray] (3x3 direction cosine matrix)\n# - modality: str (e.g., \"CT\", \"MR\", \"Unknown\")\n</code></pre> <p>Note</p> <p>Pictologics uses (X, Y, Z) axis ordering to match ITK/SimpleITK conventions. This differs from raw DICOM (which uses Rows, Columns = Y, X) and matplotlib (which expects height, width = Y, X). All loaders handle these transformations automatically.</p>"},{"location":"user_guide/data_loading/#basic-loading-with-load_image","title":"Basic Loading with <code>load_image()</code>","text":"<p>The <code>load_image()</code> function is the primary entry point for loading data. It automatically detects the file format and handles the appropriate loading strategy.</p>"},{"location":"user_guide/data_loading/#loading-nifti-files","title":"Loading NIfTI Files","text":"<pre><code>from pictologics import load_image\n\n# Load a NIfTI file (.nii or .nii.gz)\nimage = load_image(\"path/to/scan.nii.gz\")\nmask = load_image(\"path/to/segmentation.nii.gz\")\n\nprint(f\"Shape: {image.array.shape}\")\nprint(f\"Spacing: {image.spacing}\")\nprint(f\"Origin: {image.origin}\")\n</code></pre>"},{"location":"user_guide/data_loading/#loading-dicom-series","title":"Loading DICOM Series","text":"<p>For a directory containing DICOM files from a single series:</p> <pre><code># Load all DICOM files in a directory as a single volume\nimage = load_image(\"path/to/dicom_folder/\")\n\n# Pictologics automatically:\n# - Finds all DICOM files in the directory\n# - Sorts slices by spatial position\n# - Extracts spacing, origin, and direction from headers\n# - Stacks slices into a 3D volume\n</code></pre>"},{"location":"user_guide/data_loading/#loading-a-single-dicom-file","title":"Loading a Single DICOM File","text":"<p>Single DICOM files (e.g., enhanced DICOM, segmentation objects) are also supported:</p> <pre><code># Load a single DICOM file\nimage = load_image(\"path/to/image.dcm\")\n</code></pre>"},{"location":"user_guide/data_loading/#dicom-intensity-rescaling","title":"DICOM Intensity Rescaling","text":"<p>By default, <code>load_image()</code> applies RescaleSlope and RescaleIntercept transformations to DICOM data, converting stored pixel values to real-world values (e.g., Hounsfield Units for CT). This matches the behavior of NIfTI loading, which always applies its scaling factors.</p> <pre><code># Default: values are converted (e.g., to Hounsfield Units)\nct = load_image(\"ct_scan/\")\nprint(ct.array.min(), ct.array.max())  # e.g., -1024.0 to 3000.0\n\n# If you need raw stored pixel values:\nct_raw = load_image(\"ct_scan/\", apply_rescale=False)\nprint(ct_raw.array.min(), ct_raw.array.max())  # e.g., 0 to 4095\n</code></pre> Format Rescaling Behavior NIfTI Always applies <code>scl_slope</code> and <code>scl_inter</code> from header DICOM Applies <code>RescaleSlope</code> and <code>RescaleIntercept</code> when <code>apply_rescale=True</code> (default)"},{"location":"user_guide/data_loading/#handling-sentinel-na-values","title":"Handling Sentinel (NA) Values","text":"<p>Medical imaging formats often use a sentinel value to represent missing or invalid data. Common examples:</p> Modality Common Sentinel Values CT -1024, -2048, -32768 (outside tissue HU range) MR 0 (often used for background/air) PET 0 or negative values <p>Since DICOM uses integer storage and cannot represent <code>NaN</code>, these sentinel values are substituted for missing data. To exclude them from analysis, use the <code>resegment</code> preprocessing step in your pipeline:</p> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\npipeline.add_config(\"ct_analysis\", [\n    # Exclude sentinel values by filtering to valid HU range\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -100, \"range_max\": 3000}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"texture\"]}},\n])\n</code></pre> <p>This is the IBSI-recommended approach \u2014 filter invalid intensities before feature extraction rather than at load time.</p> <p>Tip</p> <p>Check your data's minimum value to identify potential sentinels: <pre><code>image = load_image(\"scan.dcm\")\nprint(f\"Min: {image.array.min()}, Max: {image.array.max()}\")\n# If min is -1024 or -2048, those are likely sentinels\n</code></pre></p>"},{"location":"user_guide/data_loading/#multi-phase-dicom-series","title":"Multi-Phase DICOM Series","text":"<p>Many clinical acquisitions contain multiple phases (e.g., cardiac CT with multiple timepoints). Pictologics can detect and load specific phases from datasets.</p>"},{"location":"user_guide/data_loading/#discovering-available-phases","title":"Discovering Available Phases","text":"<p>Use <code>get_dicom_phases()</code> to explore what's available before loading:</p> <pre><code>from pictologics.utilities import get_dicom_phases\n\n# Discover phases in a multi-phase DICOM directory\nphases = get_dicom_phases(\"path/to/cardiac_ct/\")\n\nprint(f\"Found {len(phases)} phases:\")\nfor phase in phases:\n    print(f\"--- Phase {phase.index} ---\")\n    print(f\"Label:       {phase.label}\")\n    print(f\"Split Tag:   {phase.split_tag}\")\n    print(f\"Split Value: {phase.split_value}\")\n    print(f\"Num Slices:  {phase.num_slices}\")\n    # Show first file path as example\n    print(f\"Example File: {phase.file_paths[0].name}\")\n</code></pre> <p>Example output: <pre><code>Found 10 phases:\n--- Phase 0 ---\nLabel:       Phase 0%\nSplit Tag:   NominalPercentageOfCardiacPhase\nSplit Value: 0\nNum Slices:  256\nExample File: IM-0001-0001.dcm\n--- Phase 1 ---\nLabel:       Phase 10%\nSplit Tag:   NominalPercentageOfCardiacPhase\nSplit Value: 10\nNum Slices:  256\nExample File: IM-0001-0225.dcm\n...\n</code></pre></p> <p>Each <code>DicomPhaseInfo</code> object contains:</p> <ul> <li><code>index</code>: Phase index (0, 1, 2, ...)</li> <li><code>label</code>: Human-readable label (e.g., \"CardiacPhase=0\", \"TemporalPosition=1\")</li> <li><code>num_slices</code>: Number of slices in this phase</li> <li><code>tag_name</code>: The DICOM tag used for detection</li> <li><code>tag_value</code>: The actual tag value</li> </ul>"},{"location":"user_guide/data_loading/#loading-a-specific-phase","title":"Loading a Specific Phase","text":"<p>Use the <code>dataset_index</code> parameter to load a particular phase:</p> <pre><code># Load the first phase (index 0)\nphase_0 = load_image(\"path/to/cardiac_ct/\", dataset_index=0)\n\n# Load the second phase (index 1)\nphase_1 = load_image(\"path/to/cardiac_ct/\", dataset_index=1)\n</code></pre>"},{"location":"user_guide/data_loading/#phase-detection-priority","title":"Phase Detection Priority","text":"<p>Pictologics automatically detects phases using these DICOM tags (in order of priority):</p> <ol> <li>NominalPercentageOfCardiacPhase - Cardiac phases (percentage)</li> <li>TemporalPositionIdentifier - Temporal position index</li> <li>TriggerTime - ECG trigger time</li> <li>AcquisitionNumber - Acquisition sequence number</li> <li>EchoNumber - Multi-echo MRI</li> </ol>"},{"location":"user_guide/data_loading/#4d-nifti-files","title":"4D NIfTI Files","text":"<p>NIfTI files can contain 4D data (3D + time/phase). Use <code>dataset_index</code> similarly:</p> <pre><code># Load a 4D NIfTI file - get the first volume\nvol_0 = load_image(\"path/to/4d_data.nii.gz\", dataset_index=0)\n\n# Load the second volume\nvol_1 = load_image(\"path/to/4d_data.nii.gz\", dataset_index=1)\n</code></pre>"},{"location":"user_guide/data_loading/#dicom-segmentation-seg-files","title":"DICOM Segmentation (SEG) Files","text":"<p>DICOM SEG files are specialized objects containing segmentation masks. Use <code>load_seg()</code> for full control, or let <code>load_image()</code> auto-detect them.</p>"},{"location":"user_guide/data_loading/#auto-detection-in-load_image","title":"Auto-Detection in load_image()","text":"<pre><code># load_image() automatically detects DICOM SEG files\nmask = load_image(\"path/to/segmentation.dcm\")\n</code></pre>"},{"location":"user_guide/data_loading/#detailed-control-with-load_seg","title":"Detailed Control with load_seg()","text":"<pre><code>from pictologics import load_seg\nfrom pictologics.loaders import get_segment_info\n\n# First, inspect what segments are available\nsegments = get_segment_info(\"path/to/segmentation.dcm\")\nfor seg in segments:\n    print(f\"Segment {seg['segment_number']}: {seg['segment_label']}\")\n\n# Load all segments combined into a single label mask\n# Each segment gets its numeric label (1, 2, 3, etc.)\ncombined_mask = load_seg(\"path/to/segmentation.dcm\")\nprint(np.unique(combined_mask.array))  # [0, 1, 2, 3, ...]\n# Background = 0, Segment 1 = 1, Segment 2 = 2, etc.\n\n# Load only specific segments\nliver_mask = load_seg(\n    \"path/to/segmentation.dcm\",\n    segment_numbers=[1, 2]  # Only segments 1 and 2\n)\n\n# Load segments separately (returns dict)\nseparate_masks = load_seg(\n    \"path/to/segmentation.dcm\",\n    combine_segments=False\n)\n# separate_masks = {1: Image(...), 2: Image(...), ...}\n</code></pre>"},{"location":"user_guide/data_loading/#working-with-separate-segments","title":"Working with Separate Segments","text":"<p>When using <code>combine_segments=False</code>, you can iterate over segments for individual analysis:</p> <pre><code># Get each segment as a separate binary mask\nmasks = load_seg(\"seg.dcm\", combine_segments=False)\n\n# Iterate over segments\nfor seg_num, mask in masks.items():\n    print(f\"Segment {seg_num}: {mask.array.sum()} voxels\")\n\n# Process each segment for radiomics\nfor seg_num, mask in masks.items():\n    features = pipeline.run(image=ct, mask=mask)\n</code></pre>"},{"location":"user_guide/data_loading/#combining-specific-segments-into-a-binary-mask","title":"Combining Specific Segments into a Binary Mask","text":"<p>To merge selected segments into a single binary mask:</p> <pre><code># Load specific segments separately\nmasks = load_seg(\"seg.dcm\", segment_numbers=[1, 2], combine_segments=False)\n\n# Combine into single binary mask using logical OR\ncombined = masks[1].array | masks[2].array\n</code></pre>"},{"location":"user_guide/data_loading/#aligning-seg-to-a-reference-image","title":"Aligning SEG to a Reference Image","text":"<p>SEG files may have different geometry than the source image. Use <code>reference_image</code> to align:</p> <pre><code># Load the CT image\nct = load_image(\"path/to/ct_series/\")\n\n# Load and align the segmentation to CT geometry\nmask = load_seg(\n    \"path/to/segmentation.dcm\",\n    reference_image=ct\n)\n\n# Now mask.array.shape == ct.array.shape\n</code></pre>"},{"location":"user_guide/data_loading/#merging-multiple-images-with-load_and_merge_images","title":"Merging Multiple Images with <code>load_and_merge_images()</code>","text":"<p>When you have multiple segmentation masks (e.g., different organs, or masks split across files), use <code>load_and_merge_images()</code> to combine them.</p>"},{"location":"user_guide/data_loading/#basic-merging","title":"Basic Merging","text":"<pre><code>from pictologics import load_and_merge_images\n\n# Merge multiple mask files into one\ncombined_mask = load_and_merge_images([\n    \"path/to/liver_mask.nii.gz\",\n    \"path/to/kidney_mask.nii.gz\",\n    \"path/to/spleen_mask.nii.gz\"\n])\n</code></pre>"},{"location":"user_guide/data_loading/#relabeling-masks-for-visualization","title":"Relabeling Masks for Visualization","text":"<p>When merging binary masks, assign unique labels to each:</p> <pre><code># Each mask gets a unique label (1, 2, 3, ...)\ncombined = load_and_merge_images(\n    [\"mask1.nii.gz\", \"mask2.nii.gz\", \"mask3.nii.gz\"],\n    relabel_masks=True\n)\n# Result: voxels from mask1 = 1, mask2 = 2, mask3 = 3\n</code></pre>"},{"location":"user_guide/data_loading/#merge-strategy-options","title":"Merge Strategy Options","text":"<p>Control how overlapping voxels are handled:</p> <pre><code># \"max\" (default): Take the maximum value at each voxel\ncombined = load_and_merge_images(masks, merge_strategy=\"max\")\n\n# \"sum\": Add values (useful for probability maps)\ncombined = load_and_merge_images(masks, merge_strategy=\"sum\")\n\n# \"first\": Keep the first non-zero value\ncombined = load_and_merge_images(masks, merge_strategy=\"first\")\n\n# \"last\": Keep the last non-zero value\ncombined = load_and_merge_images(masks, merge_strategy=\"last\")\n</code></pre>"},{"location":"user_guide/data_loading/#handling-cropped-masks","title":"Handling Cropped Masks","text":"<p>Medical imaging software often stores segmentation masks as cropped volumes (bounding boxes around the region of interest) to minimize storage. When loading these cropped masks, they need to be repositioned into the original image's coordinate space for proper visualization and analysis.</p> <p>The <code>pictologics</code> loader uses the spatial metadata (<code>ImagePositionPatient</code> for DICOM, affine matrix for NIfTI) to calculate where the cropped mask belongs in the full volume.</p>"},{"location":"user_guide/data_loading/#repositioning-a-single-cropped-mask","title":"Repositioning a Single Cropped Mask","text":"<pre><code># Load the full CT image\nct = load_image(\"path/to/full_ct/\")\n\n# Load a cropped mask and reposition it\ncropped_mask = load_image(\n    \"path/to/cropped_mask.nii.gz\",\n    reference_image=ct\n)\n# cropped_mask now has the same shape as ct\n</code></pre>"},{"location":"user_guide/data_loading/#merging-multiple-cropped-masks","title":"Merging Multiple Cropped Masks","text":"<pre><code># Load CT as reference\nct = load_image(\"path/to/ct/\")\n\n# Merge cropped masks into reference space\ncombined = load_and_merge_images(\n    [\"cropped_liver.nii.gz\", \"cropped_kidney.nii.gz\"],\n    reference_image=ct,\n    reposition_to_reference=True,\n    relabel_masks=True\n)\n</code></pre>"},{"location":"user_guide/data_loading/#handling-axis-transposition","title":"Handling Axis Transposition","text":"<p>If your masks have different axis ordering (e.g., from different software), specify the transformation:</p> <pre><code>combined = load_and_merge_images(\n    mask_paths,\n    reference_image=ct,\n    reposition_to_reference=True,\n    transpose_axes=(1, 0, 2)  # Swap X and Y axes\n)\n</code></pre>"},{"location":"user_guide/data_loading/#error-handling","title":"Error Handling","text":"Issue Behavior Spacing mismatch <code>ValueError</code> raised (resampling not yet supported) Orientation mismatch Warning emitted, positioning continues Mask outside reference bounds Warning emitted, empty volume returned Partial overlap Valid region is positioned, rest is clipped <p>Tip</p> <p>Label Order: When using <code>relabel_masks=True</code>, labels are assigned based on the order of files in <code>image_paths</code>. Use <code>sorted()</code> for consistent ordering, or specify the exact order you want.</p>"},{"location":"user_guide/data_loading/#creating-a-full-mask","title":"Creating a Full Mask","text":"<p>When you don't have a segmentation mask and want to analyze the entire image:</p> <pre><code>from pictologics import create_full_mask\n\n# Create a mask of all ones matching the image geometry\nimage = load_image(\"scan.nii.gz\")\nfull_mask = create_full_mask(image)\n\n# Now use full_mask for whole-image analysis\n</code></pre> <p>Tip</p> <p>If you pass <code>mask=None</code> to <code>RadiomicsPipeline.run()</code>, it automatically creates a full mask internally.</p>"},{"location":"user_guide/data_loading/#complete-workflow-examples","title":"Complete Workflow Examples","text":"<p>Example 1: Standard NIfTI Workflow</p> <pre><code>from pictologics import load_image, RadiomicsPipeline\n\n# Load image and mask\nimage = load_image(\"patient_ct.nii.gz\")\nmask = load_image(\"tumor_segmentation.nii.gz\")\n\n# Run radiomics pipeline\npipeline = RadiomicsPipeline()\nresults = pipeline.run(image, mask, config_names=[\"standard_fbn_32\"])\n</code></pre> <p>Example 2: Multi-Phase DICOM Analysis</p> <pre><code>from pictologics import load_image\nfrom pictologics.utilities import get_dicom_phases\n\n# Discover available phases\nphases = get_dicom_phases(\"cardiac_ct_folder/\")\nprint(f\"Found {len(phases)} cardiac phases\")\n\n# Analyze each phase\nfor phase in phases:\n    image = load_image(\"cardiac_ct_folder/\", dataset_index=phase.index)\n    print(f\"Phase {phase.label}: shape = {image.array.shape}\")\n    # ... run analysis on each phase\n</code></pre> <p>Example 3: DICOM SEG with Reference Alignment</p> <pre><code>from pictologics import load_image, load_seg\nfrom pictologics.loaders import get_segment_info\n\n# Load the CT series\nct = load_image(\"ct_series/\")\n\n# Check available segments\nsegments = get_segment_info(\"segmentation.dcm\")\nprint(\"Available segments:\", [s['segment_label'] for s in segments])\n\n# Load only the liver segment, aligned to CT\nliver = load_seg(\n    \"segmentation.dcm\",\n    segment_numbers=[1],\n    reference_image=ct\n)\n\n# Verify alignment\nassert liver.array.shape == ct.array.shape\n</code></pre> <p>Example 4: Merging Cropped Multi-Organ Masks</p> <pre><code>from pictologics import load_image, load_and_merge_images\n\n# Reference CT\nct = load_image(\"full_body_ct/\")\n\n# List of cropped organ masks\norgan_masks = [\n    \"liver_cropped.nii.gz\",\n    \"spleen_cropped.nii.gz\",\n    \"left_kidney_cropped.nii.gz\",\n    \"right_kidney_cropped.nii.gz\"\n]\n\n# Merge all into reference space with unique labels\ncombined = load_and_merge_images(\n    organ_masks,\n    reference_image=ct,\n    reposition_to_reference=True,\n    relabel_masks=True\n)\n\n# Result: combined.array contains:\n# 0 = background, 1 = liver, 2 = spleen, 3 = left kidney, 4 = right kidney\n</code></pre>"},{"location":"user_guide/data_loading/#summary-of-loading-functions","title":"Summary of Loading Functions","text":"Function Purpose <code>load_image()</code> Main entry point - loads NIfTI, DICOM series, single DICOM, or DICOM SEG <code>load_seg()</code> Detailed DICOM SEG loading with segment selection and alignment <code>get_segment_info()</code> Inspect available segments in a DICOM SEG file <code>load_and_merge_images()</code> Combine multiple images/masks with various strategies <code>create_full_mask()</code> Create an all-ones mask matching image geometry <code>get_dicom_phases()</code> Discover available phases in multi-phase DICOM"},{"location":"user_guide/data_loading/#next-steps","title":"Next Steps","text":"<ul> <li>Feature Calculations - Get started with your first radiomics analysis</li> <li>Pipeline Usage - Configure and run the radiomics pipeline</li> <li>Utilities - DICOM database parsing, visualization, and more</li> </ul>"},{"location":"user_guide/feature_calculations/","title":"Feature Calculations","text":"<p>This guide walks you through extracting radiomic features from medical images using Pictologics.</p>"},{"location":"user_guide/feature_calculations/#prerequisites","title":"Prerequisites","text":"<p>You will need:</p> <ol> <li>A medical image (e.g., <code>image.nii.gz</code>, DICOM folder, or a single DICOM file).</li> <li>Optionally a corresponding mask/segmentation (e.g., <code>mask.nii.gz</code>).</li> </ol> <p>Note</p> <p>Mask is optional If you omit <code>mask</code> (or pass <code>mask=None</code> / <code>mask=\"\"</code>), Pictologics treats the entire image as the ROI by generating a full (all-ones) mask internally. This can be useful for certain workflows (see the Case examples page), but it may not be scientifically appropriate for all studies.</p> <p>Detailed information on how to load images and masks can be found in the Data Loading guide.</p>"},{"location":"user_guide/feature_calculations/#method-1-the-radiomics-pipeline-recommended","title":"Method 1: The Radiomics Pipeline (Recommended)","text":"<p>For reproducible research and standardisation, use the <code>RadiomicsPipeline</code>. This ensures that all preprocessing steps (resampling, resegmentation, discretisation) are applied consistently.</p> <p>Pictologics includes a set of Standard Configurations commonly used in radiomic analyses. You can run all of them with a single command.</p> <p>Note</p> <p>Default performance behavior The built-in <code>standard_*</code> configurations disable the two most time-intensive intensity extras by default: spatial intensity (Moran's I / Geary's C) and local intensity peak features.</p> <p>If you need these metrics, see the customization examples below.</p> <p>Reproducible Research</p> <p>Save your configuration settings for reproducibility using <code>pipeline.save_configs(\"config.yaml\")</code>. See the Predefined Configurations guide for details on exporting, sharing, and version-controlling your pipeline configurations.</p> <pre><code>from pictologics import RadiomicsPipeline, format_results, save_results\n\n# 1. Initialize the pipeline\npipeline = RadiomicsPipeline()\n\n# 2. Run the \"all_standard\" configurations\nresults = pipeline.run(\n    image=\"path/to/image.nii.gz\",\n    mask=\"path/to/mask.nii.gz\",\n    subject_id=\"Subject_001\",\n    config_names=[\"all_standard\"]\n)\n\n# 3. Format and Save Results\nrow = format_results(\n    results, \n    fmt=\"wide\", \n    meta={\"subject_id\": \"Subject_001\"}\n)\nsave_results([row], \"results.csv\")\n\nprint(f\"Saved {len(results)} configurations to results.csv\")\n</code></pre>"},{"location":"user_guide/feature_calculations/#intelligent-image-routing","title":"Intelligent image routing","text":"<p>The pipeline automatically uses the correct image for each feature family. After discretisation, the pipeline maintains both the original (raw) image and the discretised image, ensuring each feature type gets the appropriate input.</p> Feature Family Image Used Why Intensity Raw image Statistics require original continuous values Morphology Raw image Volume/surface calculations use original geometry Histogram Discretised Bin-based statistics require integer bins Texture (GLCM, GLRLM, etc.) Discretised Co-occurrence matrices require discrete grey levels IVH Configurable Can use raw (continuous) or discretised values <p>Example workflow:</p> <pre><code>pipeline.add_config(\"my_config\", [\n    # Step 0: Binarize mask before resampling\n    {\"step\": \"binarize_mask\", \"params\": {\"threshold\": 0.5}},\n    # Step 1: Resample to 0.5mm isotropic\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (0.5, 0.5, 0.5)}},\n\n    # Step 2: Discretise with 32 bins (creates discretised copy)\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n\n    # Step 3: Extract features (routing happens automatically)\n    {\"step\": \"extract_features\", \"params\": {\n        \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\"]\n    }}\n])\n\n# When extract_features runs:\n# - intensity features \u2192 uses resampled raw image\n# - morphology features \u2192 uses resampled raw image  \n# - histogram features \u2192 uses discretised image\n# - texture features \u2192 uses discretised image\n</code></pre> <p>This means you don't need to worry about which image to pass \u2014 the pipeline handles it correctly.</p>"},{"location":"user_guide/feature_calculations/#working-with-results","title":"Working with results","text":"<p>The <code>format_results()</code> function converts pipeline output into different formats for analysis or export.</p> <p>Format Options (<code>fmt</code>):</p> <ol> <li> <p>Wide Format (<code>fmt=\"wide\"</code>): One row per subject with all features as columns.     Column names use the pattern <code>{config}__{feature}</code> (e.g., <code>standard_fbn_32__mean_intensity_Q4LE</code>).     <pre><code>row = format_results(results, fmt=\"wide\", meta={\"subject_id\": \"case1\"})\n# Returns: {\"subject_id\": \"case1\", \"standard_fbn_32__mean_intensity_Q4LE\": 123.4, ...}\n</code></pre></p> </li> <li> <p>Long Format (<code>fmt=\"long\"</code>): Tidy data with one row per feature. The configuration name is automatically included in a <code>config</code> column \u2014 you don't need to specify it in <code>meta</code>.     <pre><code>df = format_results(results, fmt=\"long\", meta={\"subject_id\": \"case1\"}, output_type=\"pandas\")\n# Returns DataFrame with columns: [subject_id, config, feature_name, value]\n# Example rows:\n# | subject_id | config          | feature_name        | value  |\n# |------------|-----------------|---------------------|--------|\n# | case1      | standard_fbn_32 | mean_intensity_Q4LE | 123.4  |\n# | case1      | standard_fbn_32 | volume_RNU0         | 5420.0 |\n# | case1      | standard_fbs_8  | mean_intensity_Q4LE | 123.4  |\n</code></pre></p> </li> </ol> <p>Output Types (<code>output_type</code>):</p> <ul> <li><code>\"dict\"</code> (default): Returns a Python dictionary (wide) or list of dicts (long)</li> <li><code>\"pandas\"</code>: Returns a <code>pandas.DataFrame</code></li> <li><code>\"json\"</code>: Returns a JSON string</li> </ul>"},{"location":"user_guide/feature_calculations/#batch-processing-pattern","title":"Batch Processing Pattern","text":"<pre><code>all_rows = []\nfor file in image_files:\n    res = pipeline.run(image=file, ...)\n    # Format and collect\n    all_rows.append(format_results(res, fmt=\"wide\", meta={\"filename\": file.name}))\n\n# Save everything at once (automatically merges columns)\nsave_results(all_rows, \"full_study_results.csv\")\n</code></pre>"},{"location":"user_guide/feature_calculations/#customizing-the-pipeline","title":"Customizing the pipeline","text":"<p>You can define your own steps to enable advanced features or change parameters.</p> <pre><code>from pictologics import RadiomicsPipeline\n\n# Define a config ensuring 0.5mm isotropic pixels and enabling robust texture extraction\ncfg = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (0.5, 0.5, 0.5), \"round_intensities\": True}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": True,  # Enable Moran's I / Geary's C\n            \"include_local_intensity\": True,    # Enable local intensity peaks\n        },\n    },\n]\n\npipeline = RadiomicsPipeline().add_config(\"my_custom_config\", cfg)\n# ... run as normal\n</code></pre>"},{"location":"user_guide/feature_calculations/#performance-notes-practical","title":"Performance notes (practical)","text":"<ul> <li>Spatial/local intensity can be extremely slow on large ROIs. If you do not need them, keep <code>include_spatial_intensity=False</code> and <code>include_local_intensity=False</code>.</li> <li>Texture requires discretisation. If you request <code>\"texture\"</code> without including a <code>discretise</code> step first, the pipeline will raise an error.</li> <li>If you are working with large 3D images, consider resampling to a coarser spacing for exploratory work.</li> </ul>"},{"location":"user_guide/feature_calculations/#method-2-step-by-step-manual-extraction","title":"Method 2: Step-by-Step Manual Extraction","text":"<p>If you want to understand the underlying process or need granular control over specific functions, you can call the feature extraction functions directly. This example mirrors a complete radiomics workflow, including all preprocessing steps, filtering, and feature families available in the pipeline.</p>"},{"location":"user_guide/feature_calculations/#the-hard-way-manual-extraction","title":"The \"Hard\" Way (Manual Extraction)","text":"<p>This script performs 12 distinct steps to extract all feature families.</p> <pre><code>import numpy as np\nfrom pictologics import load_image\nfrom pictologics.preprocessing import (\n    resample_image,\n    resegment_mask,\n    filter_outliers,\n    discretise_image,\n    apply_mask\n)\nfrom pictologics.features.intensity import (\n    calculate_intensity_features,\n    calculate_intensity_histogram_features,\n    calculate_ivh_features,\n    calculate_spatial_intensity_features,\n    calculate_local_intensity_features\n)\nfrom pictologics.features.morphology import calculate_morphology_features\nfrom pictologics.features.texture import calculate_all_texture_features\n\n# 1. Load Data\n# ---------------------------------------------------------\nprint(\"1. Loading data...\")\nimage = load_image(\"image.nii.gz\")\nmask = load_image(\"mask.nii.gz\")\n\n# 2. Preprocessing &amp; Standardization\n# ---------------------------------------------------------\nprint(\"2. Resampling to 1x1x1 mm...\")\nimage = resample_image(image, new_spacing=(1.0, 1.0, 1.0), interpolation=\"linear\")\nmask = resample_image(mask, new_spacing=(1.0, 1.0, 1.0), interpolation=\"nearest\")\n\nprint(\"3. Rounding intensities (for consistent binning)...\")\nimage.array = np.round(image.array)\n\nprint(\"4. Binarizing mask (thresholding)...\")\nmask.array = (mask.array &gt; 0.5).astype(np.uint8)\n\nprint(\"5. Resegmenting (range filtering)...\")\n# Exclude values outside typical range (e.g. -1000 to 400 HU for lung)\nmask = resegment_mask(image, mask, range_min=-1000, range_max=400)\n\nprint(\"6. Outlier filtering...\")\n# Remove statistical outliers (mean +/- 3 sigma) from the mask\nmask = filter_outliers(image, mask, sigma=3.0)\n\n# 3. Discretisation\n# ---------------------------------------------------------\nprint(\"7. Discretising image (FBN 32 bins)...\")\n# Essential for Texture and Histogram features\ndisc_image = discretise_image(\n    image, \n    method=\"FBN\", \n    n_bins=32, \n    roi_mask=mask\n)\n\n# 4. Feature Extraction (All Families)\n# ---------------------------------------------------------\nprint(\"8. Calculating Morphology...\")\nmorph_feat = calculate_morphology_features(mask, image=image, intensity_mask=mask)\n\nprint(\"9. Calculating Intensity (First Order)...\")\nroi_values = apply_mask(image, mask)\nint_feat = calculate_intensity_features(roi_values)\n\nprint(\"10. Calculating Histogram...\")\n# Uses discretised values\nhist_feat = calculate_intensity_histogram_features(apply_mask(disc_image, mask))\n\nprint(\"11. Calculating IVH (Intensity Volume Histogram)...\")\n# Can use raw or discretised values (usually raw)\nivh_feat = calculate_ivh_features(roi_values, bin_width=10.0)\n\nprint(\"12. Calculating Texture (GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM)...\")\ntex_feat = calculate_all_texture_features(\n    disc_array=disc_image.array,\n    mask_array=mask.array,\n    n_bins=32\n)\n\nprint(\"13. Calculating Advanced Intensity...\")\n# Very computationally intensive\nsp_feat = calculate_spatial_intensity_features(image, mask)\nloc_feat = calculate_local_intensity_features(image, mask)\n\n# 5. Combine Results\n# ---------------------------------------------------------\nall_features = {\n    **morph_feat, **int_feat, **hist_feat, \n    **ivh_feat, **tex_feat, **sp_feat, **loc_feat\n}\n\nprint(f\"\\n--- Extraction Complete: {len(all_features)} features calculated ---\")\n</code></pre>"},{"location":"user_guide/feature_calculations/#the-easy-way-pipeline-implementation","title":"The \"Easy\" Way (Pipeline Implementation)","text":"<p>The <code>RadiomicsPipeline</code> accomplishes the exact same logical workflow in a fraction of the code, with automatic image routing (handling raw vs. discretised copies transparently).</p> <pre><code>from pictologics import RadiomicsPipeline\n\n# Define the exact same workflow configuration\nconfig = [\n    # Preprocessing\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0), \"round_intensities\": True}},\n    {\"step\": \"binarize_mask\", \"params\": {\"threshold\": 0.5}},\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n    {\"step\": \"filter_outliers\", \"params\": {\"sigma\": 3.0}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n\n    # Feature Extraction (All families)\n    {\"step\": \"extract_features\", \"params\": {\n        \"families\": [\"intensity\", \"morphology\", \"histogram\", \"ivh\", \"texture\"],\n        # Enable advanced metrics\n        \"include_spatial_intensity\": True,\n        \"include_local_intensity\": True,\n        # Customize specific family parameters\n        \"ivh_params\": {\"bin_width\": 10.0}\n    }}\n]\n\n# Run it\npipeline = RadiomicsPipeline().add_config(\"comprehensive\", config)\nresults = pipeline.run(\"image.nii.gz\", \"mask.nii.gz\", config_names=[\"comprehensive\"])\n\nprint(f\"--- Extraction Complete: {len(results['comprehensive'])} features calculated ---\")\n</code></pre>"},{"location":"user_guide/feature_calculations/#image-filtering-ibsi-2","title":"Image Filtering (IBSI 2)","text":"<p>Pictologics includes IBSI 2-compliant image filters (LoG, Wavelets, Gabor, etc.) for creating response maps.</p> <p>See Image Filtering Guide</p> <p>Detailed documentation on available filters, parameters, and examples can be found in the Image Filtering guide.</p> <p>Filters are typically applied after resampling and before feature extraction. The pipeline handles this order automatically.</p>"},{"location":"user_guide/image_filtering/","title":"Image Filtering","text":"<p>Pictologics provides a comprehensive suite of IBSI 2 compliant image filters. These filters are used to generate response maps from the original image, highlighting specific textures, edges, or frequencies. Radiomic features are then extracted from these response maps to quantify patterns invisible to the naked eye.</p>"},{"location":"user_guide/image_filtering/#overview","title":"Overview","text":"<p>Applying filters is a key step in advanced radiomics. The process generally involves:</p> <ol> <li>Preprocessing: The image is resampled to isotropic spacing (e.g., 1x1x1 mm) and intensities are often rounded.</li> <li>Filtering: A convolutional filter (e.g., LoG, Wavelet) is applied to the preprocessed image.</li> <li>Feature Extraction: Features (intensity, texture, etc.) are calculated from the filtered image.</li> </ol> <p>IBSI 2 Compliance</p> <p>To maintain IBSI 2 compliance, filters should be applied after interpolation to isotropic spacing and before discretisation. Most filters also require specific boundary conditions (usually <code>mirror</code>).</p>"},{"location":"user_guide/image_filtering/#available-filters","title":"Available Filters","text":"Filter Code (IBSI) Description Mean S60F Averages intensities in a local neighborhood. Laplacian of Gaussian L6PA Detects edges and blobs at specific scales. Laws Texture Energy JTXT Measures texture energy using 1D kernels. Gabor Q88H Detects frequency content at specific orientations. Separable Wavelet - Decomposes image into frequency sub-bands (Haar, db, etc.). Simoncelli Wavelet PRT7 Isotropic, non-separable wavelet transform. Riesz Transform AYRS Steerable filter bank for texture orientation."},{"location":"user_guide/image_filtering/#mean-filter","title":"Mean Filter","text":"<p>The Mean filter replaces each voxel's intensity with the average intensity of its neighborhood. It smooths the image and reduces noise.</p>"},{"location":"user_guide/image_filtering/#parameters","title":"Parameters","text":"Parameter Type Description <code>support</code> <code>int</code> The size of the kernel (e.g., <code>3</code> for a 3x3x3 kernel). <code>boundary</code> <code>str</code> Boundary condition (<code>\"mirror\"</code>, <code>\"nearest\"</code>, <code>\"periodic\"</code>, <code>\"constant\"</code>). Default: <code>\"mirror\"</code>."},{"location":"user_guide/image_filtering/#usage","title":"Usage","text":"PipelineDirect API <pre><code>{\"step\": \"filter\", \"params\": {\n    \"type\": \"mean\",\n    \"support\": 3,\n    \"boundary\": \"mirror\"\n}}\n</code></pre> <pre><code>from pictologics.filters import mean_filter\n\nresponse = mean_filter(image_array, support=3, boundary=\"mirror\")\n</code></pre>"},{"location":"user_guide/image_filtering/#laplacian-of-gaussian-log","title":"Laplacian of Gaussian (LoG)","text":"<p>The Laplacian of Gaussian (LoG) filter highlights regions of rapid intensity change (edges) and blobs. It first smooths the image with a Gaussian kernel (scale \u03c3) and then calculates the Laplacian (second derivative).</p>"},{"location":"user_guide/image_filtering/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>sigma_mm</code> <code>float</code> Scale of the Gaussian in physical units (mm). Larger values detect larger blobs. <code>truncate</code> <code>float</code> Cutoff for the Gaussian kernel in standard deviations. Default: <code>4.0</code> (IBSI recommended). <code>spacing_mm</code> <code>tuple</code> Voxel spacing of the image (handled automatically by Pipeline)."},{"location":"user_guide/image_filtering/#usage_1","title":"Usage","text":"PipelineDirect API <pre><code>{\"step\": \"filter\", \"params\": {\n    \"type\": \"log\",\n    \"sigma_mm\": 3.0,  # e.g., coarse texture\n    \"truncate\": 4.0\n}}\n</code></pre> <pre><code>from pictologics.filters import laplacian_of_gaussian\n\nresponse = laplacian_of_gaussian(\n    image_array, \n    sigma_mm=3.0, \n    spacing_mm=(1.0, 1.0, 1.0),\n    truncate=4.0\n)\n</code></pre>"},{"location":"user_guide/image_filtering/#laws-texture-energy","title":"Laws Texture Energy","text":"<p>Laws filters use a set of 1D kernels (Level, Edge, Spot, Wave, Ripple) combined to form 3D masks. These masks detect specific types of texture energy.</p>"},{"location":"user_guide/image_filtering/#parameters_2","title":"Parameters","text":"Parameter Type Description <code>kernel</code> <code>str</code> The 3D kernel name (e.g., <code>\"E5L5S5\"</code>). See <code>pictologics.filters.laws.LAWS_KERNELS</code> for list. <code>rotation_invariant</code> <code>bool</code> If <code>True</code>, averages response across rotationally symmetric kernels (e.g., E5L5S5, L5E5S5, L5S5E5). <code>compute_energy</code> <code>bool</code> If <code>True</code> (default), computes local energy (average absolute deviation) in a window. <code>energy_distance</code> <code>int</code> Distance for the energy window (Chebyshev distance). Default: <code>7</code>."},{"location":"user_guide/image_filtering/#usage_2","title":"Usage","text":"PipelineDirect API <pre><code>{\"step\": \"filter\", \"params\": {\n    \"type\": \"laws\",\n    \"kernel\": \"E5L5S5\",\n    \"rotation_invariant\": True,\n    \"compute_energy\": True\n}}\n</code></pre> <pre><code>from pictologics.filters import laws_filter\n\n# Rotation invariant energy map\nresponse = laws_filter(\n    image_array, \n    kernel=\"E5L5S5\", \n    rotation_invariant=True\n)\n</code></pre>"},{"location":"user_guide/image_filtering/#gabor-filter","title":"Gabor Filter","text":"<p>Gabor filters are sinusoidal waves modulated by a Gaussian envelope. They are excellent for analyzing texture frequency and directionality.</p>"},{"location":"user_guide/image_filtering/#parameters_3","title":"Parameters","text":"Parameter Type Description <code>sigma_mm</code> <code>float</code> Spatial scale (Gaussian width) in mm. <code>lambda_mm</code> <code>float</code> Wavelength of the sinusoid in mm. <code>gamma</code> <code>float</code> Spatial aspect ratio. Default: <code>1.0</code>. <code>theta</code> <code>float</code> Orientation angle (if not rotation invariant). <code>rotation_invariant</code> <code>bool</code> If <code>True</code>, aggregates responses over multiple orientations."},{"location":"user_guide/image_filtering/#usage_3","title":"Usage","text":"PipelineDirect API <pre><code>{\"step\": \"filter\", \"params\": {\n    \"type\": \"gabor\",\n    \"sigma_mm\": 5.0,\n    \"lambda_mm\": 2.0,\n    \"rotation_invariant\": True\n}}\n</code></pre> <pre><code>from pictologics.filters import gabor_filter\n\nresponse = gabor_filter(\n    image_array,\n    sigma_mm=5.0,\n    lambda_mm=2.0,\n    gamma=1.0,\n    spacing_mm=(1.0, 1.0, 1.0),\n    rotation_invariant=True\n)\n</code></pre>"},{"location":"user_guide/image_filtering/#separable-wavelets","title":"Separable Wavelets","text":"<p>Separable wavelet transforms decompose the image into low-frequency (L) and high-frequency (H) components along each axis (x, y, z). This results in 8 sub-bands (LLL, LLH, ..., HHH).</p>"},{"location":"user_guide/image_filtering/#parameters_4","title":"Parameters","text":"Parameter Type Description <code>wavelet</code> <code>str</code> Wavelet family: <code>\"haar\"</code>, <code>\"dbX\"</code> (e.g., <code>db3</code>), <code>\"coifX\"</code> (e.g., <code>coif1</code>). <code>level</code> <code>int</code> Decomposition level (usually <code>1</code>). <code>decomposition</code> <code>str</code> Specific sub-band relative to the level (e.g., <code>\"LLH\"</code>, <code>\"HHL\"</code>). <code>rotation_invariant</code> <code>bool</code> If <code>True</code>, averages responses across rotationally symmetric sub-bands (e.g., HLL, LHL, LLH)."},{"location":"user_guide/image_filtering/#usage_4","title":"Usage","text":"PipelineDirect API <pre><code>{\"step\": \"filter\", \"params\": {\n    \"type\": \"wavelet\",\n    \"wavelet\": \"coif1\",\n    \"level\": 1,\n    \"decomposition\": \"LLH\",\n    \"rotation_invariant\": True\n}}\n</code></pre> <pre><code>from pictologics.filters import wavelet_transform\n\nresponse = wavelet_transform(\n    image_array,\n    wavelet_name=\"coif1\",\n    decomposition=\"LLH\",\n    level=1,\n    rotation_invariant=True\n)\n</code></pre>"},{"location":"user_guide/image_filtering/#simoncelli-wavelet","title":"Simoncelli Wavelet","text":"<p>The Simoncelli wavelet (non-separable) provides isotropic texture analysis. It is valuable when the direction of texture is unknown or irrelevant.</p>"},{"location":"user_guide/image_filtering/#parameters_5","title":"Parameters","text":"Parameter Type Description <code>level</code> <code>int</code> Decomposition level (1, 2, ...)."},{"location":"user_guide/image_filtering/#usage_5","title":"Usage","text":"PipelineDirect API <pre><code>{\"step\": \"filter\", \"params\": {\n    \"type\": \"simoncelli\",\n    \"level\": 1\n}}\n</code></pre> <pre><code>from pictologics.filters import simoncelli_wavelet\n\nresponse = simoncelli_wavelet(image_array, level=1)\n</code></pre>"},{"location":"user_guide/image_filtering/#riesz-transform","title":"Riesz Transform","text":"<p>The Riesz transform provides a steerable filter bank. It can be combined with other filters (like LoG or Simoncelli) to analyze texture orientation and phase.</p>"},{"location":"user_guide/image_filtering/#parameters_6","title":"Parameters","text":"Parameter Type Description <code>order</code> <code>int</code> Order of the Riesz transform. <code>variant</code> <code>str</code> Feature variant: <code>\"base\"</code>, <code>\"log\"</code>, or <code>\"simoncelli\"</code>."},{"location":"user_guide/image_filtering/#usage_6","title":"Usage","text":"PipelineDirect API <pre><code>{\"step\": \"filter\", \"params\": {\n    \"type\": \"riesz\",\n    \"order\": 2,\n    \"variant\": \"hessian_trace\"  # Example variant\n}}\n</code></pre> <pre><code>from pictologics.filters import riesz_transform\n\nresponse = riesz_transform(image_array, order=2)\n</code></pre>"},{"location":"user_guide/installation/","title":"Installation","text":""},{"location":"user_guide/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>pip</li> </ul>"},{"location":"user_guide/installation/#installation-via-pip","title":"Installation via Pip","text":"<pre><code>pip install pictologics\n</code></pre>"},{"location":"user_guide/installation/#installation-from-github","title":"Installation from GitHub","text":"<p>If you want the latest development version (or you want to install before the next PyPI release), you can install directly from the GitHub repository.</p>"},{"location":"user_guide/installation/#latest-from-main","title":"Latest from <code>main</code>","text":"<pre><code>pip install \"pictologics @ git+https://github.com/martonkolossvary/pictologics.git@main\"\n</code></pre>"},{"location":"user_guide/installation/#pinned-to-a-tag-or-commit","title":"Pinned to a tag or commit","text":"<pre><code># Example: install from a tag\npip install \"pictologics @ git+https://github.com/martonkolossvary/pictologics.git@v0.1.0\"\n\n# Example: install from a commit SHA\npip install \"pictologics @ git+https://github.com/martonkolossvary/pictologics.git@&lt;commit_sha&gt;\"\n</code></pre>"},{"location":"user_guide/installation/#editable-install-development","title":"Editable install (development)","text":"<p>Use this if you plan to modify the code.</p> <pre><code>git clone https://github.com/martonkolossvary/pictologics.git\ncd pictologics\npip install -e .\n</code></pre>"},{"location":"user_guide/installation/#eager-compilation-warmup","title":"Eager Compilation (Warmup)","text":"<p>Pictologics uses Numba for Just-In-Time (JIT) compilation to accelerate feature extraction. To ensure fast runtime performance, Pictologics performs an automatic warmup mechanism during import. This compiles the core functions immediately when <code>import pictologics</code> is executed.</p> <p>Note</p> <p>This may cause the <code>import pictologics</code> statement to take a few seconds (typically 2-10s depending on your CPU) to complete. This is expected behavior and guarantees that subsequent function calls are executed at full speed without initial compilation lag.</p>"},{"location":"user_guide/installation/#disabling-warmup","title":"Disabling Warmup","text":"<p>If you need fast import times (e.g., for CLI tools checking versions or lightweight scripts) and are willing to pay the compilation cost at the first function call, you can disable this behavior by setting the environment variable:</p> <pre><code>export PICTOLOGICS_DISABLE_WARMUP=1\n</code></pre>"},{"location":"user_guide/pipeline/","title":"Radiomics Pipeline","text":"<p>The <code>RadiomicsPipeline</code> is the core engine of Pictologics for executing reproducible, standardized radiomic feature extraction workflows. It manages the entire lifecycle of the data, from loading and preprocessing to feature extraction and logging.</p>"},{"location":"user_guide/pipeline/#why-use-the-pipeline","title":"Why use the Pipeline?","text":"<ol> <li>Reproducibility: By defining a configuration (a sequence of steps), you ensure that the exact same preprocessing is applied to every image.</li> <li>State Management: The pipeline automatically handles the state of the image and masks (morphological and intensity) as they pass through steps like resampling and resegmentation.</li> <li>Standardisation: It comes with built-in configurations that adhere to IBSI standards.</li> <li>Batch Processing: You can run multiple configurations (e.g., different binning strategies) on the same image in a single pass.</li> <li>Flexibility: The pipeline executes steps in a linear fashion, allowing you to arrange steps in any order, repeat steps if needed, and implement arbitrarily complex workflows.</li> </ol>"},{"location":"user_guide/pipeline/#linear-step-execution","title":"Linear Step Execution","text":"<p>The pipeline module executes steps linearly in order. This means:</p> <ul> <li>Steps are applied one after another in the exact sequence you define.</li> <li>You can repeat steps if needed (e.g., apply <code>discretise</code> multiple times with different settings).</li> <li>You can arrange steps in any order appropriate for your workflow.</li> <li>This linear design allows for implementing complex, multi-stage preprocessing while maintaining full control.</li> </ul> <pre><code># Example: Complex workflow with repeated steps\ncomplex_config = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (2.0, 2.0, 2.0)}},\n    {\"step\": \"keep_largest_component\", \"params\": {\"apply_to\": \"morph\"}},\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n    {\"step\": \"filter_outliers\", \"params\": {\"sigma\": 3.0}},\n    {\"step\": \"round_intensities\", \"params\": {}},  # Round after all preprocessing\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"texture\", \"histogram\"]}},\n]\n</code></pre>"},{"location":"user_guide/pipeline/#masks-are-optional","title":"Masks are optional","text":"<p><code>RadiomicsPipeline.run(...)</code> accepts an optional <code>mask</code> argument.</p> <ul> <li>If you pass a mask path / mask <code>Image</code>, it is used as the ROI (standard radiomics workflow).</li> <li>If you omit <code>mask</code> (or pass <code>mask=None</code> / <code>mask=\"\"</code>), Pictologics generates a full (all-ones) ROI mask internally,   meaning the entire image is treated as the initial ROI.</li> </ul> <p>Warning</p> <p>Empty ROI is an error If preprocessing removes all ROI voxels (e.g., too strict <code>resegment</code> thresholds), the pipeline raises a clear error rather than returning empty/partial feature sets.</p>"},{"location":"user_guide/pipeline/#predefined-configurations","title":"Predefined Configurations","text":"<p>Pictologics includes 6 standard configurations designed for common radiomics workflows. All standard configurations share:</p> <ul> <li>Resampling: 0.5mm \u00d7 0.5mm \u00d7 0.5mm isotropic spacing</li> <li>Feature Families: intensity, morphology, texture, histogram, and IVH</li> <li>Performance-optimized: Spatial/local intensity disabled by default</li> </ul> Configuration Method Parameters <code>standard_fbn_8</code> Fixed Bin Number <code>n_bins=8</code> <code>standard_fbn_16</code> Fixed Bin Number <code>n_bins=16</code> <code>standard_fbn_32</code> Fixed Bin Number <code>n_bins=32</code> <code>standard_fbs_8</code> Fixed Bin Size <code>bin_width=8.0</code> <code>standard_fbs_16</code> Fixed Bin Size <code>bin_width=16.0</code> <code>standard_fbs_32</code> Fixed Bin Size <code>bin_width=32.0</code> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\n\n# Run a single configuration\nresults = pipeline.run(\"standard_fbn_32\", image, mask)\n\n# Run all 6 standard configurations\nall_results = pipeline.run_all_standard_configs(image, mask)\n</code></pre> <p>Learn More</p> <p>For detailed configuration specifications, FBN vs FBS guidance, export/import capabilities, and best practices, see the Predefined Configurations guide.</p>"},{"location":"user_guide/pipeline/#custom-configurations","title":"Custom Configurations","text":"<p>For advanced users, the pipeline allows you to define custom sequences of steps. A configuration is a list of dictionaries, where each dictionary represents a step.</p>"},{"location":"user_guide/pipeline/#structure-of-a-configuration","title":"Structure of a Configuration","text":"<pre><code>config = [\n    {\n        \"step\": \"step_name\",\n        \"params\": { \"param1\": value1, \"param2\": value2 }\n    },\n    # ... more steps\n]\n</code></pre>"},{"location":"user_guide/pipeline/#practical-tips","title":"Practical tips","text":"<ul> <li>Keep preprocessing steps explicit (resampling, resegmentation, discretisation) so your results are reproducible.</li> <li>For CT in Hounsfield Units, FBS (<code>bin_width</code>) is often more interpretable; for MRI/PET, FBN (<code>n_bins</code>) can be a     reasonable choice depending on your intensity normalization.</li> <li>If you only need a subset of feature families, set <code>families</code> to avoid unnecessary work.</li> </ul>"},{"location":"user_guide/pipeline/#available-steps","title":"Available Steps","text":""},{"location":"user_guide/pipeline/#1-resample","title":"1. <code>resample</code>","text":"<p>Resamples the image and mask to a new voxel spacing.</p> <ul> <li><code>new_spacing</code>: Tuple of (x, y, z) spacing in mm (e.g., <code>(1.0, 1.0, 1.0)</code>).<ul> <li>Alias: <code>spacing</code> (older configs/tests).</li> </ul> </li> <li><code>interpolation</code>: Interpolation for the image (<code>\"linear\"</code>, <code>\"cubic\"</code>, <code>\"nearest\"</code>). Default: <code>\"linear\"</code>.</li> <li><code>mask_interpolation</code>: Interpolation for the mask (<code>\"nearest\"</code>, <code>\"linear\"</code>). Default: <code>\"nearest\"</code>.</li> <li><code>mask_threshold</code>: When using non-nearest mask interpolation, voxels above this threshold become ROI. Default: <code>0.5</code>.</li> <li><code>round_intensities</code>: Whether to round image intensities to nearest integer after resampling. Default: <code>False</code>.</li> </ul>"},{"location":"user_guide/pipeline/#2-resegment","title":"2. <code>resegment</code>","text":"<p>Refines the mask based on intensity thresholds (e.g., excluding bone from a soft tissue mask). This is also the IBSI-recommended approach for filtering out sentinel/NA values (e.g., -1024, -2048 in DICOM) that represent missing or invalid data.</p> <ul> <li><code>range_min</code>: Minimum intensity value.</li> <li><code>range_max</code>: Maximum intensity value.</li> </ul>"},{"location":"user_guide/pipeline/#3-filter_outliers","title":"3. <code>filter_outliers</code>","text":"<p>Removes outliers from the intensity mask based on standard deviations from the mean.</p> <ul> <li><code>sigma</code>: Number of standard deviations (e.g., <code>3.0</code>).</li> </ul>"},{"location":"user_guide/pipeline/#4-keep_largest_component","title":"4. <code>keep_largest_component</code>","text":"<p>Restricts the mask to the largest connected component. Useful for removing noise or disconnected artifacts.</p> <ul> <li><code>apply_to</code>: Which mask(s) to process. Options:<ul> <li><code>\"both\"</code> (default): Apply to both morphological and intensity masks.</li> <li><code>\"morph\"</code>: Apply only to the morphological mask.</li> <li><code>\"intensity\"</code>: Apply only to the intensity mask.</li> </ul> </li> </ul>"},{"location":"user_guide/pipeline/#5-round_intensities","title":"5. <code>round_intensities</code>","text":"<p>Rounds image intensities to the nearest integer. Useful before discretisation if values are close to integers.</p> <ul> <li>No parameters.</li> </ul>"},{"location":"user_guide/pipeline/#6-discretise","title":"6. <code>discretise</code>","text":"<p>Discretises the image intensities into bins. This is crucial for texture analysis.</p> <ul> <li><code>method</code>: <code>\"FBN\"</code> (Fixed Bin Number) or <code>\"FBS\"</code> (Fixed Bin Size).</li> <li><code>n_bins</code>: Number of bins (for FBN).</li> <li><code>bin_width</code>: Width of each bin (for FBS).</li> </ul>"},{"location":"user_guide/pipeline/#7-filter","title":"7. <code>filter</code>","text":"<p>Applies an image filter (convolutional filter) to the image. Supports IBSI 2 standard filters.</p> <ul> <li> <p><code>type</code>: Filter type (required). Options:</p> <ul> <li><code>\"mean\"</code>: Mean filter</li> <li><code>\"log\"</code>: Laplacian of Gaussian</li> <li><code>\"laws\"</code>: Laws' texture energy kernels</li> <li><code>\"gabor\"</code>: Gabor filter</li> <li><code>\"wavelet\"</code>: Separable wavelets (Haar, Daubechies, Coiflet)</li> <li><code>\"simoncelli\"</code>: Non-separable Simoncelli wavelet</li> <li><code>\"riesz\"</code>: Riesz transform</li> </ul> <p>See Filter Details</p> <p>For detailed explanations and visual examples of each filter, see the Image Filtering guide.</p> </li> <li> <p><code>boundary</code>: Boundary condition. Options: <code>\"mirror\"</code> (default), <code>\"nearest\"</code>, <code>\"zero\"</code>, <code>\"periodic\"</code>.</p> </li> <li>Additional filter-specific parameters (see table below).</li> </ul> <p>Filter Parameter Reference:</p> Filter Required Params Optional Params <code>mean</code> <code>support</code> <code>boundary</code> <code>log</code> <code>sigma_mm</code> <code>truncate</code>, <code>boundary</code>, <code>spacing_mm</code> <code>laws</code> <code>kernel</code> <code>rotation_invariant</code>, <code>pooling</code>, <code>compute_energy</code>, <code>energy_distance</code>, <code>boundary</code> <code>gabor</code> <code>sigma_mm</code>, <code>lambda_mm</code>, <code>gamma</code> <code>rotation_invariant</code>, <code>delta_theta</code>, <code>pooling</code>, <code>average_over_planes</code>, <code>spacing_mm</code>, <code>boundary</code> <code>wavelet</code> <code>wavelet</code>, <code>level</code>, <code>decomposition</code> <code>rotation_invariant</code>, <code>pooling</code>, <code>boundary</code> <code>simoncelli</code> <code>level</code> (no boundary) <code>riesz</code> <code>order</code> <code>variant</code> (<code>\"base\"</code>, <code>\"log\"</code>, <code>\"simoncelli\"</code>), <code>sigma_mm</code>, <code>level</code> <p>Automatic spacing injection</p> <p>For filters that require physical spacing (<code>log</code>, <code>gabor</code>), the pipeline automatically uses the image's voxel spacing if <code>spacing_mm</code> is not explicitly provided.</p> <p>IBSI 2 Compliance</p> <p>For IBSI 2 Phase 2 compliance, use <code>boundary=\"mirror\"</code> and apply filters after resampling and intensity rounding.</p>"},{"location":"user_guide/pipeline/#8-extract_features","title":"8. <code>extract_features</code>","text":"<p>Calculates the radiomic features based on the current state of the image and mask.</p> <p>Feature Calculation Inputs</p> <p>The pipeline automatically selects the appropriate image state for each feature family:</p> <ul> <li>Intensity, Morphology, Spatial Intensity, Local Intensity: Calculated on the Raw Image (non-discretised, floating-point values).</li> <li>Texture, Histogram: Calculated on the Discretised Image (integer bins).</li> <li>IVH: Configurable. Defaults to Discretised Image, but can use Raw Image (<code>ivh_use_continuous=True</code>) or a Temporary Discretisation (<code>ivh_discretisation={...}</code>).</li> </ul> <ul> <li><code>families</code>: List of feature families to extract. Options:<ul> <li><code>\"intensity\"</code>: First-order statistics (Mean, Skewness, etc.).<ul> <li>By default, this also includes spatial intensity and local intensity.</li> <li>Disable these expensive computations via <code>include_spatial_intensity=False</code> and/or     <code>include_local_intensity=False</code> in the step <code>params</code>.</li> </ul> </li> <li><code>\"spatial_intensity\"</code>: Compute only spatial intensity (Moran's I / Geary's C).</li> <li><code>\"local_intensity\"</code>: Compute only local/global intensity peak features.</li> <li><code>\"morphology\"</code>: Shape and size features (Volume, Sphericity, etc.).</li> <li><code>\"texture\"</code>: GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM.</li> <li><code>\"histogram\"</code>: Intensity histogram features.</li> <li><code>\"ivh\"</code>: Intensity-Volume Histogram features.</li> </ul> </li> </ul> <p>Additional optional parameters (advanced usage):</p> <ul> <li><code>include_spatial_intensity</code> / <code>include_local_intensity</code>: Booleans controlling whether the expensive     spatial/local intensity extras are included when <code>\"intensity\"</code> is requested.</li> <li><code>ivh_params</code>: Dict forwarded to <code>calculate_ivh_features(...)</code>. Supported keys include:     <code>bin_width</code>, <code>min_val</code>, <code>max_val</code>, <code>target_range_min</code>, <code>target_range_max</code>.     (There are also backward-compatible aliases: <code>ivh_bin_width</code>, <code>ivh_min_val</code>, <code>ivh_max_val</code>,     <code>ivh_target_range_min</code>, <code>ivh_target_range_max</code>.)</li> <li><code>ivh_discretisation</code>: Dict specifying a temporary discretisation for IVH only. This allows     using different binning for IVH vs texture features. Example: <code>{\"method\": \"FBS\", \"bin_width\": 2.5, \"min_val\": -1000}</code>.</li> <li><code>ivh_use_continuous</code>: Boolean. If <code>True</code>, uses raw (non-discretised) intensity values for IVH calculation.     Useful for \"continuous IVH\" as specified in some IBSI configurations.</li> <li><code>texture_matrix_params</code>: Dict forwarded to <code>calculate_all_texture_matrices(...)</code>.     Currently useful key: <code>ngldm_alpha</code> (IBSI default is <code>0</code>).</li> </ul>"},{"location":"user_guide/pipeline/#examples","title":"Examples","text":""},{"location":"user_guide/pipeline/#example-1-standard-suite-fast-baseline","title":"Example 1: Standard suite (fast baseline)","text":"<p>Runs all 6 built-in configurations. Spatial/local intensity extras are disabled by default.</p> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\nresults = pipeline.run(\n    image=\"path/to/image.nii.gz\",\n    mask=\"path/to/mask.nii.gz\",\n    config_names=[\"all_standard\"],\n)\n\n# Access one configuration\nprint(results[\"standard_fbn_32\"].head())\n</code></pre>"},{"location":"user_guide/pipeline/#example-1b-maskless-run-whole-image-roi","title":"Example 1b: Maskless run (whole-image ROI)","text":"<p>If you do not have a segmentation mask, you can omit the <code>mask</code> argument entirely.</p> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\nresults = pipeline.run(\n    image=\"path/to/image.nii.gz\",\n    # mask omitted -&gt; whole-image ROI\n    config_names=[\"standard_fbn_32\"],\n)\n\nprint(results[\"standard_fbn_32\"].head())\n</code></pre> <p>Note</p> <p>Morphology meaning With a maskless run, morphology features describe the ROI mask after any mask-refining steps (e.g., <code>resegment</code>, <code>keep_largest_component</code>). Starting from a whole-image ROI can be valid, but may not be scientifically meaningful for many radiomics studies.</p>"},{"location":"user_guide/pipeline/#example-2-enable-spatiallocal-intensity-extras-custom-config","title":"Example 2: Enable spatial/local intensity extras (custom config)","text":"<p>Use this when you explicitly need Moran\u2019s I / Geary\u2019s C and local intensity peak features.</p> <pre><code>from pictologics import RadiomicsPipeline\n\ncfg = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (0.5, 0.5, 0.5)}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": True,\n            \"include_local_intensity\": True,\n        },\n    },\n]\n\npipeline = RadiomicsPipeline().add_config(\"with_intensity_extras\", cfg)\nout = pipeline.run(\"path/to/image.nii.gz\", \"path/to/mask.nii.gz\", config_names=[\"with_intensity_extras\"])\nprint(out[\"with_intensity_extras\"].filter(like=\"_\"))\n</code></pre>"},{"location":"user_guide/pipeline/#example-3-only-compute-the-expensive-parts-explicit-families","title":"Example 3: Only compute the expensive parts (explicit families)","text":"<p>If you only want spatial/local intensity and not the entire first-order intensity set:</p> <pre><code>from pictologics import RadiomicsPipeline\n\ncfg = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\"families\": [\"spatial_intensity\", \"local_intensity\"]},\n    },\n]\n\npipeline = RadiomicsPipeline().add_config(\"intensity_extras_only\", cfg)\nout = pipeline.run(\"path/to/image.nii.gz\", \"path/to/mask.nii.gz\", config_names=[\"intensity_extras_only\"])\nprint(out[\"intensity_extras_only\"].head())\n</code></pre>"},{"location":"user_guide/pipeline/#example-4-ivh-with-physical-unit-mapping-advanced","title":"Example 4: IVH with physical-unit mapping (advanced)","text":"<p>When you discretise with FBS, you can map IVH to physical units by passing <code>bin_width</code> and <code>min_val</code>.</p> <pre><code>from pictologics import RadiomicsPipeline\n\ncfg = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": 25.0, \"min_val\": -1000}},\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"ivh\"],\n            \"ivh_params\": {\n                \"bin_width\": 25.0,\n                \"min_val\": -1000,\n                \"target_range_max\": 400,\n            },\n        },\n    },\n]\n\npipeline = RadiomicsPipeline().add_config(\"ivh_hu\", cfg)\nout = pipeline.run(\"path/to/image.nii.gz\", \"path/to/mask.nii.gz\", config_names=[\"ivh_hu\"])\nprint(out[\"ivh_hu\"].head())\n</code></pre>"},{"location":"user_guide/pipeline/#example-5-texture-with-ngldm-tolerance-ngldm_alpha","title":"Example 5: Texture with NGLDM tolerance (<code>ngldm_alpha</code>)","text":"<p>IBSI default is <code>ngldm_alpha=0</code> (exact match). If you want tolerance of \u00b11 grey level, set <code>ngldm_alpha=1</code>.</p> <pre><code>from pictologics import RadiomicsPipeline\n\ncfg = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"texture\"],\n            \"texture_matrix_params\": {\"ngldm_alpha\": 1},\n        },\n    },\n]\n\npipeline = RadiomicsPipeline().add_config(\"texture_ngldm_tolerant\", cfg)\nout = pipeline.run(\"path/to/image.nii.gz\", \"path/to/mask.nii.gz\", config_names=[\"texture_ngldm_tolerant\"])\nprint(out[\"texture_ngldm_tolerant\"].head())\n</code></pre>"},{"location":"user_guide/pipeline/#example-custom-ct-pipeline","title":"Example: Custom CT Pipeline","text":"<pre><code>custom_config = [\n    # 1. Resample to 1mm isotropic\n    {\n        \"step\": \"resample\",\n        \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}\n    },\n    # 2. Restrict to soft tissue window (-150 to 250 HU)\n    {\n        \"step\": \"resegment\",\n        \"params\": {\"range_min\": -150, \"range_max\": 250}\n    },\n    # 3. Discretise with Fixed Bin Number = 64\n    {\n        \"step\": \"discretise\",\n        \"params\": {\"method\": \"FBN\", \"n_bins\": 64}\n    },\n    # 4. Extract everything\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"]}\n    }\n]\n\npipeline = RadiomicsPipeline()\npipeline.add_config(\"my_custom_ct\", custom_config)\nresults = pipeline.run(image, mask, config_names=[\"my_custom_ct\"])\n</code></pre>"},{"location":"user_guide/pipeline/#example-7-laplacian-of-gaussian-log-filter","title":"Example 7: Laplacian of Gaussian (LoG) Filter","text":"<p>Apply LoG filter for edge/blob detection before feature extraction:</p> <pre><code>from pictologics import RadiomicsPipeline\n\nlog_config = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0), \"interpolation\": \"cubic\"}},\n    {\"step\": \"round_intensities\", \"params\": {}},\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n    {\"step\": \"filter\", \"params\": {\n        \"type\": \"log\",\n        \"sigma_mm\": 1.5,\n        \"truncate\": 4.0,\n    }},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"histogram\"]}},\n]\n\npipeline = RadiomicsPipeline().add_config(\"log_filtered\", log_config)\nresults = pipeline.run(\"image.nii.gz\", \"mask.nii.gz\", config_names=[\"log_filtered\"])\n</code></pre>"},{"location":"user_guide/pipeline/#example-8-laws-texture-energy-ibsi-2","title":"Example 8: Laws Texture Energy (IBSI 2)","text":"<p>Extract texture energy using Laws kernel with rotation invariance:</p> <pre><code>laws_config = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0), \"interpolation\": \"cubic\"}},\n    {\"step\": \"round_intensities\", \"params\": {}},\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n    {\"step\": \"filter\", \"params\": {\n        \"type\": \"laws\",\n        \"kernel\": \"L5E5E5\",\n        \"rotation_invariant\": True,\n        \"pooling\": \"max\",\n        \"compute_energy\": True,\n        \"energy_distance\": 7,\n    }},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"texture\", \"histogram\"]}},\n]\n</code></pre>"},{"location":"user_guide/pipeline/#example-9-wavelet-decomposition","title":"Example 9: Wavelet Decomposition","text":"<p>Apply Daubechies 3 wavelet with rotation-invariant averaging:</p> <pre><code>wavelet_config = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0), \"interpolation\": \"cubic\"}},\n    {\"step\": \"round_intensities\", \"params\": {}},\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n    {\"step\": \"filter\", \"params\": {\n        \"type\": \"wavelet\",\n        \"wavelet\": \"db3\",\n        \"level\": 1,\n        \"decomposition\": \"LLH\",\n        \"rotation_invariant\": True,\n        \"pooling\": \"average\",\n    }},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\"]}},\n]\n</code></pre>"},{"location":"user_guide/pipeline/#logging","title":"Logging","text":"<p>The pipeline maintains a detailed log of every step executed, including parameters and any errors encountered. This is vital for auditing and debugging.</p> <pre><code># After running the pipeline\npipeline.save_log(\"pipeline_execution_log.json\")\n</code></pre> <p>The log file contains:</p> <ul> <li>Timestamp</li> <li>Subject ID</li> <li>Configuration Name</li> <li>List of executed steps with their parameters</li> <li>Status of each step</li> </ul>"},{"location":"user_guide/pipeline/#configuration-export-import","title":"Configuration Export &amp; Import","text":"<p>Pictologics supports exporting and importing pipeline configurations in YAML and JSON formats for reproducible research.</p> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\npipeline.add_config(\"my_study_config\", [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\"]}},\n])\n\n# Export to YAML or JSON\npipeline.save_configs(\"my_configs.yaml\")\n\n# Import from file\npipeline = RadiomicsPipeline.load_configs(\"my_configs.yaml\")\n</code></pre> <p>Full Configuration Guide</p> <p>For complete documentation on configuration file formats, schema versioning, merging configurations,  validation, and the template system API, see the Predefined Configurations guide.</p>"},{"location":"user_guide/predefined_configurations/","title":"Predefined Configurations","text":"<p>Pictologics provides a comprehensive configuration management system designed for reproducible radiomics research. This guide covers the standard configurations, configuration file formats, and tools for sharing and managing pipeline configurations.</p>"},{"location":"user_guide/predefined_configurations/#overview","title":"Overview","text":"<p>The configuration system enables you to:</p> <ul> <li>Use standard configurations \u2013 Pre-tested, IBSI-compliant setups for common radiomics workflows</li> <li>Export and import configurations \u2013 Save your pipeline settings to YAML/JSON files for version control</li> <li>Share configurations \u2013 Collaborate by exchanging configuration files with colleagues</li> <li>Ensure reproducibility \u2013 Schema versioning ensures configurations remain compatible across versions</li> </ul>"},{"location":"user_guide/predefined_configurations/#using-standard-configurations","title":"Using Standard Configurations","text":"<p>Pictologics includes 6 standard configurations optimized for radiomics feature extraction. All standard configurations share these characteristics:</p> <ul> <li>Isotropic resampling to 0.5mm \u00d7 0.5mm \u00d7 0.5mm</li> <li>Spline interpolation (order 3) with anti-aliasing</li> <li>Complete feature extraction: intensity, morphology, texture, histogram, and IVH</li> <li>Performance-optimized: spatial and local intensity features disabled by default</li> </ul>"},{"location":"user_guide/predefined_configurations/#running-a-single-configuration","title":"Running a Single Configuration","text":"<pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\nresults = pipeline.run(\"standard_fbn_32\", image, mask)\n</code></pre>"},{"location":"user_guide/predefined_configurations/#running-all-standard-configurations","title":"Running All Standard Configurations","text":"<p>Process all 6 standard configurations in a single call:</p> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\nall_results = pipeline.run_all_standard_configs(image, mask, subject_id=\"patient_001\")\n</code></pre>"},{"location":"user_guide/predefined_configurations/#running-multiple-specific-configurations","title":"Running Multiple Specific Configurations","text":"<pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\nresults = pipeline.run_multiple(\n    [\"standard_fbn_16\", \"standard_fbn_32\", \"standard_fbs_16\"],\n    image,\n    mask,\n    subject_id=\"patient_001\"\n)\n</code></pre>"},{"location":"user_guide/predefined_configurations/#accessing-results","title":"Accessing Results","text":"<p>Results are returned as <code>RadiomicsResults</code> objects containing features organized by family:</p> <pre><code># Get all features as a flat dictionary\nfeatures = results.to_dict()\n\n# Access specific feature families\nintensity_features = results.intensity\nmorphology_features = results.morphology\ntexture_features = results.texture\n\n# Export to pandas DataFrame\ndf = results.to_dataframe()\n</code></pre>"},{"location":"user_guide/predefined_configurations/#configuration-specifications","title":"Configuration Specifications","text":""},{"location":"user_guide/predefined_configurations/#fixed-bin-number-fbn-configurations","title":"Fixed Bin Number (FBN) Configurations","text":"<p>FBN discretisation divides the intensity range into a fixed number of bins, regardless of the actual intensity values. This approach is useful when you want consistent bin counts across different images.</p> Configuration Bins Use Case <code>standard_fbn_8</code> 8 Low-resolution texture analysis, small ROIs <code>standard_fbn_16</code> 16 Balanced resolution and noise robustness <code>standard_fbn_32</code> 32 High-resolution texture analysis (recommended default)"},{"location":"user_guide/predefined_configurations/#full-specification-standard_fbn_32","title":"Full Specification: <code>standard_fbn_32</code>","text":"<pre><code>standard_fbn_32:\n  description: \"Standard FBN-32: 0.5mm isotropic resampling, 32 fixed bins\"\n  steps:\n    - step: resample\n      params:\n        new_spacing: [0.5, 0.5, 0.5]\n        interpolation_order: 3\n        anti_aliasing: true\n        anti_aliasing_sigma: null\n    - step: discretise\n      params:\n        method: FBN\n        n_bins: 32\n    - step: extract_features\n      params:\n        families:\n          - intensity\n          - morphology\n          - texture\n          - histogram\n          - ivh\n        include_spatial_intensity: false\n        include_local_intensity: false\n</code></pre>"},{"location":"user_guide/predefined_configurations/#fixed-bin-size-fbs-configurations","title":"Fixed Bin Size (FBS) Configurations","text":"<p>FBS discretisation uses a fixed bin width (in Hounsfield Units for CT), which preserves the physical meaning of intensity values. This is preferred when comparing across studies or when absolute intensity values are clinically meaningful.</p> Configuration Bin Width Use Case <code>standard_fbs_8</code> 8.0 HU High-resolution intensity preservation <code>standard_fbs_16</code> 16.0 HU Balanced resolution (recommended for CT) <code>standard_fbs_32</code> 32.0 HU Noise-robust analysis"},{"location":"user_guide/predefined_configurations/#full-specification-standard_fbs_16","title":"Full Specification: <code>standard_fbs_16</code>","text":"<pre><code>standard_fbs_16:\n  description: \"Standard FBS-16: 0.5mm isotropic resampling, 16.0 HU bin width\"\n  steps:\n    - step: resample\n      params:\n        new_spacing: [0.5, 0.5, 0.5]\n        interpolation_order: 3\n        anti_aliasing: true\n        anti_aliasing_sigma: null\n    - step: discretise\n      params:\n        method: FBS\n        bin_width: 16.0\n    - step: extract_features\n      params:\n        families:\n          - intensity\n          - morphology\n          - texture\n          - histogram\n          - ivh\n        include_spatial_intensity: false\n        include_local_intensity: false\n</code></pre>"},{"location":"user_guide/predefined_configurations/#choosing-the-right-configuration","title":"Choosing the Right Configuration","text":""},{"location":"user_guide/predefined_configurations/#fbn-vs-fbs-decision-guide","title":"FBN vs FBS: Decision Guide","text":"Factor FBN FBS Intensity range Variable (adapts to image) Fixed (preserves HU meaning) Cross-study comparison Less suitable Preferred Small ROIs Better (ensures bin coverage) May have empty bins CT imaging Acceptable Recommended MRI imaging Recommended Less suitable (no standard units) IBSI recommendation Supported Supported"},{"location":"user_guide/predefined_configurations/#performance-considerations","title":"Performance Considerations","text":"<p>The standard configurations have <code>include_spatial_intensity</code> and <code>include_local_intensity</code> set to <code>false</code> for performance:</p> <ul> <li>Spatial intensity features: Require distance calculations to ROI boundary (computationally expensive)</li> <li>Local intensity features: Require local neighborhood analysis</li> </ul> <p>To enable these features, create a custom configuration variant:</p> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\n\n# Get the standard config and modify it\nconfig = pipeline.get_config(\"standard_fbn_32\")\n\n# Find the extract_features step and enable spatial intensity\nfor step in config:\n    if step[\"step\"] == \"extract_features\":\n        step[\"params\"][\"include_spatial_intensity\"] = True\n        step[\"params\"][\"include_local_intensity\"] = True\n\n# Add as a new configuration\npipeline.add_config(\"fbn_32_with_spatial\", config)\n</code></pre>"},{"location":"user_guide/predefined_configurations/#configuration-files","title":"Configuration Files","text":"<p>Pictologics supports YAML and JSON formats for configuration files. YAML is recommended for human readability.</p>"},{"location":"user_guide/predefined_configurations/#yaml-format-specification","title":"YAML Format Specification","text":"<pre><code>schema_version: \"1.0\"\nexported_at: \"2026-01-31T12:00:00.000000\"\nconfigs:\n  my_custom_config:\n    - step: resample\n      params:\n        new_spacing: [1.0, 1.0, 1.0]\n    - step: discretise\n      params:\n        method: FBN\n        n_bins: 32\n    - step: extract_features\n      params:\n        families:\n          - intensity\n          - morphology\n          - texture\n</code></pre>"},{"location":"user_guide/predefined_configurations/#json-format-specification","title":"JSON Format Specification","text":"<pre><code>{\n  \"schema_version\": \"1.0\",\n  \"exported_at\": \"2026-01-31T12:00:00.000000\",\n  \"configs\": {\n    \"my_custom_config\": [\n      {\n        \"step\": \"resample\",\n        \"params\": {\"new_spacing\": [1.0, 1.0, 1.0]}\n      },\n      {\n        \"step\": \"discretise\",\n        \"params\": {\"method\": \"FBN\", \"n_bins\": 32}\n      },\n      {\n        \"step\": \"extract_features\",\n        \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\"]}\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"user_guide/predefined_configurations/#schema-versioning","title":"Schema Versioning","text":"<p>Configuration files include a <code>schema_version</code> field to ensure forward compatibility:</p> <ul> <li>Current version: <code>1.0</code></li> <li>Files without a version are treated as version <code>1.0</code></li> <li>Future versions will include automatic migration when loading older configs</li> </ul>"},{"location":"user_guide/predefined_configurations/#sharing-configurations","title":"Sharing Configurations","text":""},{"location":"user_guide/predefined_configurations/#exporting-configurations","title":"Exporting Configurations","text":"<p>Save your configurations to share with collaborators or for version control:</p> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\n\n# Add custom configurations\npipeline.add_config(\"my_study_config\", [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\"]}},\n])\n\n# Export to YAML (recommended for readability)\npipeline.save_configs(\"my_configs.yaml\")\n\n# Export to JSON\npipeline.save_configs(\"my_configs.json\")\n\n# Export specific configs only\npipeline.save_configs(\"study_config.yaml\", config_names=[\"my_study_config\"])\n</code></pre>"},{"location":"user_guide/predefined_configurations/#importing-configurations","title":"Importing Configurations","text":"<p>Load configurations from files:</p> <pre><code>from pictologics import RadiomicsPipeline\n\n# Load from YAML file\npipeline = RadiomicsPipeline.load_configs(\"my_configs.yaml\")\n\n# Load from JSON file\npipeline = RadiomicsPipeline.load_configs(\"my_configs.json\")\n\n# Load with validation (logs warnings for unknown parameters)\npipeline = RadiomicsPipeline.load_configs(\"my_configs.yaml\", validate=True)\n</code></pre>"},{"location":"user_guide/predefined_configurations/#string-based-exportimport","title":"String-based Export/Import","text":"<p>For integration with databases or web services:</p> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\npipeline.add_config(\"my_config\", [...])\n\n# Export to string\nyaml_string = pipeline.to_yaml()\njson_string = pipeline.to_json()\n\n# Import from string\npipeline2 = RadiomicsPipeline.from_yaml(yaml_string)\npipeline3 = RadiomicsPipeline.from_json(json_string)\n</code></pre>"},{"location":"user_guide/predefined_configurations/#merging-configurations","title":"Merging Configurations","text":"<p>Combine configurations from multiple sources:</p> <pre><code>from pictologics import RadiomicsPipeline\n\n# Load configs from different sources\npipeline1 = RadiomicsPipeline.load_configs(\"team_a_configs.yaml\")\npipeline2 = RadiomicsPipeline.load_configs(\"team_b_configs.yaml\")\n\n# Merge into pipeline1\npipeline1.merge_configs(pipeline2)\n\n# Handle duplicates explicitly\npipeline1.merge_configs(pipeline2, overwrite=True)  # Overwrite existing\npipeline1.merge_configs(pipeline2, overwrite=False)  # Keep existing (default)\n</code></pre>"},{"location":"user_guide/predefined_configurations/#configuration-validation","title":"Configuration Validation","text":"<p>When loading configurations, enable validation to catch potential issues:</p> <pre><code># Validation logs warnings for:\n# - Unknown step types\n# - Unknown parameters for known steps\n# - Missing required parameters\n\npipeline = RadiomicsPipeline.load_configs(\"config.yaml\", validate=True)\n</code></pre>"},{"location":"user_guide/predefined_configurations/#template-system-advanced","title":"Template System (Advanced)","text":"<p>For programmatic access to configuration templates, Pictologics provides a template loading API.</p>"},{"location":"user_guide/predefined_configurations/#loading-templates","title":"Loading Templates","text":"<pre><code>from pictologics.templates import (\n    list_template_files,\n    load_template_file,\n    get_standard_templates,\n    get_all_templates,\n    get_template_metadata,\n)\n\n# List available template files\nfiles = list_template_files()\n# ['standard_configs.yaml']\n\n# Load all standard configurations\nstandard_configs = get_standard_templates()\n# {'standard_fbn_8': [...], 'standard_fbn_16': [...], ...}\n\n# Get metadata from a template file\nmetadata = get_template_metadata(\"standard_configs.yaml\")\n# {'schema_version': '1.0', 'description': '...', 'config_count': 6}\n\n# Load a specific template file\nall_configs = load_template_file(\"standard_configs.yaml\")\n</code></pre>"},{"location":"user_guide/predefined_configurations/#creating-custom-template-files","title":"Creating Custom Template Files","text":"<p>You can create your own template files and load them:</p> <pre><code>from pictologics import RadiomicsPipeline\nimport yaml\n\n# Define your organization's standard configs\norg_configs = {\n    \"schema_version\": \"1.0\",\n    \"description\": \"Organization standard configurations\",\n    \"configs\": {\n        \"org_standard_ct\": [\n            {\"step\": \"resample\", \"params\": {\"new_spacing\": [0.5, 0.5, 0.5]}},\n            {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": 25.0}},\n            {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\"]}},\n        ],\n        \"org_standard_pet\": [\n            {\"step\": \"resample\", \"params\": {\"new_spacing\": [2.0, 2.0, 2.0]}},\n            {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 64}},\n            {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"texture\"]}},\n        ],\n    }\n}\n\n# Save to file\nwith open(\"org_configs.yaml\", \"w\") as f:\n    yaml.dump(org_configs, f)\n\n# Load into pipeline\npipeline = RadiomicsPipeline.load_configs(\"org_configs.yaml\")\n</code></pre>"},{"location":"user_guide/predefined_configurations/#best-practices-for-reproducibility","title":"Best Practices for Reproducibility","text":""},{"location":"user_guide/predefined_configurations/#1-version-control-your-configurations","title":"1. Version Control Your Configurations","text":"<p>Always include configuration files in your version control system:</p> <pre><code># Add to git\ngit add configs/study_configs.yaml\ngit commit -m \"Add radiomics configuration for study XYZ\"\n</code></pre>"},{"location":"user_guide/predefined_configurations/#2-document-configuration-choices","title":"2. Document Configuration Choices","text":"<p>Include comments in your YAML files explaining parameter choices:</p> <pre><code>schema_version: \"1.0\"\ndescription: |\n  Configurations for lung nodule analysis study.\n  FBS-25 chosen based on literature recommendation for CT texture analysis.\n  0.5mm resampling matches thin-slice CT acquisition protocol.\nconfigs:\n  lung_nodule_primary:\n    - step: resample\n      params:\n        new_spacing: [0.5, 0.5, 0.5]  # Match acquisition protocol\n    # ... rest of config\n</code></pre>"},{"location":"user_guide/predefined_configurations/#3-export-before-major-changes","title":"3. Export Before Major Changes","text":"<p>Before modifying your pipeline, export the current state:</p> <pre><code>pipeline.save_configs(\"configs_backup_2026-01-31.yaml\")\n</code></pre>"},{"location":"user_guide/predefined_configurations/#4-use-validation-when-loading-external-configs","title":"4. Use Validation When Loading External Configs","text":"<pre><code># Always validate configs from external sources\npipeline = RadiomicsPipeline.load_configs(\"collaborator_config.yaml\", validate=True)\n</code></pre>"},{"location":"user_guide/predefined_configurations/#5-include-schema-version-in-publications","title":"5. Include Schema Version in Publications","text":"<p>When publishing radiomics research, report:</p> <ul> <li>Pictologics version</li> <li>Configuration file (as supplementary material)</li> <li>Schema version used</li> </ul>"},{"location":"user_guide/predefined_configurations/#end-to-end-example-multi-site-radiomics-study","title":"End-to-End Example: Multi-Site Radiomics Study","text":"<p>This section provides a complete, real-world workflow demonstrating how to create, save, load, and apply configurations across a multi-site radiomics study. The scenario involves:</p> <ul> <li>Site A (Primary): Creates and validates the study configuration</li> <li>Site B (Collaborator): Receives and applies the same configuration</li> <li>Both sites: Process their local cohorts with identical settings</li> </ul>"},{"location":"user_guide/predefined_configurations/#step-1-design-the-study-configuration-site-a","title":"Step 1: Design the Study Configuration (Site A)","text":"<p>Site A designs a custom configuration tailored to their lung nodule CT analysis study:</p> <pre><code>from pictologics import RadiomicsPipeline\n\n# Initialize pipeline\npipeline = RadiomicsPipeline()\n\n# Define a study-specific configuration\n# This config is designed for thin-slice chest CT with lung nodule segmentations\nlung_nodule_config = [\n    # Step 1: Resample to 0.5mm isotropic (matches thin-slice CT)\n    {\n        \"step\": \"resample\",\n        \"params\": {\n            \"new_spacing\": [0.5, 0.5, 0.5],\n            \"interpolation_order\": 3,  # Cubic spline for smooth interpolation\n            \"anti_aliasing\": True,\n        }\n    },\n    # Step 2: Resegment to lung window and remove outliers\n    {\n        \"step\": \"resegment\",\n        \"params\": {\n            \"range_min\": -1000,  # Air\n            \"range_max\": 400,    # Soft tissue upper limit\n        }\n    },\n    # Step 3: Keep only the largest connected component (remove satellite lesions)\n    {\n        \"step\": \"largest_roi\",\n        \"params\": {}\n    },\n    # Step 4: Discretise using Fixed Bin Size (preserves HU meaning)\n    {\n        \"step\": \"discretise\",\n        \"params\": {\n            \"method\": \"FBS\",\n            \"bin_width\": 25.0,  # 25 HU bins (common for CT texture)\n        }\n    },\n    # Step 5: Extract all feature families\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": False,  # Skip for performance\n            \"include_local_intensity\": False,\n        }\n    },\n]\n\n# Add the configuration to the pipeline\npipeline.add_config(\"lung_nodule_fbs25\", lung_nodule_config)\n\n# Also add a variant with different bin width for sensitivity analysis\nlung_nodule_fbs50 = lung_nodule_config.copy()\nlung_nodule_fbs50[3] = {\n    \"step\": \"discretise\",\n    \"params\": {\"method\": \"FBS\", \"bin_width\": 50.0}\n}\npipeline.add_config(\"lung_nodule_fbs50\", lung_nodule_fbs50)\n\n# Verify the configurations are registered\nprint(\"Available configurations:\", pipeline.list_configs())\n</code></pre>"},{"location":"user_guide/predefined_configurations/#step-2-test-the-configuration-locally-site-a","title":"Step 2: Test the Configuration Locally (Site A)","text":"<p>Before sharing, validate the configuration on a sample case:</p> <pre><code>from pictologics import load_image\n\n# Load a test case\nimage = load_image(\"test_data/ct_scan.nii.gz\")\nmask = load_image(\"test_data/nodule_segmentation.nii.gz\")\n\n# Run the primary configuration\nresults = pipeline.run(\n    config_name=\"lung_nodule_fbs25\",\n    image=image,\n    mask=mask,\n    subject_id=\"test_case_001\"\n)\n\n# Inspect results\nprint(f\"Extracted {len(results.to_dict())} features\")\nprint(f\"Sample features:\")\nprint(f\"  - Volume: {results.morphology.get('volume_mesh', 'N/A'):.2f} mm\u00b3\")\nprint(f\"  - Mean intensity: {results.intensity.get('mean', 'N/A'):.2f} HU\")\n\n# Save execution log for audit\npipeline.save_log(\"logs/test_run_001.json\")\n</code></pre>"},{"location":"user_guide/predefined_configurations/#step-3-export-configuration-for-sharing-site-a","title":"Step 3: Export Configuration for Sharing (Site A)","text":"<p>Save the configuration to a file that can be shared with collaborators:</p> <pre><code># Export to YAML (human-readable, good for version control)\npipeline.save_configs(\n    \"configs/lung_nodule_study_v1.yaml\",\n    config_names=[\"lung_nodule_fbs25\", \"lung_nodule_fbs50\"]\n)\n\nprint(\"Configuration exported successfully!\")\nprint(\"Share 'configs/lung_nodule_study_v1.yaml' with Site B\")\n</code></pre> <p>The exported YAML file will look like this:</p> <pre><code>schema_version: \"1.0\"\nexported_at: \"2026-01-31T10:30:00.000000\"\nconfigs:\n  lung_nodule_fbs25:\n    - step: resample\n      params:\n        new_spacing: [0.5, 0.5, 0.5]\n        interpolation_order: 3\n        anti_aliasing: true\n    - step: resegment\n      params:\n        range_min: -1000\n        range_max: 400\n    - step: largest_roi\n      params: {}\n    - step: discretise\n      params:\n        method: FBS\n        bin_width: 25.0\n    - step: extract_features\n      params:\n        families: [intensity, morphology, texture, histogram, ivh]\n        include_spatial_intensity: false\n        include_local_intensity: false\n  lung_nodule_fbs50:\n    # ... similar structure with bin_width: 50.0\n</code></pre>"},{"location":"user_guide/predefined_configurations/#step-4-load-and-apply-configuration-site-b","title":"Step 4: Load and Apply Configuration (Site B)","text":"<p>Site B receives the configuration file and processes their cohort:</p> <pre><code>from pathlib import Path\nfrom pictologics import RadiomicsPipeline, load_image, save_results, format_results\n\n# Load the shared configuration (with validation)\npipeline = RadiomicsPipeline.load_configs(\n    \"configs/lung_nodule_study_v1.yaml\",\n    validate=True  # Logs warnings for any issues\n)\n\n# Verify loaded configurations\nprint(\"Loaded configurations:\", pipeline.list_configs())\n\n# Define the local data directory\ndata_dir = Path(\"site_b_data/\")\noutput_dir = Path(\"site_b_results/\")\noutput_dir.mkdir(exist_ok=True)\n\n# Process all cases\nall_results = []\nfor case_folder in sorted(data_dir.glob(\"patient_*\")):\n    patient_id = case_folder.name\n\n    # Load image and mask\n    image = load_image(case_folder / \"ct.nii.gz\")\n    mask = load_image(case_folder / \"nodule.nii.gz\")\n\n    # Run the primary configuration\n    results = pipeline.run(\n        config_name=\"lung_nodule_fbs25\",\n        image=image,\n        mask=mask,\n        subject_id=patient_id\n    )\n\n    # Format for CSV export\n    row = format_results(\n        results,\n        fmt=\"wide\",\n        prefix=f\"{patient_id}_lung_nodule_fbs25\"\n    )\n    all_results.append(row)\n\n    print(f\"Processed {patient_id}: {len(results.to_dict())} features\")\n\n# Save all results to CSV\nsave_results(all_results, output_dir / \"site_b_features.csv\")\nprint(f\"Results saved to {output_dir / 'site_b_features.csv'}\")\n</code></pre>"},{"location":"user_guide/predefined_configurations/#step-5-combine-results-from-both-sites","title":"Step 5: Combine Results from Both Sites","text":"<p>After both sites complete processing, merge the results:</p> <pre><code>import pandas as pd\n\n# Load results from both sites\nsite_a_df = pd.read_csv(\"site_a_results/site_a_features.csv\")\nsite_b_df = pd.read_csv(\"site_b_results/site_b_features.csv\")\n\n# Add site identifier\nsite_a_df[\"site\"] = \"A\"\nsite_b_df[\"site\"] = \"B\"\n\n# Combine into a single dataset\ncombined_df = pd.concat([site_a_df, site_b_df], ignore_index=True)\n\n# Verify consistent feature extraction\nprint(f\"Total patients: {len(combined_df)}\")\nprint(f\"Features per patient: {len(combined_df.columns) - 2}\")  # Exclude id and site\nprint(f\"Site A: {len(site_a_df)} patients\")\nprint(f\"Site B: {len(site_b_df)} patients\")\n\n# Save combined dataset\ncombined_df.to_csv(\"combined_study_features.csv\", index=False)\n</code></pre>"},{"location":"user_guide/predefined_configurations/#step-6-archive-configuration-with-study-data","title":"Step 6: Archive Configuration with Study Data","text":"<p>For long-term reproducibility, archive the configuration alongside results:</p> <pre><code>from datetime import datetime\nimport shutil\n\n# Create study archive\narchive_dir = Path(f\"study_archive_{datetime.now().strftime('%Y%m%d')}\")\narchive_dir.mkdir(exist_ok=True)\n\n# Copy configuration\nshutil.copy(\"configs/lung_nodule_study_v1.yaml\", archive_dir / \"configuration.yaml\")\n\n# Copy results\nshutil.copy(\"combined_study_features.csv\", archive_dir / \"features.csv\")\n\n# Export pipeline logs\npipeline.save_log(archive_dir / \"processing_log.json\")\n\n# Create a README with study metadata\nreadme = f\"\"\"\n# Lung Nodule Radiomics Study\nDate: {datetime.now().isoformat()}\nPictologics Version: 1.0.0\nConfiguration Schema: 1.0\n\n## Configurations Used\n- lung_nodule_fbs25: Primary analysis (25 HU bin width)\n- lung_nodule_fbs50: Sensitivity analysis (50 HU bin width)\n\n## Sites\n- Site A: {len(site_a_df)} patients\n- Site B: {len(site_b_df)} patients\n\n## Files\n- configuration.yaml: Pipeline configuration (shareable)\n- features.csv: Extracted radiomic features\n- processing_log.json: Execution audit log\n\"\"\"\n\nwith open(archive_dir / \"README.md\", \"w\") as f:\n    f.write(readme)\n\nprint(f\"Study archive created: {archive_dir}/\")\n</code></pre>"},{"location":"user_guide/predefined_configurations/#key-takeaways","title":"Key Takeaways","text":"<p>This workflow demonstrates several important practices:</p> Practice Benefit Custom configurations Tailored to specific imaging protocol and clinical question YAML export Human-readable, version-controllable, shareable Validation on load Catches configuration issues early Consistent processing Both sites use identical preprocessing and extraction Audit logging Complete record of processing steps Archival Long-term reproducibility for publications <p>Reproducibility Achieved</p> <p>By sharing the YAML configuration file, both sites process their data with identical settings,  ensuring that any differences in extracted features reflect true biological variation rather than  methodological inconsistencies.</p>"},{"location":"user_guide/predefined_configurations/#quick-reference","title":"Quick Reference","text":"Task Method Run standard config <code>pipeline.run(\"standard_fbn_32\", image, mask)</code> Run all standard configs <code>pipeline.run_all_standard_configs(image, mask)</code> List available configs <code>pipeline.list_configs()</code> Get config details <code>pipeline.get_config(\"config_name\")</code> Add custom config <code>pipeline.add_config(\"name\", steps)</code> Remove config <code>pipeline.remove_config(\"name\")</code> Export to YAML <code>pipeline.save_configs(\"file.yaml\")</code> Export to JSON <code>pipeline.save_configs(\"file.json\")</code> Import from file <code>RadiomicsPipeline.load_configs(\"file.yaml\")</code> Merge configs <code>pipeline.merge_configs(other_pipeline)</code> Export to string <code>pipeline.to_yaml()</code> / <code>pipeline.to_json()</code> Import from string <code>RadiomicsPipeline.from_yaml(s)</code> / <code>RadiomicsPipeline.from_json(s)</code> <p>See Also</p> <ul> <li>Pipeline Usage \u2013 Complete pipeline documentation</li> <li>Feature Calculations \u2013 Available feature families</li> <li>Image Filtering \u2013 Adding filters to configurations</li> </ul>"},{"location":"user_guide/utilities/","title":"Utilities","text":"<p>Pictologics provides a set of powerful utilities to help with data wrangling and organization, particularly for handling complex DICOM datasets.</p>"},{"location":"user_guide/utilities/#dicom-database-parser","title":"DICOM Database Parser","text":"<p>The <code>DicomDatabase</code> class allows you to easily parse, organize, and query DICOM folders. It automatically structures your data into a Patient -&gt; Study -&gt; Series -&gt; Instance hierarchy and extracts relevant metadata.</p>"},{"location":"user_guide/utilities/#basic-usage","title":"Basic Usage","text":"<p>To parse a directory of DICOM files:</p> <pre><code>from pictologics.utilities import DicomDatabase\n\n# 1. Parse a folder (recursive by default)\ndb = DicomDatabase.from_folders(\n    paths=[\"path/to/dicom/folder\"], \n    num_workers=4  # Use parallel processing for speed\n)\n\n# 2. Get a summary DataFrame of all series\ndf_series = db.get_series_df()\nprint(df_series.head())\n\n# 3. Get detailed DataFrame of all instances\ndf_instances = db.get_instances_df()\n</code></pre>"},{"location":"user_guide/utilities/#parallel-processing-and-progress-bar","title":"Parallel Processing and Progress Bar","text":"<p>For large DICOM datasets, parsing can take significant time. <code>DicomDatabase.from_folders()</code> supports parallel processing to speed up the scan and displays a progress bar showing progress and estimated time remaining.</p> <pre><code># Parallel processing with all available cores\ndb = DicomDatabase.from_folders(\n    paths=[\"large_dataset/\"],\n    num_workers=8,          # Use 8 parallel workers\n    show_progress=True      # Show progress bar (default: True)\n)\n\n# Disable progress bar for silent operation\ndb = DicomDatabase.from_folders(\n    paths=[\"data/\"],\n    show_progress=False\n)\n</code></pre> <p>The progress bar shows: - Number of files processed - Percentage complete - Elapsed time and estimated time remaining</p>"},{"location":"user_guide/utilities/#memory-efficient-exports","title":"Memory-Efficient Exports","text":"<p>By default, DataFrame exports exclude the large <code>InstanceSOPUIDs</code> and <code>InstanceFilePaths</code> columns to reduce memory usage. To include them:</p> <pre><code># Default: smaller DataFrames without instance lists\ndf_compact = db.get_series_df()\n\n# Include full instance lists if needed\ndf_full = db.get_series_df(include_instance_lists=True)\n</code></pre> <p>This applies to <code>get_patients_df()</code>, <code>get_studies_df()</code>, and <code>get_series_df()</code>.</p>"},{"location":"user_guide/utilities/#multi-phase-series-splitting","title":"Multi-Phase Series Splitting","text":"<p>Medical images often contain multiple phases (e.g., dynamic contrast-enhanced MRI, multiphase CT, cardiac phases) within a single \"series\" (sharing the same <code>SeriesInstanceUID</code>). <code>DicomDatabase</code> automatically detects and splits these into separate logical series for easier analysis.</p> <p>By default, <code>split_multiseries=True</code>.</p> <pre><code># Automatically splits series based on:\n# - Cardiac Phase\n# - Acquisition Number\n# - Temporal Position\n# - Echo Number\n# - Trigger Time\n# - Duplicate Spatial Positions (fallback)\n\ndb = DicomDatabase.from_folders([\"path/to/multiphase/data\"])\n\n# The resulting DataFrames will show split series with unique identifiers\n# e.g., \"1.2.3.4.5\" -&gt; \"1.2.3.4.5\" (if single phase)\n# e.g., \"1.2.3.4.5\" -&gt; \"1.2.3.4.5.1\", \"1.2.3.4.5.2\" (if multi-phase)\nseries_df = db.get_series_df()\n</code></pre>"},{"location":"user_guide/utilities/#exporting-data","title":"Exporting Data","text":"<p>You can export the structured data to standard formats:</p> <pre><code># Export all levels to CSV (default: without instance lists for smaller files)\ndb.export_csv(\"output\", levels=[\"patients\", \"studies\", \"series\", \"instances\"])\n\n# Export with full instance UIDs and file paths\ndb.export_csv(\"output\", include_instance_lists=True)\n\n# Export hierarchical JSON (includes file paths by default)\ndb.export_json(\"dataset.json\")\n\n# Export JSON without file paths for smaller files\ndb.export_json(\"dataset.json\", include_instance_lists=False)\n</code></pre>"},{"location":"user_guide/utilities/#accessing-the-hierarchy-directly","title":"Accessing the Hierarchy Directly","text":"<p>You can also traverse the object hierarchy directly if you need fine-grained control:</p> <pre><code>for patient in db.patients:\n    print(f\"Patient: {patient.patient_id}\")\n    for study in patient.studies:\n        print(f\"  Study: {study.study_date}\")\n        for series in study.series:\n            print(f\"    Series: {series.modality} ({len(series.instances)} images)\")\n\n            # Access instances\n            # series.instances is a list of DicomInstance objects\n</code></pre>"},{"location":"user_guide/utilities/#visualization","title":"Visualization","text":"<p>Pictologics provides flexible utilities for visualizing medical images and segmentation masks. The visualization functions support three display modes:</p> Mode <code>image</code> <code>mask</code> Description Overlay \u2713 \u2713 Mask overlaid on grayscale image Image Only \u2713 \u2717 Grayscale image (with optional window/level) Mask Only \u2717 \u2713 Colormap or grayscale mask display"},{"location":"user_guide/utilities/#interactive-viewer","title":"Interactive Viewer","text":"<p>Scroll through slices interactively:</p> <pre><code>from pictologics import load_image\nfrom pictologics.utilities import visualize_slices\n\nimg = load_image(\"scan.nii.gz\")\nmask = load_image(\"segmentation.nii.gz\")\n\n# Overlay mode (image + mask)\nvisualize_slices(image=img, mask=mask, alpha=0.4, colormap=\"tab20\")\n\n# Image only mode\nvisualize_slices(image=img)\n\n# Mask only mode (with colormap)\nvisualize_slices(mask=mask)\n</code></pre>"},{"location":"user_guide/utilities/#save-slices-to-files","title":"Save Slices to Files","text":"<p>Export selected slices as images:</p> <pre><code>from pictologics.utilities import save_slices\n\n# Save overlay slices\nsave_slices(\"output/\", image=img, mask=mask, slice_selection=\"10%\")\n\n# Save every 10th slice\nsave_slices(\"output/\", image=img, mask=mask, slice_selection=\"every_10\")\n\n# Save specific slices\nsave_slices(\"output/\", image=img, slice_selection=[0, 50, 100])\n</code></pre>"},{"location":"user_guide/utilities/#windowlevel-normalization","title":"Window/Level Normalization","text":"<p>For CT and MR images, use window/level controls for proper contrast:</p> <pre><code># Soft tissue window (default: center=200, width=600)\nvisualize_slices(image=img, window_center=40, window_width=400)\n\n# Bone window\nvisualize_slices(image=img, window_center=400, window_width=1800)\n\n# Lung window\nvisualize_slices(image=img, window_center=-600, window_width=1500)\n</code></pre>"},{"location":"user_guide/utilities/#colormap-options","title":"Colormap Options","text":"Colormap Labels Description <code>tab10</code> 10 Distinct categorical colors <code>tab20</code> 20 Default, 20 distinct colors <code>Set1</code> 9 Bold qualitative colors <code>Set2</code> 8 Pastel qualitative colors <code>Paired</code> 12 Paired colors"},{"location":"user_guide/utilities/#output-formats","title":"Output Formats","text":"<p>Supported formats: <code>png</code> (default), <code>jpeg</code>, <code>tiff</code></p> <pre><code>save_slices(\"output/\", image=img, mask=mask, format=\"tiff\", dpi=300)\n</code></pre>"},{"location":"user_guide/utilities/#parallel-batch-processing","title":"Parallel Batch Processing","text":"<p>For processing multiple images efficiently, use <code>concurrent.futures</code>:</p> <pre><code>from concurrent.futures import ProcessPoolExecutor\nfrom pathlib import Path\nfrom pictologics import load_image\nfrom pictologics.utilities import save_slices\n\ndef process_case(args):\n    \"\"\"Process a single image/mask pair.\"\"\"\n    image_path, mask_path, output_dir = args\n\n    # Load images\n    img = load_image(image_path, recursive=True)\n    mask = load_image(mask_path, recursive=True)\n\n    # Save slices\n    return save_slices(\n        output_dir,\n        image=img,\n        mask=mask,\n        slice_selection=\"10%\",\n        window_center=40,\n        window_width=400\n    )\n\n# Prepare list of (image, mask, output) tuples\ncases = [\n    (\"patient_001/ct/\", \"patient_001/seg.dcm\", \"output/patient_001/\"),\n    (\"patient_002/ct/\", \"patient_002/seg.dcm\", \"output/patient_002/\"),\n    (\"patient_003/ct/\", \"patient_003/seg.dcm\", \"output/patient_003/\"),\n]\n\n# Process in parallel\nwith ProcessPoolExecutor(max_workers=4) as executor:\n    results = list(executor.map(process_case, cases))\n\nprint(f\"Processed {len(results)} cases\")\n</code></pre> <p>Performance Notes</p> <ul> <li>Use <code>ProcessPoolExecutor</code> (not <code>ThreadPoolExecutor</code>) to avoid Python's GIL</li> <li>Set <code>max_workers</code> to the number of CPU cores (4-8 is typically optimal)</li> <li>Each worker loads one image at a time, so memory usage scales with <code>max_workers</code></li> </ul>"},{"location":"user_guide/utilities/#dicom-structured-reports-sr","title":"DICOM Structured Reports (SR)","text":"<p>Parse DICOM Structured Reports to extract measurements and tabular data.</p>"},{"location":"user_guide/utilities/#loading-and-parsing-sr","title":"Loading and Parsing SR","text":"<pre><code>from pictologics.utilities import SRDocument\n\nsr = SRDocument.from_file(\"measurements.dcm\")\nprint(f\"Template: {sr.template_id}\")\nprint(f\"Groups: {len(sr.measurement_groups)}\")\n</code></pre>"},{"location":"user_guide/utilities/#extracting-measurements","title":"Extracting Measurements","text":"<pre><code># Get as DataFrame\ndf = sr.get_measurements_df()\nprint(df[[\"measurement_name\", \"value\", \"unit\"]])\n\n# Export to files\nsr.export_csv(\"measurements.csv\")\nsr.export_json(\"measurements.json\")\n</code></pre>"},{"location":"user_guide/utilities/#batch-sr-processing","title":"Batch SR Processing","text":"<p>For processing multiple SR files from folders, use <code>SRDocument.from_folders()</code>:</p> <pre><code>from pictologics.utilities import SRDocument\n\n# Process all SR files in a folder (recursive by default)\nbatch = SRDocument.from_folders(\n    paths=[\"dicom_data/\"],\n    num_workers=4,  # Parallel processing\n    output_dir=\"sr_exports/\",  # Auto-export each SR\n    export_csv=True,\n    export_json=True,\n)\n\n# Access results\nprint(f\"Processed {len(batch.documents)} SR files\")\n\n# Get combined measurements from all SRs\ndf = batch.get_combined_measurements_df()\nprint(df.head())\n\n# Export combined data\nbatch.export_combined_csv(\"sr_exports/all_measurements.csv\")\nbatch.export_log(\"sr_exports/processing_log.csv\")\n</code></pre>"},{"location":"user_guide/utilities/#parallel-processing-and-progress-bar_1","title":"Parallel Processing and Progress Bar","text":"<p>For large collections of SR files, <code>SRDocument.from_folders()</code> supports parallel processing for faster parsing and displays a progress bar showing progress and estimated time remaining.</p> <pre><code># Parallel processing with 8 workers\nbatch = SRDocument.from_folders(\n    paths=[\"large_dataset/\"],\n    num_workers=8,          # Use 8 parallel workers\n    show_progress=True      # Show progress bar (default: True)\n)\n\n# Disable progress bar for silent operation\nbatch = SRDocument.from_folders(\n    paths=[\"data/\"],\n    show_progress=False\n)\n</code></pre> <p>The progress bar shows: - Number of SR files processed - Percentage complete - Elapsed time and estimated time remaining</p>"},{"location":"user_guide/utilities/#output-structure","title":"Output Structure","text":"<p>When <code>output_dir</code> is specified: <pre><code>sr_exports/\n\u251c\u2500\u2500 all_measurements.csv       # Combined measurements (optional)\n\u251c\u2500\u2500 processing_log.csv         # Log of all processed files\n\u251c\u2500\u2500 1_2_3_4_5_6.csv           # Individual SR (if export_csv=True)\n\u251c\u2500\u2500 1_2_3_4_5_6.json          # Individual SR (if export_json=True)\n\u2514\u2500\u2500 ...\n</code></pre></p>"},{"location":"user_guide/utilities/#processing-log","title":"Processing Log","text":"<p>The processing log tracks each file:</p> Column Description file_path Source SR file path sop_instance_uid SOP Instance UID status \"success\" or \"error\" num_measurements Count of measurements csv_path Path to exported CSV json_path Path to exported JSON error_message Error details if failed"}]}