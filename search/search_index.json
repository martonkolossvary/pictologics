{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Pictologics","text":"<p>Pictologics is a pure python, IBSI-compliant library for radiomic feature extraction from medical images.</p> <p>See also the NOTICE file for attribution and third-party library information.</p>"},{"location":"#why-pictologics","title":"Why Pictologics?","text":"<ul> <li>\ud83d\ude80 High Performance: Uses <code>numba</code> for JIT compilation, achieving significant speedups over other libraries (speedups between 15-300x compared to pyradiomics, see Benchmarks page for details).</li> <li>\u2705 IBSI Compliant: Implements standard algorithms verified against the IBSI digital and CT phantom (IBSI compliance page for details).</li> <li>\ud83d\udd27 Flexible: Configurable pipeline for reproducible research.</li> <li>\u2728 Easy to Use: Simple installation and a straightforward pipeline make it easy to get started quickly.</li> <li>\ud83d\udee0\ufe0f Actively Maintained: Continuously maintained and developed with the intention to provide robust latent radiomic features that can reliably describe morphological characteristics of diseases on radiological images.</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Loaders: Support for NIfTI and DICOM image formats.</li> <li>Preprocessing: Resampling, resegmentation, outlier filtering, and discretisation.</li> <li>Features:<ul> <li>Morphology: Volume, Surface Area, Compactness, etc.</li> <li>Intensity: Mean, Median, Skewness, Kurtosis, etc.</li> <li>Texture: GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM.</li> </ul> </li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Install: Follow the Installation guide.</li> <li>Learn: Check the Quick Start tutorial.</li> <li>Reference: Explore the API Documentation.</li> </ol>"},{"location":"LICENSE/","title":"LICENSE","text":"<p>Apache License                            Version 2.0, January 2004                         http://www.apache.org/licenses/</p> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright 2025 M\u00e1rton Kolossv\u00e1ry</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"NOTICE/","title":"NOTICE","text":"<p>Pictologics Copyright 2025 M\u00e1rton Kolossv\u00e1ry</p> <p>This product includes software developed by M\u00e1rton Kolossv\u00e1ry.</p> <p>This product contains code derived from the Image Biomarker Standardisation Initiative (IBSI) reference implementations and guidelines. https://theibsi.github.io/</p> <p>This product uses the following third-party libraries:</p> <ul> <li>NumPy (BSD 3-Clause)</li> <li>SciPy (BSD 3-Clause)</li> <li>Numba (BSD 2-Clause)</li> <li>NiBabel (MIT)</li> <li>PyDICOM (MIT)</li> <li>Scikit-image (BSD 3-Clause)</li> </ul>"},{"location":"benchmarks/","title":"Benchmarks","text":""},{"location":"benchmarks/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"benchmarks/#benchmark-configuration","title":"Benchmark Configuration","text":"<p>Comparisons between Pictologics and PyRadiomics (single-thread parity). </p> <p>Test Data Generation:</p> <ul> <li>Texture: 3D correlated noise generated using Gaussian smoothing.</li> <li>Mask: Blob-like structures generated via thresholded smooth noise with random holes.</li> <li>Voxel Distribution: Mean=486.04, Std=90.24, Min=0.00, Max=1000.00.</li> </ul>"},{"location":"benchmarks/#hardware-used-for-calculations","title":"HARDWARE USED FOR CALCULATIONS","text":"<ul> <li>Hardware: Apple M4 Pro, 14 cores, 48 GB</li> <li>OS: macOS 26.2 (arm64)</li> <li>Python: 3.12.10</li> <li>Core deps: pictologics 0.1.0, numpy 2.3.5, scipy 1.16.3, numba 0.62.1, pandas 2.3.3, matplotlib 3.10.7</li> <li>PyRadiomics stack (parity runs): pyradiomics 3.1.1.dev111+g8ed579383, SimpleITK 2.5.3</li> <li>BLAS/LAPACK: Apple Accelerate (from <code>numpy.show_config()</code>)</li> </ul> <p>Note: the benchmark script explicitly calls <code>warmup_jit()</code> before timing to avoid including Numba compilation overhead in the measured runtimes.</p>"},{"location":"benchmarks/#intensity","title":"Intensity","text":"Execution Time (Log-Log) Speedup <p>Pictologics-only intensity families (IVH + spatial/local intensity):</p> Size Discretization Pictologics-only Time Pictologics-only Mem 25 FBS 10.0 0.0324 s 0.81 MB 25 FBS 25.0 0.0267 s 0.81 MB 25 FBS 50.0 0.0277 s 0.81 MB 25 FBN 16 0.0281 s 0.81 MB 25 FBN 32 0.0289 s 0.81 MB 25 FBN 64 0.0301 s 0.81 MB 50 FBS 10.0 1.3024 s 6.33 MB 50 FBS 25.0 1.2929 s 6.33 MB 50 FBS 50.0 1.2979 s 6.33 MB 50 FBN 16 1.2876 s 6.33 MB 50 FBN 32 1.2959 s 6.33 MB 50 FBN 64 1.3105 s 6.33 MB 75 FBS 10.0 Not calculated Not calculated 75 FBS 25.0 Not calculated Not calculated 75 FBS 50.0 Not calculated Not calculated 75 FBN 16 Not calculated Not calculated 75 FBN 32 Not calculated Not calculated 75 FBN 64 Not calculated Not calculated 100 FBS 10.0 Not calculated Not calculated 100 FBS 25.0 Not calculated Not calculated 100 FBS 50.0 Not calculated Not calculated 100 FBN 16 Not calculated Not calculated 100 FBN 32 Not calculated Not calculated 100 FBN 64 Not calculated Not calculated"},{"location":"benchmarks/#morphology","title":"Morphology","text":"Execution Time (Log-Log) Speedup <p>Pictologics-only morphology families (intensity-weighted morphology):</p> Size Discretization Pictologics-only Time Pictologics-only Mem 25 FBS 10.0 0.0041 s 1.11 MB 25 FBS 25.0 0.0042 s 1.11 MB 25 FBS 50.0 0.0042 s 1.11 MB 25 FBN 16 0.0044 s 1.11 MB 25 FBN 32 0.0040 s 1.10 MB 25 FBN 64 0.0038 s 1.11 MB 50 FBS 10.0 0.0106 s 5.37 MB 50 FBS 25.0 0.0106 s 5.37 MB 50 FBS 50.0 0.0107 s 5.37 MB 50 FBN 16 0.0106 s 5.37 MB 50 FBN 32 0.0107 s 5.37 MB 50 FBN 64 0.0105 s 5.37 MB 75 FBS 10.0 0.0170 s 8.77 MB 75 FBS 25.0 0.0168 s 8.77 MB 75 FBS 50.0 0.0165 s 8.77 MB 75 FBN 16 0.0159 s 8.77 MB 75 FBN 32 0.0164 s 8.77 MB 75 FBN 64 0.0159 s 8.77 MB 100 FBS 10.0 0.0317 s 20.40 MB 100 FBS 25.0 0.0325 s 20.40 MB 100 FBS 50.0 0.0319 s 20.40 MB 100 FBN 16 0.0320 s 20.40 MB 100 FBN 32 0.0319 s 20.40 MB 100 FBN 64 0.0343 s 20.40 MB"},{"location":"benchmarks/#texture","title":"Texture","text":"Execution Time (Log-Log) Speedup <p>Pictologics-only texture families (GLDZM):</p> Size Discretization Pictologics-only Time Pictologics-only Mem 25 FBS 10.0 0.0002 s 0.08 MB 25 FBS 25.0 0.0002 s 0.07 MB 25 FBS 50.0 0.0002 s 0.07 MB 25 FBN 16 0.0002 s 0.07 MB 25 FBN 32 0.0002 s 0.07 MB 25 FBN 64 0.0002 s 0.07 MB 50 FBS 10.0 0.0002 s 0.10 MB 50 FBS 25.0 0.0003 s 0.08 MB 50 FBS 50.0 0.0002 s 0.07 MB 50 FBN 16 0.0002 s 0.07 MB 50 FBN 32 0.0003 s 0.07 MB 50 FBN 64 0.0003 s 0.09 MB 75 FBS 10.0 0.0003 s 0.15 MB 75 FBS 25.0 0.0003 s 0.10 MB 75 FBS 50.0 0.0003 s 0.08 MB 75 FBN 16 0.0003 s 0.08 MB 75 FBN 32 0.0003 s 0.09 MB 75 FBN 64 0.0003 s 0.12 MB 100 FBS 10.0 0.0004 s 0.14 MB 100 FBS 25.0 0.0003 s 0.09 MB 100 FBS 50.0 0.0003 s 0.08 MB 100 FBN 16 0.0003 s 0.07 MB 100 FBN 32 0.0003 s 0.09 MB 100 FBN 64 0.0003 s 0.11 MB"},{"location":"benchmarks/#detailed-parity-results","title":"Detailed Parity Results","text":"Family Size Discretization Pictologics Time PyRadiomics Time Speedup Pictologics Mem PyRadiomics Mem Intensity 25 FBN 16 0.0008 s 0.0132 s 17.38x 0.24 MB 0.71 MB Intensity 25 FBN 32 0.0009 s 0.0128 s 13.98x 0.24 MB 0.71 MB Intensity 25 FBN 64 0.0008 s 0.0125 s 14.87x 0.24 MB 0.71 MB Intensity 25 FBS 10.0 0.0008 s 0.0135 s 17.39x 0.24 MB 0.74 MB Intensity 25 FBS 25.0 0.0008 s 0.0128 s 15.20x 0.24 MB 0.71 MB Intensity 25 FBS 50.0 0.0009 s 0.0130 s 14.96x 0.24 MB 0.71 MB Intensity 50 FBN 16 0.0029 s 0.0625 s 21.72x 1.40 MB 4.61 MB Intensity 50 FBN 32 0.0029 s 0.0633 s 21.65x 1.40 MB 4.61 MB Intensity 50 FBN 64 0.0031 s 0.0657 s 21.08x 1.40 MB 4.61 MB Intensity 50 FBS 10.0 0.0028 s 0.0655 s 23.15x 1.40 MB 4.61 MB Intensity 50 FBS 25.0 0.0035 s 0.0643 s 18.22x 1.40 MB 4.61 MB Intensity 50 FBS 50.0 0.0030 s 0.0634 s 21.45x 1.40 MB 4.61 MB Intensity 75 FBN 16 0.0104 s 0.2441 s 23.46x 5.81 MB 17.95 MB Intensity 75 FBN 32 0.0108 s 0.2493 s 23.12x 5.81 MB 17.95 MB Intensity 75 FBN 64 0.0107 s 0.2501 s 23.28x 5.81 MB 17.95 MB Intensity 75 FBS 10.0 0.0116 s 0.2572 s 22.20x 5.81 MB 17.95 MB Intensity 75 FBS 25.0 0.0112 s 0.2538 s 22.62x 5.81 MB 17.95 MB Intensity 75 FBS 50.0 0.0110 s 0.2499 s 22.71x 5.81 MB 17.95 MB Intensity 100 FBN 16 0.0208 s 0.5077 s 24.36x 12.16 MB 39.01 MB Intensity 100 FBN 32 0.0214 s 0.5039 s 23.55x 12.16 MB 39.01 MB Intensity 100 FBN 64 0.0248 s 0.5128 s 20.70x 12.16 MB 39.01 MB Intensity 100 FBS 10.0 0.0225 s 0.5146 s 22.83x 12.16 MB 39.01 MB Intensity 100 FBS 25.0 0.0231 s 0.5151 s 22.29x 12.16 MB 39.01 MB Intensity 100 FBS 50.0 0.0209 s 0.5127 s 24.50x 12.16 MB 39.00 MB Morphology 25 FBN 16 0.0038 s 0.0521 s 13.56x 1.11 MB 1.18 MB Morphology 25 FBN 32 0.0040 s 0.0512 s 12.79x 1.11 MB 1.18 MB Morphology 25 FBN 64 0.0038 s 0.0501 s 13.02x 1.11 MB 1.18 MB Morphology 25 FBS 10.0 0.0041 s 0.0510 s 12.58x 1.11 MB 1.18 MB Morphology 25 FBS 25.0 0.0041 s 0.0507 s 12.30x 1.11 MB 1.18 MB Morphology 25 FBS 50.0 0.0037 s 0.0520 s 13.91x 1.11 MB 1.18 MB Morphology 50 FBN 16 0.0104 s 0.9495 s 91.09x 5.37 MB 8.68 MB Morphology 50 FBN 32 0.0103 s 0.9449 s 91.49x 5.37 MB 8.68 MB Morphology 50 FBN 64 0.0104 s 0.9577 s 91.89x 5.37 MB 8.68 MB Morphology 50 FBS 10.0 0.0104 s 0.9545 s 91.58x 5.37 MB 8.68 MB Morphology 50 FBS 25.0 0.0100 s 0.9476 s 95.23x 5.37 MB 8.68 MB Morphology 50 FBS 50.0 0.0101 s 0.9420 s 93.32x 5.37 MB 8.68 MB Morphology 75 FBN 16 0.0159 s 1.6769 s 105.70x 8.77 MB 36.60 MB Morphology 75 FBN 32 0.0162 s 1.6589 s 102.17x 8.77 MB 36.60 MB Morphology 75 FBN 64 0.0160 s 1.6684 s 104.20x 8.77 MB 36.60 MB Morphology 75 FBS 10.0 0.0168 s 1.7313 s 102.85x 8.77 MB 36.60 MB Morphology 75 FBS 25.0 0.0164 s 1.7055 s 104.18x 8.77 MB 36.60 MB Morphology 75 FBS 50.0 0.0166 s 1.7286 s 103.90x 8.77 MB 36.60 MB Morphology 100 FBN 16 0.0310 s 8.2976 s 267.42x 20.40 MB 77.49 MB Morphology 100 FBN 32 0.0315 s 8.2656 s 262.18x 20.40 MB 77.49 MB Morphology 100 FBN 64 0.0338 s 8.2164 s 242.93x 20.40 MB 77.49 MB Morphology 100 FBS 10.0 0.0310 s 8.4072 s 270.94x 20.40 MB 77.49 MB Morphology 100 FBS 25.0 0.0343 s 8.3736 s 244.10x 20.40 MB 77.49 MB Morphology 100 FBS 50.0 0.0315 s 8.2861 s 262.78x 20.40 MB 77.49 MB Texture 25 FBN 16 0.0049 s 0.0478 s 9.73x 2.13 MB 0.69 MB Texture 25 FBN 32 0.0057 s 0.0508 s 8.94x 2.07 MB 0.75 MB Texture 25 FBN 64 0.0053 s 0.0621 s 11.77x 4.53 MB 1.94 MB Texture 25 FBS 10.0 0.0073 s 0.0889 s 12.25x 10.10 MB 3.77 MB Texture 25 FBS 25.0 0.0058 s 0.0517 s 8.89x 2.16 MB 0.91 MB Texture 25 FBS 50.0 0.0050 s 0.0482 s 9.69x 2.13 MB 0.71 MB Texture 50 FBN 16 0.0220 s 0.2931 s 13.35x 19.85 MB 6.29 MB Texture 50 FBN 32 0.0237 s 0.2999 s 12.67x 19.95 MB 6.29 MB Texture 50 FBN 64 0.0180 s 0.3189 s 17.69x 9.83 MB 4.82 MB Texture 50 FBS 10.0 0.0178 s 0.3517 s 19.72x 11.96 MB 5.41 MB Texture 50 FBS 25.0 0.0237 s 0.3033 s 12.78x 19.30 MB 6.09 MB Texture 50 FBS 50.0 0.0237 s 0.2968 s 12.53x 21.08 MB 6.60 MB Texture 75 FBN 16 0.0702 s 1.1862 s 16.90x 84.43 MB 25.62 MB Texture 75 FBN 32 0.0781 s 1.1797 s 15.10x 86.59 MB 26.22 MB Texture 75 FBN 64 0.0637 s 1.2079 s 18.96x 63.72 MB 20.14 MB Texture 75 FBS 10.0 0.0486 s 1.2407 s 25.54x 13.36 MB 17.08 MB Texture 75 FBS 25.0 0.0803 s 1.1946 s 14.87x 89.44 MB 25.80 MB Texture 75 FBS 50.0 0.0753 s 1.2248 s 16.27x 88.88 MB 25.65 MB Texture 100 FBN 16 0.1675 s 2.4330 s 14.52x 213.37 MB 64.12 MB Texture 100 FBN 32 0.1859 s 2.4364 s 13.10x 224.11 MB 66.17 MB Texture 100 FBN 64 0.1922 s 2.4449 s 12.72x 204.71 MB 60.72 MB Texture 100 FBS 10.0 0.0991 s 2.5537 s 25.77x 22.17 MB 37.15 MB Texture 100 FBS 25.0 0.2054 s 2.4582 s 11.97x 230.74 MB 68.10 MB Texture 100 FBS 50.0 0.1926 s 2.4373 s 12.65x 229.77 MB 68.48 MB"},{"location":"ibsi_compliance/","title":"Comprehensive IBSI Benchmark Results","text":""},{"location":"ibsi_compliance/#how-to-run-the-benchmarks","title":"How to Run the Benchmarks","text":"<p>To reproduce these results or run the IBSI compliance benchmarks on your own system:</p>"},{"location":"ibsi_compliance/#1-download-the-data","title":"1. Download the Data","text":"<p>The IBSI reference datasets (Digital Phantom and Lung Cancer CT) are available on the IBSI GitHub repository.</p> <ul> <li>Digital Phantom: Download <code>phantom.nii.gz</code> and <code>mask.nii.gz</code> from the <code>digital_phantom</code> folder.</li> <li>Lung Cancer CT: Download the <code>PAT1</code> NIfTI files. Note that IBSI recommends converting these to at least 32-bit floating point and rounding to the nearest integer before processing.</li> </ul> <p>Place these files in <code>dev/data/</code> or provide their paths to the script.</p>"},{"location":"ibsi_compliance/#2-run-configurations-programmatically-using-radiomicspipeline","title":"2. Run Configurations Programmatically using <code>RadiomicsPipeline</code>","text":"<p>You can run any of the IBSI configurations programmatically using the <code>RadiomicsPipeline</code> class.</p> <pre><code>from pictologics.pipeline import RadiomicsPipeline\nfrom pictologics.loader import load_image\n\n# 1. Initialize Pipeline\npipeline = RadiomicsPipeline()\n\n# --- DEFINE CONFIGURATIONS ---\n\n# A. Digital Phantom Config (FBS 1.0, no resampling)\nconfig_digital_phantom = [\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": 1.0}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"], \"include_spatial_intensity\": True, \"include_local_intensity\": True}}\n]\n\n# B. Config C (2mm isotropic, resegment [-1000, 400], FBS 25 HU)\nconfig_c = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (2.0, 2.0, 2.0), \"interpolation\": \"linear\", \"round_intensities\": True}},\n    {\"step\": \"keep_largest_component\", \"params\": {\"apply_to\": \"morph\"}},\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": 25.0, \"min_val\": -1000}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"], \"include_spatial_intensity\": True, \"include_local_intensity\": True, \"ivh_discretisation\": {\"method\": \"FBS\", \"bin_width\": 2.5, \"min_val\": -1000}, \"ivh_params\": {\"target_range_max\": 400}}}\n]\n\n# C. Config D (2mm isotropic, 3-sigma outlier, FBN 32, Continuous IVH)\nconfig_d = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (2.0, 2.0, 2.0), \"interpolation\": \"linear\", \"round_intensities\": True}},\n    {\"step\": \"keep_largest_component\", \"params\": {\"apply_to\": \"morph\"}},\n    {\"step\": \"filter_outliers\", \"params\": {\"sigma\": 3.0}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"], \"include_spatial_intensity\": True, \"include_local_intensity\": True, \"ivh_use_continuous\": True}}\n]\n\n# D. Config E (Cubic resamp, 3-sigma, round last, FBN 32 for tex, FBN 1000 for IVH)\nconfig_e = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (2.0, 2.0, 2.0), \"interpolation\": \"cubic\", \"round_intensities\": False}},\n    {\"step\": \"keep_largest_component\", \"params\": {\"apply_to\": \"morph\"}},\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n    {\"step\": \"filter_outliers\", \"params\": {\"sigma\": 3.0}},\n    {\"step\": \"round_intensities\", \"params\": {}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"], \"include_spatial_intensity\": True, \"include_local_intensity\": True, \"ivh_discretisation\": {\"method\": \"FBN\", \"n_bins\": 1000}, \"ivh_params\": {\"bin_width\": 1.0, \"min_val\": 0.5, \"max_val\": 1000.5}}}\n]\n\n# --- RUN PIPELINE ---\n# Choose which config to run:\npipeline.add_config(\"ibsi_config_c\", config_c)\n\n# 3. Load image and mask\n# For Config C/D/E (Lung Cancer CT)\nimage = load_image(\"path/to/CT_image.nii.gz\")\nmask = load_image(\"path/to/CT_mask.nii.gz\")\n\n# 4. Run extraction\nresults = pipeline.run(image, \"ibsi_config_c\", mask=mask)\n\n# 5. Access results\nprint(results[\"ibsi_config_c\"])\n</code></pre>"},{"location":"ibsi_compliance/#known-deviations","title":"Known Deviations","text":"<p>For configuration D, Zone size non-uniformity and Zone size entropy are known to fail with minimal differences, but pass for all other conffigurations. The following morphological features consistently fail across all configurations due to differences in surface mesh generation algorithms:</p> <ul> <li>Compactness 2 (BQWJ): Sensitive to surface area calculation differences.</li> <li>Asphericity (25C7): Derived from Compactness 2, inheriting the same deviation.</li> </ul>"},{"location":"ibsi_compliance/#digital-phantom","title":"Digital Phantom","text":""},{"location":"ibsi_compliance/#morphology","title":"Morphology","text":"Feature Code Calc Ref Tol Status Volume RNU0 556.3 556 4 \u2705 PASS Volume voxel counting YEKZ 592 592 4 \u2705 PASS Surface area C0JK 388.1 388 3 \u2705 PASS Surface to volume ratio 2PR5 0.6976 0.698 0.004 \u2705 PASS Compactness 1 SKGS 0.04106 0.0411 0.0003 \u2705 PASS Compactness 2 BQWJ 0.5989 0.599 0.004 \u2705 PASS Spherical disproportion KRCK 1.186 1.19 0.01 \u2705 PASS Sphericity QCFX 0.8429 0.843 0.005 \u2705 PASS Asphericity 25C7 0.1863 0.186 0.001 \u2705 PASS Center of mass shift KLMA 0.6715 0.672 0.004 \u2705 PASS Maximum 3D diameter L0JK 13.11 13.1 0.1 \u2705 PASS Major axis length TDIC 11.4 11.4 0.1 \u2705 PASS Minor axis length P9VJ 9.308 9.31 0.06 \u2705 PASS Least axis length 7J51 8.536 8.54 0.05 \u2705 PASS Elongation Q3CK 0.8163 0.816 0.005 \u2705 PASS Flatness N17B 0.7486 0.749 0.005 \u2705 PASS Volume density (AABB) PBX1 0.8693 0.869 0.005 \u2705 PASS Area density (AABB) R59B 0.8662 0.866 0.005 \u2705 PASS Volume density (OMBB) ZH1A 0.458 \u2014 \u2014 \u2757 REF. Area density (OMBB) IQYR 0.5673 \u2014 \u2014 \u2757 REF. Volume density (AEE) 6BDE 1.173 1.17 0.01 \u2705 PASS Area density (AEE) RDD2 1.355 1.36 0.01 \u2705 PASS Volume density (MVEE) SWZ1 0.513 \u2014 \u2014 \u2757 REF. Area density (MVEE) BRI8 0.7909 \u2014 \u2014 \u2757 REF. Volume density (convex hull) R3ER 0.9609 0.961 0.006 \u2705 PASS Area density (convex hull) 7T7F 1.033 1.03 0.01 \u2705 PASS"},{"location":"ibsi_compliance/#intensity","title":"Intensity","text":"Feature Code Calc Ref Tol Status Integrated intensity 99N0 1195 1.2e3 10 \u2705 PASS Moran's I index N365 0.0397 0.0397 0.0003 \u2705 PASS Geary's C measure NPT7 0.974 0.974 0.006 \u2705 PASS Local intensity peak VJGA 2.6 2.6 \u2014 \u2705 PASS Global intensity peak 0F91 3.103 3.1 \u2014 \u2705 PASS Mean intensity Q4LE 2.149 2.15 \u2014 \u2705 PASS Intensity variance ECT3 3.045 3.05 \u2014 \u2705 PASS Intensity skewness KE2A 1.084 1.08 \u2014 \u2705 PASS Intensity kurtosis IPH6 -0.3546 -0.355 \u2014 \u2705 PASS Median intensity Y12H 1 1 \u2014 \u2705 PASS Minimum intensity 1GSF 1 1 \u2014 \u2705 PASS 10th intensity percentile QG58 1 1 \u2014 \u2705 PASS 90th intensity percentile 8DWT 4 4 \u2014 \u2705 PASS Maximum intensity 84IY 6 6 \u2014 \u2705 PASS Intensity interquartile range SALO 3 3 \u2014 \u2705 PASS Intensity range 2OJQ 5 5 \u2014 \u2705 PASS Intensity Mean absolute deviation 4FUA 1.552 1.55 \u2014 \u2705 PASS Intensity Robust mean absolute deviation 1128 1.114 1.11 \u2014 \u2705 PASS Intensity Median absolute deviation N72L 1.149 1.15 \u2014 \u2705 PASS Intensity Coefficient of variation 7TET 0.8122 0.812 \u2014 \u2705 PASS Intensity Quartile coefficient of dispersion 9S40 0.6 0.6 \u2014 \u2705 PASS Intensity energy N8CA 567 567 \u2014 \u2705 PASS Root mean square intensity 5ZWQ 2.768 2.77 \u2014 \u2705 PASS"},{"location":"ibsi_compliance/#intensity-histogram","title":"Intensity Histogram","text":"Feature Code Calc Ref Tol Status Mean discretised intensity X6K6 2.149 2.15 \u2014 \u2705 PASS Discretised intensity variance CH89 3.045 3.05 \u2014 \u2705 PASS Discretised intensity skewness 88K1 1.084 1.08 \u2014 \u2705 PASS Discretised intensity kurtosis C3I7 -0.3546 -0.355 \u2014 \u2705 PASS Median discretised intensity WIFQ 1 1 \u2014 \u2705 PASS Minimum discretised intensity 1PR8 1 1 \u2014 \u2705 PASS 10th discretised intensity percentile 1PR 1 1 \u2014 \u2705 PASS 90th discretised intensity percentile GPMT 4 4 \u2014 \u2705 PASS Maximum discretised intensity 3NCY 6 6 \u2014 \u2705 PASS Intensity histogram mode AMMC 1 1 \u2014 \u2705 PASS Discretised intensity interquartile range WR0O 3 3 \u2014 \u2705 PASS Discretised intensity range 5Z3W 5 5 \u2014 \u2705 PASS Intensity histogram mean absolute deviation D2ZX 1.552 1.55 \u2014 \u2705 PASS Intensity histogram robust mean absolute deviation WRZB 1.114 1.11 \u2014 \u2705 PASS Intensity histogram median absolute deviation 4RNL 1.149 1.15 \u2014 \u2705 PASS Intensity histogram coefficient of variation CWYJ 0.8122 0.812 \u2014 \u2705 PASS Intensity histogram quartile coefficient of dispersion SLWD 0.6 0.6 \u2014 \u2705 PASS Discretised intensity entropy TLU2 1.266 1.27 \u2014 \u2705 PASS Discretised intensity uniformity BJ5W 0.5124 0.512 \u2014 \u2705 PASS Maximum histogram gradient 12CE 8 8 \u2014 \u2705 PASS Maximum histogram gradient intensity 8E6O 3 3 \u2014 \u2705 PASS Minimum histogram gradient VQB3 -50 -50 \u2014 \u2705 PASS Minimum histogram gradient intensity RHQZ 1 1 \u2014 \u2705 PASS"},{"location":"ibsi_compliance/#intensity-volume-histogram","title":"Intensity-Volume Histogram","text":"Feature Code Calc Ref Tol Status Volume at intensity fraction 0.10 BC2M_10 0.3243 0.324 \u2014 \u2705 PASS Volume at intensity fraction 0.90 BC2M_90 0.09459 0.0946 \u2014 \u2705 PASS Intensity at volume fraction 0.10 GBPN_10 5 5 \u2014 \u2705 PASS Intensity at volume fraction 0.90 GBPN_90 2 2 \u2014 \u2705 PASS Volume fraction difference between intensity 0.10 and 0.90 fractions DDTU 0.2297 0.23 \u2014 \u2705 PASS Intensity fraction difference between volume 0.10 and 0.90 fractions CNV2 3 3 \u2014 \u2705 PASS Area under the IVH curve 9CMM 2.047 \u2014 \u2014 \u2757 REF."},{"location":"ibsi_compliance/#glcm","title":"GLCM","text":"Feature Code Calc Ref Tol Status Joint maximum GYBY 0.5085 0.509 \u2014 \u2705 PASS Joint average 60VM 2.149 2.15 \u2014 \u2705 PASS Joint variance UR99 3.132 3.13 \u2014 \u2705 PASS Joint entropy TU9B 2.574 2.57 \u2014 \u2705 PASS Difference average TF7R 1.38 1.38 \u2014 \u2705 PASS Difference variance D3YU 3.215 3.21 \u2014 \u2705 PASS Difference entropy NTRS 1.641 1.64 \u2014 \u2705 PASS Sum average ZGXS 4.298 4.3 \u2014 \u2705 PASS Sum variance OEEB 7.412 7.41 \u2014 \u2705 PASS Sum entropy P6QZ 2.11 2.11 \u2014 \u2705 PASS Angular second moment 8ZQL 0.291 0.291 \u2014 \u2705 PASS Contrast ACUI 5.118 5.12 \u2014 \u2705 PASS Dissimilarity 8S9J 1.38 1.38 \u2014 \u2705 PASS Inverse difference IB1Z 0.6877 0.688 \u2014 \u2705 PASS Normalised inverse difference NDRX 0.8559 0.856 \u2014 \u2705 PASS Inverse difference moment WF0Z 0.6306 0.631 \u2014 \u2705 PASS Normalised inverse difference moment 1QCO 0.9022 0.902 \u2014 \u2705 PASS Inverse variance E8JP 0.05744 0.0574 \u2014 \u2705 PASS Correlation NI2N 0.1831 0.183 \u2014 \u2705 PASS Autocorrelation QWB0 5.192 5.19 \u2014 \u2705 PASS Cluster tendency DG8W 7.412 7.41 \u2014 \u2705 PASS Cluster shade 7NFM 17.42 17.4 \u2014 \u2705 PASS Cluster prominence AE86 147.5 147 \u2014 \u2705 PASS Information correlation 1 R8DG -0.0288 -0.0288 \u2014 \u2705 PASS Information correlation 2 JN9H 0.2692 0.269 \u2014 \u2705 PASS"},{"location":"ibsi_compliance/#glrlm","title":"GLRLM","text":"Feature Code Calc Ref Tol Status Short runs emphasis 22OV 0.7291 0.729 \u2014 \u2705 PASS Long runs emphasis W4KF 2.761 2.76 \u2014 \u2705 PASS Low grey level run emphasis V3SW 0.6067 0.607 \u2014 \u2705 PASS High grey level run emphasis G3QZ 9.638 9.64 \u2014 \u2705 PASS Short run low grey level emphasis HTZT 0.3716 0.372 \u2014 \u2705 PASS Short run high grey level emphasis GD3A 8.672 8.67 \u2014 \u2705 PASS Long run low grey level emphasis IVPO 2.163 2.16 \u2014 \u2705 PASS Long run high grey level emphasis 3KUM 15.63 15.6 \u2014 \u2705 PASS Grey level non-uniformity R5YN 281.3 281 \u2014 \u2705 PASS Normalised grey level non-uniformity OVBL 0.4301 0.43 \u2014 \u2705 PASS Run length non-uniformity W92Y 327.7 328 \u2014 \u2705 PASS Normalised run length non-uniformity IC23 0.5011 0.501 \u2014 \u2705 PASS Run percentage 9ZK5 0.6798 0.68 \u2014 \u2705 PASS Grey level variance 8CE5 3.479 3.48 \u2014 \u2705 PASS Run length variance SXLW 0.5978 0.598 \u2014 \u2705 PASS Run entropy HJ9O 2.624 2.62 \u2014 \u2705 PASS"},{"location":"ibsi_compliance/#glszm","title":"GLSZM","text":"Feature Code Calc Ref Tol Status Small zone emphasis P001 0.2552 0.255 \u2014 \u2705 PASS Large zone emphasis 48P8 550 550 \u2014 \u2705 PASS Low grey level zone emphasis XMSY 0.2528 0.253 \u2014 \u2705 PASS High grey level zone emphasis 5GN9 15.6 15.6 \u2014 \u2705 PASS Small zone low grey level emphasis 5RAI 0.0256 0.0256 \u2014 \u2705 PASS Small zone high grey level emphasis HW1V 2.763 2.76 \u2014 \u2705 PASS Large zone low grey level emphasis YH51 502.8 503 \u2014 \u2705 PASS Large zone high grey level emphasis J17V 1495 1.49e3 \u2014 \u2705 PASS Grey level non-uniformity JNSA 1.4 1.4 \u2014 \u2705 PASS Normalised grey level non-uniformity Y1RO 0.28 0.28 \u2014 \u2705 PASS Zone size non-uniformity 4JP3 1 1 \u2014 \u2705 PASS Normalised zone size non-uniformity VB3A 0.2 0.2 \u2014 \u2705 PASS Zone percentage P30P 0.06757 0.0676 \u2014 \u2705 PASS Grey level variance BYLV 2.64 2.64 \u2014 \u2705 PASS Zone size variance 3NSA 331 331 \u2014 \u2705 PASS Zone size entropy GU8N 2.322 2.32 \u2014 \u2705 PASS"},{"location":"ibsi_compliance/#gldzm","title":"GLDZM","text":"Feature Code Calc Ref Tol Status Small distance emphasis 0GBI 1 1 \u2014 \u2705 PASS Large distance emphasis MB4I 1 1 \u2014 \u2705 PASS Low grey level zone emphasis S1RA 0.2528 0.253 \u2014 \u2705 PASS High grey level zone emphasis K26C 15.6 15.6 \u2014 \u2705 PASS Small distance low grey level emphasis RUVG 0.2528 0.253 \u2014 \u2705 PASS Small distance high grey level emphasis DKNJ 15.6 15.6 \u2014 \u2705 PASS Large distance low grey level emphasis A7WM 0.2528 0.253 \u2014 \u2705 PASS Large distance high grey level emphasis KLTH 15.6 15.6 \u2014 \u2705 PASS Grey level non-uniformity VFT7 1.4 1.4 \u2014 \u2705 PASS Normalised grey level non-uniformity 7HP3 0.28 0.28 \u2014 \u2705 PASS Zone distance non-uniformity V294 5 5 \u2014 \u2705 PASS Normalised zone distance non-uniformity IATH 1 1 \u2014 \u2705 PASS Zone percentage VIWW 0.06757 0.0676 \u2014 \u2705 PASS Grey level variance QK93 2.64 2.64 \u2014 \u2705 PASS Zone distance variance 7WT1 0 0 \u2014 \u2705 PASS Zone distance entropy GBDU 1.922 1.92 \u2014 \u2705 PASS"},{"location":"ibsi_compliance/#ngtdm","title":"NGTDM","text":"Feature Code Calc Ref Tol Status Coarseness QCDE 0.0296 0.0296 \u2014 \u2705 PASS Contrast 65HE 0.5837 0.584 \u2014 \u2705 PASS Busyness NQ30 6.544 6.54 \u2014 \u2705 PASS Complexity HDEZ 13.54 13.5 \u2014 \u2705 PASS Strength 1X9X 0.7635 0.763 \u2014 \u2705 PASS"},{"location":"ibsi_compliance/#ngldm","title":"NGLDM","text":"Feature Code Calc Ref Tol Status Low dependence emphasis SODN 0.045 0.045 \u2014 \u2705 PASS High dependence emphasis IMOQ 109 109 \u2014 \u2705 PASS Low grey level count emphasis TL9H 0.6933 0.693 \u2014 \u2705 PASS High grey level count emphasis OAE7 7.662 7.66 \u2014 \u2705 PASS Low dependence low grey level emphasis EQ3F 0.009631 0.00963 \u2014 \u2705 PASS Low dependence high grey level emphasis JA6D 0.7362 0.736 \u2014 \u2705 PASS High dependence low grey level emphasis NBZI 102.5 102 \u2014 \u2705 PASS High dependence high grey level emphasis 9QMG 235 235 \u2014 \u2705 PASS Grey level non-uniformity FP8K 37.92 37.9 \u2014 \u2705 PASS Normalised grey level non-uniformity 5SPA 0.5124 0.512 \u2014 \u2705 PASS Dependence count non-uniformity Z87G 4.865 4.86 \u2014 \u2705 PASS Normalised dependence count non-uniformity OKJI 0.06574 0.0657 \u2014 \u2705 PASS Dependence count percentage 6XV8 1 1 \u2014 \u2705 PASS Grey level variance 1PFV 3.045 3.05 \u2014 \u2705 PASS Dependence count variance DNX2 22.06 22.1 \u2014 \u2705 PASS Dependence count entropy FCBV 4.404 4.4 \u2014 \u2705 PASS Dependence count energy CAS9 0.05332 0.0533 \u2014 \u2705 PASS"},{"location":"ibsi_compliance/#config-c","title":"Config C","text":""},{"location":"ibsi_compliance/#morphology_1","title":"Morphology","text":"Feature Code Calc Ref Tol Status Volume RNU0 3.672e+05 3.67e5 6e3 \u2705 PASS Volume voxel counting YEKZ 3.675e+05 3.68e5 6e3 \u2705 PASS Surface area C0JK 3.446e+04 3.43e4 400 \u2705 PASS Surface to volume ratio 2PR5 0.09384 0.0934 0.0007 \u2705 PASS Compactness 1 SKGS 0.03239 \u2014 \u2014 \u2757 REF. Compactness 2 BQWJ 0.3727 0.378 0.004 \u274c FAIL Spherical disproportion KRCK 1.39 1.38 0.01 \u2705 PASS Sphericity QCFX 0.7196 0.723 0.003 \u2705 PASS Asphericity 25C7 0.3896 0.383 0.004 \u274c FAIL Center of mass shift KLMA 45.64 45.6 2.8 \u2705 PASS Maximum 3D diameter L0JK 125.1 125 1 \u2705 PASS Major axis length TDIC 93.26 93.3 0.5 \u2705 PASS Minor axis length P9VJ 81.98 82 0.5 \u2705 PASS Least axis length 7J51 70.88 70.9 0.4 \u2705 PASS Elongation Q3CK 0.879 0.879 0.001 \u2705 PASS Flatness N17B 0.76 0.76 0.001 \u2705 PASS Volume density (AABB) PBX1 0.4779 0.478 0.003 \u2705 PASS Area density (AABB) R59B 0.6814 0.678 0.003 \u2705 PASS Volume density (OMBB) ZH1A 0.3405 \u2014 \u2014 \u2757 REF. Area density (OMBB) IQYR 0.5412 \u2014 \u2014 \u2757 REF. Volume density (AEE) 6BDE 1.294 1.29 0.01 \u2705 PASS Area density (AEE) RDD2 1.625 1.62 0.01 \u2705 PASS Volume density (MVEE) SWZ1 0.4956 \u2014 \u2014 \u2757 REF. Area density (MVEE) BRI8 0.8844 \u2014 \u2014 \u2757 REF. Volume density (convex hull) R3ER 0.8331 0.834 0.002 \u2705 PASS Area density (convex hull) 7T7F 1.135 1.13 0.01 \u2705 PASS"},{"location":"ibsi_compliance/#intensity_1","title":"Intensity","text":"Feature Code Calc Ref Tol Status Integrated intensity 99N0 -1.794e+07 -1.8e7 1.4e6 \u2705 PASS Moran's I index N365 0.08246 0.0824 0.0003 \u2705 PASS Geary's C measure NPT7 0.8461 0.846 0.001 \u2705 PASS Local intensity peak VJGA 168.6 169 10 \u2705 PASS Global intensity peak 0F91 179.7 180 5 \u2705 PASS Mean intensity Q4LE -48.85 -49 2.9 \u2705 PASS Intensity variance ECT3 5.056e+04 5.06e4 1.4e3 \u2705 PASS Intensity skewness KE2A -2.142 -2.14 0.05 \u2705 PASS Intensity kurtosis IPH6 3.534 3.53 0.23 \u2705 PASS Median intensity Y12H 40 40 0.4 \u2705 PASS Minimum intensity 1GSF -939 -939 4 \u2705 PASS 10th intensity percentile QG58 -424 -424 14 \u2705 PASS 90th intensity percentile 8DWT 86 86 0.1 \u2705 PASS Maximum intensity 84IY 393 393 10 \u2705 PASS Intensity interquartile range SALO 67 67 4.9 \u2705 PASS Intensity range 2OJQ 1332 1.33e3 20 \u2705 PASS Intensity Mean absolute deviation 4FUA 157.8 158 4 \u2705 PASS Intensity Robust mean absolute deviation 1128 66.74 66.8 3.5 \u2705 PASS Intensity Median absolute deviation N72L 119 119 4 \u2705 PASS Intensity Coefficient of variation 7TET -4.603 -4.59 0.29 \u2705 PASS Intensity Quartile coefficient of dispersion 9S40 1.031 1.03 0.4 \u2705 PASS Intensity energy N8CA 2.432e+09 2.44e9 1.2e8 \u2705 PASS Root mean square intensity 5ZWQ 230.1 230 4 \u2705 PASS"},{"location":"ibsi_compliance/#intensity-histogram_1","title":"Intensity Histogram","text":"Feature Code Calc Ref Tol Status Mean discretised intensity X6K6 38.56 38.6 0.2 \u2705 PASS Discretised intensity variance CH89 80.99 81.1 2.1 \u2705 PASS Discretised intensity skewness 88K1 -2.139 -2.14 0.05 \u2705 PASS Discretised intensity kurtosis C3I7 3.528 3.52 0.23 \u2705 PASS Median discretised intensity WIFQ 42 42 \u2014 \u2705 PASS Minimum discretised intensity 1PR8 3 3 0.16 \u2705 PASS 10th discretised intensity percentile 1PR 24 24 0.7 \u2705 PASS 90th discretised intensity percentile GPMT 44 44 \u2014 \u2705 PASS Maximum discretised intensity 3NCY 56 56 0.5 \u2705 PASS Intensity histogram mode AMMC 43 43 0.1 \u2705 PASS Discretised intensity interquartile range WR0O 3 3 0.21 \u2705 PASS Discretised intensity range 5Z3W 53 53 0.6 \u2705 PASS Intensity histogram mean absolute deviation D2ZX 6.314 6.32 0.15 \u2705 PASS Intensity histogram robust mean absolute deviation WRZB 2.587 2.59 0.14 \u2705 PASS Intensity histogram median absolute deviation 4RNL 4.745 4.75 0.12 \u2705 PASS Intensity histogram coefficient of variation CWYJ 0.2334 0.234 0.005 \u2705 PASS Intensity histogram quartile coefficient of dispersion SLWD 0.03614 0.0361 0.0027 \u2705 PASS Discretised intensity entropy TLU2 3.734 3.73 0.04 \u2705 PASS Discretised intensity uniformity BJ5W 0.1396 0.14 0.003 \u2705 PASS Maximum histogram gradient 12CE 4740 4.75e3 30 \u2705 PASS Maximum histogram gradient intensity 8E6O 41 41 \u2014 \u2705 PASS Minimum histogram gradient VQB3 -4672 -4.68e3 50 \u2705 PASS Minimum histogram gradient intensity RHQZ 44 44 \u2014 \u2705 PASS"},{"location":"ibsi_compliance/#intensity-volume-histogram_1","title":"Intensity-Volume Histogram","text":"Feature Code Calc Ref Tol Status Volume at intensity fraction 0.10 BC2M_10 0.9976 0.998 0.001 \u2705 PASS Volume at intensity fraction 0.90 BC2M_90 0.0001524 0.000152 2e-5 \u2705 PASS Intensity at volume fraction 0.10 GBPN_10 88.75 88.8 0.2 \u2705 PASS Intensity at volume fraction 0.90 GBPN_90 -421.2 -421 14 \u2705 PASS Volume fraction difference between intensity 0.10 and 0.90 fractions DDTU 0.9974 0.997 0.001 \u2705 PASS Intensity fraction difference between volume 0.10 and 0.90 fractions CNV2 510 510 14 \u2705 PASS Area under the IVH curve 9CMM 891.4 \u2014 \u2014 \u2757 REF."},{"location":"ibsi_compliance/#glcm_1","title":"GLCM","text":"Feature Code Calc Ref Tol Status Joint maximum GYBY 0.1109 0.111 0.002 \u2705 PASS Joint average 60VM 38.98 39 0.2 \u2705 PASS Joint variance UR99 73.68 73.8 2 \u2705 PASS Joint entropy TU9B 6.419 6.42 0.06 \u2705 PASS Difference average TF7R 2.162 2.16 0.05 \u2705 PASS Difference variance D3YU 14.42 14.4 0.5 \u2705 PASS Difference entropy NTRS 2.642 2.64 0.03 \u2705 PASS Sum average ZGXS 77.96 78 0.3 \u2705 PASS Sum variance OEEB 275.6 276 8 \u2705 PASS Sum entropy P6QZ 4.559 4.56 0.04 \u2705 PASS Angular second moment 8ZQL 0.04471 0.0447 0.001 \u2705 PASS Contrast ACUI 19.09 19.1 0.7 \u2705 PASS Dissimilarity 8S9J 2.162 2.16 0.05 \u2705 PASS Inverse difference IB1Z 0.5828 0.583 0.004 \u2705 PASS Normalised inverse difference NDRX 0.9652 0.966 0.001 \u2705 PASS Inverse difference moment WF0Z 0.5479 0.548 0.004 \u2705 PASS Normalised inverse difference moment 1QCO 0.994 0.994 0.001 \u2705 PASS Inverse variance E8JP 0.3906 0.39 0.003 \u2705 PASS Correlation NI2N 0.8704 0.871 0.001 \u2705 PASS Autocorrelation QWB0 1584 1.58e3 10 \u2705 PASS Cluster tendency DG8W 275.6 276 8 \u2705 PASS Cluster shade 7NFM -1.062e+04 -1.06e4 300 \u2705 PASS Cluster prominence AE86 5.69e+05 5.7e5 1.1e4 \u2705 PASS Information correlation 1 R8DG -0.2283 -0.228 0.001 \u2705 PASS Information correlation 2 JN9H 0.8993 0.899 0.001 \u2705 PASS"},{"location":"ibsi_compliance/#glrlm_1","title":"GLRLM","text":"Feature Code Calc Ref Tol Status Short runs emphasis 22OV 0.7872 0.787 0.003 \u2705 PASS Long runs emphasis W4KF 3.275 3.28 0.04 \u2705 PASS Low grey level run emphasis V3SW 0.001544 0.00155 5e-5 \u2705 PASS High grey level run emphasis G3QZ 1473 1.47e3 10 \u2705 PASS Short run low grey level emphasis HTZT 0.001358 0.00136 5e-5 \u2705 PASS Short run high grey level emphasis GD3A 1100 1.1e3 10 \u2705 PASS Long run low grey level emphasis IVPO 0.003141 0.00314 4e-5 \u2705 PASS Long run high grey level emphasis 3KUM 5523 5.53e3 80 \u2705 PASS Grey level non-uniformity R5YN 4.129e+04 4.13e4 100 \u2705 PASS Normalised grey level non-uniformity OVBL 0.1018 0.102 0.003 \u2705 PASS Run length non-uniformity W92Y 2.334e+05 2.34e5 6e3 \u2705 PASS Normalised run length non-uniformity IC23 0.5755 0.575 0.004 \u2705 PASS Run percentage 9ZK5 0.6792 0.679 0.003 \u2705 PASS Grey level variance 8CE5 101.2 101 3 \u2705 PASS Run length variance SXLW 1.107 1.11 0.02 \u2705 PASS Run entropy HJ9O 5.349 5.35 0.03 \u2705 PASS"},{"location":"ibsi_compliance/#glszm_1","title":"GLSZM","text":"Feature Code Calc Ref Tol Status Small zone emphasis P001 0.6951 0.695 0.001 \u2705 PASS Large zone emphasis 48P8 3.892e+04 3.89e4 900 \u2705 PASS Low grey level zone emphasis XMSY 0.002342 0.00235 6e-5 \u2705 PASS High grey level zone emphasis 5GN9 971.7 971 7 \u2705 PASS Small zone low grey level emphasis 5RAI 0.001588 0.0016 4e-5 \u2705 PASS Small zone high grey level emphasis HW1V 657.8 657 4 \u2705 PASS Large zone low grey level emphasis YH51 21.55 21.6 0.5 \u2705 PASS Large zone high grey level emphasis J17V 7.071e+07 7.07e7 1.5e6 \u2705 PASS Grey level non-uniformity JNSA 195 195 6 \u2705 PASS Normalised grey level non-uniformity Y1RO 0.02868 0.0286 0.0003 \u2705 PASS Zone size non-uniformity 4JP3 3039 3.04e3 100 \u2705 PASS Normalised zone size non-uniformity VB3A 0.447 0.447 0.001 \u2705 PASS Zone percentage P30P 0.148 0.148 0.003 \u2705 PASS Grey level variance BYLV 105.7 106 1 \u2705 PASS Zone size variance 3NSA 3.888e+04 3.89e4 900 \u2705 PASS Zone size entropy GU8N 6.997 7 0.01 \u2705 PASS"},{"location":"ibsi_compliance/#gldzm_1","title":"GLDZM","text":"Feature Code Calc Ref Tol Status Small distance emphasis 0GBI 0.5312 0.531 0.006 \u2705 PASS Large distance emphasis MB4I 11.01 11 0.3 \u2705 PASS Low grey level zone emphasis S1RA 0.002342 0.00235 6e-5 \u2705 PASS High grey level zone emphasis K26C 971.7 971 7 \u2705 PASS Small distance low grey level emphasis RUVG 0.001487 0.00149 4e-5 \u2705 PASS Small distance high grey level emphasis DKNJ 476.7 476 11 \u2705 PASS Large distance low grey level emphasis A7WM 0.0154 0.0154 0.0005 \u2705 PASS Large distance high grey level emphasis KLTH 1.334e+04 1.34e4 200 \u2705 PASS Grey level non-uniformity VFT7 195 195 6 \u2705 PASS Normalised grey level non-uniformity 7HP3 0.02868 0.0286 0.0003 \u2705 PASS Zone distance non-uniformity V294 1864 1.87e3 40 \u2705 PASS Normalised zone distance non-uniformity IATH 0.2742 0.274 0.005 \u2705 PASS Zone percentage VIWW 0.148 0.148 0.003 \u2705 PASS Grey level variance QK93 105.7 106 1 \u2705 PASS Zone distance variance 7WT1 4.594 4.6 0.06 \u2705 PASS Zone distance entropy GBDU 7.562 7.56 0.03 \u2705 PASS"},{"location":"ibsi_compliance/#ngtdm_1","title":"NGTDM","text":"Feature Code Calc Ref Tol Status Coarseness QCDE 0.0002163 0.000216 4e-6 \u2705 PASS Contrast 65HE 0.0872 0.0873 0.0019 \u2705 PASS Busyness NQ30 1.388 1.39 0.01 \u2705 PASS Complexity HDEZ 1808 1.81e3 60 \u2705 PASS Strength 1X9X 0.6517 0.651 0.015 \u2705 PASS"},{"location":"ibsi_compliance/#ngldm_1","title":"NGLDM","text":"Feature Code Calc Ref Tol Status Low dependence emphasis SODN 0.1368 0.137 0.003 \u2705 PASS High dependence emphasis IMOQ 126.4 126 2 \u2705 PASS Low grey level count emphasis TL9H 0.001296 0.0013 4e-5 \u2705 PASS High grey level count emphasis OAE7 1568 1.57e3 10 \u2705 PASS Low dependence low grey level emphasis EQ3F 0.0003046 0.000306 1.2e-5 \u2705 PASS Low dependence high grey level emphasis JA6D 140.7 141 2 \u2705 PASS High dependence low grey level emphasis NBZI 0.08278 0.0828 0.0003 \u2705 PASS High dependence high grey level emphasis 9QMG 2.266e+05 2.27e5 3e3 \u2705 PASS Grey level non-uniformity FP8K 6412 6.42e3 10 \u2705 PASS Normalised grey level non-uniformity 5SPA 0.1396 0.14 0.003 \u2705 PASS Dependence count non-uniformity Z87G 2445 2.45e3 60 \u2705 PASS Normalised dependence count non-uniformity OKJI 0.05322 0.0532 0.0005 \u2705 PASS Dependence count percentage 6XV8 1 1 \u2014 \u2705 PASS Grey level variance 1PFV 80.99 81.1 2.1 \u2705 PASS Dependence count variance DNX2 39.18 39.2 0.1 \u2705 PASS Dependence count entropy FCBV 7.536 7.54 0.03 \u2705 PASS Dependence count energy CAS9 0.007893 0.00789 0.00011 \u2705 PASS"},{"location":"ibsi_compliance/#config-d","title":"Config D","text":""},{"location":"ibsi_compliance/#morphology_2","title":"Morphology","text":"Feature Code Calc Ref Tol Status Volume RNU0 3.672e+05 3.67e5 6e3 \u2705 PASS Volume voxel counting YEKZ 3.675e+05 3.68e5 6e3 \u2705 PASS Surface area C0JK 3.446e+04 3.43e4 400 \u2705 PASS Surface to volume ratio 2PR5 0.09384 0.0934 0.0007 \u2705 PASS Compactness 1 SKGS 0.03239 0.0326 0.0002 \u2705 PASS Compactness 2 BQWJ 0.3727 0.378 0.004 \u274c FAIL Spherical disproportion KRCK 1.39 1.38 0.01 \u2705 PASS Sphericity QCFX 0.7196 0.723 0.003 \u2705 PASS Asphericity 25C7 0.3896 0.383 0.004 \u274c FAIL Center of mass shift KLMA 65.29 64.9 2.8 \u2705 PASS Maximum 3D diameter L0JK 125.1 125 1 \u2705 PASS Major axis length TDIC 93.26 93.3 0.5 \u2705 PASS Minor axis length P9VJ 81.98 82 0.5 \u2705 PASS Least axis length 7J51 70.88 70.9 0.4 \u2705 PASS Elongation Q3CK 0.879 0.879 0.001 \u2705 PASS Flatness N17B 0.76 0.76 0.001 \u2705 PASS Volume density (AABB) PBX1 0.4779 0.478 0.003 \u2705 PASS Area density (AABB) R59B 0.6814 0.678 0.003 \u2705 PASS Volume density (OMBB) ZH1A 0.3405 \u2014 \u2014 \u2757 REF. Area density (OMBB) IQYR 0.5412 \u2014 \u2014 \u2757 REF. Volume density (AEE) 6BDE 1.294 1.29 0.01 \u2705 PASS Area density (AEE) RDD2 1.625 1.62 0.01 \u2705 PASS Volume density (MVEE) SWZ1 0.4956 \u2014 \u2014 \u2757 REF. Area density (MVEE) BRI8 0.8844 \u2014 \u2014 \u2757 REF. Volume density (convex hull) R3ER 0.8331 0.834 0.002 \u2705 PASS Area density (convex hull) 7T7F 1.135 1.13 0.01 \u2705 PASS"},{"location":"ibsi_compliance/#intensity_2","title":"Intensity","text":"Feature Code Calc Ref Tol Status Integrated intensity 99N0 -8.551e+06 -8.64e6 1.56e6 \u2705 PASS Moran's I index N365 0.06202 0.0622 0.0013 \u2705 PASS Geary's C measure NPT7 0.8512 0.851 0.001 \u2705 PASS Local intensity peak VJGA 200.8 201 10 \u2705 PASS Global intensity peak 0F91 200.8 201 5 \u2705 PASS Mean intensity Q4LE -23.29 -23.5 3.9 \u2705 PASS Intensity variance ECT3 3.264e+04 3.28e4 2.1e3 \u2705 PASS Intensity skewness KE2A -2.282 -2.28 0.06 \u2705 PASS Intensity kurtosis IPH6 4.361 4.35 0.32 \u2705 PASS Median intensity Y12H 42 42 0.4 \u2705 PASS Minimum intensity 1GSF -723 -724 12 \u2705 PASS 10th intensity percentile QG58 -304 -304 20 \u2705 PASS 90th intensity percentile 8DWT 86 86 0.1 \u2705 PASS Maximum intensity 84IY 521 521 22 \u2705 PASS Intensity interquartile range SALO 57 57 4.1 \u2705 PASS Intensity range 2OJQ 1244 1.24e3 40 \u2705 PASS Intensity Mean absolute deviation 4FUA 122.2 123 6 \u2705 PASS Intensity Robust mean absolute deviation 1128 46.82 46.8 3.6 \u2705 PASS Intensity Median absolute deviation N72L 94.51 94.7 3.8 \u2705 PASS Intensity Coefficient of variation 7TET -7.758 -7.7 1.01 \u2705 PASS Intensity Quartile coefficient of dispersion 9S40 0.7403 0.74 0.011 \u2705 PASS Intensity energy N8CA 1.474e+09 1.48e9 1.4e8 \u2705 PASS Root mean square intensity 5ZWQ 182.2 183 7 \u2705 PASS"},{"location":"ibsi_compliance/#intensity-histogram_2","title":"Intensity Histogram","text":"Feature Code Calc Ref Tol Status Mean discretised intensity X6K6 18.51 18.5 0.5 \u2705 PASS Discretised intensity variance CH89 21.65 21.7 0.4 \u2705 PASS Discretised intensity skewness 88K1 -2.27 -2.27 0.06 \u2705 PASS Discretised intensity kurtosis C3I7 4.316 4.31 0.32 \u2705 PASS Median discretised intensity WIFQ 20 20 0.5 \u2705 PASS Minimum discretised intensity 1PR8 1 1 \u2014 \u2705 PASS 10th discretised intensity percentile 1PR 11 11 0.7 \u2705 PASS 90th discretised intensity percentile GPMT 21 21 0.5 \u2705 PASS Maximum discretised intensity 3NCY 32 32 \u2014 \u2705 PASS Intensity histogram mode AMMC 20 20 0.4 \u2705 PASS Discretised intensity interquartile range WR0O 2 2 0.06 \u2705 PASS Discretised intensity range 5Z3W 31 31 \u2014 \u2705 PASS Intensity histogram mean absolute deviation D2ZX 3.148 3.15 0.05 \u2705 PASS Intensity histogram robust mean absolute deviation WRZB 1.33 1.33 0.06 \u2705 PASS Intensity histogram median absolute deviation 4RNL 2.404 2.41 0.04 \u2705 PASS Intensity histogram coefficient of variation CWYJ 0.2515 0.252 0.006 \u2705 PASS Intensity histogram quartile coefficient of dispersion SLWD 0.05 0.05 0.0021 \u2705 PASS Discretised intensity entropy TLU2 2.937 2.94 0.01 \u2705 PASS Discretised intensity uniformity BJ5W 0.2289 0.229 0.003 \u2705 PASS Maximum histogram gradient 12CE 7256 7.26e3 200 \u2705 PASS Maximum histogram gradient intensity 8E6O 19 19 0.4 \u2705 PASS Minimum histogram gradient VQB3 -6676 -6.67e3 230 \u2705 PASS Minimum histogram gradient intensity RHQZ 22 22 0.4 \u2705 PASS"},{"location":"ibsi_compliance/#intensity-volume-histogram_2","title":"Intensity-Volume Histogram","text":"Feature Code Calc Ref Tol Status Volume at intensity fraction 0.10 BC2M_10 0.9716 0.972 0.003 \u2705 PASS Volume at intensity fraction 0.90 BC2M_90 9.005e-05 9e-5 0.000415 \u2705 PASS Intensity at volume fraction 0.10 GBPN_10 87 87 0.1 \u2705 PASS Intensity at volume fraction 0.90 GBPN_90 -303 -303 20 \u2705 PASS Volume fraction difference between intensity 0.10 and 0.90 fractions DDTU 0.9715 0.971 0.001 \u2705 PASS Intensity fraction difference between volume 0.10 and 0.90 fractions CNV2 390 390 20 \u2705 PASS Area under the IVH curve 9CMM 700.2 \u2014 \u2014 \u2757 REF."},{"location":"ibsi_compliance/#glcm_2","title":"GLCM","text":"Feature Code Calc Ref Tol Status Joint maximum GYBY 0.2322 0.232 0.007 \u2705 PASS Joint average 60VM 18.85 18.9 0.5 \u2705 PASS Joint variance UR99 17.59 17.6 0.4 \u2705 PASS Joint entropy TU9B 4.96 4.96 0.03 \u2705 PASS Difference average TF7R 1.29 1.29 0.01 \u2705 PASS Difference variance D3YU 5.391 5.38 0.11 \u2705 PASS Difference entropy NTRS 2.139 2.14 0.01 \u2705 PASS Sum average ZGXS 37.71 37.7 0.8 \u2705 PASS Sum variance OEEB 63.31 63.5 1.3 \u2705 PASS Sum entropy P6QZ 3.677 3.68 0.02 \u2705 PASS Angular second moment 8ZQL 0.1094 0.109 0.003 \u2705 PASS Contrast ACUI 7.056 7.05 0.13 \u2705 PASS Dissimilarity 8S9J 1.29 1.29 0.01 \u2705 PASS Inverse difference IB1Z 0.6822 0.682 0.003 \u2705 PASS Normalised inverse difference NDRX 0.9651 0.965 0.001 \u2705 PASS Inverse difference moment WF0Z 0.657 0.657 0.003 \u2705 PASS Normalised inverse difference moment 1QCO 0.9937 0.994 0.001 \u2705 PASS Inverse variance E8JP 0.3404 0.34 0.005 \u2705 PASS Correlation NI2N 0.7995 0.8 0.005 \u2705 PASS Autocorrelation QWB0 369.6 370 16 \u2705 PASS Cluster tendency DG8W 63.31 63.5 1.3 \u2705 PASS Cluster shade 7NFM -1271 -1.28e3 40 \u2705 PASS Cluster prominence AE86 3.558e+04 3.57e4 1.5e3 \u2705 PASS Information correlation 1 R8DG -0.2249 -0.225 0.003 \u2705 PASS Information correlation 2 JN9H 0.8459 0.846 0.003 \u2705 PASS"},{"location":"ibsi_compliance/#glrlm_2","title":"GLRLM","text":"Feature Code Calc Ref Tol Status Short runs emphasis 22OV 0.7355 0.736 0.001 \u2705 PASS Long runs emphasis W4KF 6.558 6.56 0.18 \u2705 PASS Low grey level run emphasis V3SW 0.02563 0.0257 0.0012 \u2705 PASS High grey level run emphasis G3QZ 326 326 17 \u2705 PASS Short run low grey level emphasis HTZT 0.02319 0.0232 0.001 \u2705 PASS Short run high grey level emphasis GD3A 219.3 219 13 \u2705 PASS Long run low grey level emphasis IVPO 0.04771 0.0478 0.0031 \u2705 PASS Long run high grey level emphasis 3KUM 2627 2.63e3 30 \u2705 PASS Grey level non-uniformity R5YN 4.279e+04 4.28e4 200 \u2705 PASS Normalised grey level non-uniformity OVBL 0.1339 0.134 0.002 \u2705 PASS Run length non-uniformity W92Y 1.601e+05 1.6e5 3e3 \u2705 PASS Normalised run length non-uniformity IC23 0.5009 0.501 0.001 \u2705 PASS Run percentage 9ZK5 0.5536 0.554 0.005 \u2705 PASS Grey level variance 8CE5 31.4 31.4 0.4 \u2705 PASS Run length variance SXLW 3.295 3.29 0.13 \u2705 PASS Run entropy HJ9O 5.08 5.08 0.02 \u2705 PASS"},{"location":"ibsi_compliance/#glszm_2","title":"GLSZM","text":"Feature Code Calc Ref Tol Status Small zone emphasis P001 0.6395 0.637 0.005 \u2705 PASS Large zone emphasis 48P8 9.863e+04 9.91e4 2.8e3 \u2705 PASS Low grey level zone emphasis XMSY 0.04051 0.0409 0.0005 \u2705 PASS High grey level zone emphasis 5GN9 187.2 188 10 \u2705 PASS Small zone low grey level emphasis 5RAI 0.02478 0.0248 0.0004 \u2705 PASS Small zone high grey level emphasis HW1V 116.4 117 7 \u2705 PASS Large zone low grey level emphasis YH51 239.8 241 14 \u2705 PASS Large zone high grey level emphasis J17V 4.122e+07 4.14e7 3e5 \u2705 PASS Grey level non-uniformity JNSA 212.9 212 6 \u2705 PASS Normalised grey level non-uniformity Y1RO 0.04908 0.0491 0.0008 \u2705 PASS Zone size non-uniformity 4JP3 1648 1.63e3 10 \u274c FAIL Normalised zone size non-uniformity VB3A 0.3801 0.377 0.006 \u2705 PASS Zone percentage P30P 0.09764 0.0972 0.0007 \u2705 PASS Grey level variance BYLV 32.64 32.7 1.6 \u2705 PASS Zone size variance 3NSA 9.853e+04 9.9e4 2.8e3 \u2705 PASS Zone size entropy GU8N 6.498 6.52 0.01 \u274c FAIL"},{"location":"ibsi_compliance/#gldzm_2","title":"GLDZM","text":"Feature Code Calc Ref Tol Status Small distance emphasis 0GBI 0.5791 0.579 0.004 \u2705 PASS Large distance emphasis MB4I 10.21 10.3 0.1 \u2705 PASS Low grey level zone emphasis S1RA 0.04051 0.0409 0.0005 \u2705 PASS High grey level zone emphasis K26C 187.2 188 10 \u2705 PASS Small distance low grey level emphasis RUVG 0.02965 0.0302 0.0006 \u2705 PASS Small distance high grey level emphasis DKNJ 99.26 99.3 5.1 \u2705 PASS Large distance low grey level emphasis A7WM 0.1844 0.183 0.004 \u2705 PASS Large distance high grey level emphasis KLTH 2587 2.62e3 110 \u2705 PASS Grey level non-uniformity VFT7 212.9 212 6 \u2705 PASS Normalised grey level non-uniformity 7HP3 0.04908 0.0491 0.0008 \u2705 PASS Zone distance non-uniformity V294 1373 1.37e3 20 \u2705 PASS Normalised zone distance non-uniformity IATH 0.3165 0.317 0.004 \u2705 PASS Zone percentage VIWW 0.09764 0.0972 0.0007 \u2705 PASS Grey level variance QK93 32.64 32.7 1.6 \u2705 PASS Zone distance variance 7WT1 4.586 4.61 0.04 \u2705 PASS Zone distance entropy GBDU 6.613 6.61 0.03 \u2705 PASS"},{"location":"ibsi_compliance/#ngtdm_2","title":"NGTDM","text":"Feature Code Calc Ref Tol Status Coarseness QCDE 0.0002084 0.000208 4e-6 \u2705 PASS Contrast 65HE 0.04598 0.046 0.0005 \u2705 PASS Busyness NQ30 5.143 5.14 0.14 \u2705 PASS Complexity HDEZ 400.5 400 5 \u2705 PASS Strength 1X9X 0.1617 0.162 0.008 \u2705 PASS"},{"location":"ibsi_compliance/#ngldm_2","title":"NGLDM","text":"Feature Code Calc Ref Tol Status Low dependence emphasis SODN 0.09168 0.0912 0.0007 \u2705 PASS High dependence emphasis IMOQ 222.9 223 5 \u2705 PASS Low grey level count emphasis TL9H 0.01673 0.0168 0.0009 \u2705 PASS High grey level count emphasis OAE7 364.1 364 16 \u2705 PASS Low dependence low grey level emphasis EQ3F 0.003592 0.00357 4e-5 \u2705 PASS Low dependence high grey level emphasis JA6D 18.95 18.9 1.1 \u2705 PASS High dependence low grey level emphasis NBZI 0.7971 0.798 0.072 \u2705 PASS High dependence high grey level emphasis 9QMG 9.282e+04 9.28e4 1.3e3 \u2705 PASS Grey level non-uniformity FP8K 1.017e+04 1.02e4 300 \u2705 PASS Normalised grey level non-uniformity 5SPA 0.2289 0.229 0.003 \u2705 PASS Dependence count non-uniformity Z87G 1834 1.84e3 30 \u2705 PASS Normalised dependence count non-uniformity OKJI 0.0413 0.0413 0.0003 \u2705 PASS Dependence count percentage 6XV8 1 1 \u2014 \u2705 PASS Grey level variance 1PFV 21.65 21.7 0.4 \u2705 PASS Dependence count variance DNX2 63.94 63.9 1.3 \u2705 PASS Dependence count entropy FCBV 6.979 6.98 0.01 \u2705 PASS Dependence count energy CAS9 0.0113 0.0113 0.0002 \u2705 PASS"},{"location":"ibsi_compliance/#config-e","title":"Config E","text":""},{"location":"ibsi_compliance/#morphology_3","title":"Morphology","text":"Feature Code Calc Ref Tol Status Volume RNU0 3.672e+05 3.67e5 6e3 \u2705 PASS Volume voxel counting YEKZ 3.675e+05 3.68e5 6e3 \u2705 PASS Surface area C0JK 3.446e+04 3.43e4 400 \u2705 PASS Surface to volume ratio 2PR5 0.09384 0.0934 0.0007 \u2705 PASS Compactness 1 SKGS 0.03239 0.0326 0.0002 \u2705 PASS Compactness 2 BQWJ 0.3727 0.378 0.004 \u274c FAIL Spherical disproportion KRCK 1.39 1.38 0.01 \u2705 PASS Sphericity QCFX 0.7196 0.723 0.003 \u2705 PASS Asphericity 25C7 0.3896 0.383 0.004 \u274c FAIL Center of mass shift KLMA 68.89 68.5 2.1 \u2705 PASS Maximum 3D diameter L0JK 125.1 125 1 \u2705 PASS Major axis length TDIC 93.26 93.3 0.5 \u2705 PASS Minor axis length P9VJ 81.98 82 0.5 \u2705 PASS Least axis length 7J51 70.88 70.9 0.4 \u2705 PASS Elongation Q3CK 0.879 0.879 0.001 \u2705 PASS Flatness N17B 0.76 0.76 0.001 \u2705 PASS Volume density (AABB) PBX1 0.4779 0.478 0.003 \u2705 PASS Area density (AABB) R59B 0.6814 0.678 0.003 \u2705 PASS Volume density (OMBB) ZH1A 0.3405 \u2014 \u2014 \u2757 REF. Area density (OMBB) IQYR 0.5412 \u2014 \u2014 \u2757 REF. Volume density (AEE) 6BDE 1.294 1.29 0.01 \u2705 PASS Area density (AEE) RDD2 1.625 1.62 0.01 \u2705 PASS Volume density (MVEE) SWZ1 0.4956 \u2014 \u2014 \u2757 REF. Area density (MVEE) BRI8 0.8844 \u2014 \u2014 \u2757 REF. Volume density (convex hull) R3ER 0.8331 0.834 0.002 \u2705 PASS Area density (convex hull) 7T7F 1.135 1.13 0.01 \u2705 PASS"},{"location":"ibsi_compliance/#intensity_3","title":"Intensity","text":"Feature Code Calc Ref Tol Status Integrated intensity 99N0 -8.244e+06 -8.31e6 1.6e6 \u2705 PASS Moran's I index N365 0.05951 0.0596 0.0014 \u2705 PASS Geary's C measure NPT7 0.8531 0.853 0.001 \u2705 PASS Local intensity peak VJGA 180.8 181 13 \u2705 PASS Global intensity peak 0F91 180.8 181 5 \u2705 PASS Mean intensity Q4LE -22.45 -22.6 4.1 \u2705 PASS Intensity variance ECT3 3.499e+04 3.51e4 2.2e3 \u2705 PASS Intensity skewness KE2A -2.302 -2.3 0.07 \u2705 PASS Intensity kurtosis IPH6 4.449 4.44 0.33 \u2705 PASS Median intensity Y12H 43 43 0.5 \u2705 PASS Minimum intensity 1GSF -743 -743 13 \u2705 PASS 10th intensity percentile QG58 -310 -310 21 \u2705 PASS 90th intensity percentile 8DWT 93 93 0.2 \u2705 PASS Maximum intensity 84IY 345 345 9 \u2705 PASS Intensity interquartile range SALO 62 62 3.5 \u2705 PASS Intensity range 2OJQ 1088 1.09e3 30 \u2705 PASS Intensity Mean absolute deviation 4FUA 125.1 125 6 \u2705 PASS Intensity Robust mean absolute deviation 1128 46.44 46.5 3.7 \u2705 PASS Intensity Median absolute deviation N72L 97.71 97.9 3.9 \u2705 PASS Intensity Coefficient of variation 7TET -8.331 -8.28 0.95 \u2705 PASS Intensity Quartile coefficient of dispersion 9S40 0.7949 0.795 0.337 \u2705 PASS Intensity energy N8CA 1.577e+09 1.58e9 1.4e8 \u2705 PASS Root mean square intensity 5ZWQ 188.4 189 7 \u2705 PASS"},{"location":"ibsi_compliance/#intensity-histogram_3","title":"Intensity Histogram","text":"Feature Code Calc Ref Tol Status Mean discretised intensity X6K6 21.71 21.7 0.3 \u2705 PASS Discretised intensity variance CH89 30.33 30.4 0.8 \u2705 PASS Discretised intensity skewness 88K1 -2.291 -2.29 0.07 \u2705 PASS Discretised intensity kurtosis C3I7 4.415 4.4 0.33 \u2705 PASS Median discretised intensity WIFQ 24 24 0.2 \u2705 PASS Minimum discretised intensity 1PR8 1 1 \u2014 \u2705 PASS 10th discretised intensity percentile 1PR 13 13 0.7 \u2705 PASS 90th discretised intensity percentile GPMT 25 25 0.2 \u2705 PASS Maximum discretised intensity 3NCY 32 32 \u2014 \u2705 PASS Intensity histogram mode AMMC 24 24 0.1 \u2705 PASS Discretised intensity interquartile range WR0O 1 1 0.06 \u2705 PASS Discretised intensity range 5Z3W 31 31 \u2014 \u2705 PASS Intensity histogram mean absolute deviation D2ZX 3.681 3.69 0.1 \u2705 PASS Intensity histogram robust mean absolute deviation WRZB 1.455 1.46 0.09 \u2705 PASS Intensity histogram median absolute deviation 4RNL 2.889 2.89 0.07 \u2705 PASS Intensity histogram coefficient of variation CWYJ 0.2537 0.254 0.006 \u2705 PASS Intensity histogram quartile coefficient of dispersion SLWD 0.02128 0.0213 0.0015 \u2705 PASS Discretised intensity entropy TLU2 3.221 3.22 0.02 \u2705 PASS Discretised intensity uniformity BJ5W 0.1837 0.184 0.001 \u2705 PASS Maximum histogram gradient 12CE 6002 6.01e3 130 \u2705 PASS Maximum histogram gradient intensity 8E6O 23 23 0.2 \u2705 PASS Minimum histogram gradient VQB3 -6102 -6.11e3 180 \u2705 PASS Minimum histogram gradient intensity RHQZ 25 25 0.2 \u2705 PASS"},{"location":"ibsi_compliance/#intensity-volume-histogram_3","title":"Intensity-Volume Histogram","text":"Feature Code Calc Ref Tol Status Volume at intensity fraction 0.10 BC2M_10 0.9748 0.975 0.002 \u2705 PASS Volume at intensity fraction 0.90 BC2M_90 0.0001575 0.000157 0.000248 \u2705 PASS Intensity at volume fraction 0.10 GBPN_10 770 770 5 \u2705 PASS Intensity at volume fraction 0.90 GBPN_90 399 399 17 \u2705 PASS Volume fraction difference between intensity 0.10 and 0.90 fractions DDTU 0.9746 0.974 0.001 \u2705 PASS Intensity fraction difference between volume 0.10 and 0.90 fractions CNV2 371 371 13 \u2705 PASS Area under the IVH curve 9CMM 662.3 \u2014 \u2014 \u2757 REF."},{"location":"ibsi_compliance/#glcm_3","title":"GLCM","text":"Feature Code Calc Ref Tol Status Joint maximum GYBY 0.153 0.153 0.003 \u2705 PASS Joint average 60VM 22.14 22.1 0.3 \u2705 PASS Joint variance UR99 24.35 24.4 0.9 \u2705 PASS Joint entropy TU9B 5.613 5.61 0.03 \u2705 PASS Difference average TF7R 1.695 1.7 0.01 \u2705 PASS Difference variance D3YU 8.228 8.23 0.06 \u2705 PASS Difference entropy NTRS 2.397 2.4 0.01 \u2705 PASS Sum average ZGXS 44.27 44.3 0.4 \u2705 PASS Sum variance OEEB 86.3 86.7 3.3 \u2705 PASS Sum entropy P6QZ 3.966 3.97 0.02 \u2705 PASS Angular second moment 8ZQL 0.06354 0.0635 0.0009 \u2705 PASS Contrast ACUI 11.1 11.1 0.1 \u2705 PASS Dissimilarity 8S9J 1.695 1.7 0.01 \u2705 PASS Inverse difference IB1Z 0.6084 0.608 0.001 \u2705 PASS Normalised inverse difference NDRX 0.9552 0.955 0.001 \u2705 PASS Inverse difference moment WF0Z 0.5769 0.577 0.001 \u2705 PASS Normalised inverse difference moment 1QCO 0.9905 0.99 0.001 \u2705 PASS Inverse variance E8JP 0.4101 0.41 0.004 \u2705 PASS Correlation NI2N 0.7721 0.773 0.006 \u2705 PASS Autocorrelation QWB0 508.8 509 8 \u2705 PASS Cluster tendency DG8W 86.3 86.7 3.3 \u2705 PASS Cluster shade 7NFM -2065 -2.08e3 70 \u2705 PASS Cluster prominence AE86 6.86e+04 6.9e4 2.1e3 \u2705 PASS Information correlation 1 R8DG -0.1752 -0.175 0.003 \u2705 PASS Information correlation 2 JN9H 0.8122 0.813 0.004 \u2705 PASS"},{"location":"ibsi_compliance/#glrlm_3","title":"GLRLM","text":"Feature Code Calc Ref Tol Status Short runs emphasis 22OV 0.7771 0.777 0.001 \u2705 PASS Long runs emphasis W4KF 3.521 3.52 0.07 \u2705 PASS Low grey level run emphasis V3SW 0.0201 0.0204 0.0008 \u2705 PASS High grey level run emphasis G3QZ 471.5 471 9 \u2705 PASS Short run low grey level emphasis HTZT 0.01843 0.0186 0.0007 \u2705 PASS Short run high grey level emphasis GD3A 347.3 347 7 \u2705 PASS Long run low grey level emphasis IVPO 0.0307 0.0311 0.0016 \u2705 PASS Long run high grey level emphasis 3KUM 1888 1.89e3 20 \u2705 PASS Grey level non-uniformity R5YN 5.194e+04 5.19e4 200 \u2705 PASS Normalised grey level non-uniformity OVBL 0.1354 0.135 0.003 \u2705 PASS Run length non-uniformity W92Y 2.149e+05 2.15e5 4e3 \u2705 PASS Normalised run length non-uniformity IC23 0.5602 0.56 0.001 \u2705 PASS Run percentage 9ZK5 0.6639 0.664 0.003 \u2705 PASS Grey level variance 8CE5 39.62 39.7 0.9 \u2705 PASS Run length variance SXLW 1.252 1.25 0.05 \u2705 PASS Run entropy HJ9O 4.869 4.87 0.03 \u2705 PASS"},{"location":"ibsi_compliance/#glszm_3","title":"GLSZM","text":"Feature Code Calc Ref Tol Status Small zone emphasis P001 0.6763 0.676 0.003 \u2705 PASS Large zone emphasis 48P8 5.849e+04 5.86e4 800 \u2705 PASS Low grey level zone emphasis XMSY 0.03423 0.034 0.0004 \u2705 PASS High grey level zone emphasis 5GN9 285.6 286 6 \u2705 PASS Small zone low grey level emphasis 5RAI 0.02236 0.0224 0.0004 \u2705 PASS Small zone high grey level emphasis HW1V 185.8 186 4 \u2705 PASS Large zone low grey level emphasis YH51 104.8 105 4 \u2705 PASS Large zone high grey level emphasis J17V 3.352e+07 3.36e7 3e5 \u2705 PASS Grey level non-uniformity JNSA 231.1 231 6 \u2705 PASS Normalised grey level non-uniformity Y1RO 0.04137 0.0414 0.0003 \u2705 PASS Zone size non-uniformity 4JP3 2367 2.37e3 40 \u2705 PASS Normalised zone size non-uniformity VB3A 0.4236 0.424 0.004 \u2705 PASS Zone percentage P30P 0.1257 0.126 0.001 \u2705 PASS Grey level variance BYLV 50.84 50.8 0.9 \u2705 PASS Zone size variance 3NSA 5.842e+04 5.85e4 800 \u2705 PASS Zone size entropy GU8N 6.563 6.57 0.01 \u2705 PASS"},{"location":"ibsi_compliance/#gldzm_3","title":"GLDZM","text":"Feature Code Calc Ref Tol Status Small distance emphasis 0GBI 0.5276 0.527 0.004 \u2705 PASS Large distance emphasis MB4I 12.54 12.6 0.1 \u2705 PASS Low grey level zone emphasis S1RA 0.03423 0.034 0.0004 \u2705 PASS High grey level zone emphasis K26C 285.6 286 6 \u2705 PASS Small distance low grey level emphasis RUVG 0.02311 0.0228 0.0003 \u2705 PASS Small distance high grey level emphasis DKNJ 136.2 136 4 \u2705 PASS Large distance low grey level emphasis A7WM 0.179 0.179 0.004 \u2705 PASS Large distance high grey level emphasis KLTH 4844 4.85e3 60 \u2705 PASS Grey level non-uniformity VFT7 231.1 231 6 \u2705 PASS Normalised grey level non-uniformity 7HP3 0.04137 0.0414 0.0003 \u2705 PASS Zone distance non-uniformity V294 1506 1.5e3 30 \u2705 PASS Normalised zone distance non-uniformity IATH 0.2695 0.269 0.003 \u2705 PASS Zone percentage VIWW 0.1257 0.126 0.001 \u2705 PASS Grey level variance QK93 50.84 50.8 0.9 \u2705 PASS Zone distance variance 7WT1 5.55 5.56 0.05 \u2705 PASS Zone distance entropy GBDU 7.061 7.06 0.01 \u2705 PASS"},{"location":"ibsi_compliance/#ngtdm_3","title":"NGTDM","text":"Feature Code Calc Ref Tol Status Coarseness QCDE 0.0001886 0.000188 4e-6 \u2705 PASS Contrast 65HE 0.07502 0.0752 0.0019 \u2705 PASS Busyness NQ30 4.645 4.65 0.1 \u2705 PASS Complexity HDEZ 574.2 574 1 \u2705 PASS Strength 1X9X 0.1676 0.167 0.006 \u2705 PASS"},{"location":"ibsi_compliance/#ngldm_3","title":"NGLDM","text":"Feature Code Calc Ref Tol Status Low dependence emphasis SODN 0.1183 0.118 0.001 \u2705 PASS High dependence emphasis IMOQ 134.3 134 3 \u2705 PASS Low grey level count emphasis TL9H 0.01519 0.0154 0.0007 \u2705 PASS High grey level count emphasis OAE7 501.6 502 8 \u2705 PASS Low dependence low grey level emphasis EQ3F 0.00386 0.00388 4e-5 \u2705 PASS Low dependence high grey level emphasis JA6D 36.65 36.7 0.5 \u2705 PASS High dependence low grey level emphasis NBZI 0.4499 0.457 0.031 \u2705 PASS High dependence high grey level emphasis 9QMG 7.599e+04 7.6e4 600 \u2705 PASS Grey level non-uniformity FP8K 8162 8.17e3 130 \u2705 PASS Normalised grey level non-uniformity 5SPA 0.1837 0.184 0.001 \u2705 PASS Dependence count non-uniformity Z87G 2245 2.25e3 30 \u2705 PASS Normalised dependence count non-uniformity OKJI 0.05051 0.0505 0.0003 \u2705 PASS Dependence count percentage 6XV8 1 1 \u2014 \u2705 PASS Grey level variance 1PFV 30.33 30.4 0.8 \u2705 PASS Dependence count variance DNX2 39.44 39.4 1 \u2705 PASS Dependence count entropy FCBV 7.064 7.06 0.02 \u2705 PASS Dependence count energy CAS9 0.01062 0.0106 0.0001 \u2705 PASS"},{"location":"quality/","title":"Code Quality Report","text":"<p>Generated on 2025-12-30 17:33:38</p>"},{"location":"quality/#test-coverage","title":"Test Coverage","text":"<p>Total Coverage: 100.00%</p> Module Coverage <code>pictologics/__init__.py</code> 100.00% <code>pictologics/features/__init__.py</code> 100.00% <code>pictologics/features/intensity.py</code> 100.00% <code>pictologics/features/morphology.py</code> 100.00% <code>pictologics/features/texture.py</code> 100.00% <code>pictologics/loader.py</code> 100.00% <code>pictologics/pipeline.py</code> 100.00% <code>pictologics/preprocessing.py</code> 100.00% <code>pictologics/results.py</code> 100.00% <code>pictologics/warmup.py</code> 100.00%"},{"location":"quality/#static-type-checking-mypy","title":"Static Type Checking (Mypy)","text":"<p>Status: Pass Errors: 0</p> <pre><code>Success: no issues found in 10 source files\n</code></pre>"},{"location":"quality/#linting-ruff","title":"Linting (Ruff)","text":"<p>Status: Pass Total Issues: 0</p> <p>No issues found.</p>"},{"location":"api/loader/","title":"Loader API","text":""},{"location":"api/loader/#pictologics.loader","title":"<code>pictologics.loader</code>","text":""},{"location":"api/loader/#pictologics.loader--image-loading-module","title":"Image Loading Module","text":"<p>This module handles the loading of medical images from various formats (NIfTI, DICOM) into a standardized <code>Image</code> class. It abstracts away file format differences to provide a consistent interface for the rest of the library.</p>"},{"location":"api/loader/#pictologics.loader--key-features","title":"Key Features:","text":"<ul> <li>Unified Image Class: Stores 3D data, spacing, origin, direction, and modality.</li> <li>Format Support:<ul> <li>NIfTI (.nii, .nii.gz) via <code>nibabel</code>.</li> <li>DICOM Series (directory of DICOM files) via <code>pydicom</code>.</li> <li>Single DICOM files.</li> </ul> </li> <li>Automatic Detection: <code>load_image</code> automatically detects format and dimensionality.</li> <li>Robust DICOM Sorting: Sorts slices based on spatial position and orientation.</li> </ul>"},{"location":"api/loader/#pictologics.loader.Image","title":"<code>Image</code>  <code>dataclass</code>","text":"<p>A standardized container for 3D medical image data and metadata.</p> <p>This class serves as the common interface for all image processing operations in the library, abstracting away the differences between file formats like DICOM and NIfTI.</p> <p>Attributes:</p> Name Type Description <code>array</code> <code>ndarray</code> <p>The 3D image data with shape (x, y, z).</p> <code>spacing</code> <code>Tuple[float, float, float]</code> <p>Voxel spacing in millimeters (mm) along the (x, y, z) axes.</p> <code>origin</code> <code>Tuple[float, float, float]</code> <p>World coordinates of the image origin (center of the first voxel) in millimeters (mm).</p> <code>direction</code> <code>Optional[ndarray]</code> <p>3x3 direction cosine matrix defining the orientation of the image axes in world space. Defaults to identity matrix.</p> <code>modality</code> <code>str</code> <p>The imaging modality (e.g., 'CT', 'MR', 'PT'). Defaults to 'Unknown'.</p> Source code in <code>pictologics/loader.py</code> <pre><code>@dataclass\nclass Image:\n    \"\"\"\n    A standardized container for 3D medical image data and metadata.\n\n    This class serves as the common interface for all image processing operations\n    in the library, abstracting away the differences between file formats like\n    DICOM and NIfTI.\n\n    Attributes:\n        array (np.ndarray): The 3D image data with shape (x, y, z).\n        spacing (Tuple[float, float, float]): Voxel spacing in millimeters (mm)\n            along the (x, y, z) axes.\n        origin (Tuple[float, float, float]): World coordinates of the image origin\n            (center of the first voxel) in millimeters (mm).\n        direction (Optional[np.ndarray]): 3x3 direction cosine matrix defining the\n            orientation of the image axes in world space. Defaults to identity matrix.\n        modality (str): The imaging modality (e.g., 'CT', 'MR', 'PT'). Defaults to 'Unknown'.\n    \"\"\"\n\n    array: np.ndarray\n    spacing: Tuple[float, float, float]\n    origin: Tuple[float, float, float]\n    direction: Optional[np.ndarray] = None\n    modality: str = \"Unknown\"\n</code></pre>"},{"location":"api/loader/#pictologics.loader.load_image","title":"<code>load_image(path, dataset_index=0, recursive=False)</code>","text":"<p>Load a medical image from a file path or directory.</p> <p>This is the main entry point for loading data. It automatically detects whether the input is a NIfTI file or a DICOM directory/file (single DICOM or series) and standardizes it into an <code>Image</code> object.</p> <p>The resulting image array is always 3D with dimensions (x, y, z).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The absolute or relative path to the image file (e.g., .nii.gz, .dcm or file with no extension) or the directory containing DICOM files.</p> required <code>dataset_index</code> <code>int</code> <p>For 4D datasets (like fMRI or dynamic PET), this specifies which volume (time point) to extract. Defaults to 0 (the first volume).</p> <code>0</code> <code>recursive</code> <code>bool</code> <p>If True and <code>path</code> is a directory, recursively searches subdirectories and loads the DICOM series from the folder containing the most DICOM files. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Image</code> <code>Image</code> <p>An <code>Image</code> object containing the 3D numpy array and metadata (spacing, origin, etc.).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path does not exist, the file format is not supported, or the file is corrupt/unreadable.</p> <p>Examples:</p> <p>Loading a NIfTI file: <pre><code>from pictologics.loader import load_image\n\n# Load a standard brain scan\nimg = load_image(\"data/brain.nii.gz\")\nprint(f\"Image shape: {img.array.shape}\")\n# Output: Image shape: (256, 256, 128)\n</code></pre></p> <p>Loading a DICOM series: <pre><code># Load a CT scan from a folder of DICOM files\nimg_ct = load_image(\"data/patients/001/CT_scan/\")\nprint(f\"Voxel spacing: {img_ct.spacing}\")\n# Output: Voxel spacing: (0.97, 0.97, 2.5)\n</code></pre></p> <p>Loading a single DICOM file: <pre><code># Load a single DICOM file (even without .dcm extension)\nimg_slice = load_image(\"data/slice_001\")\nprint(f\"Modality: {img_slice.modality}\")\n</code></pre></p> <p>Recursive DICOM loading: <pre><code># Finds the deep subfolder with actual DICOM files\nimg = load_image(\"data/patients/001/\", recursive=True)\n</code></pre></p> <p>Loading a specific volume from a 4D file: <pre><code># Load the 5th time point from a 4D fMRI file\nfmri_vol = load_image(\"data/fmri.nii.gz\", dataset_index=4)\n</code></pre></p> Source code in <code>pictologics/loader.py</code> <pre><code>def load_image(path: str, dataset_index: int = 0, recursive: bool = False) -&gt; Image:\n    \"\"\"\n    Load a medical image from a file path or directory.\n\n    This is the main entry point for loading data. It automatically detects whether\n    the input is a NIfTI file or a DICOM directory/file (single DICOM or series)\n    and standardizes it into an `Image` object.\n\n    The resulting image array is always 3D with dimensions (x, y, z).\n\n    Args:\n        path (str): The absolute or relative path to the image file (e.g., .nii.gz,\n            .dcm or file with no extension) or the directory containing DICOM files.\n        dataset_index (int, optional): For 4D datasets (like fMRI or dynamic PET),\n            this specifies which volume (time point) to extract. Defaults to 0 (the first volume).\n        recursive (bool, optional): If True and `path` is a directory, recursively searches\n            subdirectories and loads the DICOM series from the folder containing the most\n            DICOM files. Defaults to False.\n\n    Returns:\n        Image: An `Image` object containing the 3D numpy array and metadata (spacing, origin, etc.).\n\n    Raises:\n        ValueError: If the path does not exist, the file format is not supported,\n            or the file is corrupt/unreadable.\n\n    Examples:\n        **Loading a NIfTI file:**\n        ```python\n        from pictologics.loader import load_image\n\n        # Load a standard brain scan\n        img = load_image(\"data/brain.nii.gz\")\n        print(f\"Image shape: {img.array.shape}\")\n        # Output: Image shape: (256, 256, 128)\n        ```\n\n        **Loading a DICOM series:**\n        ```python\n        # Load a CT scan from a folder of DICOM files\n        img_ct = load_image(\"data/patients/001/CT_scan/\")\n        print(f\"Voxel spacing: {img_ct.spacing}\")\n        # Output: Voxel spacing: (0.97, 0.97, 2.5)\n        ```\n\n        **Loading a single DICOM file:**\n        ```python\n        # Load a single DICOM file (even without .dcm extension)\n        img_slice = load_image(\"data/slice_001\")\n        print(f\"Modality: {img_slice.modality}\")\n        ```\n\n        **Recursive DICOM loading:**\n        ```python\n        # Finds the deep subfolder with actual DICOM files\n        img = load_image(\"data/patients/001/\", recursive=True)\n        ```\n\n        **Loading a specific volume from a 4D file:**\n        ```python\n        # Load the 5th time point from a 4D fMRI file\n        fmri_vol = load_image(\"data/fmri.nii.gz\", dataset_index=4)\n        ```\n    \"\"\"\n    path_obj = Path(path)\n    if not path_obj.exists():\n        raise ValueError(f\"The specified path does not exist: {path}\")\n\n    try:\n        if path_obj.is_dir():\n            target_path = path_obj\n            if recursive:\n                target_path = _find_best_dicom_series_dir(path_obj)\n            return _load_dicom_series(target_path)\n        elif path.lower().endswith((\".nii\", \".nii.gz\")):\n\n            return _load_nifti(path, dataset_index)\n        else:\n            # Attempt to load as a single DICOM file if extension is not NIfTI\n            try:\n                return _load_dicom_file(path)\n            except Exception:\n                raise ValueError(\n                    f\"Unsupported file format or unable to read file: {path}\"\n                ) from None\n    except Exception as e:\n        # Re-raise ValueErrors directly, wrap others\n        if isinstance(e, ValueError):\n            raise e\n        raise ValueError(f\"Failed to load image from '{path}': {e}\") from e\n</code></pre>"},{"location":"api/loader/#pictologics.loader.load_and_merge_images","title":"<code>load_and_merge_images(image_paths, reference_image=None, conflict_resolution='max', dataset_index=0, recursive=False, binarize=None)</code>","text":"<p>Load multiple images (e.g., masks or partial scans) and merge them into a single image.</p> <p>This function loads images from the provided paths, validates that they all share the same geometry (dimensions, spacing, origin, direction), and merges them according to the specified conflict resolution strategy.</p> <p>Use Cases: - Merging multiple segmentation masks into a single ROI. - Merging split image volumes (though typically less common than mask merging).</p> <p>Format &amp; Path Support: Since this function uses <code>load_image</code> internally for each path, it supports: - NIfTI files (.nii, .nii.gz). - DICOM series (directories containing DICOM files). - Single DICOM files (with or without .dcm extension). - Nested directories (if paths point to folders containing DICOMs).</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of absolute or relative paths to the images. These can be file paths or directory paths.</p> required <code>reference_image</code> <code>Optional[Image]</code> <p>An optional reference image (e.g., the scan corresponding to the masks). If provided, the merged image is validated against this image's geometry.</p> <code>None</code> <code>conflict_resolution</code> <code>str</code> <p>Strategy to resolve voxel values when multiple images have non-zero values at the same location. Options: - 'max': Use the maximum value (default). - 'min': Use the minimum value. - 'first': Keep the value from the first image encountered (earlier in list). - 'last': Overwrite with the value from the last image encountered (later in list).</p> <code>'max'</code> <code>dataset_index</code> <code>int</code> <p>For 4D datasets, this specifies which volume (time point) to extract for all images. Defaults to 0.</p> <code>0</code> <code>recursive</code> <code>bool</code> <p>If True, recursively searches subdirectories for each path in <code>image_paths</code>. Defaults to False.</p> <code>False</code> <code>binarize</code> <code>Union[bool, int, List[int], Tuple[int, int], None]</code> <p>Rules for binarizing the merged image. - <code>None</code> (default): No binarization. - <code>True</code>: Sets all voxels &gt; 0 to 1, others to 0. - <code>int</code> (e.g., 2): Sets voxels == value to 1, others to 0. - <code>List[int]</code> (e.g., [1, 2]): Sets voxels in list to 1, others to 0. - <code>Tuple[int, int]</code> (e.g., (1, 10)): Sets voxels in inclusive range to 1, others to 0.</p> <code>None</code> <p>Note on Filtering: The <code>binarize</code> parameter is intended for mask filtering (e.g., selecting specific ROI labels). To filter image intensity values (e.g., HU ranges), use the preprocessing steps in the radiomics pipeline configuration instead.</p> <p>Returns:</p> Name Type Description <code>Image</code> <code>Image</code> <p>A new <code>Image</code> object containing the merged data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>image_paths</code> is empty, if an invalid <code>conflict_resolution</code> is provided, or if the images (or reference) have mismatched geometries.</p> Source code in <code>pictologics/loader.py</code> <pre><code>def load_and_merge_images(\n    image_paths: List[str],\n    reference_image: Optional[Image] = None,\n    conflict_resolution: str = \"max\",\n    dataset_index: int = 0,\n    recursive: bool = False,\n    binarize: Union[bool, int, List[int], Tuple[int, int], None] = None,\n) -&gt; Image:\n    \"\"\"\n    Load multiple images (e.g., masks or partial scans) and merge them into a single image.\n\n    This function loads images from the provided paths, validates that they all share\n    the same geometry (dimensions, spacing, origin, direction), and merges them\n    according to the specified conflict resolution strategy.\n\n    **Use Cases:**\n    - Merging multiple segmentation masks into a single ROI.\n    - Merging split image volumes (though typically less common than mask merging).\n\n    **Format &amp; Path Support:**\n    Since this function uses `load_image` internally for each path, it supports:\n    - **NIfTI files** (.nii, .nii.gz).\n    - **DICOM series** (directories containing DICOM files).\n    - **Single DICOM files** (with or without .dcm extension).\n    - **Nested directories** (if paths point to folders containing DICOMs).\n\n    Args:\n        image_paths (List[str]): List of absolute or relative paths to the images.\n            These can be file paths or directory paths.\n        reference_image (Optional[Image]): An optional reference image (e.g., the scan\n            corresponding to the masks). If provided, the merged image is validated\n            against this image's geometry.\n        conflict_resolution (str): Strategy to resolve voxel values when multiple images\n            have non-zero values at the same location. Options:\n            - 'max': Use the maximum value (default).\n            - 'min': Use the minimum value.\n            - 'first': Keep the value from the first image encountered (earlier in list).\n            - 'last': Overwrite with the value from the last image encountered (later in list).\n        dataset_index (int, optional): For 4D datasets, this specifies which volume\n            (time point) to extract for all images. Defaults to 0.\n        recursive (bool, optional): If True, recursively searches subdirectories\n            for each path in `image_paths`. Defaults to False.\n        binarize (Union[bool, int, List[int], Tuple[int, int], None], optional):\n            Rules for binarizing the merged image.\n            - `None` (default): No binarization.\n            - `True`: Sets all voxels &gt; 0 to 1, others to 0.\n            - `int` (e.g., 2): Sets voxels == value to 1, others to 0.\n            - `List[int]` (e.g., [1, 2]): Sets voxels in list to 1, others to 0.\n            - `Tuple[int, int]` (e.g., (1, 10)): Sets voxels in inclusive range to 1, others to 0.\n\n    **Note on Filtering:**\n    The `binarize` parameter is intended for **mask filtering** (e.g., selecting specific ROI labels).\n    To filter image intensity values (e.g., HU ranges), use the preprocessing steps in the\n    radiomics pipeline configuration instead.\n\n    Returns:\n        Image: A new `Image` object containing the merged data.\n\n    Raises:\n        ValueError: If `image_paths` is empty, if an invalid `conflict_resolution` is provided,\n            or if the images (or reference) have mismatched geometries.\n    \"\"\"\n    if not image_paths:\n        raise ValueError(\"image_paths cannot be empty.\")\n\n    valid_strategies = {\"max\", \"min\", \"first\", \"last\"}\n    if conflict_resolution not in valid_strategies:\n        raise ValueError(\n            f\"Invalid conflict_resolution '{conflict_resolution}'. \"\n            f\"Must be one of {valid_strategies}.\"\n        )\n\n    # Load the first image to serve as the consensus geometry\n    try:\n        consensus_image = load_image(\n            image_paths[0], dataset_index=dataset_index, recursive=recursive\n        )\n    except Exception as e:\n        raise ValueError(f\"Failed to load first image '{image_paths[0]}': {e}\") from e\n\n    merged_array = consensus_image.array.copy()\n\n    # Geometry validation helper\n    def _validate_geometry(target: Image, ref: Image, name: str, ref_name: str) -&gt; None:\n        if target.array.shape != ref.array.shape:\n            raise ValueError(\n                f\"Dimension mismatch between {name} {target.array.shape} \"\n                f\"and {ref_name} {ref.array.shape}.\"\n            )\n        if not np.allclose(target.spacing, ref.spacing, atol=1e-5):\n            raise ValueError(\n                f\"Spacing mismatch between {name} {target.spacing} \"\n                f\"and {ref_name} {ref.spacing}.\"\n            )\n        if not np.allclose(target.origin, ref.origin, atol=1e-5):\n            raise ValueError(\n                f\"Origin mismatch between {name} {target.origin} \"\n                f\"and {ref_name} {ref.origin}.\"\n            )\n        if target.direction is not None and ref.direction is not None:\n            if not np.allclose(target.direction, ref.direction, atol=1e-5):\n                raise ValueError(f\"Direction mismatch between {name} and {ref_name}.\")\n\n    # Iterate through remaining images\n    for path in image_paths[1:]:\n        try:\n            current_image = load_image(\n                path, dataset_index=dataset_index, recursive=recursive\n            )\n        except Exception as e:\n            raise ValueError(f\"Failed to load image '{path}': {e}\") from e\n\n        _validate_geometry(\n            current_image, consensus_image, f\"image '{path}'\", \"consensus image\"\n        )\n\n        current_array = current_image.array\n\n        # Identify regions\n        # Overlap: non-zero in both\n        overlap_mask = (merged_array != 0) &amp; (current_array != 0)\n        # New data: zero in merged, non-zero in current\n        new_data_mask = (merged_array == 0) &amp; (current_array != 0)\n\n        # Apply new data (always acceptable)\n        merged_array[new_data_mask] = current_array[new_data_mask]\n\n        # Resolve conflicts in overlapping regions\n        if np.any(overlap_mask):\n            if conflict_resolution == \"max\":\n                merged_array[overlap_mask] = np.maximum(\n                    merged_array[overlap_mask], current_array[overlap_mask]\n                )\n            elif conflict_resolution == \"min\":\n                merged_array[overlap_mask] = np.minimum(\n                    merged_array[overlap_mask], current_array[overlap_mask]\n                )\n            elif conflict_resolution == \"last\":\n                merged_array[overlap_mask] = current_array[overlap_mask]\n            elif conflict_resolution == \"first\":\n                pass  # Already have the 'first' value, do nothing\n\n    # Apply binarization if requested\n    if binarize is not None:\n        mask_out = np.zeros_like(merged_array, dtype=np.uint8)\n        if isinstance(binarize, bool) and binarize is True:\n            mask_out[merged_array &gt; 0] = 1\n        elif isinstance(binarize, int) and not isinstance(binarize, bool):\n            mask_out[merged_array == binarize] = 1\n        elif isinstance(binarize, list):\n            mask_out[np.isin(merged_array, binarize)] = 1\n        elif isinstance(binarize, tuple) and len(binarize) == 2:\n            mask_out[(merged_array &gt;= binarize[0]) &amp; (merged_array &lt;= binarize[1])] = 1\n        else:\n            # Fallback for False or unknown types, though type hint restricts this\n            if binarize is not False:\n                raise ValueError(f\"Unsupported binarize value: {binarize}\")\n            mask_out = merged_array  # return original as fallback\n\n        if binarize is not False:\n            merged_array = mask_out\n\n    # Validate against reference image if provided\n    if reference_image is not None:\n        # Create a dummy image wrapping the merged array for validation\n        final_merged_image = Image(\n            array=merged_array,\n            spacing=consensus_image.spacing,\n            origin=consensus_image.origin,\n            direction=consensus_image.direction,\n            modality=\"Image\",\n        )\n        _validate_geometry(\n            final_merged_image, reference_image, \"merged image\", \"reference image\"\n        )\n\n    return Image(\n        array=merged_array,\n        spacing=consensus_image.spacing,\n        origin=consensus_image.origin,\n        direction=consensus_image.direction,\n        modality=\"MergedImage\",\n    )\n</code></pre>"},{"location":"api/loader/#pictologics.loader.create_full_mask","title":"<code>create_full_mask(reference_image, dtype=np.uint8)</code>","text":"<p>Create a whole-image ROI mask matching a reference image.</p> <p>This utility is primarily used when a user does not provide a segmentation mask. The returned mask has the same geometry (shape, spacing, origin, direction) as the reference image and contains a value of 1 for every voxel.</p> <p>Parameters:</p> Name Type Description Default <code>reference_image</code> <code>Image</code> <p>Image whose geometry should be copied.</p> required <code>dtype</code> <code>DTypeLike</code> <p>Numpy dtype to use for the mask array. Defaults to <code>np.uint8</code>.</p> <code>uint8</code> <p>Returns:</p> Type Description <code>Image</code> <p>An <code>Image</code> mask with <code>array == 1</code> everywhere.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the reference image does not have a valid 3D array.</p> Source code in <code>pictologics/loader.py</code> <pre><code>def create_full_mask(reference_image: Image, dtype: DTypeLike = np.uint8) -&gt; Image:\n    \"\"\"Create a whole-image ROI mask matching a reference image.\n\n    This utility is primarily used when a user does not provide a segmentation mask.\n    The returned mask has the same geometry (shape, spacing, origin, direction) as\n    the reference image and contains a value of 1 for every voxel.\n\n    Args:\n        reference_image: Image whose geometry should be copied.\n        dtype: Numpy dtype to use for the mask array. Defaults to `np.uint8`.\n\n    Returns:\n        An `Image` mask with `array == 1` everywhere.\n\n    Raises:\n        ValueError: If the reference image does not have a valid 3D array.\n    \"\"\"\n    if reference_image.array.ndim != 3:\n        raise ValueError(\n            f\"reference_image.array must be 3D, got shape {reference_image.array.shape}\"\n        )\n\n    mask_array = np.ones(reference_image.array.shape, dtype=dtype)\n    return Image(\n        array=mask_array,\n        spacing=reference_image.spacing,\n        origin=reference_image.origin,\n        direction=reference_image.direction,\n        modality=\"mask\",\n    )\n</code></pre>"},{"location":"api/pipeline/","title":"Pipeline API","text":""},{"location":"api/pipeline/#pictologics.pipeline","title":"<code>pictologics.pipeline</code>","text":""},{"location":"api/pipeline/#pictologics.pipeline--radiomics-pipeline-module","title":"Radiomics Pipeline Module","text":"<p>This module provides a flexible, configurable pipeline for executing radiomic feature extraction workflows. It allows users to define sequences of preprocessing steps and feature extraction tasks.</p>"},{"location":"api/pipeline/#pictologics.pipeline--key-features","title":"Key Features:","text":"<ul> <li>Configurable Workflows: Define steps like resampling, resegmentation, filtering,   discretisation, and feature extraction in a declarative manner.</li> <li>State Management: Tracks the state of the image and masks (morphological and intensity)   throughout the pipeline.</li> <li>Logging: Records execution details, parameters, and errors for reproducibility.</li> <li>Batch Processing: Can process multiple configurations on the same input data.</li> </ul>"},{"location":"api/pipeline/#pictologics.pipeline.EmptyROIMaskError","title":"<code>EmptyROIMaskError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when preprocessing yields an empty ROI mask.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>class EmptyROIMaskError(ValueError):\n    \"\"\"Raised when preprocessing yields an empty ROI mask.\"\"\"\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.PipelineState","title":"<code>PipelineState</code>  <code>dataclass</code>","text":"<p>Holds the current state of the image and masks during pipeline execution.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>@dataclass\nclass PipelineState:\n    \"\"\"\n    Holds the current state of the image and masks during pipeline execution.\n    \"\"\"\n\n    image: Image  # May be discretised after discretise step\n    raw_image: Image  # Always the non-discretised image (for intensity/morphology)\n    morph_mask: Image\n    intensity_mask: Image\n    is_discretised: bool = False\n    n_bins: Optional[int] = None\n    bin_width: Optional[float] = None\n    discretisation_method: Optional[str] = None\n    discretisation_min: Optional[float] = None\n    discretisation_max: Optional[float] = None\n    mask_was_generated: bool = False\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline","title":"<code>RadiomicsPipeline</code>","text":"<p>A flexible, configurable pipeline for radiomic feature extraction. Allows defining multiple processing configurations (sequences of steps) to be run on data.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>class RadiomicsPipeline:\n    \"\"\"\n    A flexible, configurable pipeline for radiomic feature extraction.\n    Allows defining multiple processing configurations (sequences of steps) to be run on data.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self._configs: Dict[str, List[Dict[str, Any]]] = {}\n        self._log: List[Dict[str, Any]] = []\n        self._load_predefined_configs()\n\n    def _load_predefined_configs(self) -&gt; None:\n        \"\"\"\n        Load predefined, commonly used pipeline configurations.\n        \"\"\"\n        # Common resampling parameters\n        resample_05mm = {\n            \"step\": \"resample\",\n            \"params\": {\"new_spacing\": (0.5, 0.5, 0.5), \"interpolation\": \"linear\"},\n        }\n\n        # Common feature extraction parameters (all families)\n        extract_all = {\n            \"step\": \"extract_features\",\n            \"params\": {\n                \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n                # Performance-friendly defaults: spatial/local intensity extras can be\n                # extremely slow on larger ROIs. Users can enable explicitly in custom configs.\n                \"include_spatial_intensity\": False,\n                \"include_local_intensity\": False,\n            },\n        }\n\n        # --- FBN Configurations ---\n\n        # Steps1: 0.5mm, FBN 8\n        self.add_config(\n            \"standard_fbn_8\",\n            [\n                resample_05mm,\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 8}},\n                extract_all,\n            ],\n        )\n\n        # Steps2: 0.5mm, FBN 16\n        self.add_config(\n            \"standard_fbn_16\",\n            [\n                resample_05mm,\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 16}},\n                extract_all,\n            ],\n        )\n\n        # Steps3: 0.5mm, FBN 32\n        self.add_config(\n            \"standard_fbn_32\",\n            [\n                resample_05mm,\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n                extract_all,\n            ],\n        )\n\n        # --- FBS Configurations ---\n\n        # Steps4: 0.5mm, FBS 8.0\n        self.add_config(\n            \"standard_fbs_8\",\n            [\n                resample_05mm,\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": 8.0}},\n                extract_all,\n            ],\n        )\n\n        # Steps5: 0.5mm, FBS 16.0\n        self.add_config(\n            \"standard_fbs_16\",\n            [\n                resample_05mm,\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": 16.0}},\n                extract_all,\n            ],\n        )\n\n        # Steps6: 0.5mm, FBS 32.0\n        self.add_config(\n            \"standard_fbs_32\",\n            [\n                resample_05mm,\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": 32.0}},\n                extract_all,\n            ],\n        )\n\n    def get_all_standard_config_names(self) -&gt; List[str]:\n        \"\"\"\n        Returns the list of all standard configuration names.\n        \"\"\"\n        return [\n            \"standard_fbn_8\",\n            \"standard_fbn_16\",\n            \"standard_fbn_32\",\n            \"standard_fbs_8\",\n            \"standard_fbs_16\",\n            \"standard_fbs_32\",\n        ]\n\n    def add_config(self, name: str, steps: List[Dict[str, Any]]) -&gt; \"RadiomicsPipeline\":\n        \"\"\"\n        Add a processing configuration.\n\n        Args:\n            name: Unique name for this configuration.\n            steps: List of steps. Each step is a dict with 'step' (name) and 'params' (dict).\n                   Supported steps:\n                   - 'resample': params: new_spacing (required), interpolation (optional)\n                   - 'resegment': params: range_min, range_max\n                   - 'filter_outliers': params: sigma\n                   - 'keep_largest_component': params: None\n                   - 'round_intensities': params: None\n                   - 'discretise': params: method, n_bins/bin_width, etc.\n                   - 'extract_features': params: families (list), etc.\n                     Note: Texture features require a prior 'discretise' step.\n                     IVH features are configured via 'ivh_params' dict.\n        \"\"\"\n        if not isinstance(steps, list):\n            raise ValueError(\"Configuration must be a list of steps\")\n\n        for step in steps:\n            if not isinstance(step, dict):\n                raise ValueError(\"Each step must be a dictionary\")\n            if \"step\" not in step:\n                raise ValueError(\"Each step must have a 'step' key\")\n\n        self._configs[name] = steps\n        return self\n\n    def run(\n        self,\n        image: Union[str, Image],\n        mask: Optional[Union[str, Image]] = None,\n        subject_id: Optional[str] = None,\n        config_names: Optional[List[str]] = None,\n    ) -&gt; Dict[str, pd.Series]:\n        \"\"\"\n        Run configurations on the provided image and mask.\n\n        Args:\n            image: Path to image or Image object.\n            mask: Optional path to mask or Image object.\n                If omitted (or passed as `None` / empty string), the pipeline will\n                treat the **entire image** as the ROI by generating a full (all-ones)\n                mask matching the input image geometry.\n            subject_id: Optional identifier for the subject.\n            config_names: List of specific configuration names to run.\n                          If None, runs all registered configurations.\n                          Supports \"all_standard\" to run all 6 standard configs.\n\n        Returns:\n            Dictionary mapping config names to pandas Series of features.\n        \"\"\"\n        # 1. Load Data\n        if isinstance(image, str):\n            orig_img = load_image(image)\n            img_source = image\n        else:\n            orig_img = image\n            img_source = \"InMemory\"\n\n        mask_was_generated = False\n        if mask is None or (isinstance(mask, str) and mask.strip() == \"\"):\n            orig_mask = create_full_mask(orig_img)\n            mask_source = \"GeneratedFullMask\"\n            mask_was_generated = True\n        elif isinstance(mask, str):\n            orig_mask = load_image(mask)\n            mask_source = mask\n        else:\n            orig_mask = mask\n            mask_source = \"InMemory\"\n\n        all_results = {}\n\n        # Determine which configs to run\n        if config_names is None:\n            target_configs = list(self._configs.keys())\n        else:\n            target_configs = []\n            for name in config_names:\n                if name == \"all_standard\":\n                    target_configs.extend(self.get_all_standard_config_names())\n                elif name in self._configs:\n                    target_configs.append(name)\n                else:\n                    raise ValueError(f\"Configuration '{name}' not found.\")\n\n        # Run each configuration\n        for config_name in target_configs:\n            steps = self._configs[config_name]\n\n            # Initialize State\n            # We start with fresh copies for each config\n            state = PipelineState(\n                image=orig_img,\n                raw_image=orig_img,  # Track non-discretised image\n                morph_mask=orig_mask,\n                intensity_mask=Image(\n                    array=orig_mask.array.copy(),\n                    spacing=orig_mask.spacing,\n                    origin=orig_mask.origin,\n                    direction=orig_mask.direction,\n                    modality=orig_mask.modality,\n                ),\n                mask_was_generated=mask_was_generated,\n            )\n\n            self._ensure_nonempty_roi(state, context=\"initialization\")\n\n            config_log: Dict[str, Any] = {\n                \"timestamp\": datetime.datetime.now().isoformat(),\n                \"subject_id\": subject_id,\n                \"config_name\": config_name,\n                \"image_source\": img_source,\n                \"mask_source\": mask_source,\n                \"steps_executed\": [],\n            }\n\n            config_features = {}\n\n            try:\n                for step_def in steps:\n                    step_name = step_def[\"step\"]\n                    params = step_def.get(\"params\", {})\n\n                    # Execute Step\n                    if step_name == \"extract_features\":\n                        features = self._extract_features(state, params)\n                        config_features.update(features)\n                    else:\n                        self._execute_preprocessing_step(state, step_name, params)\n\n                    # Log\n                    config_log[\"steps_executed\"].append(\n                        {\"step\": step_name, \"params\": params, \"status\": \"completed\"}\n                    )\n\n            except Exception as e:\n                config_log[\"error\"] = str(e)\n                config_log[\"failed_step\"] = step_def\n                print(f\"Error in config '{config_name}', step '{step_def}': {e}\")\n\n                # For empty ROI, fail fast (do not silently return empty/partial features).\n                if isinstance(e, EmptyROIMaskError):\n                    self._log.append(config_log)\n                    raise\n\n            self._log.append(config_log)\n\n            # Create Series\n            series = pd.Series(config_features)\n            if subject_id:\n                series[\"subject_id\"] = subject_id\n            all_results[config_name] = series\n\n        return all_results\n\n    def clear_log(self) -&gt; None:\n        \"\"\"Clear the in-memory processing log.\"\"\"\n        self._log.clear()\n\n    def _ensure_nonempty_roi(self, state: PipelineState, context: str) -&gt; None:\n        \"\"\"Raise a clear error if the ROI is empty.\n\n        The pipeline uses `mask_values=1` semantics throughout (see `apply_mask`).\n        \"\"\"\n        has_intensity_roi = bool(np.any(state.intensity_mask.array == 1))\n        has_morph_roi = bool(np.any(state.morph_mask.array == 1))\n\n        if not has_intensity_roi or not has_morph_roi:\n            raise EmptyROIMaskError(\n                \"ROI is empty after preprocessing \"\n                f\"({context}). Ensure your mask contains at least one voxel with value 1, \"\n                \"or relax resegmentation/outlier filtering thresholds.\"\n            )\n\n    def _execute_preprocessing_step(\n        self, state: PipelineState, step_name: str, params: Dict[str, Any]\n    ) -&gt; None:\n        \"\"\"\n        Execute a single preprocessing step and update the state in-place.\n        \"\"\"\n        if step_name == \"resample\":\n            # Params\n            if \"new_spacing\" not in params:\n                raise ValueError(\"Resample step requires 'new_spacing' parameter.\")\n\n            spacing = params[\"new_spacing\"]\n            interp_img = params.get(\"interpolation\", \"linear\")\n            interp_mask = params.get(\"mask_interpolation\", \"nearest\")\n            mask_thresh = params.get(\"mask_threshold\", 0.5)\n            round_intensities_flag = params.get(\"round_intensities\", False)\n\n            # Update Image and raw_image\n            state.image = resample_image(\n                state.image,\n                spacing,\n                interpolation=interp_img,\n                round_intensities=round_intensities_flag,\n            )\n            state.raw_image = (\n                state.image\n            )  # Keep raw_image in sync before discretisation\n\n            # Update Masks\n            thresh_arg = mask_thresh if interp_mask != \"nearest\" else None\n            state.morph_mask = resample_image(\n                state.morph_mask,\n                spacing,\n                interpolation=interp_mask,\n                mask_threshold=thresh_arg,\n            )\n            state.intensity_mask = resample_image(\n                state.intensity_mask,\n                spacing,\n                interpolation=interp_mask,\n                mask_threshold=thresh_arg,\n            )\n\n            self._ensure_nonempty_roi(state, context=\"resample\")\n\n        elif step_name == \"resegment\":\n            range_min = params.get(\"range_min\")\n            range_max = params.get(\"range_max\")\n            state.intensity_mask = resegment_mask(\n                state.image, state.intensity_mask, range_min, range_max\n            )\n\n            # If the mask was auto-generated (mask omitted), treat resegmentation as ROI definition\n            # for both intensity and morphology features.\n            if state.mask_was_generated:\n                state.morph_mask = resegment_mask(\n                    state.image, state.morph_mask, range_min, range_max\n                )\n\n            self._ensure_nonempty_roi(state, context=\"resegment\")\n\n        elif step_name == \"filter_outliers\":\n            sigma = params.get(\"sigma\", 3.0)\n            state.intensity_mask = filter_outliers(\n                state.image, state.intensity_mask, sigma\n            )\n\n            if state.mask_was_generated:\n                state.morph_mask = filter_outliers(state.image, state.morph_mask, sigma)\n\n            self._ensure_nonempty_roi(state, context=\"filter_outliers\")\n\n        elif step_name == \"round_intensities\":\n            state.image = round_intensities(state.image)\n            state.raw_image = (\n                state.image\n            )  # Keep raw_image in sync before discretisation\n\n        elif step_name == \"keep_largest_component\":\n            # apply_to: \"morph\", \"intensity\", or \"both\" (default)\n            apply_to = params.get(\"apply_to\", \"both\")\n            if apply_to in (\"morph\", \"both\"):\n                state.morph_mask = keep_largest_component(state.morph_mask)\n            if apply_to in (\"intensity\", \"both\"):\n                state.intensity_mask = keep_largest_component(state.intensity_mask)\n\n            self._ensure_nonempty_roi(state, context=\"keep_largest_component\")\n\n        elif step_name == \"discretise\":\n            self._ensure_nonempty_roi(state, context=\"discretise\")\n            method = params.get(\"method\", \"FBN\")\n\n            # Avoid passing 'method' twice\n            disc_params = params.copy()\n            if \"method\" in disc_params:\n                del disc_params[\"method\"]\n\n            state.image = cast(\n                Image,\n                discretise_image(\n                    state.image,\n                    method=method,\n                    roi_mask=state.intensity_mask,\n                    **disc_params,\n                ),\n            )\n\n            state.is_discretised = True\n            state.discretisation_method = method\n            state.n_bins = params.get(\"n_bins\")\n            state.bin_width = params.get(\"bin_width\")\n\n            # If FBS, n_bins is dynamic. We can estimate it from the result.\n            if method == \"FBS\":\n                masked_vals = apply_mask(state.image, state.intensity_mask)\n                if len(masked_vals) &gt; 0:\n                    state.n_bins = int(np.max(masked_vals))\n                else:\n                    raise EmptyROIMaskError(\n                        \"ROI is empty after preprocessing (discretise). \"\n                        \"Cannot infer FBS bin count from an empty ROI.\"\n                    )\n\n        else:\n            raise ValueError(f\"Unknown preprocessing step: {step_name}\")\n\n    def _extract_features(\n        self, state: PipelineState, params: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extract features based on current state.\n        \"\"\"\n        results = {}\n        families = params.get(\n            \"families\", [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"]\n        )\n\n        # Optional kwargs pass-through (advanced usage)\n        spatial_intensity_params = params.get(\"spatial_intensity_params\", {})\n        local_intensity_params = params.get(\"local_intensity_params\", {})\n        ivh_params = params.get(\"ivh_params\", {})\n        texture_matrix_params = params.get(\"texture_matrix_params\", {})\n\n        if spatial_intensity_params is None:\n            spatial_intensity_params = {}\n        if local_intensity_params is None:\n            local_intensity_params = {}\n        if ivh_params is None:\n            ivh_params = {}\n        if texture_matrix_params is None:\n            texture_matrix_params = {}\n\n        if not isinstance(spatial_intensity_params, dict):\n            raise ValueError(\"spatial_intensity_params must be a dict\")\n        if not isinstance(local_intensity_params, dict):\n            raise ValueError(\"local_intensity_params must be a dict\")\n        if not isinstance(ivh_params, dict):\n            raise ValueError(\"ivh_params must be a dict\")\n        if not isinstance(texture_matrix_params, dict):\n            raise ValueError(\"texture_matrix_params must be a dict\")\n\n        # Morphology - uses raw_image (non-discretised) for intensity-based features\n        if \"morphology\" in families:\n            results.update(\n                calculate_morphology_features(\n                    state.morph_mask,\n                    state.raw_image,\n                    intensity_mask=state.intensity_mask,\n                )\n            )\n\n        # Intensity - uses raw_image (non-discretised)\n        if \"intensity\" in families:\n            masked_values = apply_mask(state.raw_image, state.intensity_mask)\n            results.update(calculate_intensity_features(masked_values))\n\n            include_spatial = bool(params.get(\"include_spatial_intensity\", False))\n            include_local = bool(params.get(\"include_local_intensity\", False))\n\n            if include_spatial:\n                results.update(\n                    calculate_spatial_intensity_features(\n                        state.raw_image,\n                        state.intensity_mask,\n                        **spatial_intensity_params,\n                    )\n                )\n            if include_local:\n                results.update(\n                    calculate_local_intensity_features(\n                        state.raw_image, state.intensity_mask, **local_intensity_params\n                    )\n                )\n\n        # Optional explicit families (no-op unless requested)\n        if \"spatial_intensity\" in families and \"intensity\" not in families:\n            results.update(\n                calculate_spatial_intensity_features(\n                    state.raw_image, state.intensity_mask, **spatial_intensity_params\n                )\n            )\n\n        if \"local_intensity\" in families and \"intensity\" not in families:\n            results.update(\n                calculate_local_intensity_features(\n                    state.raw_image, state.intensity_mask, **local_intensity_params\n                )\n            )\n\n        # Histogram / IVH\n        if \"histogram\" in families:\n            # Usually on discretised image\n            if not state.is_discretised:\n                warnings.warn(\n                    \"Histogram features requested but image is not discretised. \"\n                    \"Features may be unreliable or fail if integer bins are expected.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n\n            masked_values = apply_mask(state.image, state.intensity_mask)\n            results.update(calculate_intensity_histogram_features(masked_values))\n\n        if \"ivh\" in families:\n            # IVH computation supports three modes:\n            # 1. ivh_use_continuous=True: Use raw (pre-discretised) intensity values\n            # 2. ivh_discretisation={...}: Apply temporary discretisation just for IVH\n            # 3. Default: Use the pipeline's discretised image (if discretised)\n\n            ivh_use_continuous = params.get(\"ivh_use_continuous\", False)\n            ivh_discretisation = params.get(\"ivh_discretisation\", None)\n\n            # Track discretisation params for IVH calculation\n            ivh_disc_bin_width: Optional[float] = None\n            ivh_disc_min_val: Optional[float] = None\n\n            if ivh_use_continuous:\n                # Use raw intensity values (non-discretised)\n                # This is used for continuous IVH (e.g., IBSI Config D)\n                ivh_values = apply_mask(state.raw_image, state.intensity_mask)\n            elif ivh_discretisation:\n                # Apply temporary discretisation for IVH only\n                # This allows different binning for IVH vs texture features\n                # Uses raw_image as the base to discretise from raw values\n                ivh_disc_params = ivh_discretisation.copy()\n                ivh_method = ivh_disc_params.pop(\"method\", \"FBS\")\n                # Save bin_width and min_val for passing to calculate_ivh_features\n                ivh_disc_bin_width = ivh_disc_params.get(\"bin_width\")\n                ivh_disc_min_val = ivh_disc_params.get(\"min_val\")\n                temp_ivh_disc = discretise_image(\n                    state.raw_image,\n                    method=ivh_method,\n                    roi_mask=state.intensity_mask,\n                    **ivh_disc_params,\n                )\n                ivh_values = apply_mask(temp_ivh_disc, state.intensity_mask)\n            else:\n                # Default: use the current image (which may be discretised)\n                ivh_values = apply_mask(state.image, state.intensity_mask)\n\n            # IVH accepts several optional arguments; support both explicit top-level\n            # keys and an \"ivh_params\" dict for full control.\n            ivh_kwargs: Dict[str, Any] = {}\n\n            # If ivh_discretisation was used, pass its bin_width and min_val\n            if ivh_disc_bin_width is not None:\n                ivh_kwargs[\"bin_width\"] = ivh_disc_bin_width\n            if ivh_disc_min_val is not None:\n                ivh_kwargs[\"min_val\"] = ivh_disc_min_val\n\n            # Dict-based params (preferred) - these override discretisation defaults\n            if \"bin_width\" in ivh_params:\n                ivh_kwargs[\"bin_width\"] = ivh_params.get(\"bin_width\")\n            if \"min_val\" in ivh_params:\n                ivh_kwargs[\"min_val\"] = ivh_params.get(\"min_val\")\n            if \"max_val\" in ivh_params:\n                ivh_kwargs[\"max_val\"] = ivh_params.get(\"max_val\")\n            if \"target_range_min\" in ivh_params:\n                ivh_kwargs[\"target_range_min\"] = ivh_params.get(\"target_range_min\")\n            if \"target_range_max\" in ivh_params:\n                ivh_kwargs[\"target_range_max\"] = ivh_params.get(\"target_range_max\")\n\n            # If not provided, and we are discretised (and not using continuous mode),\n            # default bin_width to 1.0 (bin indices)\n            if (\n                not ivh_use_continuous\n                and state.is_discretised\n                and ivh_kwargs.get(\"bin_width\") is None\n                and not ivh_discretisation\n            ):\n                ivh_kwargs[\"bin_width\"] = 1.0\n\n            # Only pass non-None arguments\n            ivh_kwargs = {k: v for k, v in ivh_kwargs.items() if v is not None}\n\n            results.update(calculate_ivh_features(ivh_values, **ivh_kwargs))\n\n        # Texture\n        if \"texture\" in families:\n            if not state.is_discretised:\n                raise ValueError(\n                    \"Texture features requested but image is not discretised. \"\n                    \"You must include a 'discretise' step before extracting texture features.\"\n                )\n\n            disc_image = state.image\n            n_bins = state.n_bins if state.n_bins else 32  # Fallback\n\n            # Calculate Matrices\n            # Use morphological mask for distance map (GLDZM)\n            # Advanced: allow overriding matrix computation parameters via texture_matrix_params.\n            matrix_kwargs: Dict[str, Any] = {}\n            if \"ngldm_alpha\" in texture_matrix_params:\n                matrix_kwargs[\"ngldm_alpha\"] = texture_matrix_params.get(\"ngldm_alpha\")\n            if \"ngldm_alpha\" not in matrix_kwargs and \"ngldm_alpha\" in params:\n                matrix_kwargs[\"ngldm_alpha\"] = params.get(\"ngldm_alpha\")\n            matrix_kwargs = {k: v for k, v in matrix_kwargs.items() if v is not None}\n\n            texture_matrices = calculate_all_texture_matrices(\n                disc_image.array,\n                state.intensity_mask.array,\n                n_bins,\n                distance_mask=state.morph_mask.array,\n                **matrix_kwargs,\n            )\n\n            results.update(\n                calculate_glcm_features(\n                    disc_image.array,\n                    state.intensity_mask.array,\n                    n_bins,\n                    glcm_matrix=texture_matrices[\"glcm\"],\n                )\n            )\n            results.update(\n                calculate_glrlm_features(\n                    disc_image.array,\n                    state.intensity_mask.array,\n                    n_bins,\n                    glrlm_matrix=texture_matrices[\"glrlm\"],\n                )\n            )\n            results.update(\n                calculate_glszm_features(\n                    disc_image.array,\n                    state.intensity_mask.array,\n                    n_bins,\n                    glszm_matrix=texture_matrices[\"glszm\"],\n                )\n            )\n            results.update(\n                calculate_gldzm_features(\n                    disc_image.array,\n                    state.intensity_mask.array,\n                    n_bins,\n                    gldzm_matrix=texture_matrices[\"gldzm\"],\n                    distance_mask=state.morph_mask.array,\n                )\n            )\n            results.update(\n                calculate_ngtdm_features(\n                    disc_image.array,\n                    state.intensity_mask.array,\n                    n_bins,\n                    ngtdm_matrices=(\n                        texture_matrices[\"ngtdm_s\"],\n                        texture_matrices[\"ngtdm_n\"],\n                    ),\n                )\n            )\n            results.update(\n                calculate_ngldm_features(\n                    disc_image.array,\n                    state.intensity_mask.array,\n                    n_bins,\n                    ngldm_matrix=texture_matrices[\"ngldm\"],\n                )\n            )\n\n        return results\n\n    def save_log(self, output_path: str) -&gt; None:\n        \"\"\"\n        Save the processing log to a JSON file.\n        \"\"\"\n        if not output_path.endswith(\".json\"):\n            output_path += \".json\"\n\n        with open(output_path, \"w\") as f:\n            json.dump(self._log, f, indent=4, default=str)\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.add_config","title":"<code>add_config(name, steps)</code>","text":"<p>Add a processing configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name for this configuration.</p> required <code>steps</code> <code>List[Dict[str, Any]]</code> <p>List of steps. Each step is a dict with 'step' (name) and 'params' (dict).    Supported steps:    - 'resample': params: new_spacing (required), interpolation (optional)    - 'resegment': params: range_min, range_max    - 'filter_outliers': params: sigma    - 'keep_largest_component': params: None    - 'round_intensities': params: None    - 'discretise': params: method, n_bins/bin_width, etc.    - 'extract_features': params: families (list), etc.      Note: Texture features require a prior 'discretise' step.      IVH features are configured via 'ivh_params' dict.</p> required Source code in <code>pictologics/pipeline.py</code> <pre><code>def add_config(self, name: str, steps: List[Dict[str, Any]]) -&gt; \"RadiomicsPipeline\":\n    \"\"\"\n    Add a processing configuration.\n\n    Args:\n        name: Unique name for this configuration.\n        steps: List of steps. Each step is a dict with 'step' (name) and 'params' (dict).\n               Supported steps:\n               - 'resample': params: new_spacing (required), interpolation (optional)\n               - 'resegment': params: range_min, range_max\n               - 'filter_outliers': params: sigma\n               - 'keep_largest_component': params: None\n               - 'round_intensities': params: None\n               - 'discretise': params: method, n_bins/bin_width, etc.\n               - 'extract_features': params: families (list), etc.\n                 Note: Texture features require a prior 'discretise' step.\n                 IVH features are configured via 'ivh_params' dict.\n    \"\"\"\n    if not isinstance(steps, list):\n        raise ValueError(\"Configuration must be a list of steps\")\n\n    for step in steps:\n        if not isinstance(step, dict):\n            raise ValueError(\"Each step must be a dictionary\")\n        if \"step\" not in step:\n            raise ValueError(\"Each step must have a 'step' key\")\n\n    self._configs[name] = steps\n    return self\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.clear_log","title":"<code>clear_log()</code>","text":"<p>Clear the in-memory processing log.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def clear_log(self) -&gt; None:\n    \"\"\"Clear the in-memory processing log.\"\"\"\n    self._log.clear()\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.get_all_standard_config_names","title":"<code>get_all_standard_config_names()</code>","text":"<p>Returns the list of all standard configuration names.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def get_all_standard_config_names(self) -&gt; List[str]:\n    \"\"\"\n    Returns the list of all standard configuration names.\n    \"\"\"\n    return [\n        \"standard_fbn_8\",\n        \"standard_fbn_16\",\n        \"standard_fbn_32\",\n        \"standard_fbs_8\",\n        \"standard_fbs_16\",\n        \"standard_fbs_32\",\n    ]\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.run","title":"<code>run(image, mask=None, subject_id=None, config_names=None)</code>","text":"<p>Run configurations on the provided image and mask.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Image]</code> <p>Path to image or Image object.</p> required <code>mask</code> <code>Optional[Union[str, Image]]</code> <p>Optional path to mask or Image object. If omitted (or passed as <code>None</code> / empty string), the pipeline will treat the entire image as the ROI by generating a full (all-ones) mask matching the input image geometry.</p> <code>None</code> <code>subject_id</code> <code>Optional[str]</code> <p>Optional identifier for the subject.</p> <code>None</code> <code>config_names</code> <code>Optional[List[str]]</code> <p>List of specific configuration names to run.           If None, runs all registered configurations.           Supports \"all_standard\" to run all 6 standard configs.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Series]</code> <p>Dictionary mapping config names to pandas Series of features.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def run(\n    self,\n    image: Union[str, Image],\n    mask: Optional[Union[str, Image]] = None,\n    subject_id: Optional[str] = None,\n    config_names: Optional[List[str]] = None,\n) -&gt; Dict[str, pd.Series]:\n    \"\"\"\n    Run configurations on the provided image and mask.\n\n    Args:\n        image: Path to image or Image object.\n        mask: Optional path to mask or Image object.\n            If omitted (or passed as `None` / empty string), the pipeline will\n            treat the **entire image** as the ROI by generating a full (all-ones)\n            mask matching the input image geometry.\n        subject_id: Optional identifier for the subject.\n        config_names: List of specific configuration names to run.\n                      If None, runs all registered configurations.\n                      Supports \"all_standard\" to run all 6 standard configs.\n\n    Returns:\n        Dictionary mapping config names to pandas Series of features.\n    \"\"\"\n    # 1. Load Data\n    if isinstance(image, str):\n        orig_img = load_image(image)\n        img_source = image\n    else:\n        orig_img = image\n        img_source = \"InMemory\"\n\n    mask_was_generated = False\n    if mask is None or (isinstance(mask, str) and mask.strip() == \"\"):\n        orig_mask = create_full_mask(orig_img)\n        mask_source = \"GeneratedFullMask\"\n        mask_was_generated = True\n    elif isinstance(mask, str):\n        orig_mask = load_image(mask)\n        mask_source = mask\n    else:\n        orig_mask = mask\n        mask_source = \"InMemory\"\n\n    all_results = {}\n\n    # Determine which configs to run\n    if config_names is None:\n        target_configs = list(self._configs.keys())\n    else:\n        target_configs = []\n        for name in config_names:\n            if name == \"all_standard\":\n                target_configs.extend(self.get_all_standard_config_names())\n            elif name in self._configs:\n                target_configs.append(name)\n            else:\n                raise ValueError(f\"Configuration '{name}' not found.\")\n\n    # Run each configuration\n    for config_name in target_configs:\n        steps = self._configs[config_name]\n\n        # Initialize State\n        # We start with fresh copies for each config\n        state = PipelineState(\n            image=orig_img,\n            raw_image=orig_img,  # Track non-discretised image\n            morph_mask=orig_mask,\n            intensity_mask=Image(\n                array=orig_mask.array.copy(),\n                spacing=orig_mask.spacing,\n                origin=orig_mask.origin,\n                direction=orig_mask.direction,\n                modality=orig_mask.modality,\n            ),\n            mask_was_generated=mask_was_generated,\n        )\n\n        self._ensure_nonempty_roi(state, context=\"initialization\")\n\n        config_log: Dict[str, Any] = {\n            \"timestamp\": datetime.datetime.now().isoformat(),\n            \"subject_id\": subject_id,\n            \"config_name\": config_name,\n            \"image_source\": img_source,\n            \"mask_source\": mask_source,\n            \"steps_executed\": [],\n        }\n\n        config_features = {}\n\n        try:\n            for step_def in steps:\n                step_name = step_def[\"step\"]\n                params = step_def.get(\"params\", {})\n\n                # Execute Step\n                if step_name == \"extract_features\":\n                    features = self._extract_features(state, params)\n                    config_features.update(features)\n                else:\n                    self._execute_preprocessing_step(state, step_name, params)\n\n                # Log\n                config_log[\"steps_executed\"].append(\n                    {\"step\": step_name, \"params\": params, \"status\": \"completed\"}\n                )\n\n        except Exception as e:\n            config_log[\"error\"] = str(e)\n            config_log[\"failed_step\"] = step_def\n            print(f\"Error in config '{config_name}', step '{step_def}': {e}\")\n\n            # For empty ROI, fail fast (do not silently return empty/partial features).\n            if isinstance(e, EmptyROIMaskError):\n                self._log.append(config_log)\n                raise\n\n        self._log.append(config_log)\n\n        # Create Series\n        series = pd.Series(config_features)\n        if subject_id:\n            series[\"subject_id\"] = subject_id\n        all_results[config_name] = series\n\n    return all_results\n</code></pre>"},{"location":"api/pipeline/#pictologics.pipeline.RadiomicsPipeline.save_log","title":"<code>save_log(output_path)</code>","text":"<p>Save the processing log to a JSON file.</p> Source code in <code>pictologics/pipeline.py</code> <pre><code>def save_log(self, output_path: str) -&gt; None:\n    \"\"\"\n    Save the processing log to a JSON file.\n    \"\"\"\n    if not output_path.endswith(\".json\"):\n        output_path += \".json\"\n\n    with open(output_path, \"w\") as f:\n        json.dump(self._log, f, indent=4, default=str)\n</code></pre>"},{"location":"api/preprocessing/","title":"Preprocessing API","text":""},{"location":"api/preprocessing/#pictologics.preprocessing","title":"<code>pictologics.preprocessing</code>","text":""},{"location":"api/preprocessing/#pictologics.preprocessing--image-preprocessing-module","title":"Image Preprocessing Module","text":"<p>This module provides a collection of preprocessing functions essential for radiomics analysis. These functions are designed to be IBSI-compliant where applicable.</p>"},{"location":"api/preprocessing/#pictologics.preprocessing--key-features","title":"Key Features:","text":"<ul> <li>Resampling: Voxel resampling using 'Align grid centers' (IBSI compliant).</li> <li>Discretisation: Fixed Bin Number (FBN) and Fixed Bin Size (FBS) algorithms.</li> <li>Filtering: Outlier filtering (mean +/- sigma).</li> <li>Mask Operations: Resegmentation (thresholding), ROI extraction, Largest Connected Component.</li> <li>Utilities: Rounding intensities, applying masks.</li> </ul>"},{"location":"api/preprocessing/#pictologics.preprocessing.apply_mask","title":"<code>apply_mask(image, mask, mask_values=1)</code>","text":"<p>Apply mask to image and return flattened array of voxel values.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[Image, ndarray]</code> <p>Image object or numpy array.</p> required <code>mask</code> <code>Union[Image, ndarray]</code> <p>Image object (mask) or numpy array.</p> required <code>mask_values</code> <code>Optional[Union[int, List[int]]]</code> <p>Value(s) in the mask to consider as ROI. Default is 1.          Can be a single integer or a list of integers.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>1D numpy array of values within the mask.</p> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def apply_mask(\n    image: Union[Image, np.ndarray],\n    mask: Union[Image, np.ndarray],\n    mask_values: Optional[Union[int, List[int]]] = 1,\n) -&gt; np.ndarray:\n    \"\"\"\n    Apply mask to image and return flattened array of voxel values.\n\n    Args:\n        image: Image object or numpy array.\n        mask: Image object (mask) or numpy array.\n        mask_values: Value(s) in the mask to consider as ROI. Default is 1.\n                     Can be a single integer or a list of integers.\n\n    Returns:\n        1D numpy array of values within the mask.\n    \"\"\"\n    # Handle inputs\n    img_arr = image.array if isinstance(image, Image) else image\n    mask_arr = mask.array if isinstance(mask, Image) else mask\n\n    # Ensure shapes match\n    if img_arr.shape != mask_arr.shape:\n        raise ValueError(\n            f\"Image shape {img_arr.shape} and mask shape {mask_arr.shape} do not match\"\n        )\n\n    # Handle mask values\n    if mask_values is None:\n        mask_values = [1]\n    elif isinstance(mask_values, int):\n        mask_values = [mask_values]\n\n    # Create boolean mask\n    roi_mask = np.isin(mask_arr, mask_values)\n\n    if not np.any(roi_mask):\n        return np.array([])\n\n    # Apply mask\n    return img_arr[roi_mask]\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.discretise_image","title":"<code>discretise_image(image, method, roi_mask=None, n_bins=None, bin_width=None, min_val=None, max_val=None, cutoffs=None)</code>","text":"<p>Discretise image intensities.</p> <p>Supports IBSI-compliant Fixed Bin Number (FBN) and Fixed Bin Size (FBS).</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[Image, ndarray]</code> <p>Input Image object or numpy array.</p> required <code>method</code> <code>str</code> <p>'FBN' (Fixed Bin Number), 'FBS' (Fixed Bin Size), or 'FIXED_CUTOFFS'.</p> required <code>roi_mask</code> <code>Optional[Union[Image, ndarray]]</code> <p>Optional mask to define the ROI for determining min/max values.</p> <code>None</code> <code>n_bins</code> <code>Optional[int]</code> <p>Number of bins (required for FBN).</p> <code>None</code> <code>bin_width</code> <code>Optional[float]</code> <p>Bin width (required for FBS).</p> <code>None</code> <code>min_val</code> <code>Optional[float]</code> <p>Minimum value for discretisation.      For FBS, defaults to ROI minimum (or global minimum).      For FBN, defaults to ROI minimum.</p> <code>None</code> <code>max_val</code> <code>Optional[float]</code> <p>Maximum value for discretisation (FBN only).      Defaults to ROI maximum.</p> <code>None</code> <code>cutoffs</code> <code>Optional[List[float]]</code> <p>List of cutoffs (required for FIXED_CUTOFFS).</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Image, ndarray]</code> <p>Discretised Image object or numpy array (depending on input).</p> <code>Union[Image, ndarray]</code> <p>Values are 1-based indices.</p> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def discretise_image(\n    image: Union[Image, np.ndarray],\n    method: str,\n    roi_mask: Optional[Union[Image, np.ndarray]] = None,\n    n_bins: Optional[int] = None,\n    bin_width: Optional[float] = None,\n    min_val: Optional[float] = None,\n    max_val: Optional[float] = None,\n    cutoffs: Optional[List[float]] = None,\n) -&gt; Union[Image, np.ndarray]:\n    \"\"\"\n    Discretise image intensities.\n\n    Supports IBSI-compliant Fixed Bin Number (FBN) and Fixed Bin Size (FBS).\n\n    Args:\n        image: Input Image object or numpy array.\n        method: 'FBN' (Fixed Bin Number), 'FBS' (Fixed Bin Size), or 'FIXED_CUTOFFS'.\n        roi_mask: Optional mask to define the ROI for determining min/max values.\n        n_bins: Number of bins (required for FBN).\n        bin_width: Bin width (required for FBS).\n        min_val: Minimum value for discretisation.\n                 For FBS, defaults to ROI minimum (or global minimum).\n                 For FBN, defaults to ROI minimum.\n        max_val: Maximum value for discretisation (FBN only).\n                 Defaults to ROI maximum.\n        cutoffs: List of cutoffs (required for FIXED_CUTOFFS).\n\n    Returns:\n        Discretised Image object or numpy array (depending on input).\n        Values are 1-based indices.\n    \"\"\"\n    # Handle input type\n    if isinstance(image, Image):\n        array = image.array\n        is_image_obj = True\n    else:\n        array = image\n        is_image_obj = False\n\n    # Determine ROI values for default min/max\n    if roi_mask is not None:\n        if isinstance(roi_mask, Image):\n            mask_arr = roi_mask.array\n        else:\n            mask_arr = roi_mask\n\n        if mask_arr.shape != array.shape:\n            raise ValueError(\n                f\"Shape mismatch: Image {array.shape} vs Mask {mask_arr.shape}\"\n            )\n\n        # Extract ROI values (ignoring NaNs)\n        roi_values = array[(mask_arr &gt; 0) &amp; (~np.isnan(array))]\n    else:\n        roi_values = array[~np.isnan(array)]\n\n    # Initialize result\n    discretised = np.zeros(array.shape, dtype=int)\n\n    # We process all non-NaN pixels in the image\n    valid_mask = ~np.isnan(array)\n    values = array[valid_mask]\n\n    if values.size == 0:\n        if is_image_obj:\n            # Create new Image with discretised array\n            return Image(\n                array=discretised,\n                spacing=image.spacing,  # type: ignore\n                origin=image.origin,  # type: ignore\n                direction=image.direction,  # type: ignore\n                modality=image.modality,  # type: ignore\n            )\n        return discretised\n\n    if method == \"FBN\":\n        if n_bins is None:\n            raise ValueError(\"n_bins required for FBN\")\n        if n_bins &lt;= 0:\n            raise ValueError(\"n_bins must be positive\")\n\n        # Determine min/max\n        current_min = min_val\n        if current_min is None:\n            current_min = np.min(roi_values) if roi_values.size &gt; 0 else np.min(values)\n\n        current_max = max_val\n        if current_max is None:\n            current_max = np.max(roi_values) if roi_values.size &gt; 0 else np.max(values)\n\n        if current_max &lt;= current_min:\n            # Edge case: flat region or invalid range\n            discretised[valid_mask] = 1\n        else:\n            # IBSI FBN: floor(N_g * (X - X_min) / (X_max - X_min)) + 1\n            temp_discretised = (\n                np.floor(n_bins * (values - current_min) / (current_max - current_min))\n                + 1\n            )\n\n            # Handle max value case (it falls into N_g + 1 with this formula)\n            # Also clip outliers\n            temp_discretised[values &gt;= current_max] = n_bins\n            temp_discretised = np.clip(temp_discretised, 1, n_bins)\n\n            discretised[valid_mask] = temp_discretised.astype(int)\n\n    elif method == \"FBS\":\n        if bin_width is None:\n            raise ValueError(\"bin_width required for FBS\")\n        if bin_width &lt;= 0:\n            raise ValueError(\"bin_width must be positive\")\n\n        current_min = min_val\n        if current_min is None:\n            current_min = np.min(roi_values) if roi_values.size &gt; 0 else np.min(values)\n\n        # IBSI FBS: floor((X - X_min) / w_b) + 1\n        temp_discretised = np.floor((values - current_min) / bin_width) + 1\n\n        # Ensure minimum bin is 1\n        temp_discretised[temp_discretised &lt; 1] = 1\n        discretised[valid_mask] = temp_discretised.astype(int)\n\n    elif method == \"FIXED_CUTOFFS\":\n        if cutoffs is None:\n            raise ValueError(\"cutoffs required for FIXED_CUTOFFS\")\n\n        temp_discretised = np.digitize(values, bins=np.array(cutoffs))\n        discretised[valid_mask] = temp_discretised.astype(int)\n\n    else:\n        raise ValueError(f\"Unknown discretisation method: {method}\")\n\n    if is_image_obj:\n        return Image(\n            array=discretised,\n            spacing=image.spacing,  # type: ignore\n            origin=image.origin,  # type: ignore\n            direction=image.direction,  # type: ignore\n            modality=image.modality,  # type: ignore\n        )\n    return discretised\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.extract_roi","title":"<code>extract_roi(image, mask, mask_values=1)</code>","text":"<p>Extract ROI from image. Voxels outside the mask are set to NaN. IBSI 'ROI extraction'.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image object.</p> required <code>mask</code> <code>Image</code> <p>Image object (mask).</p> required <code>mask_values</code> <code>Optional[Union[int, List[int]]]</code> <p>Value(s) in the mask to consider as ROI. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Image</code> <p>New Image object with non-ROI voxels set to NaN.</p> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def extract_roi(\n    image: Image,\n    mask: Image,\n    mask_values: Optional[Union[int, List[int]]] = 1,\n) -&gt; Image:\n    \"\"\"\n    Extract ROI from image. Voxels outside the mask are set to NaN.\n    IBSI 'ROI extraction'.\n\n    Args:\n        image: Image object.\n        mask: Image object (mask).\n        mask_values: Value(s) in the mask to consider as ROI. Default is 1.\n\n    Returns:\n        New Image object with non-ROI voxels set to NaN.\n    \"\"\"\n    if image.array.shape != mask.array.shape:\n        raise ValueError(\"Image and mask must have the same shape.\")\n\n    # Handle mask values\n    if mask_values is None:\n        mask_values = [1]\n    elif isinstance(mask_values, int):\n        mask_values = [mask_values]\n\n    roi_mask = np.isin(mask.array, mask_values)\n\n    new_array = image.array.astype(float).copy()\n    new_array[~roi_mask] = np.nan\n\n    return Image(\n        array=new_array,\n        spacing=image.spacing,\n        origin=image.origin,\n        direction=image.direction,\n        modality=image.modality,\n    )\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.filter_outliers","title":"<code>filter_outliers(image, mask, sigma=3.0)</code>","text":"<p>Exclude outliers from the mask based on mean +/- sigma * std. IBSI 3.6.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image object.</p> required <code>mask</code> <code>Image</code> <p>Image object (mask).</p> required <code>sigma</code> <code>float</code> <p>Number of standard deviations.</p> <code>3.0</code> <p>Returns:</p> Type Description <code>Image</code> <p>New Image object (mask) with outliers removed.</p> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def filter_outliers(image: Image, mask: Image, sigma: float = 3.0) -&gt; Image:\n    \"\"\"\n    Exclude outliers from the mask based on mean +/- sigma * std.\n    IBSI 3.6.\n\n    Args:\n        image: Image object.\n        mask: Image object (mask).\n        sigma: Number of standard deviations.\n\n    Returns:\n        New Image object (mask) with outliers removed.\n    \"\"\"\n    # Extract values within the mask\n    values = apply_mask(image, mask)\n\n    if values.size == 0:\n        return mask\n\n    mean_val = np.mean(values)\n    # IBSI uses population std (no bias correction, ddof=0)\n    std_val = np.std(values, ddof=0)\n\n    lower_bound = mean_val - sigma * std_val\n    upper_bound = mean_val + sigma * std_val\n\n    # Create outlier mask\n    # Keep values within [lower, upper]\n    valid_mask = (image.array &gt;= lower_bound) &amp; (image.array &lt;= upper_bound)\n\n    # Update original mask\n    new_mask_array = mask.array.copy()\n\n    # Ensure boolean or integer type for bitwise operation\n    # Assuming mask.array is binary (0/1) or boolean\n    if new_mask_array.dtype == bool:\n        new_mask_array = new_mask_array &amp; valid_mask\n    else:\n        new_mask_array = (new_mask_array * valid_mask).astype(np.uint8)\n\n    return Image(\n        array=new_mask_array,\n        spacing=mask.spacing,\n        origin=mask.origin,\n        direction=mask.direction,\n        modality=mask.modality,\n    )\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.keep_largest_component","title":"<code>keep_largest_component(mask)</code>","text":"<p>Keep only the largest connected component in the mask.</p> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def keep_largest_component(mask: Image) -&gt; Image:\n    \"\"\"\n    Keep only the largest connected component in the mask.\n    \"\"\"\n    mask_array = mask.array\n    labeled_mask, num_features = label(mask_array)\n    if num_features &lt;= 1:\n        return mask\n\n    max_size = 0\n    max_label = 0\n    for i in range(1, num_features + 1):\n        size = np.sum(labeled_mask == i)\n        if size &gt; max_size:\n            max_size = size\n            max_label = i\n\n    new_array = (labeled_mask == max_label).astype(np.uint8)\n\n    return Image(\n        array=new_array,\n        spacing=mask.spacing,\n        origin=mask.origin,\n        direction=mask.direction,\n        modality=mask.modality,\n    )\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.resample_image","title":"<code>resample_image(image, new_spacing, interpolation='linear', boundary_mode='nearest', round_intensities=False, mask_threshold=None)</code>","text":"<p>Resample image to new voxel spacing using IBSI-compliant 'Align grid centers' method.</p> <p>Uses scipy.ndimage.affine_transform for memory efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input Image object.</p> required <code>new_spacing</code> <code>Tuple[float, float, float]</code> <p>Target spacing (x, y, z). Must be positive.</p> required <code>interpolation</code> <code>str</code> <p>Interpolation method. 'nearest': Nearest neighbour (order 0). 'linear': Trilinear (order 1). 'cubic': Tricubic spline (order 3).</p> <code>'linear'</code> <code>boundary_mode</code> <code>str</code> <p>Padding mode for extrapolation. 'nearest' (default): Replicates edge values (aaaa|abcd|dddd). 'constant': Pads with constant value (0). 'reflect': Reflects at boundary. 'wrap': Wraps around.</p> <code>'nearest'</code> <code>round_intensities</code> <code>bool</code> <p>If True, round resulting intensities to nearest integer.</p> <code>False</code> <code>mask_threshold</code> <code>Optional[float]</code> <p>If provided, treat output as a binary mask.             Values &gt;= threshold become 1, others 0.             Commonly 0.5 for partial volume correction.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>Resampled Image object.</p> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def resample_image(\n    image: Image,\n    new_spacing: Tuple[float, float, float],\n    interpolation: str = \"linear\",\n    boundary_mode: str = \"nearest\",\n    round_intensities: bool = False,\n    mask_threshold: Optional[float] = None,\n) -&gt; Image:\n    \"\"\"\n    Resample image to new voxel spacing using IBSI-compliant 'Align grid centers' method.\n\n    Uses scipy.ndimage.affine_transform for memory efficiency.\n\n    Args:\n        image: Input Image object.\n        new_spacing: Target spacing (x, y, z). Must be positive.\n        interpolation: Interpolation method.\n            'nearest': Nearest neighbour (order 0).\n            'linear': Trilinear (order 1).\n            'cubic': Tricubic spline (order 3).\n        boundary_mode: Padding mode for extrapolation.\n            'nearest' (default): Replicates edge values (aaaa|abcd|dddd).\n            'constant': Pads with constant value (0).\n            'reflect': Reflects at boundary.\n            'wrap': Wraps around.\n        round_intensities: If True, round resulting intensities to nearest integer.\n        mask_threshold: If provided, treat output as a binary mask.\n                        Values &gt;= threshold become 1, others 0.\n                        Commonly 0.5 for partial volume correction.\n\n    Returns:\n        Resampled Image object.\n    \"\"\"\n    if any(s &lt;= 0 for s in new_spacing):\n        raise ValueError(f\"New spacing must be positive, got {new_spacing}\")\n\n    # Map interpolation string to spline order\n    interpolation_map = {\n        \"nearest\": 0,\n        \"linear\": 1,\n        \"cubic\": 3,\n    }\n\n    if interpolation not in interpolation_map:\n        raise ValueError(\n            f\"Unknown interpolation method: {interpolation}. \"\n            f\"Supported: {list(interpolation_map.keys())}\"\n        )\n\n    order = interpolation_map[interpolation]\n\n    # Calculate new shape\n    # IBSI: nb = ceil(na * sa / sb)\n    original_spacing = np.array(image.spacing)\n    target_spacing = np.array(new_spacing)\n\n    # Scale factor for dimensions (how many new voxels per old voxel)\n    # dim_scale = s_old / s_new\n    dim_scale = original_spacing / target_spacing\n\n    new_shape = np.ceil(image.array.shape * dim_scale).astype(int)\n\n    # Calculate affine transform parameters\n    # We map Output Coordinate (x_out) -&gt; Input Coordinate (x_in)\n    # x_in = matrix * x_out + offset\n\n    # Scale factor for coordinates (step size in input space per step in output space)\n    # step_in = s_new / s_old\n    coord_scale = target_spacing / original_spacing\n    matrix = coord_scale  # Diagonal matrix elements\n\n    # Calculate offset for 'Align Grid Centers\n    center_orig = (np.array(image.array.shape) - 1) / 2.0\n    center_new = (new_shape - 1) / 2.0\n\n    offset = center_orig - matrix * center_new\n\n    resampled_array = affine_transform(\n        image.array,\n        matrix=matrix,\n        offset=offset,\n        output_shape=new_shape,\n        order=order,\n        mode=boundary_mode,\n    )\n\n    # Post-processing\n    if mask_threshold is not None:\n        # Binarize mask\n        resampled_array = (resampled_array &gt;= mask_threshold).astype(np.uint8)\n    elif round_intensities:\n        # Round intensities\n        resampled_array = np.round(resampled_array)\n\n    # Update origin to maintain center alignment\n    # O_new = O_old + 0.5 * ( (N_old-1)*S_old - (N_new-1)*S_new )\n    extent_orig = (np.array(image.array.shape) - 1) * original_spacing\n    extent_new = (new_shape - 1) * target_spacing\n    origin_shift = 0.5 * (extent_orig - extent_new)\n    new_origin = tuple(np.array(image.origin) + origin_shift)\n\n    return Image(\n        array=resampled_array,\n        spacing=new_spacing,\n        origin=new_origin,\n        direction=image.direction,\n        modality=image.modality,\n    )\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.resegment_mask","title":"<code>resegment_mask(image, mask, range_min=None, range_max=None)</code>","text":"<p>Update mask to exclude voxels where image intensity is outside the specified range. Used for IBSI re-segmentation (e.g. [-1000, 400] HU).</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image object.</p> required <code>mask</code> <code>Image</code> <p>Image object (mask).</p> required <code>range_min</code> <code>Optional[float]</code> <p>Minimum intensity value (inclusive). If None, no lower bound.</p> <code>None</code> <code>range_max</code> <code>Optional[float]</code> <p>Maximum intensity value (inclusive). If None, no upper bound.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>Updated Image object (mask) with re-segmentation applied.</p> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def resegment_mask(\n    image: Image,\n    mask: Image,\n    range_min: Optional[float] = None,\n    range_max: Optional[float] = None,\n) -&gt; Image:\n    \"\"\"\n    Update mask to exclude voxels where image intensity is outside the specified range.\n    Used for IBSI re-segmentation (e.g. [-1000, 400] HU).\n\n    Args:\n        image: Image object.\n        mask: Image object (mask).\n        range_min: Minimum intensity value (inclusive). If None, no lower bound.\n        range_max: Maximum intensity value (inclusive). If None, no upper bound.\n\n    Returns:\n        Updated Image object (mask) with re-segmentation applied.\n    \"\"\"\n    if image.array.shape != mask.array.shape:\n        raise ValueError(\"Image and mask must have the same shape for re-segmentation.\")\n\n    new_mask_array = mask.array.copy()\n\n    # Identify outliers\n    outliers = np.zeros(image.array.shape, dtype=bool)\n\n    if range_min is not None:\n        outliers |= image.array &lt; range_min\n\n    if range_max is not None:\n        outliers |= image.array &gt; range_max\n\n    # Set mask to 0 where outliers exist\n    new_mask_array[outliers] = 0\n\n    return Image(\n        array=new_mask_array,\n        spacing=mask.spacing,\n        origin=mask.origin,\n        direction=mask.direction,\n        modality=mask.modality,\n    )\n</code></pre>"},{"location":"api/preprocessing/#pictologics.preprocessing.round_intensities","title":"<code>round_intensities(image)</code>","text":"<p>Round image intensities to the nearest integer.</p> Source code in <code>pictologics/preprocessing.py</code> <pre><code>def round_intensities(image: Image) -&gt; Image:\n    \"\"\"\n    Round image intensities to the nearest integer.\n    \"\"\"\n    new_array = np.round(image.array)\n    return Image(\n        array=new_array,\n        spacing=image.spacing,\n        origin=image.origin,\n        direction=image.direction,\n        modality=image.modality,\n    )\n</code></pre>"},{"location":"api/results/","title":"Results Support","text":""},{"location":"api/results/#pictologics.results","title":"<code>pictologics.results</code>","text":""},{"location":"api/results/#pictologics.results.format_results","title":"<code>format_results(results, fmt='wide', meta=None, output_type='dict', config_col='config')</code>","text":"<p>Format the output of RadiomicsPipeline.run() into a structured format.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict[str, Series]</code> <p>Dictionary mapping configuration names to pandas Series of features      (the standard output of RadiomicsPipeline.run).</p> required <code>fmt</code> <code>str</code> <p>\"wide\" or \"long\".  - \"wide\": Flattens keys to '{config}__{feature}'. Returns 1 row (dict/df).  - \"long\": Tidy format with columns for config, feature_name, and value.</p> <code>'wide'</code> <code>meta</code> <code>dict[str, Any] | None</code> <p>Optional dictionary of metadata to prepend to the result (e.g., subject ID).</p> <code>None</code> <code>output_type</code> <code>str</code> <p>Format of the returned object: \"dict\", \"pandas\", or \"json\".</p> <code>'dict'</code> <code>config_col</code> <code>str</code> <p>Name of the column holding the configuration name (only used if fmt=\"long\").</p> <code>'config'</code> <p>Returns:</p> Type Description <code>dict[str, Any] | DataFrame | str | list[dict[str, Any]]</code> <p>Formatted data in the specified output_type.</p> Source code in <code>pictologics/results.py</code> <pre><code>def format_results(\n    results: dict[str, pd.Series],\n    fmt: str = \"wide\",\n    meta: dict[str, Any] | None = None,\n    output_type: str = \"dict\",\n    config_col: str = \"config\",\n) -&gt; dict[str, Any] | pd.DataFrame | str | list[dict[str, Any]]:\n    \"\"\"\n    Format the output of RadiomicsPipeline.run() into a structured format.\n\n    Args:\n        results: Dictionary mapping configuration names to pandas Series of features\n                 (the standard output of RadiomicsPipeline.run).\n        fmt: \"wide\" or \"long\".\n             - \"wide\": Flattens keys to '{config}__{feature}'. Returns 1 row (dict/df).\n             - \"long\": Tidy format with columns for config, feature_name, and value.\n        meta: Optional dictionary of metadata to prepend to the result (e.g., subject ID).\n        output_type: Format of the returned object: \"dict\", \"pandas\", or \"json\".\n        config_col: Name of the column holding the configuration name (only used if fmt=\"long\").\n\n    Returns:\n        Formatted data in the specified output_type.\n    \"\"\"\n    if meta is None:\n        meta = {}\n\n    if fmt == \"wide\":\n        # Wide format: { \"meta_key\": val, \"config__feature\": val }\n        formatted_data = meta.copy()\n        for config_name, series in results.items():\n            for feature_name, value in series.items():\n                col_name = f\"{config_name}__{feature_name}\"\n                formatted_data[col_name] = value\n\n        if output_type == \"dict\":\n            return formatted_data\n        elif output_type == \"pandas\":\n            return pd.DataFrame([formatted_data])\n        elif output_type == \"json\":\n            return json.dumps(formatted_data)\n        else:\n            raise ValueError(f\"Unknown output_type: {output_type}\")\n\n    elif fmt == \"long\":\n        # Long format: Rows of [meta_cols..., config, feature_name, value]\n        rows = []\n        for config_name, series in results.items():\n            for feature_name, value in series.items():\n                row = meta.copy()\n                row[config_col] = config_name\n                row[\"feature_name\"] = feature_name\n                row[\"value\"] = value\n                rows.append(row)\n\n        if not rows:\n            # Handle empty results case\n            # For pure python output, empty list is fine.\n            # For pandas, we need a dataframe with columns.\n            if output_type != \"pandas\":\n                if output_type == \"dict\":\n                    return []\n                elif output_type == \"json\":\n                    return \"[]\"\n\n            df = pd.DataFrame(\n                columns=list(meta.keys()) + [config_col, \"feature_name\", \"value\"]\n            )\n            return df  # Columns are already in order\n\n        # Reorder keys/columns\n        # Determine strict order\n        meta_keys = list(meta.keys())\n        standard_cols = [config_col, \"feature_name\", \"value\"]\n        # Ensure we don't duplicate keys\n        cols_order = meta_keys + [c for c in standard_cols if c not in meta_keys]\n\n        # If output is dict/json, reorder the dictionaries directly\n        if output_type in (\"dict\", \"json\"):\n            # Ensure each row has keys in the desired order\n            ordered_rows = []\n            for r in rows:\n                new_r = {k: r.get(k) for k in cols_order if k in r}\n\n                ordered_rows.append(new_r)\n\n            if output_type == \"dict\":\n                return ordered_rows\n            else:  # json\n                return json.dumps(ordered_rows)\n\n        # Output type is 'pandas'\n        df = pd.DataFrame(rows)\n        # Verify which columns actually exist in the dataframe\n        existing_cols = list(df.columns)\n        final_order = [c for c in cols_order if c in existing_cols] + [\n            c for c in existing_cols if c not in cols_order\n        ]\n        df = df.reindex(columns=final_order)\n\n        return df\n\n    else:\n        raise ValueError(f\"Unknown format: {fmt}. Use 'wide' or 'long'.\")\n</code></pre>"},{"location":"api/results/#pictologics.results.save_results","title":"<code>save_results(data, path, file_format=None)</code>","text":"<p>Save results to a file (CSV, JSON, etc.), automatically handling merging of lists.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any] | list[dict[str, Any]] | DataFrame | list[DataFrame] | str | list[str]</code> <p>The data to save. Supported types:   - Dict or List[Dict]   - DataFrame or List[DataFrame]   - JSON string or List[JSON strings]</p> required <code>path</code> <code>str | Path</code> <p>Output file path.</p> required <code>file_format</code> <code>str | None</code> <p>\"csv\" or \"json\". If None, inferred from file extension.</p> <code>None</code> Source code in <code>pictologics/results.py</code> <pre><code>def save_results(\n    data: (\n        dict[str, Any]\n        | list[dict[str, Any]]\n        | pd.DataFrame\n        | list[pd.DataFrame]\n        | str\n        | list[str]\n    ),\n    path: str | Path,\n    file_format: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Save results to a file (CSV, JSON, etc.), automatically handling merging of lists.\n\n    Args:\n        data: The data to save. Supported types:\n              - Dict or List[Dict]\n              - DataFrame or List[DataFrame]\n              - JSON string or List[JSON strings]\n        path: Output file path.\n        file_format: \"csv\" or \"json\". If None, inferred from file extension.\n    \"\"\"\n    path = Path(path)\n    if file_format is None:\n        if path.suffix.lower() == \".csv\":\n            file_format = \"csv\"\n        elif path.suffix.lower() == \".json\":\n            file_format = \"json\"\n        else:\n            file_format = \"csv\"\n\n    # If format is JSON and data is already a dict or list of dicts, bypass pandas\n    # to avoid overhead and potential C-extension conflicts in coverage/threading.\n    if file_format == \"json\":\n        # Check if data is already in a compatible format\n        is_dict = isinstance(data, dict)\n        is_list_of_dicts = isinstance(data, list) and (\n            not data or isinstance(data[0], dict)\n        )\n\n        if is_dict:\n            with open(path, \"w\") as f:\n                json.dump([data], f, indent=2)\n            return\n\n        if is_list_of_dicts:\n            with open(path, \"w\") as f:\n                json.dump(data, f, indent=2)\n            return\n\n    # Normalize input to a single DataFrame\n    try:\n        final_df = _normalize_to_dataframe(data)\n    except Exception as e:\n        # If normalization fails but we want to debug, re-raise\n        raise e\n\n    # Export\n    if file_format == \"csv\":\n        final_df.to_csv(path, index=False)\n    elif file_format == \"json\":\n        # Use standard json library to avoid potential pandas C-extension issues during coverage\n        with open(path, \"w\") as f:\n            json.dump(final_df.to_dict(orient=\"records\"), f, indent=2)\n    else:\n        raise ValueError(f\"Unsupported export format: {file_format}\")\n</code></pre>"},{"location":"api/features/intensity/","title":"Intensity Features API","text":""},{"location":"api/features/intensity/#pictologics.features.intensity","title":"<code>pictologics.features.intensity</code>","text":""},{"location":"api/features/intensity/#pictologics.features.intensity--intensity-feature-extraction-module","title":"Intensity Feature Extraction Module","text":"<p>This module provides functions for calculating First Order Statistics (Intensity) features from medical images. It implements the Image Biomarker Standardisation Initiative (IBSI) compliant algorithms.</p>"},{"location":"api/features/intensity/#pictologics.features.intensity--key-features","title":"Key Features:","text":"<ul> <li>First Order Statistics: Mean, Variance, Skewness, Kurtosis, Percentiles, etc.</li> <li>Intensity Histogram: Features based on discretised intensity histograms.</li> <li>Intensity-Volume Histogram (IVH): Volume fractions and intensity fractions, AUC.</li> <li>Spatial Intensity: Moran's I and Geary's C (spatial autocorrelation) [Optimized].</li> <li>Local Intensity: Local and Global Intensity Peaks.</li> </ul>"},{"location":"api/features/intensity/#pictologics.features.intensity--optimization","title":"Optimization:","text":"<p>Uses <code>numba</code> for JIT compilation, with parallel execution for computationally intensive spatial feature calculations.</p>"},{"location":"api/features/intensity/#pictologics.features.intensity.calculate_intensity_features","title":"<code>calculate_intensity_features(values)</code>","text":"<p>Calculate intensity-based features (First Order Statistics) as defined in IBSI.</p> Source code in <code>pictologics/features/intensity.py</code> <pre><code>def calculate_intensity_features(values: np.ndarray) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate intensity-based features (First Order Statistics) as defined in IBSI.\n    \"\"\"\n    if len(values) == 0:\n        return {}\n\n    features: Dict[str, float] = {}\n\n    # 4.1.1 Mean intensity (Q4LE)\n    mean_val = np.mean(values)\n    features[\"mean_intensity_Q4LE\"] = float(mean_val)\n\n    # 4.1.2 Intensity variance (ECT3)\n    var_val = float(np.var(values, ddof=0))\n    features[\"intensity_variance_ECT3\"] = float(var_val)\n\n    # 4.1.3 Intensity skewness (KE2A)\n    if var_val == 0.0:\n        features[\"intensity_skewness_KE2A\"] = np.nan\n        features[\"intensity_kurtosis_IPH6\"] = np.nan\n    else:\n        m2, m3, m4 = _central_moments_2_3_4(values, float(mean_val))\n        denom = m2**1.5\n        if denom != 0.0:\n            features[\"intensity_skewness_KE2A\"] = float(m3 / denom)\n            features[\"intensity_kurtosis_IPH6\"] = float((m4 / (m2 * m2)) - 3.0)\n\n    # 4.1.5 Median intensity (Y12H)\n    median_val = np.median(values)\n    features[\"median_intensity_Y12H\"] = float(median_val)\n\n    # 4.1.6 Minimum intensity (1GSF)\n    min_val = np.min(values)\n    features[\"minimum_intensity_1GSF\"] = float(min_val)\n\n    p10, p25, p75, p90 = np.percentile(values, [10, 25, 75, 90])\n    features[\"10th_intensity_percentile_QG58\"] = float(p10)\n    features[\"90th_intensity_percentile_8DWT\"] = float(p90)\n\n    # 4.1.9 Maximum intensity (84IY)\n    max_val = np.max(values)\n    features[\"maximum_intensity_84IY\"] = float(max_val)\n\n    # 4.1.10 Intensity interquartile range (SALO)\n    features[\"intensity_interquartile_range_SALO\"] = float(p75 - p25)\n\n    # 4.1.11 Intensity range (2OJQ)\n    features[\"intensity_range_2OJQ\"] = float(max_val - min_val)\n\n    # 4.1.12 Mean absolute deviation (4FUA)\n    features[\"intensity_mean_absolute_deviation_4FUA\"] = float(\n        _mean_abs_dev(values, float(mean_val))\n    )\n\n    # 4.1.13 Robust mean absolute deviation (1128)\n    features[\"intensity_robust_mean_absolute_deviation_1128\"] = float(\n        _robust_mean_abs_dev(values, float(p10), float(p90))\n    )\n\n    # 4.1.14 Median absolute deviation (N72L)\n    features[\"intensity_median_absolute_deviation_N72L\"] = float(\n        _mean_abs_dev(values, float(median_val))\n    )\n\n    # 4.1.15 Coefficient of variation (7TET)\n    if mean_val != 0:\n        features[\"intensity_coefficient_of_variation_7TET\"] = float(\n            np.sqrt(var_val) / mean_val\n        )\n    else:\n        features[\"intensity_coefficient_of_variation_7TET\"] = np.nan\n\n    # 4.1.16 Quartile coefficient of dispersion (9S40)\n    if (p75 + p25) != 0:\n        features[\"intensity_quartile_coefficient_of_dispersion_9S40\"] = float(\n            (p75 - p25) / (p75 + p25)\n        )\n    else:\n        features[\"intensity_quartile_coefficient_of_dispersion_9S40\"] = np.nan\n\n    # 4.1.17 Energy (N8CA)\n    energy = float(np.dot(values, values))\n    features[\"intensity_energy_N8CA\"] = energy\n\n    # 4.1.18 Root mean square (5ZWQ)\n    features[\"root_mean_square_intensity_5ZWQ\"] = float(np.sqrt(energy / len(values)))\n\n    return features\n</code></pre>"},{"location":"api/features/intensity/#pictologics.features.intensity.calculate_intensity_histogram_features","title":"<code>calculate_intensity_histogram_features(discretised_values)</code>","text":"<p>Calculate intensity histogram features as defined in IBSI 4.2.</p> Source code in <code>pictologics/features/intensity.py</code> <pre><code>def calculate_intensity_histogram_features(\n    discretised_values: np.ndarray,\n) -&gt; Dict[str, float]:\n    \"\"\"Calculate intensity histogram features as defined in IBSI 4.2.\"\"\"\n    if len(discretised_values) == 0:\n        return {}\n\n    features: Dict[str, float] = {}\n\n    disc = np.asarray(discretised_values)\n    n = disc.size\n\n    # Support negative values by shifting for bincount compatibility\n    min_val_i = int(np.min(disc))\n    max_val_i = int(np.max(disc))\n\n    shifted = disc.astype(np.int64) - min_val_i\n    counts_full = np.bincount(shifted, minlength=(max_val_i - min_val_i + 1))\n    total = float(n)\n    p = counts_full[counts_full &gt; 0].astype(np.float64) / total\n\n    # 4.2.1 Mean discretised intensity (X6K6)\n    mean_disc = float(np.mean(disc))\n    features[\"mean_discretised_intensity_X6K6\"] = float(mean_disc)\n\n    # 4.2.2 Discretised intensity variance (CH89)\n    var_disc = float(np.var(disc, ddof=0))\n    features[\"discretised_intensity_variance_CH89\"] = float(var_disc)\n\n    # 4.2.3 Discretised intensity skewness (88K1)\n    if var_disc == 0.0:\n        features[\"discretised_intensity_skewness_88K1\"] = np.nan\n        features[\"discretised_intensity_kurtosis_C3I7\"] = np.nan\n    else:\n        m2, m3, m4 = _central_moments_2_3_4(disc, float(mean_disc))\n        denom = m2**1.5\n        if denom != 0.0:\n            features[\"discretised_intensity_skewness_88K1\"] = float(m3 / denom)\n            features[\"discretised_intensity_kurtosis_C3I7\"] = float(\n                (m4 / (m2 * m2)) - 3.0\n            )\n\n    p10, p25, median_val, p75, p90 = np.percentile(disc, [10, 25, 50, 75, 90])\n\n    features[\"median_discretised_intensity_WIFQ\"] = float(median_val)\n    features[\"minimum_discretised_intensity_1PR8\"] = float(min_val_i)\n    features[\"10th_discretised_intensity_percentile_1PR\"] = float(p10)\n    features[\"90th_discretised_intensity_percentile_GPMT\"] = float(p90)\n    features[\"maximum_discretised_intensity_3NCY\"] = float(max_val_i)\n\n    mode_index = int(np.argmax(counts_full))\n    features[\"intensity_histogram_mode_AMMC\"] = float(min_val_i + mode_index)\n\n    features[\"discretised_intensity_interquartile_range_WR0O\"] = float(p75 - p25)\n\n    features[\"discretised_intensity_range_5Z3W\"] = float(\n        features[\"maximum_discretised_intensity_3NCY\"]\n        - features[\"minimum_discretised_intensity_1PR8\"]\n    )\n\n    features[\"intensity_histogram_mean_absolute_deviation_D2ZX\"] = float(\n        _mean_abs_dev(disc, float(mean_disc))\n    )\n\n    features[\"intensity_histogram_robust_mean_absolute_deviation_WRZB\"] = float(\n        _robust_mean_abs_dev(disc, float(p10), float(p90))\n    )\n\n    features[\"intensity_histogram_median_absolute_deviation_4RNL\"] = float(\n        _mean_abs_dev(disc, float(median_val))\n    )\n\n    if mean_disc != 0:\n        features[\"intensity_histogram_coefficient_of_variation_CWYJ\"] = float(\n            np.sqrt(var_disc) / mean_disc\n        )\n    else:\n        features[\"intensity_histogram_coefficient_of_variation_CWYJ\"] = np.nan\n\n    if (p75 + p25) != 0:\n        features[\"intensity_histogram_quartile_coefficient_of_dispersion_SLWD\"] = float(\n            (p75 - p25) / (p75 + p25)\n        )\n    else:\n        features[\"intensity_histogram_quartile_coefficient_of_dispersion_SLWD\"] = np.nan\n\n    # Vectorized entropy/uniformity\n    features[\"discretised_intensity_entropy_TLU2\"] = float(-np.sum(p * np.log2(p)))\n    features[\"discretised_intensity_uniformity_BJ5W\"] = float(np.sum(p * p))\n\n    hist_counts = counts_full.astype(np.float64)\n    if len(hist_counts) &lt; 2:\n        features[\"maximum_histogram_gradient_12CE\"] = np.nan\n        features[\"maximum_histogram_gradient_intensity_8E6O\"] = np.nan\n        features[\"minimum_histogram_gradient_VQB3\"] = np.nan\n        features[\"minimum_histogram_gradient_intensity_RHQZ\"] = np.nan\n    else:\n        gradient = np.gradient(hist_counts)\n        features[\"maximum_histogram_gradient_12CE\"] = float(np.max(gradient))\n        max_grad_idx = int(np.argmax(gradient))\n        features[\"maximum_histogram_gradient_intensity_8E6O\"] = float(\n            min_val_i + max_grad_idx\n        )\n        features[\"minimum_histogram_gradient_VQB3\"] = float(np.min(gradient))\n        min_grad_idx = int(np.argmin(gradient))\n        features[\"minimum_histogram_gradient_intensity_RHQZ\"] = float(\n            min_val_i + min_grad_idx\n        )\n\n    return features\n</code></pre>"},{"location":"api/features/intensity/#pictologics.features.intensity.calculate_ivh_features","title":"<code>calculate_ivh_features(discretised_values, bin_width=None, min_val=None, max_val=None, target_range_min=None, target_range_max=None)</code>","text":"<p>Calculate Intensity-Volume Histogram (IVH) features.</p> <p>Includes Volume Fraction difference and Area Under the IVH Curve (AUC).</p> Source code in <code>pictologics/features/intensity.py</code> <pre><code>def calculate_ivh_features(\n    discretised_values: np.ndarray,\n    bin_width: Optional[float] = None,\n    min_val: Optional[float] = None,\n    max_val: Optional[float] = None,\n    target_range_min: Optional[float] = None,\n    target_range_max: Optional[float] = None,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate Intensity-Volume Histogram (IVH) features.\n\n    Includes Volume Fraction difference and Area Under the IVH Curve (AUC).\n    \"\"\"\n    if len(discretised_values) == 0:\n        return {}\n\n    features: Dict[str, float] = {}\n    N = len(discretised_values)\n\n    vals = np.asarray(discretised_values)\n    sorted_vals = np.sort(vals)\n\n    # -------------------------------------------------------------------------\n    # 1. Volume Fractions\n    # -------------------------------------------------------------------------\n    t_min = target_range_min if target_range_min is not None else min_val\n    t_max = target_range_max if target_range_max is not None else max_val\n\n    # Fallback to data min/max if still None\n    if t_min is None or t_max is None:\n        t_min_idx = float(sorted_vals[0])\n        t_max_idx = float(sorted_vals[-1])\n        val_range_idx = t_max_idx - t_min_idx\n\n        def get_volume_fraction_at_intensity_fraction_indices(frac: float) -&gt; float:\n            threshold_idx = t_min_idx + frac * val_range_idx\n            idx = int(np.searchsorted(sorted_vals, threshold_idx, side=\"left\"))\n            count = N - idx\n            return float(count / N)\n\n        features[\"volume_at_intensity_fraction_0.10_BC2M_10\"] = (\n            get_volume_fraction_at_intensity_fraction_indices(0.10)\n        )\n        features[\"volume_at_intensity_fraction_0.90_BC2M_90\"] = (\n            get_volume_fraction_at_intensity_fraction_indices(0.90)\n        )\n\n    else:\n        # We have physical units for the target range.\n        t_range = t_max - t_min\n\n        def get_volume_fraction_at_intensity_fraction_physical(frac: float) -&gt; float:\n            threshold_val = t_min + frac * t_range  # type: ignore\n            if bin_width is not None and min_val is not None:\n                # Convert to index based on FBS\n                mv = min_val\n                bw = bin_width\n                threshold_idx = np.floor((threshold_val - mv) / bw) + 1  # type: ignore\n            else:\n                threshold_idx = threshold_val\n\n            idx = int(np.searchsorted(sorted_vals, threshold_idx, side=\"left\"))\n            count = N - idx\n            return float(count / N)\n\n        features[\"volume_at_intensity_fraction_0.10_BC2M_10\"] = (\n            get_volume_fraction_at_intensity_fraction_physical(0.10)\n        )\n        features[\"volume_at_intensity_fraction_0.90_BC2M_90\"] = (\n            get_volume_fraction_at_intensity_fraction_physical(0.90)\n        )\n\n    features[\n        \"volume_fraction_difference_between_intensity_0.10_and_0.90_fractions_DDTU\"\n    ] = float(\n        features[\"volume_at_intensity_fraction_0.10_BC2M_10\"]\n        - features[\"volume_at_intensity_fraction_0.90_BC2M_90\"]\n    )\n\n    # -------------------------------------------------------------------------\n    # 2. Intensity Fractions\n    # -------------------------------------------------------------------------\n    def get_intensity_at_volume_fraction(vol_frac: float) -&gt; float:\n        # Fast path for standard integer bins (step=1)\n        if (\n            bin_width is not None\n            and min_val is None\n            and bin_width &gt; 0\n            and float(bin_width) == 1.0\n            and np.issubdtype(sorted_vals.dtype, np.integer)\n        ):\n            target_count = int(np.floor(vol_frac * N))\n            if target_count &lt;= 0:\n                return float(sorted_vals[-1])\n\n            # Smallest integer threshold t such that count(vals &gt;= t) &lt;= target_count.\n            # Let k = N - target_count. We need searchsorted(t) &gt;= k.\n            k = N - target_count\n            v = int(sorted_vals[k - 1])\n            t = v + 1\n            vmax = int(sorted_vals[-1])\n            if t &gt; vmax:\n                t = vmax\n            return float(t)\n\n        # Determine candidates\n        if bin_width is not None:\n            g_min = min_val if min_val is not None else np.min(discretised_values)\n            if max_val is not None:\n                g_max = max_val\n            elif min_val is not None:\n                g_max = min_val + np.max(discretised_values) * bin_width\n            else:\n                g_max = np.max(discretised_values)\n\n            if bin_width &gt; 0:\n                num_steps = int(np.round((g_max - g_min) / bin_width))\n                if min_val is not None:\n                    # Candidates are bin centers\n                    idx = np.arange(num_steps, dtype=np.float64)\n                    candidates = g_min + (idx + 0.5) * bin_width\n                else:\n                    idx = np.arange(num_steps + 1, dtype=np.float64)\n                    candidates = g_min + idx * bin_width\n            else:\n                candidates = sorted_vals.astype(np.float64)\n        else:\n            candidates = sorted_vals\n\n        target_count = int(np.floor(vol_frac * N))\n\n        # Binary search\n        low = 0\n        high = len(candidates) - 1\n        ans_idx = -1\n\n        while low &lt;= high:\n            mid = (low + high) // 2\n            val = candidates[mid]\n\n            # Convert physical value to index if in discrete mode\n            if bin_width is not None and min_val is not None and bin_width &gt; 0:\n                check_val = np.floor((val - min_val) / bin_width) + 1\n            else:\n                check_val = val\n\n            idx = np.searchsorted(sorted_vals, check_val, side=\"left\")\n            count = N - idx\n\n            if count &lt;= target_count:\n                ans_idx = mid\n                high = mid - 1\n            else:\n                low = mid + 1\n\n        if ans_idx != -1:\n            return float(candidates[ans_idx])\n        else:\n            return float(candidates[-1])\n\n    features[\"intensity_at_volume_fraction_0.10_GBPN_10\"] = (\n        get_intensity_at_volume_fraction(0.10)\n    )\n    features[\"intensity_at_volume_fraction_0.90_GBPN_90\"] = (\n        get_intensity_at_volume_fraction(0.90)\n    )\n\n    features[\n        \"intensity_fraction_difference_between_volume_0.10_and_0.90_fractions_CNV2\"\n    ] = float(\n        features[\"intensity_at_volume_fraction_0.10_GBPN_10\"]\n        - features[\"intensity_at_volume_fraction_0.90_GBPN_90\"]\n    )\n\n    # -------------------------------------------------------------------------\n    # 3. Area Under the IVH Curve (AUC)\n    # -------------------------------------------------------------------------\n    # IVH Curve: Volume Fraction (phi) vs Intensity (I)\n    # We construct the curve points from the unique values in the data.\n    unique_vals = np.unique(sorted_vals)\n    if len(unique_vals) == 1:\n        # If there is only one discretised intensity, AUC is 0 by definition.\n        features[\"area_under_the_ivh_curve_9CMM\"] = 0.0\n    else:\n        # P(X &gt;= i)\n        # For each unique value, calculate fraction &gt;= value\n\n        # If we have physical mapping, map unique_vals to physical intensities\n        if bin_width is not None and min_val is not None:\n            # Map index to physical center: min_val + (idx - 0.5) * w ?\n            # Standard FBS mapping: index k corresponds to [min + (k-1)w, min + kw)\n            # center = min_val + (k - 1 + 0.5) * w\n            # k is the value in unique_vals\n            intensities_arr = (\n                min_val + (unique_vals.astype(np.float64) - 0.5) * bin_width\n            )\n        else:\n            intensities_arr = unique_vals.astype(np.float64)\n\n        # Calculate volume fractions\n        # searchsorted returns first index where val fits.\n        # Since sorted_vals is sorted, all elements &gt;= val are from searchsorted(val) onwards.\n        indices = np.searchsorted(sorted_vals, unique_vals, side=\"left\")\n        counts = N - indices\n        fractions = counts.astype(np.float64) / float(N)\n\n        # Riemann Sum (Trapezoidal)\n        # Integrate fraction(I) over I.\n        auc = 0.0\n        for k in range(1, len(intensities_arr)):\n            i_curr = intensities_arr[k]\n            i_prev = intensities_arr[k - 1]\n            phi_curr = fractions[k]\n            phi_prev = fractions[k - 1]\n\n            # Trapezoid area\n            width = i_curr - i_prev\n            avg_height = (phi_curr + phi_prev) * 0.5\n            auc += width * avg_height\n\n        features[\"area_under_the_ivh_curve_9CMM\"] = float(auc)\n\n    return features\n</code></pre>"},{"location":"api/features/intensity/#pictologics.features.intensity.calculate_local_intensity_features","title":"<code>calculate_local_intensity_features(image, mask, *, enabled=True)</code>","text":"<p>Calculate local intensity features: Local and Global Intensity Peak (IBSI 4.5).</p> Source code in <code>pictologics/features/intensity.py</code> <pre><code>def calculate_local_intensity_features(\n    image: Image,\n    mask: Image,\n    *,\n    enabled: bool = True,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate local intensity features: Local and Global Intensity Peak (IBSI 4.5).\n    \"\"\"\n    if not enabled:\n        return {}\n\n    features: Dict[str, float] = {}\n\n    mask_array = mask.array\n    data = image.array\n    spacing_tuple = (\n        float(image.spacing[0]),\n        float(image.spacing[1]),\n        float(image.spacing[2]),\n    )\n\n    # Radius for 1 cm^3 sphere\n    radius_mm = 6.2035\n\n    # Get ROI indices\n    x_idx, y_idx, z_idx = np.where(mask_array &gt; 0)\n    if len(x_idx) == 0:\n        return features\n\n    mask_indices = np.ascontiguousarray(\n        np.stack([x_idx, y_idx, z_idx], axis=1).astype(np.int32)\n    )\n    offsets = _sphere_offsets_for_radius(spacing_tuple, radius_mm)\n\n    # Calculate local means only for ROI voxels\n    roi_means = _calculate_local_mean_numba(data, mask_indices, offsets)\n\n    # Compute both peaks without allocating ROI intensity arrays.\n    global_peak, local_peak = _calculate_local_peaks_numba(\n        data, mask_indices, roi_means\n    )\n    features[\"global_intensity_peak_0F91\"] = float(global_peak)\n    features[\"local_intensity_peak_VJGA\"] = float(local_peak)\n\n    return features\n</code></pre>"},{"location":"api/features/intensity/#pictologics.features.intensity.calculate_spatial_intensity_features","title":"<code>calculate_spatial_intensity_features(image, mask, *, enabled=True)</code>","text":"<p>Calculate spatial intensity features: Moran's I and Geary's C (IBSI 4.4).</p> Source code in <code>pictologics/features/intensity.py</code> <pre><code>def calculate_spatial_intensity_features(\n    image: Image,\n    mask: Image,\n    *,\n    enabled: bool = True,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate spatial intensity features: Moran's I and Geary's C (IBSI 4.4).\n    \"\"\"\n    if not enabled:\n        return {}\n\n    features: Dict[str, float] = {}\n\n    mask_array = mask.array\n    data = image.array\n    sx, sy, sz = (\n        float(image.spacing[0]),\n        float(image.spacing[1]),\n        float(image.spacing[2]),\n    )\n\n    # Get ROI indices (X, Y, Z)\n    x_idx, y_idx, z_idx = np.where(mask_array &gt; 0)\n\n    if len(x_idx) &lt; 2:\n        features[\"morans_i_index_N365\"] = np.nan\n        features[\"gearys_c_measure_NPT7\"] = np.nan\n        return features\n\n    xi = np.ascontiguousarray(x_idx.astype(np.int32))\n    yi = np.ascontiguousarray(y_idx.astype(np.int32))\n    zi = np.ascontiguousarray(z_idx.astype(np.int32))\n\n    intensities = np.ascontiguousarray(data[mask_array &gt; 0].astype(np.float64))\n\n    N = len(intensities)\n    mean_int = np.mean(intensities)\n\n    # Calculate terms using Parallelized Numba Function\n    numer_moran, numer_geary_1, numer_geary_2, sum_weights = (\n        _calculate_spatial_features_numba(\n            xi, yi, zi, intensities, float(mean_int), sx, sy, sz\n        )\n    )\n\n    # Moran's I - N365\n    denom = _sum_sq_centered(intensities, float(mean_int))\n\n    if denom != 0 and sum_weights != 0:\n        moran_i = (N / sum_weights) * (numer_moran / denom)\n        features[\"morans_i_index_N365\"] = float(moran_i)\n    else:\n        features[\"morans_i_index_N365\"] = np.nan\n\n    # Geary's C - NPT7\n    if denom != 0 and sum_weights != 0:\n        numer = 2 * numer_geary_1 - 2 * numer_geary_2\n        geary_c = ((N - 1) / (2 * sum_weights)) * (numer / denom)\n        features[\"gearys_c_measure_NPT7\"] = float(geary_c)\n    else:\n        features[\"gearys_c_measure_NPT7\"] = np.nan\n\n    return features\n</code></pre>"},{"location":"api/features/morphology/","title":"Morphology Features API","text":""},{"location":"api/features/morphology/#pictologics.features.morphology","title":"<code>pictologics.features.morphology</code>","text":""},{"location":"api/features/morphology/#pictologics.features.morphology--morphology-feature-extraction-module","title":"Morphology Feature Extraction Module","text":"<p>This module provides functions for calculating Morphological (Shape and Size) features from medical images. It implements the Image Biomarker Standardisation Initiative (IBSI) compliant algorithms.</p>"},{"location":"api/features/morphology/#pictologics.features.morphology--key-features","title":"Key Features:","text":"<ul> <li>Voxel-based: Volume (voxel counting).</li> <li>Mesh-based: Surface Area, Volume (mesh), Compactness, Sphericity.</li> <li>PCA-based: Major/Minor/Least Axis Length, Elongation, Flatness.</li> <li>Convex Hull: Volume, Area, Max 3D Diameter.</li> <li>Bounding Box: Oriented (OMBB) and Axis-Aligned (AABB) Bounding Boxes.</li> <li>Minimum Volume Enclosing Ellipsoid (MVEE): Volume, Area.</li> <li>Intensity-Weighted: Center of Mass Shift, Integrated Intensity.</li> </ul>"},{"location":"api/features/morphology/#pictologics.features.morphology--optimization","title":"Optimization:","text":"<p>Uses <code>numba</code> for optimizing the Khachiyan algorithm for MVEE calculation.</p>"},{"location":"api/features/morphology/#pictologics.features.morphology.calculate_morphology_features","title":"<code>calculate_morphology_features(mask, image=None, intensity_mask=None)</code>","text":"<p>Calculate morphological features from the ROI mask. Includes both voxel-based and mesh-based features (IBSI compliant).</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Image</code> <p>Image object containing the binary mask (Morphological Mask).</p> required <code>image</code> <code>Optional[Image]</code> <p>Optional Image object containing intensity data (required for some features).</p> <code>None</code> <code>intensity_mask</code> <code>Optional[Image]</code> <p>Optional Image object containing the intensity mask (e.g. after outlier filtering).             If provided, used for intensity-weighted features (99N0, KLMA).             If None, defaults to <code>mask</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of calculated features.</p> Source code in <code>pictologics/features/morphology.py</code> <pre><code>def calculate_morphology_features(\n    mask: Image, image: Optional[Image] = None, intensity_mask: Optional[Image] = None\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate morphological features from the ROI mask.\n    Includes both voxel-based and mesh-based features (IBSI compliant).\n\n    Args:\n        mask: Image object containing the binary mask (Morphological Mask).\n        image: Optional Image object containing intensity data (required for some features).\n        intensity_mask: Optional Image object containing the intensity mask (e.g. after outlier filtering).\n                        If provided, used for intensity-weighted features (99N0, KLMA).\n                        If None, defaults to `mask`.\n\n    Returns:\n        Dictionary of calculated features.\n    \"\"\"\n    features: Dict[str, float] = {}\n    i_mask = intensity_mask if intensity_mask is not None else mask\n\n    # 1. Voxel Based Features\n    voxel_volume = np.prod(mask.spacing)\n    n_voxels = np.sum(mask.array)\n    features[\"volume_voxel_counting_YEKZ\"] = float(n_voxels * voxel_volume)\n\n    # 2. Mesh Based Features\n    mesh_feats, verts, faces = _get_mesh_features(mask)\n    features.update(mesh_feats)\n\n    if verts is None or faces is None:\n        return features\n\n    mesh_volume = features.get(\"volume_RNU0\", 0.0)\n    surface_area = features.get(\"surface_area_C0JK\", 0.0)\n\n    # 3. Shape Features\n    features.update(_get_shape_features(surface_area, mesh_volume))\n\n    # 4. PCA Based Features\n    pca_feats, evals, evecs = _get_pca_features(mask, mesh_volume, surface_area)\n    features.update(pca_feats)\n\n    # 5. Convex Hull Features\n    hull_feats, hull = _get_convex_hull_features(verts, mesh_volume, surface_area)\n    features.update(hull_feats)\n\n    # 6. Bounding Box Features\n    features.update(_get_bounding_box_features(verts, evecs, mesh_volume, surface_area))\n\n    # 7. MVEE Features\n    features.update(_get_mvee_features(hull, verts, mesh_volume, surface_area))\n\n    # 8. Intensity Based Features\n    if image is not None:\n        features.update(\n            _get_intensity_morphology_features(mask, image, i_mask, mesh_volume)\n        )\n\n    return features\n</code></pre>"},{"location":"api/features/texture/","title":"Texture Features API","text":""},{"location":"api/features/texture/#pictologics.features.texture","title":"<code>pictologics.features.texture</code>","text":""},{"location":"api/features/texture/#pictologics.features.texture--texture-feature-extraction-module","title":"Texture Feature Extraction Module","text":"<p>This module provides a comprehensive suite of functions for calculating 3D texture features from medical images. It implements the Image Biomarker Standardisation Initiative (IBSI) compliant algorithms for various texture matrices.</p>"},{"location":"api/features/texture/#pictologics.features.texture--key-concepts","title":"Key Concepts:","text":"<p>Texture analysis quantifies the spatial arrangement of grey levels in an image. It assumes that the texture (e.g., \"smooth\", \"coarse\", \"regular\") is contained in the spatial relationship between the grey levels of the voxels.</p>"},{"location":"api/features/texture/#pictologics.features.texture--implemented-matrices","title":"Implemented Matrices:","text":"<ol> <li> <p>GLCM (Grey Level Co-occurrence Matrix):     Counts how often pairs of grey levels occur at a specific distance and direction.     Captures: Contrast, homogeneity, correlation.</p> </li> <li> <p>GLRLM (Grey Level Run Length Matrix):     Counts the lengths of consecutive runs of the same grey level.     Captures: Coarseness, directionality.</p> </li> <li> <p>GLSZM (Grey Level Size Zone Matrix):     Counts the size of zones (connected components) of the same grey level.     Captures: Regional homogeneity, size distribution of texture elements.</p> </li> <li> <p>GLDZM (Grey Level Distance Zone Matrix):     Counts zones based on their distance from the ROI border.     Captures: Spatial distribution relative to the boundary.</p> </li> <li> <p>NGTDM (Neighbourhood Grey Tone Difference Matrix):     Quantifies the difference between a voxel and its neighbours.     Captures: Human perception of texture (coarseness, contrast, busyness).</p> </li> <li> <p>NGLDM (Neighbourhood Grey Level Dependence Matrix):     Captures the dependence of grey levels on their neighbours.     Captures: Dependence, spatial relationships.</p> </li> </ol>"},{"location":"api/features/texture/#pictologics.features.texture--optimization","title":"Optimization:","text":"<p>This module uses <code>numba</code> for Just-In-Time (JIT) compilation to achieve high performance. The core calculations are parallelized and optimized for memory usage. - Single-pass calculation: Multiple matrices are computed in a single pass over the image   to minimize memory access overhead. - Flattened DFS: Zone-based features (GLSZM, GLDZM) use a memory-efficient Depth-First Search   with flattened stack indices.</p>"},{"location":"api/features/texture/#pictologics.features.texture--usage","title":"Usage:","text":"<p>The main entry point is <code>calculate_all_texture_matrices</code>, which computes all raw matrices. Then, specific feature calculation functions (e.g., <code>calculate_glcm_features</code>) can be called using these matrices.</p> Example <p>import numpy as np from pictologics.features.texture import calculate_all_texture_matrices, calculate_glcm_features</p>"},{"location":"api/features/texture/#pictologics.features.texture--create-dummy-data","title":"Create dummy data","text":"<p>data = np.random.randint(1, 33, (50, 50, 50)) mask = np.ones((50, 50, 50))</p>"},{"location":"api/features/texture/#pictologics.features.texture--calculate-matrices","title":"Calculate matrices","text":"<p>matrices = calculate_all_texture_matrices(data, mask, n_bins=32)</p>"},{"location":"api/features/texture/#pictologics.features.texture--extract-features","title":"Extract features","text":"<p>glcm_feats = calculate_glcm_features(data, mask, n_bins=32, glcm_matrix=matrices['glcm']) print(glcm_feats['contrast_ACUI'])</p>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_all_texture_features","title":"<code>calculate_all_texture_features(disc_array, mask_array, n_bins, distance_mask_array=None, ngldm_alpha=0)</code>","text":"<p>Calculate all texture features (GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM).</p> <p>This is a convenience wrapper that computes all texture matrices and then extracts all available features.</p> <p>Parameters:</p> Name Type Description Default <code>disc_array</code> <code>ndarray</code> <p>Discretised image array.</p> required <code>mask_array</code> <code>ndarray</code> <p>Mask array (ROI).</p> required <code>n_bins</code> <code>int</code> <p>Number of bins.</p> required <code>distance_mask_array</code> <code>Optional[ndarray]</code> <p>Optional mask for GLDZM distance calculation.                  If None, mask_array is used.</p> <code>None</code> <code>ngldm_alpha</code> <code>int</code> <p>The coarseness parameter \u03b1 for NGLDM. Two grey levels are considered dependent if their absolute difference is \u2264 \u03b1. Default is 0 (IBSI standard).</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of all texture features.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_all_texture_features(\n    disc_array: np.ndarray,\n    mask_array: np.ndarray,\n    n_bins: int,\n    distance_mask_array: Optional[np.ndarray] = None,\n    ngldm_alpha: int = 0,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate all texture features (GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM).\n\n    This is a convenience wrapper that computes all texture matrices and then\n    extracts all available features.\n\n    Args:\n        disc_array: Discretised image array.\n        mask_array: Mask array (ROI).\n        n_bins: Number of bins.\n        distance_mask_array: Optional mask for GLDZM distance calculation.\n                             If None, mask_array is used.\n        ngldm_alpha: The coarseness parameter \u03b1 for NGLDM. Two grey levels are considered\n            dependent if their absolute difference is \u2264 \u03b1. Default is 0 (IBSI standard).\n\n    Returns:\n        Dictionary of all texture features.\n    \"\"\"\n    results = {}\n\n    # Calculate all matrices once\n    texture_matrices = calculate_all_texture_matrices(\n        disc_array,\n        mask_array,\n        n_bins,\n        distance_mask=distance_mask_array,\n        ngldm_alpha=ngldm_alpha,\n    )\n\n    # GLCM\n    results.update(\n        calculate_glcm_features(\n            disc_array, mask_array, n_bins, glcm_matrix=texture_matrices[\"glcm\"]\n        )\n    )\n\n    # GLRLM\n    results.update(\n        calculate_glrlm_features(\n            disc_array, mask_array, n_bins, glrlm_matrix=texture_matrices[\"glrlm\"]\n        )\n    )\n\n    # GLSZM\n    results.update(\n        calculate_glszm_features(\n            disc_array, mask_array, n_bins, glszm_matrix=texture_matrices[\"glszm\"]\n        )\n    )\n\n    # GLDZM\n    results.update(\n        calculate_gldzm_features(\n            disc_array,\n            mask_array,\n            n_bins,\n            gldzm_matrix=texture_matrices[\"gldzm\"],\n            distance_mask=(\n                distance_mask_array if distance_mask_array is not None else mask_array\n            ),\n        )\n    )\n\n    # NGTDM\n    results.update(\n        calculate_ngtdm_features(\n            disc_array,\n            mask_array,\n            n_bins,\n            ngtdm_matrices=(texture_matrices[\"ngtdm_s\"], texture_matrices[\"ngtdm_n\"]),\n        )\n    )\n    # NGLDM\n    results.update(\n        calculate_ngldm_features(\n            disc_array, mask_array, n_bins, ngldm_matrix=texture_matrices[\"ngldm\"]\n        )\n    )\n\n    return results\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_all_texture_matrices","title":"<code>calculate_all_texture_matrices(data, mask, n_bins, distance_mask=None, ngldm_alpha=0)</code>","text":"<p>Calculate all texture matrices (GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM) in an optimized single pass.</p> <p>This function serves as the computational backbone for texture analysis. It computes the raw matrices required to extract specific texture features. By aggregating these calculations, it minimizes the number of passes over the image data, significantly improving performance.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The 3D image array containing discretised grey levels. Values should be integers in the range [1, n_bins].</p> required <code>mask</code> <code>ndarray</code> <p>The 3D binary mask array defining the Region of Interest (ROI). Must have the same shape as <code>data</code>. Non-zero values indicate the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels used for discretization (e.g., 16, 32, 64). This determines the size of the resulting matrices.</p> required <code>distance_mask</code> <code>Optional[ndarray]</code> <p>Optional mask used to calculate the distance map for GLDZM. If None, <code>mask</code> is used. This allows calculating distances based on the morphological mask while analyzing intensities from the intensity mask (e.g., after outlier filtering).</p> <code>None</code> <code>ngldm_alpha</code> <code>int</code> <p>The coarseness parameter \u03b1 for NGLDM calculation. Two grey levels are considered dependent if their absolute difference is \u2264 \u03b1. Default is 0 (exact match), which is the IBSI standard. Use \u03b1=1 for tolerance of \u00b11 grey level difference.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the calculated texture matrices: - 'glcm' (np.ndarray): Grey Level Co-occurrence Matrix. Shape: (n_dirs, n_bins, n_bins). - 'glrlm' (np.ndarray): Grey Level Run Length Matrix. Shape: (n_dirs, n_bins, max_run_length). - 'ngtdm_s' (np.ndarray): NGTDM Sum of absolute differences. Shape: (n_bins,). - 'ngtdm_n' (np.ndarray): NGTDM Number of valid voxels. Shape: (n_bins,). - 'ngldm' (np.ndarray): Neighbouring Grey Level Dependence Matrix. Shape: (n_bins, n_dependence). - 'glszm' (np.ndarray): Grey Level Size Zone Matrix. Shape: (n_bins, max_zone_size). - 'gldzm' (np.ndarray): Grey Level Distance Zone Matrix. Shape: (n_bins, max_distance).</p> Example <p>import numpy as np data = np.random.randint(1, 33, (50, 50, 50)) mask = np.ones((50, 50, 50)) matrices = calculate_all_texture_matrices(data, mask, n_bins=32) print(matrices['glcm'].shape) (13, 32, 32)</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_all_texture_matrices(\n    data: np.ndarray,\n    mask: np.ndarray,\n    n_bins: int,\n    distance_mask: Optional[np.ndarray] = None,\n    ngldm_alpha: int = 0,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Calculate all texture matrices (GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM) in an optimized single pass.\n\n    This function serves as the computational backbone for texture analysis. It computes the raw\n    matrices required to extract specific texture features. By aggregating these calculations,\n    it minimizes the number of passes over the image data, significantly improving performance.\n\n    Args:\n        data (np.ndarray): The 3D image array containing discretised grey levels.\n            Values should be integers in the range [1, n_bins].\n        mask (np.ndarray): The 3D binary mask array defining the Region of Interest (ROI).\n            Must have the same shape as `data`. Non-zero values indicate the ROI.\n        n_bins (int): The number of grey levels used for discretization (e.g., 16, 32, 64).\n            This determines the size of the resulting matrices.\n        distance_mask (Optional[np.ndarray]): Optional mask used to calculate the distance map for GLDZM.\n            If None, `mask` is used. This allows calculating distances based on the morphological mask\n            while analyzing intensities from the intensity mask (e.g., after outlier filtering).\n        ngldm_alpha (int): The coarseness parameter \u03b1 for NGLDM calculation. Two grey levels are\n            considered dependent if their absolute difference is \u2264 \u03b1. Default is 0 (exact match),\n            which is the IBSI standard. Use \u03b1=1 for tolerance of \u00b11 grey level difference.\n\n    Returns:\n        Dict[str, Any]: A dictionary containing the calculated texture matrices:\n            - 'glcm' (np.ndarray): Grey Level Co-occurrence Matrix. Shape: (n_dirs, n_bins, n_bins).\n            - 'glrlm' (np.ndarray): Grey Level Run Length Matrix. Shape: (n_dirs, n_bins, max_run_length).\n            - 'ngtdm_s' (np.ndarray): NGTDM Sum of absolute differences. Shape: (n_bins,).\n            - 'ngtdm_n' (np.ndarray): NGTDM Number of valid voxels. Shape: (n_bins,).\n            - 'ngldm' (np.ndarray): Neighbouring Grey Level Dependence Matrix. Shape: (n_bins, n_dependence).\n            - 'glszm' (np.ndarray): Grey Level Size Zone Matrix. Shape: (n_bins, max_zone_size).\n            - 'gldzm' (np.ndarray): Grey Level Distance Zone Matrix. Shape: (n_bins, max_distance).\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; data = np.random.randint(1, 33, (50, 50, 50))\n        &gt;&gt;&gt; mask = np.ones((50, 50, 50))\n        &gt;&gt;&gt; matrices = calculate_all_texture_matrices(data, mask, n_bins=32)\n        &gt;&gt;&gt; print(matrices['glcm'].shape)\n        (13, 32, 32)\n    \"\"\"\n    # Fast exit for empty ROI\n    if not bool(np.any(mask != 0)):\n        return {\n            \"glcm\": np.zeros((13, n_bins, n_bins), dtype=np.uint64),\n            \"glrlm\": np.zeros((13, n_bins, 1), dtype=np.uint64),\n            \"ngtdm_s\": np.zeros((n_bins,), dtype=np.float64),\n            \"ngtdm_n\": np.zeros((n_bins,), dtype=np.float64),\n            \"ngldm\": np.zeros((n_bins, 27), dtype=np.uint64),\n            \"glszm\": np.zeros((n_bins, 1), dtype=np.uint32),\n            \"gldzm\": np.zeros((n_bins, 1), dtype=np.uint32),\n        }\n\n    # Crop to ROI bounding box (union with distance_mask when provided) to reduce memory traffic.\n    data_c, mask_c, distmask_c = _maybe_crop_to_bbox(data, mask, distance_mask)\n\n    # Use a compact mask representation for kernels.\n    if mask_c.dtype == np.uint8:\n        mask_u8 = mask_c\n    else:\n        mask_u8 = (mask_c != 0).astype(np.uint8)\n    # 1. Local Features (GLCM, GLRLM, NGTDM, NGLDM)\n    # Pre-cast data to smallest possible int type (0-based)\n    # Input data is 1-based, so we subtract 1.\n    if n_bins &lt;= 256:\n        data_int = (data_c - 1).astype(np.uint8)\n    else:\n        data_int = (data_c - 1).astype(np.int32)\n\n    # Determine n_threads for JIT call\n    try:\n        n_threads = int(numba.config.NUMBA_NUM_THREADS)\n    except (ValueError, TypeError):\n        n_threads = 1  # Fallback\n\n    glcm, glrlm, ngtdm_s, ngtdm_n, ngldm = _calculate_local_features_numba(\n        data_int,\n        mask_u8,\n        n_bins,\n        calc_glcm=True,\n        calc_glrlm=True,\n        calc_ngtdm=True,\n        calc_ngldm=True,\n        offsets_26=OFFSETS_26,\n        directions_13=DIRECTIONS_13,\n        ngldm_alpha=ngldm_alpha,\n        n_threads=n_threads,\n    )\n\n    # 2. Zone Features (GLSZM, GLDZM)\n    # Pre-calculate distance map for GLDZM\n    # Use distance_mask if provided, else mask\n    d_mask = distmask_c if distmask_c is not None else mask_u8\n\n    # Pad the mask with 0s to ensure the image border is treated as an edge.\n    mask_bool = d_mask &gt; 0\n    mask_padded = np.pad(mask_bool, 1, mode=\"constant\", constant_values=0)\n    dist_map_padded = distance_transform_cdt(mask_padded, metric=\"taxicab\").astype(\n        np.int32\n    )\n    dist_map = dist_map_padded[1:-1, 1:-1, 1:-1]\n\n    glszm, gldzm = calculate_zone_features(\n        data_c,\n        mask_u8,\n        dist_map,\n        n_bins,\n        calc_glszm=True,\n        calc_gldzm=True,\n    )\n\n    return {\n        \"glcm\": glcm,\n        \"glrlm\": glrlm,\n        \"ngtdm_s\": ngtdm_s,\n        \"ngtdm_n\": ngtdm_n,\n        \"ngldm\": ngldm,\n        \"glszm\": glszm,\n        \"gldzm\": gldzm,\n    }\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_glcm_features","title":"<code>calculate_glcm_features(data, mask, n_bins, glcm_matrix=None)</code>","text":"<p>Calculate Grey Level Co-occurrence Matrix (GLCM) features.</p> <p>The GLCM describes the second-order statistical distribution of grey levels in the ROI. It counts how often pairs of grey levels occur at a specific distance and direction. This implementation computes features based on the 3D merged GLCM (averaged over all 13 directions), making the features rotationally invariant.</p> <p>IBSI Reference: Section 3.6 (Grey Level Co-occurrence Based Features).</p> <p>Mathematical Definition: Let $P(i,j)$ be the co-occurrence matrix, where $i$ and $j$ are grey levels. The matrix is normalized such that $\\sum_{i,j} P(i,j) = 1$.</p> <p>Calculated Features: *   Joint Maximum (GYBY) *   Joint Average (60VM) *   Joint Variance (UR99) *   Joint Entropy (TU9B) *   Difference Average (TF7R) *   Difference Variance (D3YU) *   Difference Entropy (NTRS) *   Sum Average (ZGXS) *   Sum Variance (OEEB) *   Sum Entropy (P6QZ) *   Angular Second Moment (8ZQL) *   Contrast (ACUI) *   Dissimilarity (8S9J) *   Inverse Difference (IB1Z) *   Normalised Inverse Difference (NDRX) *   Inverse Difference Moment (WF0Z) *   Normalised Inverse Difference Moment (1QCO) *   Inverse Variance (E8JP) *   Correlation (NI2N) *   Autocorrelation (QWB0) *   Cluster Tendency (DG8W) *   Cluster Shade (7NFM) *   Cluster Prominence (AE86) *   Information Correlation 1 (R8DG) *   Information Correlation 2 (JN9H)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The 3D image array containing discretised grey levels.</p> required <code>mask</code> <code>ndarray</code> <p>The 3D binary mask array defining the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels.</p> required <code>glcm_matrix</code> <code>Optional[ndarray]</code> <p>Pre-calculated GLCM matrix. If provided, <code>data</code> and <code>mask</code> are ignored for matrix calculation, but <code>data</code> is still used for <code>Ng</code> estimation if needed. If None, the matrix is calculated from scratch.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary of calculated GLCM features, keyed by their name and IBSI code. Example keys: 'joint_maximum_GYBY', 'contrast_ACUI', 'correlation_NI2N'.</p> Example <p>features = calculate_glcm_features(data, mask, n_bins=32) print(features['contrast_ACUI']) 12.5</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_glcm_features(\n    data: np.ndarray,\n    mask: np.ndarray,\n    n_bins: int,\n    glcm_matrix: Optional[np.ndarray] = None,\n) -&gt; Dict[str, float]:\n    r\"\"\"\n    Calculate Grey Level Co-occurrence Matrix (GLCM) features.\n\n    The GLCM describes the second-order statistical distribution of grey levels in the ROI.\n    It counts how often pairs of grey levels occur at a specific distance and direction.\n    This implementation computes features based on the 3D merged GLCM (averaged over all 13 directions),\n    making the features rotationally invariant.\n\n    **IBSI Reference**: Section 3.6 (Grey Level Co-occurrence Based Features).\n\n    **Mathematical Definition**:\n    Let $P(i,j)$ be the co-occurrence matrix, where $i$ and $j$ are grey levels.\n    The matrix is normalized such that $\\sum_{i,j} P(i,j) = 1$.\n\n    **Calculated Features**:\n    *   Joint Maximum (GYBY)\n    *   Joint Average (60VM)\n    *   Joint Variance (UR99)\n    *   Joint Entropy (TU9B)\n    *   Difference Average (TF7R)\n    *   Difference Variance (D3YU)\n    *   Difference Entropy (NTRS)\n    *   Sum Average (ZGXS)\n    *   Sum Variance (OEEB)\n    *   Sum Entropy (P6QZ)\n    *   Angular Second Moment (8ZQL)\n    *   Contrast (ACUI)\n    *   Dissimilarity (8S9J)\n    *   Inverse Difference (IB1Z)\n    *   Normalised Inverse Difference (NDRX)\n    *   Inverse Difference Moment (WF0Z)\n    *   Normalised Inverse Difference Moment (1QCO)\n    *   Inverse Variance (E8JP)\n    *   Correlation (NI2N)\n    *   Autocorrelation (QWB0)\n    *   Cluster Tendency (DG8W)\n    *   Cluster Shade (7NFM)\n    *   Cluster Prominence (AE86)\n    *   Information Correlation 1 (R8DG)\n    *   Information Correlation 2 (JN9H)\n\n    Args:\n        data (np.ndarray): The 3D image array containing discretised grey levels.\n        mask (np.ndarray): The 3D binary mask array defining the ROI.\n        n_bins (int): The number of grey levels.\n        glcm_matrix (Optional[np.ndarray]): Pre-calculated GLCM matrix. If provided, `data` and `mask`\n            are ignored for matrix calculation, but `data` is still used for `Ng` estimation if needed.\n            If None, the matrix is calculated from scratch.\n\n    Returns:\n        Dict[str, float]: A dictionary of calculated GLCM features, keyed by their name and IBSI code.\n            Example keys: 'joint_maximum_GYBY', 'contrast_ACUI', 'correlation_NI2N'.\n\n    Example:\n        &gt;&gt;&gt; features = calculate_glcm_features(data, mask, n_bins=32)\n        &gt;&gt;&gt; print(features['contrast_ACUI'])\n        12.5\n    \"\"\"\n    if glcm_matrix is None:\n        data_c, mask_c, _ = _maybe_crop_to_bbox(data, mask, None)\n        if mask_c.dtype == np.uint8:\n            mask_u8 = mask_c\n        else:\n            mask_u8 = (mask_c != 0).astype(np.uint8)\n\n        if n_bins &lt;= 256:\n            data_int = (data_c - 1).astype(np.uint8)\n        else:\n            data_int = (data_c - 1).astype(np.int32)\n\n        # Determine n_threads for JIT call\n        try:\n            n_threads = int(numba.config.NUMBA_NUM_THREADS)\n        except (ValueError, TypeError):\n            n_threads = 1  # Fallback\n\n        # Call combined kernel to calculate only GLCM\n        glcm, _, _, _, _ = _calculate_local_features_numba(\n            data_int,\n            mask_u8,\n            n_bins,\n            calc_glcm=True,\n            calc_glrlm=False,\n            calc_ngtdm=False,\n            calc_ngldm=False,\n            offsets_26=OFFSETS_26,\n            directions_13=DIRECTIONS_13,\n            ngldm_alpha=0,\n            n_threads=n_threads,\n        )\n    else:\n        glcm = glcm_matrix\n\n    # Merge (Sum) -&gt; IAZD\n    glcm_sum = np.sum(glcm, axis=0)\n    glcm_sym = glcm_sum + glcm_sum.T\n\n    # Normalize\n    total_sum = np.sum(glcm_sym)\n    if total_sum == 0:\n        return {}\n\n    P = glcm_sym / total_sum\n\n    # Indices (0-based from np.indices, convert to 1-based for IBSI)\n    i_idx, j_idx = np.indices((n_bins, n_bins))\n    I = i_idx + 1  # noqa: E741\n    J = j_idx + 1\n\n    features = {}\n\n    # Joint Maximum - GYBY\n    features[\"joint_maximum_GYBY\"] = np.max(P)\n\n    # Joint Average - 60VM\n    features[\"joint_average_60VM\"] = np.sum(I * P)\n\n    # Joint Variance - UR99\n    mu = features[\"joint_average_60VM\"]\n    features[\"joint_variance_UR99\"] = np.sum(((I - mu) ** 2) * P)\n\n    # Joint Entropy - TU9B\n    mask_p = P &gt; 0\n    features[\"joint_entropy_TU9B\"] = -np.sum(P[mask_p] * np.log2(P[mask_p]))\n\n    # Difference Average - TF7R\n    k_diff = np.abs(I - J)\n    features[\"difference_average_TF7R\"] = np.sum(k_diff * P)\n\n    # Optimized using bincount\n    k_diff_flat = k_diff.ravel().astype(np.int32)\n    P_flat = P.ravel()\n    p_diff = np.bincount(k_diff_flat, weights=P_flat)\n\n    mu_diff = features[\"difference_average_TF7R\"]\n    k_vals = np.arange(n_bins)\n    features[\"difference_variance_D3YU\"] = np.sum(((k_vals - mu_diff) ** 2) * p_diff)\n\n    # Difference Entropy - NTRS\n    mask_pd = p_diff &gt; 0\n    features[\"difference_entropy_NTRS\"] = -np.sum(\n        p_diff[mask_pd] * np.log2(p_diff[mask_pd])\n    )\n\n    # Sum Average - ZGXS\n    k_sum_grid = I + J\n\n    # Optimized using bincount\n    k_sum_flat = k_sum_grid.ravel().astype(np.int32)\n    # P_flat is already defined in Difference Variance block\n    p_sum_full = np.bincount(k_sum_flat, weights=P_flat)\n\n    # Slice from 2.\n    p_sum = p_sum_full[2:]\n\n    k_vals_sum = np.arange(2, 2 * n_bins + 1)\n    features[\"sum_average_ZGXS\"] = np.sum(k_vals_sum * p_sum)\n\n    # Sum Variance - OEEB\n    mu_sum = features[\"sum_average_ZGXS\"]\n    features[\"sum_variance_OEEB\"] = np.sum(((k_vals_sum - mu_sum) ** 2) * p_sum)\n\n    # Sum Entropy - P6QZ\n    mask_ps = p_sum &gt; 0\n    features[\"sum_entropy_P6QZ\"] = -np.sum(p_sum[mask_ps] * np.log2(p_sum[mask_ps]))\n\n    # Angular Second Moment (Energy) - 8ZQL\n    features[\"angular_second_moment_8ZQL\"] = np.sum(P**2)\n\n    # Contrast - ACUI\n    features[\"contrast_ACUI\"] = np.sum(((I - J) ** 2) * P)\n\n    # Dissimilarity - 8S9J\n    features[\"dissimilarity_8S9J\"] = np.sum(np.abs(I - J) * P)\n\n    # Inverse Difference - IB1Z\n    features[\"inverse_difference_IB1Z\"] = np.sum(P / (1 + np.abs(I - J)))\n\n    roi_data = data[mask &gt; 0]\n    if len(roi_data) &gt; 0:\n        Ng_eff = int(np.max(roi_data) - np.min(roi_data) + 1)\n    else:\n        Ng_eff = 1  # Fallback\n\n    # Normalised Inverse Difference - NDRX\n    features[\"normalised_inverse_difference_NDRX\"] = np.sum(\n        P / (1 + np.abs(I - J) / Ng_eff)\n    )\n\n    # Inverse Difference Moment - WF0Z\n    features[\"inverse_difference_moment_WF0Z\"] = np.sum(P / (1 + (I - J) ** 2))\n\n    # Normalised Inverse Difference Moment - 1QCO\n    features[\"normalised_inverse_difference_moment_1QCO\"] = np.sum(\n        P / (1 + ((I - J) ** 2) / (Ng_eff**2))\n    )\n\n    # Inverse Variance - E8JP\n    mask_neq = I != J\n    features[\"inverse_variance_E8JP\"] = np.sum(\n        P[mask_neq] / ((I[mask_neq] - J[mask_neq]) ** 2)\n    )\n\n    # Correlation - NI2N\n    term1 = np.sum((I - mu) * (J - mu) * P)\n    if features[\"joint_variance_UR99\"] != 0:\n        features[\"correlation_NI2N\"] = term1 / features[\"joint_variance_UR99\"]\n    else:\n        features[\"correlation_NI2N\"] = (\n            1.0  # Or NaN? IBSI doesn't specify for 0 variance.\n        )\n\n    # Autocorrelation - QWB0\n    features[\"autocorrelation_QWB0\"] = np.sum(I * J * P)\n\n    # Cluster Tendency - DG8W\n    features[\"cluster_tendency_DG8W\"] = np.sum(((I + J - 2 * mu) ** 2) * P)\n\n    # Cluster Shade - 7NFM\n    features[\"cluster_shade_7NFM\"] = np.sum(((I + J - 2 * mu) ** 3) * P)\n\n    # Cluster Prominence - AE86\n    features[\"cluster_prominence_AE86\"] = np.sum(((I + J - 2 * mu) ** 4) * P)\n\n    # Information Correlation 1 - R8DG\n    HXY = features[\"joint_entropy_TU9B\"]\n    p_x = np.sum(P, axis=1)\n    mask_px = p_x &gt; 0\n    HX = -np.sum(p_x[mask_px] * np.log2(p_x[mask_px]))\n\n    HXY1 = -np.sum(P[mask_p] * np.log2(p_x[I[mask_p] - 1] * p_x[J[mask_p] - 1]))\n\n    if HX != 0:\n        features[\"information_correlation_1_R8DG\"] = (HXY - HXY1) / HX\n    else:\n        features[\"information_correlation_1_R8DG\"] = np.nan\n\n    # Information Correlation 2 - JN9H\n    P_prod = np.outer(p_x, p_x)\n    mask_prod = P_prod &gt; 0\n    HXY2 = -np.sum(P_prod[mask_prod] * np.log2(P_prod[mask_prod]))\n\n    features[\"information_correlation_2_JN9H\"] = np.sqrt(1 - np.exp(-2 * (HXY2 - HXY)))\n    return features\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_gldzm_features","title":"<code>calculate_gldzm_features(data, mask, n_bins, gldzm_matrix=None, distance_mask=None)</code>","text":"<p>Calculate Grey Level Distance Zone Matrix (GLDZM) features.</p> <p>The GLDZM counts the number of zones of linked voxels with the same grey level, categorized by the distance of the zone from the ROI border. This captures information about the spatial distribution of textures relative to the boundary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The 3D image array containing discretised grey levels.</p> required <code>mask</code> <code>ndarray</code> <p>The 3D binary mask array defining the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels.</p> required <code>gldzm_matrix</code> <code>Optional[ndarray]</code> <p>Pre-calculated GLDZM matrix.</p> <code>None</code> <code>distance_mask</code> <code>Optional[ndarray]</code> <p>Optional mask used to calculate the distance map. If None, <code>mask</code> is used. This allows calculating distances based on the morphological mask while analyzing intensities from the intensity mask (e.g., after outlier filtering).</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary of calculated GLDZM features. Example keys: 'small_distance_emphasis_0GBI', 'zone_distance_entropy_GBDU'.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_gldzm_features(\n    data: np.ndarray,\n    mask: np.ndarray,\n    n_bins: int,\n    gldzm_matrix: Optional[np.ndarray] = None,\n    distance_mask: Optional[np.ndarray] = None,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate Grey Level Distance Zone Matrix (GLDZM) features.\n\n    The GLDZM counts the number of zones of linked voxels with the same grey level,\n    categorized by the distance of the zone from the ROI border.\n    This captures information about the spatial distribution of textures relative to the boundary.\n\n    Args:\n        data (np.ndarray): The 3D image array containing discretised grey levels.\n        mask (np.ndarray): The 3D binary mask array defining the ROI.\n        n_bins (int): The number of grey levels.\n        gldzm_matrix (Optional[np.ndarray]): Pre-calculated GLDZM matrix.\n        distance_mask (Optional[np.ndarray]): Optional mask used to calculate the distance map.\n            If None, `mask` is used. This allows calculating distances based on the morphological mask\n            while analyzing intensities from the intensity mask (e.g., after outlier filtering).\n\n    Returns:\n        Dict[str, float]: A dictionary of calculated GLDZM features.\n            Example keys: 'small_distance_emphasis_0GBI', 'zone_distance_entropy_GBDU'.\n    \"\"\"\n    if gldzm_matrix is None:\n        # Calculate distance map\n        # Use distance_mask if provided, else mask\n        d_mask = distance_mask if distance_mask is not None else mask\n\n        # Pad the mask with 0s to ensure the image border is treated as an edge.\n        mask_bool = d_mask &gt; 0\n        mask_padded = np.pad(mask_bool, 1, mode=\"constant\", constant_values=0)\n        dist_map_padded = distance_transform_cdt(mask_padded, metric=\"taxicab\").astype(\n            np.int32\n        )\n        dist_map = dist_map_padded[1:-1, 1:-1, 1:-1]\n\n        _, gldzm = calculate_zone_features(\n            data,\n            mask,\n            dist_map,\n            n_bins,\n            calc_glszm=False,\n            calc_gldzm=True,\n        )\n    else:\n        gldzm = gldzm_matrix\n\n    N_zones = np.sum(gldzm)\n    if N_zones == 0:\n        return {}\n\n    P = gldzm / N_zones\n\n    n_g, n_d = gldzm.shape\n    i_idx, j_idx = np.indices((n_g, n_d))\n    I = i_idx + 1  # noqa: E741\n    J = j_idx + 1  # Distance\n\n    features = {}\n\n    # Small Distance Emphasis (SDE) - 0GBI\n    features[\"small_distance_emphasis_0GBI\"] = np.sum(P / (J**2))\n\n    # Large Distance Emphasis (LDE) - MB4I\n    features[\"large_distance_emphasis_MB4I\"] = np.sum(P * (J**2))\n\n    # Grey Level Non-Uniformity (GLNU) - VFT7\n    s_i = np.sum(gldzm, axis=1)\n    features[\"grey_level_non_uniformity_VFT7\"] = np.sum(s_i**2) / N_zones\n\n    # Normalised Grey Level Non-Uniformity (GLNN) - 7HP3\n    features[\"normalised_grey_level_non_uniformity_7HP3\"] = np.sum(s_i**2) / (\n        N_zones**2\n    )\n\n    # Zone Distance Non-Uniformity (ZDNU) - V294\n    s_j = np.sum(gldzm, axis=0)\n    features[\"zone_distance_non_uniformity_V294\"] = np.sum(s_j**2) / N_zones\n\n    # Normalised Zone Distance Non-Uniformity (ZDNN) - IATH\n    features[\"normalised_zone_distance_non_uniformity_IATH\"] = np.sum(s_j**2) / (\n        N_zones**2\n    )\n\n    # Zone Percentage (ZP) - VIWW\n    n_voxels = np.sum(mask)\n    features[\"zone_percentage_VIWW\"] = N_zones / n_voxels\n\n    # Grey Level Variance (GLV) - QK93\n    mu_i = np.sum(I * P)\n    features[\"grey_level_variance_QK93\"] = np.sum(((I - mu_i) ** 2) * P)\n\n    # Zone Distance Variance (ZDV) - 7WT1\n    mu_j = np.sum(J * P)\n    features[\"zone_distance_variance_7WT1\"] = np.sum(((J - mu_j) ** 2) * P)\n\n    # Zone Distance Entropy (ZDE) - GBDU\n    mask_p = P &gt; 0\n    features[\"zone_distance_entropy_GBDU\"] = -np.sum(P[mask_p] * np.log2(P[mask_p]))\n\n    # Low Grey Level Zone Emphasis (LGLZE) - S1RA\n    features[\"low_grey_level_zone_emphasis_S1RA\"] = np.sum(P / (I**2))\n\n    # High Grey Level Zone Emphasis (HGLZE) - K26C\n    features[\"high_grey_level_zone_emphasis_K26C\"] = np.sum(P * (I**2))\n\n    # Small Distance Low Grey Level Emphasis (SDLGLE) - RUVG\n    features[\"small_distance_low_grey_level_emphasis_RUVG\"] = np.sum(\n        P / ((I**2) * (J**2))\n    )\n\n    # Small Distance High Grey Level Emphasis (SDHGLE) - DKNJ\n    features[\"small_distance_high_grey_level_emphasis_DKNJ\"] = np.sum(\n        P * (I**2) / (J**2)\n    )\n\n    # Large Distance Low Grey Level Emphasis (LDLGLE) - A7WM\n    features[\"large_distance_low_grey_level_emphasis_A7WM\"] = np.sum(\n        P * (J**2) / (I**2)\n    )\n\n    # Large Distance High Grey Level Emphasis (LDHGLE) - KLTH\n    features[\"large_distance_high_grey_level_emphasis_KLTH\"] = np.sum(\n        P * (I**2) * (J**2)\n    )\n\n    return features\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_glrlm_features","title":"<code>calculate_glrlm_features(data, mask, n_bins, glrlm_matrix=None)</code>","text":"<p>Calculate Grey Level Run Length Matrix (GLRLM) features.</p> <p>The GLRLM quantifies grey level runs, which are defined as the length in number of pixels, of consecutive pixels that have the same grey level value. This implementation computes features based on the 3D merged GLRLM (averaged over all 13 directions).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The 3D image array containing discretised grey levels.</p> required <code>mask</code> <code>ndarray</code> <p>The 3D binary mask array defining the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels.</p> required <code>glrlm_matrix</code> <code>Optional[ndarray]</code> <p>Pre-calculated GLRLM matrix.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary of calculated GLRLM features. Example keys: 'short_runs_emphasis_22OV', 'grey_level_non_uniformity_R5YN'.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_glrlm_features(\n    data: np.ndarray,\n    mask: np.ndarray,\n    n_bins: int,\n    glrlm_matrix: Optional[np.ndarray] = None,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate Grey Level Run Length Matrix (GLRLM) features.\n\n    The GLRLM quantifies grey level runs, which are defined as the length in number of pixels,\n    of consecutive pixels that have the same grey level value.\n    This implementation computes features based on the 3D merged GLRLM (averaged over all 13 directions).\n\n    Args:\n        data (np.ndarray): The 3D image array containing discretised grey levels.\n        mask (np.ndarray): The 3D binary mask array defining the ROI.\n        n_bins (int): The number of grey levels.\n        glrlm_matrix (Optional[np.ndarray]): Pre-calculated GLRLM matrix.\n\n    Returns:\n        Dict[str, float]: A dictionary of calculated GLRLM features.\n            Example keys: 'short_runs_emphasis_22OV', 'grey_level_non_uniformity_R5YN'.\n    \"\"\"\n    if glrlm_matrix is None:\n        if n_bins &lt;= 256:\n            data_int = (data - 1).astype(np.uint8)\n        else:\n            data_int = (data - 1).astype(np.int32)  # pragma: no cover\n\n        # Determine n_threads for JIT call\n        try:\n            n_threads = int(numba.config.NUMBA_NUM_THREADS)\n        except (ValueError, TypeError):\n            n_threads = 1  # Fallback\n\n        # Call combined kernel\n        _, glrlm, _, _, _ = _calculate_local_features_numba(\n            data_int,\n            mask,\n            n_bins,\n            calc_glcm=False,\n            calc_glrlm=True,\n            calc_ngtdm=False,\n            calc_ngldm=False,\n            offsets_26=OFFSETS_26,\n            directions_13=DIRECTIONS_13,\n            ngldm_alpha=0,\n            n_threads=n_threads,\n        )\n    else:\n        glrlm = glrlm_matrix\n\n    # Merge (Sum) -&gt; IAZD\n    glrlm_sum = np.sum(glrlm, axis=0)\n\n    # Remove length 0 (column 0)\n    glrlm = glrlm_sum[:, 1:]\n\n    N_runs = np.sum(glrlm)\n    if N_runs == 0:\n        return {}\n\n    P = glrlm / N_runs\n\n    n_g, n_r = glrlm.shape\n    i_idx, j_idx = np.indices((n_g, n_r))\n    I = i_idx + 1  # noqa: E741\n    J = j_idx + 1\n\n    features = {}\n\n    # Short Run Emphasis (SRE) - 22OV\n    features[\"short_runs_emphasis_22OV\"] = np.sum(P / (J**2))\n\n    # Long Run Emphasis (LRE) - W4KF\n    features[\"long_runs_emphasis_W4KF\"] = np.sum(P * (J**2))\n\n    # Grey Level Non-Uniformity (GLNU) - R5YN\n    s_i = np.sum(glrlm, axis=1)\n    features[\"grey_level_non_uniformity_R5YN\"] = np.sum(s_i**2) / N_runs\n\n    # Normalised Grey Level Non-Uniformity (GLNN) - OVBL\n    features[\"normalised_grey_level_non_uniformity_OVBL\"] = np.sum(s_i**2) / (N_runs**2)\n\n    # Run Length Non-Uniformity (RLNU) - W92Y\n    s_j = np.sum(glrlm, axis=0)\n    features[\"run_length_non_uniformity_W92Y\"] = np.sum(s_j**2) / N_runs\n\n    # Normalised Run Length Non-Uniformity (RLNN) - IC23\n    features[\"normalised_run_length_non_uniformity_IC23\"] = np.sum(s_j**2) / (N_runs**2)\n\n    # Run Percentage (RP) - 9ZK5\n    n_voxels = np.sum(mask)\n    n_dirs = 13  # Fixed for 3D\n    features[\"run_percentage_9ZK5\"] = N_runs / (n_voxels * n_dirs)\n\n    # Grey Level Variance (GLV) - 8CE5\n    mu_i = np.sum(I * P)\n    features[\"grey_level_variance_8CE5\"] = np.sum(((I - mu_i) ** 2) * P)\n\n    # Run Length Variance (RLV) - SXLW\n    mu_j = np.sum(J * P)\n    features[\"run_length_variance_SXLW\"] = np.sum(((J - mu_j) ** 2) * P)\n\n    # Run Entropy (RE) - HJ9O\n    mask_p = P &gt; 0\n    features[\"run_entropy_HJ9O\"] = -np.sum(P[mask_p] * np.log2(P[mask_p]))\n\n    # Low Grey Level Run Emphasis (LGLRE) - V3SW\n    features[\"low_grey_level_run_emphasis_V3SW\"] = np.sum(P / (I**2))\n\n    # High Grey Level Run Emphasis (HGLRE) - G3QZ\n    features[\"high_grey_level_run_emphasis_G3QZ\"] = np.sum(P * (I**2))\n\n    # Short Run Low Grey Level Emphasis (SRLGLE) - HTZT\n    features[\"short_run_low_grey_level_emphasis_HTZT\"] = np.sum(P / ((I**2) * (J**2)))\n\n    # Short Run High Grey Level Emphasis (SRHGLE) - GD3A\n    features[\"short_run_high_grey_level_emphasis_GD3A\"] = np.sum(P * (I**2) / (J**2))\n\n    # Long Run Low Grey Level Emphasis (LRLGLE) - IVPO\n    features[\"long_run_low_grey_level_emphasis_IVPO\"] = np.sum(P * (J**2) / (I**2))\n\n    # Long Run High Grey Level Emphasis (LRHGLE) - 3KUM\n    features[\"long_run_high_grey_level_emphasis_3KUM\"] = np.sum(P * (I**2) * (J**2))\n\n    return features\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_glszm_features","title":"<code>calculate_glszm_features(data, mask, n_bins, glszm_matrix=None)</code>","text":"<p>Calculate Grey Level Size Zone Matrix (GLSZM) features.</p> <p>The GLSZM counts the number of zones (connected components) of linked voxels that share the same grey level intensity. A zone is defined as a group of connected voxels with the same grey level. This matrix is rotationally invariant by definition.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The 3D image array containing discretised grey levels.</p> required <code>mask</code> <code>ndarray</code> <p>The 3D binary mask array defining the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels.</p> required <code>glszm_matrix</code> <code>Optional[ndarray]</code> <p>Pre-calculated GLSZM matrix.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary of calculated GLSZM features. Example keys: 'small_zone_emphasis_P001', 'zone_percentage_P30P'.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_glszm_features(\n    data: np.ndarray,\n    mask: np.ndarray,\n    n_bins: int,\n    glszm_matrix: Optional[np.ndarray] = None,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate Grey Level Size Zone Matrix (GLSZM) features.\n\n    The GLSZM counts the number of zones (connected components) of linked voxels\n    that share the same grey level intensity. A zone is defined as a group of connected voxels\n    with the same grey level. This matrix is rotationally invariant by definition.\n\n    Args:\n        data (np.ndarray): The 3D image array containing discretised grey levels.\n        mask (np.ndarray): The 3D binary mask array defining the ROI.\n        n_bins (int): The number of grey levels.\n        glszm_matrix (Optional[np.ndarray]): Pre-calculated GLSZM matrix.\n\n    Returns:\n        Dict[str, float]: A dictionary of calculated GLSZM features.\n            Example keys: 'small_zone_emphasis_P001', 'zone_percentage_P30P'.\n    \"\"\"\n    if glszm_matrix is None:\n        data_c, mask_c, _ = _maybe_crop_to_bbox(data, mask, None)\n        if mask_c.dtype == np.uint8:\n            mask_u8 = mask_c\n        else:\n            mask_u8 = (mask_c != 0).astype(np.uint8)\n\n        # We need dist_map for the combined kernel signature, even if unused for GLSZM\n        dummy_dist = np.zeros_like(data_c, dtype=np.int32)\n\n        glszm, _ = calculate_zone_features(\n            data_c, mask_u8, dummy_dist, n_bins, calc_glszm=True, calc_gldzm=False\n        )\n    else:\n        glszm = glszm_matrix\n\n    N_zones = np.sum(glszm)\n    if N_zones == 0:\n        return {}\n\n    P = glszm / N_zones\n\n    n_g, n_s = glszm.shape\n    i_idx, j_idx = np.indices((n_g, n_s))\n    I = i_idx + 1  # noqa: E741\n    J = j_idx + 1  # Zone size\n\n    features = {}\n\n    # Small Zone Emphasis (SZE) - P001\n    features[\"small_zone_emphasis_P001\"] = np.sum(P / (J**2))\n\n    # Large Zone Emphasis (LZE) - 48P8\n    features[\"large_zone_emphasis_48P8\"] = np.sum(P * (J**2))\n\n    # Grey Level Non-Uniformity (GLNU) - JNSA\n    s_i = np.sum(glszm, axis=1)\n    features[\"grey_level_non_uniformity_JNSA\"] = np.sum(s_i**2) / N_zones\n\n    # Normalised Grey Level Non-Uniformity (GLNN) - Y1RO\n    features[\"normalised_grey_level_non_uniformity_Y1RO\"] = np.sum(s_i**2) / (\n        N_zones**2\n    )\n\n    # Zone Size Non-Uniformity (ZSNU) - 4JP3\n    s_j = np.sum(glszm, axis=0)\n    features[\"zone_size_non_uniformity_4JP3\"] = np.sum(s_j**2) / N_zones\n\n    # Normalised Zone Size Non-Uniformity (ZSNN) - VB3A\n    features[\"normalised_zone_size_non_uniformity_VB3A\"] = np.sum(s_j**2) / (N_zones**2)\n\n    # Zone Percentage (ZP) - P30P\n    n_voxels = np.sum(mask)\n    features[\"zone_percentage_P30P\"] = N_zones / n_voxels\n\n    # Grey Level Variance (GLV) - BYLV\n    mu_i = np.sum(I * P)\n    features[\"grey_level_variance_BYLV\"] = np.sum(((I - mu_i) ** 2) * P)\n\n    # Zone Size Variance (ZSV) - 3NSA\n    mu_j = np.sum(J * P)\n    features[\"zone_size_variance_3NSA\"] = np.sum(((J - mu_j) ** 2) * P)\n\n    # Zone Size Entropy (ZSE) - GU8N\n    mask_p = P &gt; 0\n    features[\"zone_size_entropy_GU8N\"] = -np.sum(P[mask_p] * np.log2(P[mask_p]))\n\n    # Low Grey Level Zone Emphasis (LGLZE) - XMSY\n    features[\"low_grey_level_zone_emphasis_XMSY\"] = np.sum(P / (I**2))\n\n    # High Grey Level Zone Emphasis (HGLZE) - 5GN9\n    features[\"high_grey_level_zone_emphasis_5GN9\"] = np.sum(P * (I**2))\n\n    # Small Zone Low Grey Level Emphasis (SZLGLE) - 5RAI\n    features[\"small_zone_low_grey_level_emphasis_5RAI\"] = np.sum(P / ((I**2) * (J**2)))\n\n    # Small Zone High Grey Level Emphasis (SZHGLE) - HW1V\n    features[\"small_zone_high_grey_level_emphasis_HW1V\"] = np.sum(P * (I**2) / (J**2))\n\n    # Large Zone Low Grey Level Emphasis (LZLGLE) - YH51\n    features[\"large_zone_low_grey_level_emphasis_YH51\"] = np.sum(P * (J**2) / (I**2))\n\n    # Large Zone High Grey Level Emphasis (LZHGLE) - J17V\n    features[\"large_zone_high_grey_level_emphasis_J17V\"] = np.sum(P * (I**2) * (J**2))\n\n    return features\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_ngldm_features","title":"<code>calculate_ngldm_features(data, mask, n_bins, ngldm_matrix=None, ngldm_alpha=0)</code>","text":"<p>Calculate Neighbourhood Grey Level Dependence Matrix (NGLDM) features.</p> <p>The NGLDM captures the dependence of grey levels on their neighbours. A \"dependence\" is defined as a connected voxel having a similar grey level (within a tolerance \u03b1).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The 3D image array containing discretised grey levels.</p> required <code>mask</code> <code>ndarray</code> <p>The 3D binary mask array defining the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels.</p> required <code>ngldm_matrix</code> <code>Optional[ndarray]</code> <p>Pre-calculated NGLDM matrix.</p> <code>None</code> <code>ngldm_alpha</code> <code>int</code> <p>The coarseness parameter \u03b1. Two grey levels are considered dependent if their absolute difference is \u2264 \u03b1. Default is 0 (exact match, IBSI standard).</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary of calculated NGLDM features. Example keys: 'low_dependence_emphasis_SODN', 'dependence_count_entropy_FCBV'.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_ngldm_features(\n    data: np.ndarray,\n    mask: np.ndarray,\n    n_bins: int,\n    ngldm_matrix: Optional[np.ndarray] = None,\n    ngldm_alpha: int = 0,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate Neighbourhood Grey Level Dependence Matrix (NGLDM) features.\n\n    The NGLDM captures the dependence of grey levels on their neighbours.\n    A \"dependence\" is defined as a connected voxel having a similar grey level (within a tolerance \u03b1).\n\n    Args:\n        data (np.ndarray): The 3D image array containing discretised grey levels.\n        mask (np.ndarray): The 3D binary mask array defining the ROI.\n        n_bins (int): The number of grey levels.\n        ngldm_matrix (Optional[np.ndarray]): Pre-calculated NGLDM matrix.\n        ngldm_alpha (int): The coarseness parameter \u03b1. Two grey levels are considered dependent\n            if their absolute difference is \u2264 \u03b1. Default is 0 (exact match, IBSI standard).\n\n    Returns:\n        Dict[str, float]: A dictionary of calculated NGLDM features.\n            Example keys: 'low_dependence_emphasis_SODN', 'dependence_count_entropy_FCBV'.\n    \"\"\"\n    if ngldm_matrix is None:\n        if n_bins &lt;= 256:\n            data_int = (data - 1).astype(np.uint8)\n        elif n_bins &lt;= 65536:\n            data_int = (data - 1).astype(np.uint16)\n        else:\n            data_int = (data - 1).astype(np.int32)  # pragma: no cover\n\n        # Determine n_threads for JIT call\n        try:\n            n_threads = int(numba.config.NUMBA_NUM_THREADS)\n        except (ValueError, TypeError):\n            n_threads = 1  # Fallback\n\n        _, _, _, _, ngldm = _calculate_local_features_numba(\n            data_int,\n            mask,\n            n_bins,\n            calc_glcm=False,\n            calc_glrlm=False,\n            calc_ngtdm=False,\n            calc_ngldm=True,\n            offsets_26=OFFSETS_26,\n            directions_13=DIRECTIONS_13,\n            ngldm_alpha=ngldm_alpha,\n            n_threads=n_threads,\n        )\n    else:\n        ngldm = ngldm_matrix\n\n    N_s = np.sum(ngldm)\n    if N_s == 0:\n        return {}\n\n    P = ngldm / N_s\n\n    n_g, n_d = ngldm.shape\n    i_idx, j_idx = np.indices((n_g, n_d))\n    I = i_idx + 1  # noqa: E741\n    J = j_idx + 1  # Dependence count\n\n    features = {}\n\n    # Low Dependence Emphasis (LDE) - SODN\n    features[\"low_dependence_emphasis_SODN\"] = np.sum(P / (J**2))\n\n    # High Dependence Emphasis (HDE) - IMOQ\n    features[\"high_dependence_emphasis_IMOQ\"] = np.sum(P * (J**2))\n\n    # Low Grey Level Count Emphasis (LGCE) - TL9H\n    features[\"low_grey_level_count_emphasis_TL9H\"] = np.sum(P / (I**2))\n\n    # High Grey Level Count Emphasis (HGCE) - OAE7\n    features[\"high_grey_level_count_emphasis_OAE7\"] = np.sum(P * (I**2))\n\n    # Low Dependence Low Grey Level Emphasis (LDLGE) - EQ3F\n    features[\"low_dependence_low_grey_level_emphasis_EQ3F\"] = np.sum(\n        P / ((I**2) * (J**2))\n    )\n\n    # Low Dependence High Grey Level Emphasis (LDHGE) - JA6D\n    features[\"low_dependence_high_grey_level_emphasis_JA6D\"] = np.sum(\n        P * (I**2) / (J**2)\n    )\n\n    # High Dependence Low Grey Level Emphasis (HDLGE) - NBZI\n    features[\"high_dependence_low_grey_level_emphasis_NBZI\"] = np.sum(\n        P * (J**2) / (I**2)\n    )\n\n    # High Dependence High Grey Level Emphasis (HDHGE) - 9QMG\n    features[\"high_dependence_high_grey_level_emphasis_9QMG\"] = np.sum(\n        P * (I**2) * (J**2)\n    )\n\n    # Grey Level Non-Uniformity - FP8K\n    s_i = np.sum(ngldm, axis=1)\n    features[\"grey_level_non_uniformity_FP8K\"] = np.sum(s_i**2) / N_s\n\n    # Normalised Grey Level Non-Uniformity - 5SPA\n    features[\"normalised_grey_level_non_uniformity_5SPA\"] = np.sum(s_i**2) / (N_s**2)\n\n    # Dependence Count Non-Uniformity - Z87G\n    s_j = np.sum(ngldm, axis=0)\n    features[\"dependence_count_non_uniformity_Z87G\"] = np.sum(s_j**2) / N_s\n\n    # Normalised Dependence Count Non-Uniformity - OKJI\n    features[\"normalised_dependence_count_non_uniformity_OKJI\"] = np.sum(s_j**2) / (\n        N_s**2\n    )\n\n    # Dependence Count Percentage - 6XV8\n    n_voxels = np.sum(mask)\n    features[\"dependence_count_percentage_6XV8\"] = N_s / n_voxels\n\n    # Grey Level Variance - 1PFV\n    mu_i = np.sum(I * P)\n    features[\"grey_level_variance_1PFV\"] = np.sum(((I - mu_i) ** 2) * P)\n\n    # Dependence Count Variance - DNX2\n    mu_j = np.sum(J * P)\n    features[\"dependence_count_variance_DNX2\"] = np.sum(((J - mu_j) ** 2) * P)\n\n    # Dependence Count Entropy - FCBV\n    mask_p = P &gt; 0\n    features[\"dependence_count_entropy_FCBV\"] = -np.sum(P[mask_p] * np.log2(P[mask_p]))\n\n    # Dependence Count Energy - CAS9\n    features[\"dependence_count_energy_CAS9\"] = np.sum(P**2)\n\n    return features\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_ngtdm_features","title":"<code>calculate_ngtdm_features(data, mask, n_bins, ngtdm_matrices=None)</code>","text":"<p>Calculate Neighbourhood Grey Tone Difference Matrix (NGTDM) features.</p> <p>The NGTDM quantifies the difference between a grey value and the average grey value of its neighbours. It captures the coarseness and contrast of the texture.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>The 3D image array containing discretised grey levels.</p> required <code>mask</code> <code>ndarray</code> <p>The 3D binary mask array defining the ROI.</p> required <code>n_bins</code> <code>int</code> <p>The number of grey levels.</p> required <code>ngtdm_matrices</code> <code>Optional[Tuple[ndarray, ndarray]]</code> <p>Pre-calculated NGTDM matrices (sum of absolute differences <code>s</code>, and count <code>n</code>).</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, float]: A dictionary of calculated NGTDM features. Example keys: 'coarseness_QCDE', 'contrast_65HE', 'busyness_NQ30'.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_ngtdm_features(\n    data: np.ndarray,\n    mask: np.ndarray,\n    n_bins: int,\n    ngtdm_matrices: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Calculate Neighbourhood Grey Tone Difference Matrix (NGTDM) features.\n\n    The NGTDM quantifies the difference between a grey value and the average grey value\n    of its neighbours. It captures the coarseness and contrast of the texture.\n\n    Args:\n        data (np.ndarray): The 3D image array containing discretised grey levels.\n        mask (np.ndarray): The 3D binary mask array defining the ROI.\n        n_bins (int): The number of grey levels.\n        ngtdm_matrices (Optional[Tuple[np.ndarray, np.ndarray]]): Pre-calculated NGTDM matrices\n            (sum of absolute differences `s`, and count `n`).\n\n    Returns:\n        Dict[str, float]: A dictionary of calculated NGTDM features.\n            Example keys: 'coarseness_QCDE', 'contrast_65HE', 'busyness_NQ30'.\n    \"\"\"\n    if ngtdm_matrices is None:\n        if n_bins &lt;= 256:\n            data_int = (data - 1).astype(np.uint8)\n        elif n_bins &lt;= 65536:\n            data_int = (data - 1).astype(np.uint16)\n        else:\n            data_int = (data - 1).astype(np.int32)  # pragma: no cover\n\n        # Determine n_threads for JIT call\n        try:\n            n_threads = int(numba.config.NUMBA_NUM_THREADS)\n        except (ValueError, TypeError):\n            n_threads = 1  # Fallback\n\n        _, _, s, n, _ = _calculate_local_features_numba(\n            data_int,\n            mask,\n            n_bins,\n            calc_glcm=False,\n            calc_glrlm=False,\n            calc_ngtdm=True,\n            calc_ngldm=False,\n            offsets_26=OFFSETS_26,\n            directions_13=DIRECTIONS_13,\n            ngldm_alpha=0,\n            n_threads=n_threads,\n        )\n    else:\n        s, n = ngtdm_matrices\n\n    # s[i] is sum of absolute differences for grey level i+1\n    # n[i] is number of voxels of grey level i+1 with valid neighborhood\n\n    N_vp = np.sum(n)\n    if N_vp == 0:\n        return {}\n\n    p = n / N_vp\n\n    # Indices\n    i_idx = np.arange(n_bins)\n    I = i_idx + 1  # noqa: E741\n\n    features = {}\n\n    # Filter for non-zero probabilities (required for Busyness, Complexity, Strength)\n    mask_p = p &gt; 0\n    p_nz = p[mask_p]\n    s_nz = s[mask_p]\n    I_nz = I[mask_p]\n\n    # Coarseness - QCDE\n    sum_ps = np.sum(p_nz * s_nz)\n    if sum_ps &gt; 1e-10:\n        features[\"coarseness_QCDE\"] = 1 / sum_ps\n    else:\n        features[\"coarseness_QCDE\"] = 1e6\n\n    # Contrast - 65HE\n    Ng_p = len(p_nz)\n\n    if Ng_p &gt; 1:\n        # Term 1: Dynamic range variance\n        Pi, Pj = np.meshgrid(p_nz, p_nz, indexing=\"ij\")\n        Ii, Ij = np.meshgrid(I_nz, I_nz, indexing=\"ij\")\n\n        term1_sum = np.sum(Pi * Pj * ((Ii - Ij) ** 2))\n        term1 = term1_sum / (Ng_p * (Ng_p - 1))\n\n        # Term 2: Intensity change\n        sum_s = np.sum(s)\n        term2 = sum_s / N_vp\n\n        features[\"contrast_65HE\"] = term1 * term2\n    else:\n        features[\"contrast_65HE\"] = 0.0\n\n    # Busyness - NQ30\n    IPi = I_nz * p_nz\n\n    # Grid\n    IPi_grid, IPj_grid = np.meshgrid(IPi, IPi, indexing=\"ij\")\n    denom_busyness = np.sum(np.abs(IPi_grid - IPj_grid))\n\n    if denom_busyness &gt; 1e-10:\n        features[\"busyness_NQ30\"] = sum_ps / denom_busyness\n    else:\n        features[\"busyness_NQ30\"] = 0.0\n\n    # Complexity - HDEZ\n    Pi, Pj = np.meshgrid(p_nz, p_nz, indexing=\"ij\")\n    Si, Sj = np.meshgrid(s_nz, s_nz, indexing=\"ij\")\n    Ii, Ij = np.meshgrid(I_nz, I_nz, indexing=\"ij\")\n\n    denom_comp = Pi + Pj\n    term_comp = np.abs(Ii - Ij) * (Pi * Si + Pj * Sj) / denom_comp\n\n    features[\"complexity_HDEZ\"] = (1 / N_vp) * np.sum(term_comp)\n\n    # Strength - 1X9X\n    sum_s = np.sum(s)\n\n    term_str = (Pi + Pj) * ((Ii - Ij) ** 2)\n    sum_term_str = np.sum(term_str)\n\n    if sum_s &gt; 1e-10:\n        features[\"strength_1X9X\"] = sum_term_str / sum_s\n    else:\n        features[\"strength_1X9X\"] = 0.0\n\n    return features\n</code></pre>"},{"location":"api/features/texture/#pictologics.features.texture.calculate_zone_features","title":"<code>calculate_zone_features(data, mask, dist_map, n_bins, calc_glszm=True, calc_gldzm=True)</code>","text":"<p>Wrapper for _calculate_zone_features_numba with buffer pooling.</p> <p>This function manages pre-allocated buffers to reduce memory allocation overhead for repeated calls (e.g., during batch processing).</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>3D discretized image data.</p> required <code>mask</code> <code>ndarray</code> <p>3D mask array (not modified - copied internally by JIT function).</p> required <code>dist_map</code> <code>ndarray</code> <p>3D distance map for GLDZM.</p> required <code>n_bins</code> <code>int</code> <p>Number of grey level bins.</p> required <code>calc_glszm</code> <code>bool</code> <p>Whether to calculate GLSZM.</p> <code>True</code> <code>calc_gldzm</code> <code>bool</code> <p>Whether to calculate GLDZM.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple of (glszm, gldzm) matrices.</p> Source code in <code>pictologics/features/texture.py</code> <pre><code>def calculate_zone_features(\n    data: np.ndarray,\n    mask: np.ndarray,\n    dist_map: np.ndarray,\n    n_bins: int,\n    calc_glszm: bool = True,\n    calc_gldzm: bool = True,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Wrapper for _calculate_zone_features_numba with buffer pooling.\n\n    This function manages pre-allocated buffers to reduce memory allocation\n    overhead for repeated calls (e.g., during batch processing).\n\n    Args:\n        data: 3D discretized image data.\n        mask: 3D mask array (not modified - copied internally by JIT function).\n        dist_map: 3D distance map for GLDZM.\n        n_bins: Number of grey level bins.\n        calc_glszm: Whether to calculate GLSZM.\n        calc_gldzm: Whether to calculate GLDZM.\n\n    Returns:\n        Tuple of (glszm, gldzm) matrices.\n    \"\"\"\n    # For zone features, the worst-case number of zones is bounded by ROI voxel count.\n    # Sizing buffers to full image volume is extremely costly for sparse ROIs.\n    max_zones = int(np.count_nonzero(mask))\n    if max_zones &lt; 1:\n        max_zones = 1\n\n    # Get pre-allocated buffers from pool\n    pool = _ZoneBufferPool.get_instance()\n    res_gl, res_size, res_dist, stack = pool.get_buffers(max_zones)\n\n    return cast(\n        Tuple[np.ndarray, np.ndarray],\n        _calculate_zone_features_numba(\n            data,\n            mask,\n            dist_map,\n            n_bins,\n            res_gl,\n            res_size,\n            res_dist,\n            stack,\n            calc_glszm,\n            calc_gldzm,\n        ),\n    )\n</code></pre>"},{"location":"user_guide/case_examples/","title":"Case examples","text":"<p>This section provides practical, end-to-end examples for common Pictologics workflows. The goal is to show how to combine loading, preprocessing, feature extraction, and result export patterns into scripts you can reuse. Many of these common processing techniques are in development to make the batch processing of cases even more easy. Therefore check back often to see any of these processing steps being implemented into the core package.</p>"},{"location":"user_guide/case_examples/#case-1-batch-radiomics-from-a-folder-of-nifti-files-no-masks","title":"Case 1: Batch radiomics from a folder of NIfTI files (no masks)","text":""},{"location":"user_guide/case_examples/#scenario","title":"Scenario","text":"<p>You have a folder of NIfTI volumes where each file is a separate exported segmentation-like volume. There are no mask files.</p> <p>You want to:</p> <ul> <li>Process every <code>*.nii</code> / <code>*.nii.gz</code> file in a folder.</li> <li>Use the whole image as the initial ROI (because no mask is provided).</li> <li>Resample to 0.5\u00d70.5\u00d70.5 mm.</li> <li>Restrict the ROI to intensities in [-100, 3000] (CT HU example).</li> <li>Remove disjoint parts by keeping the largest connected component.</li> <li>Compute all radiomic feature families for four discretisation settings:</li> <li>FBN with <code>n_bins=8</code></li> <li>FBN with <code>n_bins=16</code></li> <li>FBS with <code>bin_width=8</code></li> <li>FBS with <code>bin_width=16</code></li> <li>Export a single wide CSV where:</li> <li>Each row is one input NIfTI file.</li> <li>Columns include metadata (e.g., filename) + all computed features.</li> <li>Save pipeline logs to a separate folder.</li> </ul>"},{"location":"user_guide/case_examples/#important-notes","title":"Important notes","text":"<ul> <li>Maskless pipeline runs: <code>RadiomicsPipeline.run(...)</code> accepts <code>mask=None</code>, <code>mask=\"\"</code>, or an omitted mask argument.   In that case, Pictologics generates a full (all-ones) ROI mask internally (whole-image ROI).</li> <li>Empty ROI is an error: If your preprocessing removes all voxels (e.g., a too-tight HU range), the pipeline raises   a clear error instead of silently returning empty outputs.</li> <li>Morphology on whole-image ROI: Shape features will describe the shape of the ROI mask.   With a maskless run, that ROI starts as the full image volume, then becomes whatever remains after resegmentation   and connected-component filtering. This is valid computationally, but make sure it matches your scientific intent.</li> </ul>"},{"location":"user_guide/case_examples/#full-example-script","title":"Full example script","text":"<pre><code>from pathlib import Path\n\nfrom pictologics import RadiomicsPipeline\nfrom pictologics.results import format_results, save_results\n\ndef strip_nii_suffix(name):\n    \"\"\"Remove .nii or .nii.gz suffix from filename.\"\"\"\n    if name.endswith(\".nii.gz\"):\n        return name[: -len(\".nii.gz\")]\n    if name.endswith(\".nii\"):\n        return name[: -len(\".nii\")]\n    return name\n\ndef main():\n    # Configure paths\n    input_dir = Path(\"path/to/nifti_folder\")\n    output_csv = Path(\"results.csv\")\n    log_dir = Path(\"logs\")\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Define common preprocessing steps\n    base_steps = [\n        # Resample to 0.5mm isotropic. Round intensities to integers (useful for HU).\n        {\"step\": \"resample\", \"params\": {\"new_spacing\": (0.5, 0.5, 0.5), \"round_intensities\": True}},\n        # Exclude voxels outside standard HU range\n        {\"step\": \"resegment\", \"params\": {\"range_min\": -100, \"range_max\": 3000}},\n        # Keep only the largest connected component of the ROI mask\n        {\"step\": \"keep_largest_component\", \"params\": {\"apply_to\": \"morph\"}},\n    ]\n\n    # Shared feature extraction configuration\n    extract_all = {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": False,\n            \"include_local_intensity\": False,\n        },\n    }\n\n    # Initialize the pipeline\n    pipeline = RadiomicsPipeline()\n\n    # Add 4 configurations (2 FBN, 2 FBS)\n    for n_bins in (8, 16):\n        pipeline.add_config(\n            f\"case1_fbn_{n_bins}\",\n            base_steps + [\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": n_bins}},\n                extract_all,\n            ],\n        )\n\n    for bin_width in (8, 16):\n        pipeline.add_config(\n            f\"case1_fbs_{bin_width}\",\n            base_steps + [\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": bin_width}},\n                extract_all,\n            ],\n        )\n\n    # Prepare for batch processing\n    rows = []\n    nifti_paths = sorted(input_dir.glob(\"*.nii*\"))\n    if not nifti_paths:\n        raise ValueError(f\"No NIfTI files found in: {input_dir}\")\n\n    # Process each NIfTI file\n    for path in nifti_paths:\n        subject_id = strip_nii_suffix(path.name)\n        pipeline.clear_log()\n\n        # Run pipeline (mask omitted -&gt; whole-image ROI)\n        results = pipeline.run(\n            image=str(path),\n            subject_id=subject_id,\n            config_names=[\"case1_fbn_8\", \"case1_fbn_16\", \"case1_fbs_8\", \"case1_fbs_16\"],\n        )\n\n        # Format results as flat dictionary and add metadata\n        row = format_results(\n            results,\n            fmt=\"wide\",\n            meta={\"subject_id\": subject_id, \"file\": str(path)}\n        )\n        rows.append(row)\n\n        # Save per-case logs\n        pipeline.save_log(str(log_dir / f\"{subject_id}.json\"))\n\n    # Consolidated export of all results\n    save_results(rows, output_csv)\n    print(f\"Wrote {len(rows)} rows to {output_csv}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user_guide/case_examples/#output-format","title":"Output format","text":"<ul> <li>One row per file.</li> <li>Columns include:</li> <li><code>subject_id</code></li> <li><code>file</code></li> <li>Feature columns prefixed by configuration name (e.g., <code>case1_fbn_8__mean_intensity_Q4LE</code>).</li> </ul>"},{"location":"user_guide/case_examples/#case-2-batch-radiomics-from-dicom-case-folders-image-segmentation","title":"Case 2: Batch radiomics from DICOM case folders (Image + Segmentation)","text":""},{"location":"user_guide/case_examples/#scenario_1","title":"Scenario","text":"<p>You have a folder of cases. Each case is a separate folder and contains two subfolders:</p> <ul> <li><code>Image/</code>: the DICOM image series (CT/MR/etc.), stored at an arbitrary depth.</li> <li><code>Segmentation/</code>: the DICOM segmentation, also stored at an arbitrary depth.</li> </ul> <p>You want to:</p> <ul> <li>For each case, recursively discover the DICOM series folders.</li> <li>Load the image series and segmentation series.</li> <li>Convert the segmentation to a binary ROI mask by keeping all voxels where the segmentation value is <code>&gt; 0</code> (handled automatically during loading).</li> <li>Resample to 1\u00d71\u00d71 mm.</li> <li>Do not apply intensity filtering/resegmentation, and do not keep the largest connected component.</li> <li>Compute all radiomic feature families for two discretisations:</li> <li>FBS with <code>bin_width=256</code></li> <li>FBN with <code>n_bins=64</code></li> <li>Export a long-format CSV (tidy data, one row per feature).</li> <li>Save pipeline logs into a separate folder (one JSON per case).</li> <li>Show a progress bar during batch processing.</li> </ul>"},{"location":"user_guide/case_examples/#notes","title":"Notes","text":"<ul> <li>Progress bar dependency: This example uses <code>tqdm</code>.<ul> <li>If you are running this outside the library repo, install it with <code>pip install tqdm</code>.</li> <li>If you are adding it to your Poetry-managed project, use <code>poetry add tqdm</code>.</li> </ul> </li> <li>Segmentation DICOM at arbitrary depth: The helper <code>_find_dicom_series_root(...)</code> looks for the subfolder with     the most <code>.dcm</code> files and uses that as the series root.</li> <li>Multiple masks in one SEG: If your segmentation DICOM encodes multiple labels (e.g., values 1..N), the     <code>_binarize_segmentation_mask(...)</code> step turns it into a single ROI by keeping all voxels where the value is <code>&gt; 0</code>.</li> </ul>"},{"location":"user_guide/case_examples/#full-example-script_1","title":"Full example script","text":"<pre><code>from pathlib import Path\nimport numpy as np\nfrom pictologics import Image, RadiomicsPipeline, load_image, load_and_merge_images\nfrom pictologics.results import format_results, save_results\n\n\ndef main():\n    # Configure paths\n    cases_dir = Path(\"path/to/cases_root\")\n    output_csv = Path(\"dicom_results_long.csv\")\n    log_dir = Path(\"dicom_logs\")\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Find all case directories\n    case_dirs = sorted([p for p in cases_dir.iterdir() if p.is_dir()])\n    if not case_dirs:\n        raise ValueError(f\"No case folders found in: {cases_dir}\")\n\n    # Shared feature extraction settings\n    extract_all = {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": False,\n            \"include_local_intensity\": False,\n        },\n    }\n\n    # Initialize the pipeline\n    pipeline = RadiomicsPipeline()\n\n    # Configuration A: Fixed Bin Size\n    pipeline.add_config(\n        \"case2_fbs_256\",\n        [\n            {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n            {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": 256.0}},\n            extract_all,\n        ],\n    )\n\n    # Configuration B: Fixed Bin Number\n    pipeline.add_config(\n        \"case2_fbn_64\",\n        [\n            {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n            {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 64}},\n            extract_all,\n        ],\n    )\n\n    # Use tqdm for a progress bar\n    from tqdm import tqdm\n\n    rows = []\n    for case_dir in tqdm(case_dirs, desc=\"Radiomics (DICOM cases)\", unit=\"case\"):\n        subject_id = case_dir.name\n        image_root = case_dir / \"Image\"\n        seg_root = case_dir / \"Segmentation\"\n\n        # load_image with recursive=True finds the best series folder automatically\n        image = load_image(str(image_root), recursive=True)\n\n        # Load the segmentation using load_and_merge_images with binarize=True\n        # recursive=True ensures we find the DICOM series inside the Segmentation folder\n        mask = load_and_merge_images(\n            [str(seg_root)], \n            binarize=True, \n            recursive=True\n        )\n\n        pipeline.clear_log()\n\n        # Execute extraction for both configurations\n        results = pipeline.run(\n            image=image,\n            mask=mask,\n            subject_id=subject_id,\n            config_names=[\"case2_fbs_256\", \"case2_fbn_64\"],\n        )\n\n        # Format results and store\n        row = format_results(\n            results,\n            fmt=\"long\",  # Tidy format: [subject_id, config, feature_name, value]\n            meta={\n                \"subject_id\": subject_id,\n                \"image_root\": str(image_root),\n                \"seg_root\": str(seg_root),\n            },\n        )\n        rows.append(row)\n\n        # Save per-case logs\n        pipeline.save_log(str(log_dir / f\"{subject_id}.json\"))\n\n    # Final export\n    save_results(rows, output_csv)\n    print(f\"Wrote {len(rows)} rows to {output_csv}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user_guide/case_examples/#case-3-batch-radiomics-from-a-flat-nifti-folder-multiple-masks-per-image","title":"Case 3: Batch radiomics from a flat NIfTI folder (multiple masks per image)","text":""},{"location":"user_guide/case_examples/#scenario_2","title":"Scenario","text":"<p>You have a single large folder containing both images and masks as NIfTI files.</p> <ul> <li>Image files end with <code>_IMG</code> (e.g., <code>CASE001_IMG.nii.gz</code>).</li> <li>Mask files end with <code>MASKn</code> where <code>n</code> is a number (e.g., <code>CASE001_MASK1.nii.gz</code>, <code>CASE001_MASK2.nii.gz</code>).</li> <li>There can be multiple segmentation masks per image.</li> </ul> <p>You want to:</p> <ul> <li>For each image, automatically find all its corresponding masks.</li> <li>Merge all masks into a single ROI using <code>load_and_merge_images(...)</code>.</li> <li>Do not apply any preprocessing (no resampling, no thresholding, no connected components).</li> <li>Compute radiomics for the six standard discretisations (FBN 8/16/32 and FBS 8/16/32).</li> <li>Export a long-format JSON file for easy ingestion into NoSQL databases or web apps.</li> <li>Save logs for each case into a separate folder.</li> <li>Show a progress bar during batch processing.</li> </ul> <p>Tip</p> <p>Flexible Image Loading The <code>load_and_merge_images</code> function uses <code>load_image</code> for each path in its input list. This means you can merge: - Multiple NIfTI files (as shown here). - DICOM series folders. - Single DICOM slice files. - Or a mix of these formats, provided they share the same spatial geometry.</p> <p>You can also pass <code>recursive=True</code> (to search subfolders) or <code>dataset_index=N</code> (for 4D files) to control how each mask is loaded.</p>"},{"location":"user_guide/case_examples/#full-example-script_2","title":"Full example script","text":"<pre><code>from pathlib import Path\n\nimport numpy as np\n\nfrom pictologics import Image, RadiomicsPipeline, load_and_merge_images, load_image\nfrom pictologics.results import format_results, save_results\n\n\n\n\n\ndef strip_suffixes(name):\n    \"\"\"Strip .nii/.nii.gz and trailing _IMG to get a stable case id.\"\"\"\n    if name.endswith(\".nii.gz\"):\n        name = name[: -len(\".nii.gz\")]\n    elif name.endswith(\".nii\"):\n        name = name[: -len(\".nii\")]\n    if name.endswith(\"_IMG\"):\n        name = name[: -len(\"_IMG\")]\n    return name\n\n\ndef main():\n    # Configure paths\n    input_dir = Path(\"path/to/nifti_folder\")\n    output_file = Path(\"case3_results_long.json\")\n    log_dir = Path(\"case3_logs\")\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Use tqdm for a progress bar\n    from tqdm import tqdm\n\n    # Identify all images (files ending in _IMG)\n    image_paths = sorted(input_dir.glob(\"*_IMG.nii*\"))\n    if not image_paths:\n        raise ValueError(f\"No *_IMG NIfTI files found in: {input_dir}\")\n\n    # Initialize the pipeline\n    pipeline = RadiomicsPipeline()\n\n    # Shared feature extraction settings\n    extract_all = {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": False,\n            \"include_local_intensity\": False,\n        },\n    }\n\n    # Add 6 target configurations (no preprocessing requested)\n    for n_bins in (8, 16, 32):\n        pipeline.add_config(\n            f\"case3_fbn_{n_bins}\",\n            [\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": n_bins}},\n                extract_all,\n            ],\n        )\n\n    for bin_width in (8.0, 16.0, 32.0):\n        pipeline.add_config(\n            f\"case3_fbs_{int(bin_width)}\",\n            [\n                {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": bin_width}},\n                extract_all,\n            ],\n        )\n\n    target_configs = [\n        \"case3_fbn_8\", \"case3_fbn_16\", \"case3_fbn_32\",\n        \"case3_fbs_8\", \"case3_fbs_16\", \"case3_fbs_32\",\n    ]\n\n    # Process each image and its associated masks\n    rows = []\n    for img_path in tqdm(image_paths, desc=\"Radiomics (NIfTI images)\", unit=\"image\"):\n        subject_id = strip_suffixes(img_path.name)\n\n        # Find all corresponding masks (e.g., CASE001_MASK1.nii.gz, etc.)\n        mask_paths = sorted(input_dir.glob(f\"{subject_id}_MASK*.nii*\"))\n        if not mask_paths:\n            raise ValueError(f\"No masks found for {subject_id}\")\n\n        image = load_image(str(img_path))\n\n        # Merge multiple masks into one and ensure binary semantics\n        mask = load_and_merge_images(\n            [str(p) for p in mask_paths],\n            reference_image=image,\n            binarize=True\n        )\n\n        pipeline.clear_log()\n\n        # Run extraction for all target configurations\n        results = pipeline.run(\n            image=image,\n            mask=mask,\n            subject_id=subject_id,\n            config_names=target_configs,\n        )\n\n        # Format results and store metadata\n        row = format_results(\n            results,\n            fmt=\"long\",\n            meta={\n                \"subject_id\": subject_id,\n                \"image\": str(img_path),\n                \"masks\": \";\".join(str(p) for p in mask_paths),\n            },\n        )\n        rows.append(row)\n\n        # Save per-case logs\n        pipeline.save_log(str(log_dir / f\"{subject_id}.json\"))\n\n    # Consolidated export (save_results handles list of DataFrames for long format automatically)\n    save_results(rows, output_file)\n    print(f\"Wrote {len(rows)} cases to {output_file}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user_guide/case_examples/#case-4-parallel-batch-radiomics-from-dicom-cases-merge-multiple-segmentation-folders","title":"Case 4: Parallel batch radiomics from DICOM cases (merge multiple segmentation folders)","text":""},{"location":"user_guide/case_examples/#scenario_3","title":"Scenario","text":"<p>You have a folder of cases. Each case is a separate folder and contains:</p> <ul> <li><code>Image/</code>: the DICOM image series (CT/MR/etc.), stored at an arbitrary depth.</li> <li><code>Segmentation/</code>: multiple subfolders, each containing a segmentation series at an arbitrary depth.</li> </ul> <p>You want to:</p> <ul> <li>For each case, recursively discover the DICOM image series folder.</li> <li>Discover all segmentation series folders under <code>Segmentation/</code> and load them.</li> <li>Convert each segmentation to a binary mask (values <code>&gt; 0</code> become 1).</li> <li>Merge all binary masks into a single ROI mask per case.</li> <li>Apply all preprocessing steps supported by the pipeline (resample, resegment, outlier filtering,   intensity rounding, and largest connected component).</li> <li>Compute radiomics for six discretisations (FBN 8/16/32 and FBS 8/16/32).</li> <li>Run cases in parallel on multiple CPU cores, with a user-controlled <code>n_jobs</code>.</li> <li>Export a single wide JSON file (one object per case) and save per-case logs.</li> </ul>"},{"location":"user_guide/case_examples/#notes_1","title":"Notes","text":"<ul> <li>Progress bar dependency: This example uses <code>tqdm</code>.<ul> <li>If you are running this outside the library repo, install it with <code>pip install tqdm</code>.</li> <li>If you are adding it to your Poetry-managed project, use <code>poetry add tqdm</code>.</li> </ul> </li> <li>Multiprocessing requirement: On Windows/macOS, keep the parallel execution inside   <code>if __name__ == \"__main__\":</code> (as shown) to avoid process-spawn issues.</li> <li>JIT warmup in parallel workers: Pictologics performs a Numba JIT warmup at package import.     With <code>ProcessPoolExecutor</code>, each worker is a separate Python process, so warmup happens once per worker process     (on its first import of <code>pictologics</code>) and then stays warm for all cases that worker processes.     It is not re-run for every case unless you explicitly call <code>warmup_jit()</code> inside your per-case function.     You can disable auto-warmup via <code>PICTOLOGICS_DISABLE_WARMUP=1</code> if you prefer to skip the upfront cost.</li> <li>Preprocessing parameters are dataset-dependent: The <code>resegment</code> range here uses the CT HU example   <code>[-100, 3000]</code>. Adjust or remove it for non-CT data.</li> </ul>"},{"location":"user_guide/case_examples/#full-example-script_3","title":"Full example script","text":"<pre><code>from concurrent.futures import ProcessPoolExecutor, as_completed\nfrom pathlib import Path\nimport numpy as np\nfrom pictologics import Image, RadiomicsPipeline, load_image, load_and_merge_images\nfrom pictologics.results import format_results, save_results\n\n\ndef collect_segmentation_series_roots(seg_root):\n    \"\"\"Collect all segmentation series subfolders.\"\"\"\n    if not seg_root.exists():\n        raise ValueError(f\"Folder does not exist: {seg_root}\")\n\n    subdirs = sorted([p for p in seg_root.iterdir() if p.is_dir()])\n    if not subdirs:\n        return [seg_root]\n\n    return subdirs\n\ndef build_case4_pipeline():\n    \"\"\"Define the pipeline with preprocessing and all feature families.\"\"\"\n    pipeline = RadiomicsPipeline()\n\n    # Define standard CT preprocessing\n    preprocess_steps = [\n        {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n        {\"step\": \"resegment\", \"params\": {\"range_min\": -100, \"range_max\": 3000}},\n        {\"step\": \"filter_outliers\", \"params\": {\"sigma\": 3.0}},\n        {\"step\": \"round_intensities\"},\n        {\"step\": \"keep_largest_component\", \"params\": {\"apply_to\": \"morph\"}},\n    ]\n\n    extract_all = {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": False,\n            \"include_local_intensity\": False,\n        },\n    }\n\n    # Add discretisation variants\n    for n_bins in (8, 16, 32):\n        pipeline.add_config(\n            f\"case4_fbn_{n_bins}\",\n            preprocess_steps + [{\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": n_bins}}, extract_all],\n        )\n\n    for bin_width in (8.0, 16.0, 32.0):\n        pipeline.add_config(\n            f\"case4_fbs_{int(bin_width)}\",\n            preprocess_steps + [{\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": bin_width}}, extract_all],\n        )\n\n    config_names = [f\"case4_fbn_{b}\" for b in (8, 16, 32)] + [f\"case4_fbs_{b}\" for b in (8, 16, 32)]\n    return pipeline, config_names\n\ndef process_case(case_dir, log_dir):\n    \"\"\"Worker function for single case processing.\"\"\"\n    case_path = Path(case_dir)\n    subject_id = case_path.name\n    image_root = case_path / \"Image\"\n    seg_root = case_path / \"Segmentation\"\n\n    # Load image recursively\n    image = load_image(str(image_root), recursive=True)\n\n    # Load and merge all found segmentations\n    seg_roots = collect_segmentation_series_roots(seg_root)\n\n    # load_and_merge_images handles multiple paths, geometry checking, and binarization\n    try:\n        mask = load_and_merge_images(\n            [str(p) for p in seg_roots], \n            reference_image=image, \n            binarize=True, \n            recursive=True\n        )\n    except ValueError as e:\n         raise ValueError(f\"Failed to load/merge masks for {subject_id}: {e}\")\n\n    # Setup and run pipeline\n    pipeline, config_names = build_case4_pipeline()\n    pipeline.clear_log()\n    results = pipeline.run(image=image, mask=mask, subject_id=subject_id, config_names=config_names)\n\n    # Save results and log\n    Path(log_dir).mkdir(parents=True, exist_ok=True)\n    pipeline.save_log(str(Path(log_dir) / f\"{subject_id}.json\"))\n\n    return format_results(\n        results,\n        fmt=\"wide\",\n        meta={\n            \"subject_id\": subject_id,\n            \"image_root\": str(image_root),\n            \"seg_roots\": \";\".join(str(p) for p in seg_roots),\n        },\n    )\n\ndef main():\n    # Configure paths\n    cases_dir = Path(\"path/to/cases_root\")\n    output_file = Path(\"case4_parallel_results.json\")\n    log_dir = Path(\"case4_logs\")\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Start parallel processing\n    n_jobs = 4\n    case_dirs = sorted([p for p in cases_dir.iterdir() if p.is_dir()])\n    if not case_dirs:\n        raise ValueError(f\"No case folders found in: {cases_dir}\")\n\n    from tqdm import tqdm\n    rows = []\n    errors = []\n\n    # Map each case to the worker function\n    with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n        futures = {\n            executor.submit(process_case, str(case_dir), str(log_dir)): case_dir\n            for case_dir in case_dirs\n        }\n\n        with tqdm(total=len(futures), desc=\"Radiomics (parallel cases)\", unit=\"case\") as pbar:\n            for fut in as_completed(futures):\n                case_dir = futures[fut]\n                try:\n                    rows.append(fut.result())\n                except Exception as e:\n                    errors.append((str(case_dir), repr(e)))\n                finally:\n                    pbar.update(1)\n\n    if errors:\n        msg = \"\\n\".join(f\"- {case}: {err}\" for case, err in errors)\n        raise RuntimeError(f\"One or more cases failed:\\n{msg}\")\n\n    # Final data export\n    save_results(rows, output_file)\n    print(f\"Wrote {len(rows)} cases to {output_file}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user_guide/case_examples/#output-options-and-formats","title":"Output Options and Formats","text":"<p>You can customize how results are formatted and exported using <code>format_results</code> and <code>save_results</code>.</p>"},{"location":"user_guide/case_examples/#wide-format-default","title":"Wide Format (Default)","text":"<p>Produces a single row per case, with columns like <code>configName__featureName</code>.</p> <pre><code># Format as a flat dictionary for spreadsheet-style output\nrow = format_results(\n    results, \n    fmt=\"wide\", \n    meta={\"subject_id\": \"001\"}\n)\n# Returns: {\"subject_id\": \"001\", \"configA__val\": 1.5, ...}\n</code></pre>"},{"location":"user_guide/case_examples/#long-format-tidy","title":"Long Format (Tidy)","text":"<p>Produces multiple rows per case. This is often better for analysis in R/tidyverse or seaborn.</p> <pre><code># Format as a tidy DataFrame (long format)\ndf = format_results(\n    results, \n    fmt=\"long\", \n    meta={\"subject_id\": \"001\"},\n    output_type=\"pandas\"\n)\n# Returns DataFrame columns: [subject_id, config, feature_name, value]\n</code></pre> <p>To export long-format results for a batch:</p> <pre><code>dfs = []\nfor case in cases:\n    # ... process image and run pipeline ...\n    # Format current result as tidy DataFrame\n    df = format_results(results, fmt=\"long\", meta={\"subject_id\": case.id}, output_type=\"pandas\")\n    dfs.append(df)\n\n# save_results automatically concatenates list of DataFrames\nsave_results(dfs, \"results_long.csv\")\n</code></pre>"},{"location":"user_guide/case_examples/#json-export","title":"JSON Export","text":"<p>If you prefer JSON output (e.g. for Web/NoSQL):</p> <pre><code># Save result list directly to JSON\nsave_results(rows, \"results.json\")\n</code></pre> <p>Or get a JSON string directly for a single result:</p> <pre><code># Convert result to JSON string\njson_str = format_results(results, fmt=\"wide\", output_type=\"json\")\n</code></pre>"},{"location":"user_guide/installation/","title":"Installation","text":""},{"location":"user_guide/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>pip</li> </ul>"},{"location":"user_guide/installation/#installation-via-pip","title":"Installation via Pip","text":"<pre><code>pip install pictologics\n</code></pre>"},{"location":"user_guide/installation/#installation-from-github","title":"Installation from GitHub","text":"<p>If you want the latest development version (or you want to install before the next PyPI release), you can install directly from the GitHub repository.</p>"},{"location":"user_guide/installation/#latest-from-main","title":"Latest from <code>main</code>","text":"<pre><code>pip install \"pictologics @ git+https://github.com/martonkolossvary/pictologics.git@main\"\n</code></pre>"},{"location":"user_guide/installation/#pinned-to-a-tag-or-commit","title":"Pinned to a tag or commit","text":"<pre><code># Example: install from a tag\npip install \"pictologics @ git+https://github.com/martonkolossvary/pictologics.git@v0.1.0\"\n\n# Example: install from a commit SHA\npip install \"pictologics @ git+https://github.com/martonkolossvary/pictologics.git@&lt;commit_sha&gt;\"\n</code></pre>"},{"location":"user_guide/installation/#editable-install-development","title":"Editable install (development)","text":"<p>Use this if you plan to modify the code.</p> <pre><code>git clone https://github.com/martonkolossvary/pictologics.git\ncd pictologics\npip install -e .\n</code></pre>"},{"location":"user_guide/installation/#eager-compilation-warmup","title":"Eager Compilation (Warmup)","text":"<p>Pictologics uses Numba for Just-In-Time (JIT) compilation to accelerate feature extraction. To ensure fast runtime performance, Pictologics performs an automatic warmup mechanism during import. This compiles the core functions immediately when <code>import pictologics</code> is executed.</p> <p>Note: This may cause the <code>import pictologics</code> statement to take a few seconds (typically 2-10s depending on your CPU) to complete. This is expected behavior and guarantees that subsequent function calls are executed at full speed without initial compilation lag.</p>"},{"location":"user_guide/installation/#disabling-warmup","title":"Disabling Warmup","text":"<p>If you need fast import times (e.g., for CLI tools checking versions or lightweight scripts) and are willing to pay the compilation cost at the first function call, you can disable this behavior by setting the environment variable:</p> <pre><code>export PICTOLOGICS_DISABLE_WARMUP=1\n</code></pre>"},{"location":"user_guide/pipeline/","title":"Radiomics Pipeline","text":"<p>The <code>RadiomicsPipeline</code> is the core engine of Pictologics for executing reproducible, standardized radiomic feature extraction workflows. It manages the entire lifecycle of the data, from loading and preprocessing to feature extraction and logging.</p>"},{"location":"user_guide/pipeline/#why-use-the-pipeline","title":"Why use the Pipeline?","text":"<ol> <li>Reproducibility: By defining a configuration (a sequence of steps), you ensure that the exact same preprocessing is applied to every image.</li> <li>State Management: The pipeline automatically handles the state of the image and masks (morphological and intensity) as they pass through steps like resampling and resegmentation.</li> <li>Standardisation: It comes with built-in configurations that adhere to IBSI standards.</li> <li>Batch Processing: You can run multiple configurations (e.g., different binning strategies) on the same image in a single pass.</li> <li>Flexibility: The pipeline executes steps in a linear fashion, allowing you to arrange steps in any order, repeat steps if needed, and implement arbitrarily complex workflows.</li> </ol>"},{"location":"user_guide/pipeline/#linear-step-execution","title":"Linear Step Execution","text":"<p>The pipeline module executes steps linearly in order. This means:</p> <ul> <li>Steps are applied one after another in the exact sequence you define.</li> <li>You can repeat steps if needed (e.g., apply <code>discretise</code> multiple times with different settings).</li> <li>You can arrange steps in any order appropriate for your workflow.</li> <li>This linear design allows for implementing complex, multi-stage preprocessing while maintaining full control.</li> </ul> <pre><code># Example: Complex workflow with repeated steps\ncomplex_config = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (2.0, 2.0, 2.0)}},\n    {\"step\": \"keep_largest_component\", \"params\": {\"apply_to\": \"morph\"}},\n    {\"step\": \"resegment\", \"params\": {\"range_min\": -1000, \"range_max\": 400}},\n    {\"step\": \"filter_outliers\", \"params\": {\"sigma\": 3.0}},\n    {\"step\": \"round_intensities\", \"params\": {}},  # Round after all preprocessing\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\"step\": \"extract_features\", \"params\": {\"families\": [\"texture\", \"histogram\"]}},\n]\n</code></pre>"},{"location":"user_guide/pipeline/#masks-are-optional","title":"Masks are optional","text":"<p><code>RadiomicsPipeline.run(...)</code> accepts an optional <code>mask</code> argument.</p> <ul> <li>If you pass a mask path / mask <code>Image</code>, it is used as the ROI (standard radiomics workflow).</li> <li>If you omit <code>mask</code> (or pass <code>mask=None</code> / <code>mask=\"\"</code>), Pictologics generates a full (all-ones) ROI mask internally,   meaning the entire image is treated as the initial ROI.</li> </ul> <p>Warning</p> <p>Empty ROI is an error If preprocessing removes all ROI voxels (e.g., too strict <code>resegment</code> thresholds), the pipeline raises a clear error rather than returning empty/partial feature sets.</p>"},{"location":"user_guide/pipeline/#predefined-configurations","title":"Predefined Configurations","text":"<p>Pictologics includes a suite of Standard Configurations designed to cover the most common radiomics analysis scenarios. These configurations are compliant with general best practices (e.g., IBSI).</p>"},{"location":"user_guide/pipeline/#common-characteristics","title":"Common Characteristics","text":"<p>All standard configurations share the following preprocessing steps:</p> <ul> <li>Resampling: Images are resampled to 0.5mm x 0.5mm x 0.5mm isotropic spacing using Linear interpolation (Nearest Neighbor for masks).</li> <li>Feature Families: All feature families are extracted: <code>intensity</code>, <code>morphology</code>, <code>texture</code>, <code>histogram</code>, and <code>ivh</code>.</li> </ul> <p>Note</p> <p>Performance-friendly default for standard configs The built-in <code>standard_*</code> configurations disable the two most time-intensive intensity extras by default: <code>include_spatial_intensity=False</code> and <code>include_local_intensity=False</code>.</p> <p>This does not change the general behavior of the <code>extract_features</code> step when you build your own custom configuration: if you request <code>\"intensity\"</code> in a custom config and do not specify these flags, spatial/local intensity will still be included (backward compatible).</p> <p>Warning</p> <p>Spatial/local intensity features can be very time consuming. Spatial intensity (Moran's I / Geary's C) and local intensity peak features can dominate runtime for larger ROIs. For images larger than 50\u00d750\u00d750 voxels, this is generally not recommended unless you explicitly need them.</p> <ul> <li>Standard configs: disabled by default</li> <li>Custom configs: enabled by default (unless you set <code>include_spatial_intensity=False</code> / <code>include_local_intensity=False</code>)</li> </ul>"},{"location":"user_guide/pipeline/#available-configurations","title":"Available Configurations","text":"Configuration Name Discretisation Method Parameters Description <code>standard_fbn_8</code> Fixed Bin Number (FBN) <code>n_bins=8</code> Coarse texture analysis. <code>standard_fbn_16</code> Fixed Bin Number (FBN) <code>n_bins=16</code> Medium texture analysis. <code>standard_fbn_32</code> Fixed Bin Number (FBN) <code>n_bins=32</code> Fine texture analysis (Common default). <code>standard_fbs_8</code> Fixed Bin Size (FBS) <code>bin_width=8.0</code> For absolute intensity units (e.g., HU). <code>standard_fbs_16</code> Fixed Bin Size (FBS) <code>bin_width=16.0</code> For absolute intensity units. <code>standard_fbs_32</code> Fixed Bin Size (FBS) <code>bin_width=32.0</code> For absolute intensity units."},{"location":"user_guide/pipeline/#running-standard-configurations","title":"Running Standard Configurations","text":"<p>You can run specific configurations or use the special <code>\"all_standard\"</code> keyword to run all 6 at once.</p> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\n\n# Option A: Run specific configurations\nresults = pipeline.run(\n    image=\"path/to/image.nii.gz\",\n    mask=\"path/to/mask.nii.gz\",\n    config_names=[\"standard_fbn_32\", \"standard_fbs_16\"]\n)\n\n# Option B: Run ALL 6 standard configurations (Recommended for exploration)\nall_results = pipeline.run(\n    image=\"path/to/image.nii.gz\",\n    mask=\"path/to/mask.nii.gz\",\n    config_names=[\"all_standard\"]\n)\n\n# Accessing results\nprint(all_results[\"standard_fbn_32\"])\n</code></pre>"},{"location":"user_guide/pipeline/#what-you-get-and-what-you-dont","title":"What you get (and what you don't)","text":"<p>The standard configurations are meant to be a fast, reproducible baseline:</p> <ul> <li>You get full first-order intensity statistics (<code>\"intensity\"</code>), morphology, textures, histogram, and IVH.</li> <li>You do not get spatial/local intensity extras unless you build a custom configuration and enable them.</li> </ul> <p>If you need spatial/local intensity metrics for a specific study, use a custom configuration (examples below).</p>"},{"location":"user_guide/pipeline/#custom-configurations","title":"Custom Configurations","text":"<p>For advanced users, the pipeline allows you to define custom sequences of steps. A configuration is a list of dictionaries, where each dictionary represents a step.</p>"},{"location":"user_guide/pipeline/#structure-of-a-configuration","title":"Structure of a Configuration","text":"<pre><code>config = [\n    {\n        \"step\": \"step_name\",\n        \"params\": { \"param1\": value1, \"param2\": value2 }\n    },\n    # ... more steps\n]\n</code></pre>"},{"location":"user_guide/pipeline/#practical-tips","title":"Practical tips","text":"<ul> <li>Keep preprocessing steps explicit (resampling, resegmentation, discretisation) so your results are reproducible.</li> <li>For CT in Hounsfield Units, FBS (<code>bin_width</code>) is often more interpretable; for MRI/PET, FBN (<code>n_bins</code>) can be a     reasonable choice depending on your intensity normalization.</li> <li>If you only need a subset of feature families, set <code>families</code> to avoid unnecessary work.</li> </ul>"},{"location":"user_guide/pipeline/#available-steps","title":"Available Steps","text":""},{"location":"user_guide/pipeline/#1-resample","title":"1. <code>resample</code>","text":"<p>Resamples the image and mask to a new voxel spacing.</p> <ul> <li><code>new_spacing</code>: Tuple of (x, y, z) spacing in mm (e.g., <code>(1.0, 1.0, 1.0)</code>).<ul> <li>Alias: <code>spacing</code> (older configs/tests).</li> </ul> </li> <li><code>interpolation</code>: Interpolation for the image (<code>\"linear\"</code>, <code>\"cubic\"</code>, <code>\"nearest\"</code>). Default: <code>\"linear\"</code>.</li> <li><code>mask_interpolation</code>: Interpolation for the mask (<code>\"nearest\"</code>, <code>\"linear\"</code>). Default: <code>\"nearest\"</code>.</li> <li><code>mask_threshold</code>: When using non-nearest mask interpolation, voxels above this threshold become ROI. Default: <code>0.5</code>.</li> <li><code>round_intensities</code>: Whether to round image intensities to nearest integer after resampling. Default: <code>False</code>.</li> </ul>"},{"location":"user_guide/pipeline/#2-resegment","title":"2. <code>resegment</code>","text":"<p>Refines the mask based on intensity thresholds (e.g., excluding bone from a soft tissue mask).</p> <ul> <li><code>range_min</code>: Minimum intensity value.</li> <li><code>range_max</code>: Maximum intensity value.</li> </ul>"},{"location":"user_guide/pipeline/#3-filter_outliers","title":"3. <code>filter_outliers</code>","text":"<p>Removes outliers from the intensity mask based on standard deviations from the mean.</p> <ul> <li><code>sigma</code>: Number of standard deviations (e.g., <code>3.0</code>).</li> </ul>"},{"location":"user_guide/pipeline/#4-keep_largest_component","title":"4. <code>keep_largest_component</code>","text":"<p>Restricts the mask to the largest connected component. Useful for removing noise or disconnected artifacts.</p> <ul> <li><code>apply_to</code>: Which mask(s) to process. Options:<ul> <li><code>\"both\"</code> (default): Apply to both morphological and intensity masks.</li> <li><code>\"morph\"</code>: Apply only to the morphological mask.</li> <li><code>\"intensity\"</code>: Apply only to the intensity mask.</li> </ul> </li> </ul>"},{"location":"user_guide/pipeline/#5-round_intensities","title":"5. <code>round_intensities</code>","text":"<p>Rounds image intensities to the nearest integer. Useful before discretisation if values are close to integers.</p> <ul> <li>No parameters.</li> </ul>"},{"location":"user_guide/pipeline/#6-discretise","title":"6. <code>discretise</code>","text":"<p>Discretises the image intensities into bins. This is crucial for texture analysis.</p> <ul> <li><code>method</code>: <code>\"FBN\"</code> (Fixed Bin Number) or <code>\"FBS\"</code> (Fixed Bin Size).</li> <li><code>n_bins</code>: Number of bins (for FBN).</li> <li><code>bin_width</code>: Width of each bin (for FBS).</li> </ul>"},{"location":"user_guide/pipeline/#7-extract_features","title":"7. <code>extract_features</code>","text":"<p>Calculates the radiomic features based on the current state of the image and mask.</p> <p>Feature Calculation Inputs</p> <p>The pipeline automatically selects the appropriate image state for each feature family:</p> <ul> <li>Intensity, Morphology, Spatial Intensity, Local Intensity: Calculated on the Raw Image (non-discretised, floating-point values).</li> <li>Texture, Histogram: Calculated on the Discretised Image (integer bins).</li> <li>IVH: Configurable. Defaults to Discretised Image, but can use Raw Image (<code>ivh_use_continuous=True</code>) or a Temporary Discretisation (<code>ivh_discretisation={...}</code>).</li> </ul> <ul> <li><code>families</code>: List of feature families to extract. Options:<ul> <li><code>\"intensity\"</code>: First-order statistics (Mean, Skewness, etc.).<ul> <li>By default, this also includes spatial intensity and local intensity.</li> <li>Disable these expensive computations via <code>include_spatial_intensity=False</code> and/or     <code>include_local_intensity=False</code> in the step <code>params</code>.</li> </ul> </li> <li><code>\"spatial_intensity\"</code>: Compute only spatial intensity (Moran's I / Geary's C).</li> <li><code>\"local_intensity\"</code>: Compute only local/global intensity peak features.</li> <li><code>\"morphology\"</code>: Shape and size features (Volume, Sphericity, etc.).</li> <li><code>\"texture\"</code>: GLCM, GLRLM, GLSZM, GLDZM, NGTDM, NGLDM.</li> <li><code>\"histogram\"</code>: Intensity histogram features.</li> <li><code>\"ivh\"</code>: Intensity-Volume Histogram features.</li> </ul> </li> </ul> <p>Additional optional parameters (advanced usage):</p> <ul> <li><code>include_spatial_intensity</code> / <code>include_local_intensity</code>: Booleans controlling whether the expensive     spatial/local intensity extras are included when <code>\"intensity\"</code> is requested.</li> <li><code>ivh_params</code>: Dict forwarded to <code>calculate_ivh_features(...)</code>. Supported keys include:     <code>bin_width</code>, <code>min_val</code>, <code>max_val</code>, <code>target_range_min</code>, <code>target_range_max</code>.     (There are also backward-compatible aliases: <code>ivh_bin_width</code>, <code>ivh_min_val</code>, <code>ivh_max_val</code>,     <code>ivh_target_range_min</code>, <code>ivh_target_range_max</code>.)</li> <li><code>ivh_discretisation</code>: Dict specifying a temporary discretisation for IVH only. This allows     using different binning for IVH vs texture features. Example: <code>{\"method\": \"FBS\", \"bin_width\": 2.5, \"min_val\": -1000}</code>.</li> <li><code>ivh_use_continuous</code>: Boolean. If <code>True</code>, uses raw (non-discretised) intensity values for IVH calculation.     Useful for \"continuous IVH\" as specified in some IBSI configurations.</li> <li><code>texture_matrix_params</code>: Dict forwarded to <code>calculate_all_texture_matrices(...)</code>.     Currently useful key: <code>ngldm_alpha</code> (IBSI default is <code>0</code>).</li> </ul>"},{"location":"user_guide/pipeline/#examples","title":"Examples","text":""},{"location":"user_guide/pipeline/#example-1-standard-suite-fast-baseline","title":"Example 1: Standard suite (fast baseline)","text":"<p>Runs all 6 built-in configurations. Spatial/local intensity extras are disabled by default.</p> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\nresults = pipeline.run(\n    image=\"path/to/image.nii.gz\",\n    mask=\"path/to/mask.nii.gz\",\n    config_names=[\"all_standard\"],\n)\n\n# Access one configuration\nprint(results[\"standard_fbn_32\"].head())\n</code></pre>"},{"location":"user_guide/pipeline/#example-1b-maskless-run-whole-image-roi","title":"Example 1b: Maskless run (whole-image ROI)","text":"<p>If you do not have a segmentation mask, you can omit the <code>mask</code> argument entirely.</p> <pre><code>from pictologics import RadiomicsPipeline\n\npipeline = RadiomicsPipeline()\nresults = pipeline.run(\n    image=\"path/to/image.nii.gz\",\n    # mask omitted -&gt; whole-image ROI\n    config_names=[\"standard_fbn_32\"],\n)\n\nprint(results[\"standard_fbn_32\"].head())\n</code></pre> <p>Note</p> <p>Morphology meaning With a maskless run, morphology features describe the ROI mask after any mask-refining steps (e.g., <code>resegment</code>, <code>keep_largest_component</code>). Starting from a whole-image ROI can be valid, but may not be scientifically meaningful for many radiomics studies.</p>"},{"location":"user_guide/pipeline/#example-2-enable-spatiallocal-intensity-extras-custom-config","title":"Example 2: Enable spatial/local intensity extras (custom config)","text":"<p>Use this when you explicitly need Moran\u2019s I / Geary\u2019s C and local intensity peak features.</p> <pre><code>from pictologics import RadiomicsPipeline\n\ncfg = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (0.5, 0.5, 0.5)}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": True,\n            \"include_local_intensity\": True,\n        },\n    },\n]\n\npipeline = RadiomicsPipeline().add_config(\"with_intensity_extras\", cfg)\nout = pipeline.run(\"path/to/image.nii.gz\", \"path/to/mask.nii.gz\", config_names=[\"with_intensity_extras\"])\nprint(out[\"with_intensity_extras\"].filter(like=\"_\"))\n</code></pre>"},{"location":"user_guide/pipeline/#example-3-only-compute-the-expensive-parts-explicit-families","title":"Example 3: Only compute the expensive parts (explicit families)","text":"<p>If you only want spatial/local intensity and not the entire first-order intensity set:</p> <pre><code>from pictologics import RadiomicsPipeline\n\ncfg = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\"families\": [\"spatial_intensity\", \"local_intensity\"]},\n    },\n]\n\npipeline = RadiomicsPipeline().add_config(\"intensity_extras_only\", cfg)\nout = pipeline.run(\"path/to/image.nii.gz\", \"path/to/mask.nii.gz\", config_names=[\"intensity_extras_only\"])\nprint(out[\"intensity_extras_only\"].head())\n</code></pre>"},{"location":"user_guide/pipeline/#example-4-ivh-with-physical-unit-mapping-advanced","title":"Example 4: IVH with physical-unit mapping (advanced)","text":"<p>When you discretise with FBS, you can map IVH to physical units by passing <code>bin_width</code> and <code>min_val</code>.</p> <pre><code>from pictologics import RadiomicsPipeline\n\ncfg = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBS\", \"bin_width\": 25.0, \"min_val\": -1000}},\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"ivh\"],\n            \"ivh_params\": {\n                \"bin_width\": 25.0,\n                \"min_val\": -1000,\n                \"target_range_max\": 400,\n            },\n        },\n    },\n]\n\npipeline = RadiomicsPipeline().add_config(\"ivh_hu\", cfg)\nout = pipeline.run(\"path/to/image.nii.gz\", \"path/to/mask.nii.gz\", config_names=[\"ivh_hu\"])\nprint(out[\"ivh_hu\"].head())\n</code></pre>"},{"location":"user_guide/pipeline/#example-5-texture-with-ngldm-tolerance-ngldm_alpha","title":"Example 5: Texture with NGLDM tolerance (<code>ngldm_alpha</code>)","text":"<p>IBSI default is <code>ngldm_alpha=0</code> (exact match). If you want tolerance of \u00b11 grey level, set <code>ngldm_alpha=1</code>.</p> <pre><code>from pictologics import RadiomicsPipeline\n\ncfg = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"texture\"],\n            \"texture_matrix_params\": {\"ngldm_alpha\": 1},\n        },\n    },\n]\n\npipeline = RadiomicsPipeline().add_config(\"texture_ngldm_tolerant\", cfg)\nout = pipeline.run(\"path/to/image.nii.gz\", \"path/to/mask.nii.gz\", config_names=[\"texture_ngldm_tolerant\"])\nprint(out[\"texture_ngldm_tolerant\"].head())\n</code></pre>"},{"location":"user_guide/pipeline/#example-custom-ct-pipeline","title":"Example: Custom CT Pipeline","text":"<pre><code>custom_config = [\n    # 1. Resample to 1mm isotropic\n    {\n        \"step\": \"resample\",\n        \"params\": {\"new_spacing\": (1.0, 1.0, 1.0)}\n    },\n    # 2. Restrict to soft tissue window (-150 to 250 HU)\n    {\n        \"step\": \"resegment\",\n        \"params\": {\"range_min\": -150, \"range_max\": 250}\n    },\n    # 3. Discretise with Fixed Bin Number = 64\n    {\n        \"step\": \"discretise\",\n        \"params\": {\"method\": \"FBN\", \"n_bins\": 64}\n    },\n    # 4. Extract everything\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"]}\n    }\n]\n\npipeline = RadiomicsPipeline()\npipeline.add_config(\"my_custom_ct\", custom_config)\nresults = pipeline.run(image, mask, config_names=[\"my_custom_ct\"])\n</code></pre>"},{"location":"user_guide/pipeline/#logging","title":"Logging","text":"<p>The pipeline maintains a detailed log of every step executed, including parameters and any errors encountered. This is vital for auditing and debugging.</p> <pre><code># After running the pipeline\npipeline.save_log(\"pipeline_execution_log.json\")\n</code></pre> <p>The log file contains:</p> <ul> <li>Timestamp</li> <li>Subject ID</li> <li>Configuration Name</li> <li>List of executed steps with their parameters</li> <li>Status of each step</li> </ul>"},{"location":"user_guide/quickstart/","title":"Quick Start Guide","text":"<p>This guide will walk you through the process of extracting radiomic features from a medical image using Pictologics.</p>"},{"location":"user_guide/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have installed the package: <pre><code>pip install pictologics\n</code></pre></p> <p>You will need: 1.  A medical image (e.g., <code>image.nii.gz</code>, DICOM folder, or a single DICOM file). 2.  Optionally a corresponding mask/segmentation (e.g., <code>mask.nii.gz</code>).</p> <p>Note</p> <p>Mask is optional If you omit <code>mask</code> (or pass <code>mask=None</code> / <code>mask=\"\"</code>), Pictologics treats the entire image as the ROI by generating a full (all-ones) mask internally. This can be useful for certain workflows (see the Case examples page), but it may not be scientifically appropriate for all studies.</p>"},{"location":"user_guide/quickstart/#method-1-the-radiomics-pipeline-recommended","title":"Method 1: The Radiomics Pipeline (Recommended)","text":"<p>For reproducible research and standardisation, we recommend using the <code>RadiomicsPipeline</code>. This ensures that all preprocessing steps (resampling, resegmentation, discretisation) are applied consistently.</p> <p>Pictologics includes a set of Standard Configurations commonly used in radiomic analyses. You can run all of them with a single command.</p> <p>Note</p> <p>Default performance behavior The built-in <code>standard_*</code> configurations disable the two most time-intensive intensity extras by default: spatial intensity (Moran\u2019s I / Geary\u2019s C) and local intensity peak features.</p> <p>If you need these metrics, see the customization examples below.</p> <pre><code>from pictologics import RadiomicsPipeline, format_results, save_results\n\n# 1. Initialize the pipeline\npipeline = RadiomicsPipeline()\n\n# 2. Run the \"all_standard\" configurations\nresults = pipeline.run(\n    image=\"path/to/image.nii.gz\",\n    mask=\"path/to/mask.nii.gz\",\n    subject_id=\"Subject_001\",\n    config_names=[\"all_standard\"]\n)\n\n# 3. Format and Export Results\nprint(f\"Successfully ran {len(results)} configurations.\")\n\n#  Option A: Get a flat dictionary (Wide Format)\n#  Inject subject ID or other metadata directly into the row\nrow = format_results(\n    results, \n    fmt=\"wide\", \n    meta={\"subject_id\": \"Subject_001\", \"group\": \"control\"}\n)\n\n#  Option B: Save to CSV immediately\n#  save_results handles list of dicts/dataframes automatically\nsave_results([row], \"results.csv\")\n</code></pre>"},{"location":"user_guide/quickstart/#working-with-results","title":"Working with results","text":"<p>The <code>pictologics.results</code> module makes it easy to handle outputs, especially for batch processing.</p> <ol> <li> <p>Wide Format: One row per case, huge column count (e.g. <code>standard_fbn_32__original_glcm_Energy</code>).     <pre><code>row = format_results(results, fmt=\"wide\", meta={\"id\": \"case1\"})\n</code></pre></p> </li> <li> <p>Long Format: Tidy data, one row per feature. Great for analysis in R/Seaborn.     <pre><code>df = format_results(results, fmt=\"long\", meta={\"id\": \"case1\"}, output_type=\"pandas\")\n# Columns: [id, config, feature_name, value]\n</code></pre></p> </li> </ol>"},{"location":"user_guide/quickstart/#batch-processing-pattern","title":"Batch Processing Pattern","text":"<pre><code>all_rows = []\nfor file in image_files:\n    res = pipeline.run(image=file, ...)\n    # Format and collect\n    all_rows.append(format_results(res, fmt=\"wide\", meta={\"filename\": file.name}))\n\n# Save everything at once (automatically merges columns)\nsave_results(all_rows, \"full_study_results.csv\")\n</code></pre>"},{"location":"user_guide/quickstart/#customizing-the-pipeline","title":"Customizing the pipeline","text":"<p>You can define your own steps to enable advanced features or change parameters.</p> <pre><code>from pictologics import RadiomicsPipeline\n\n# Define a config ensuring 0.5mm isotropic pixels and enabling robust texture extraction\ncfg = [\n    {\"step\": \"resample\", \"params\": {\"new_spacing\": (0.5, 0.5, 0.5), \"round_intensities\": True}},\n    {\"step\": \"discretise\", \"params\": {\"method\": \"FBN\", \"n_bins\": 32}},\n    {\n        \"step\": \"extract_features\",\n        \"params\": {\n            \"families\": [\"intensity\", \"morphology\", \"texture\", \"histogram\", \"ivh\"],\n            \"include_spatial_intensity\": True,  # Enable Moran's I / Geary's C\n            \"include_local_intensity\": True,    # Enable local intensity peaks\n        },\n    },\n]\n\npipeline = RadiomicsPipeline().add_config(\"my_custom_config\", cfg)\n# ... run as normal\n</code></pre>"},{"location":"user_guide/quickstart/#performance-notes-practical","title":"Performance notes (practical)","text":"<ul> <li>Spatial/local intensity can be extremely slow on large ROIs. If you do not need them, keep <code>include_spatial_intensity=False</code> and <code>include_local_intensity=False</code>.</li> <li>Texture requires discretisation. If you request <code>\"texture\"</code> without discretising first, the pipeline can temporarily discretise only if you provide <code>texture_settings</code> in <code>extract_features</code> (e.g., <code>{\"method\": \"FBN\", \"n_bins\": 32}</code>). However, doing this repeatedly for every texture family is slower than a single <code>discretise</code> step.</li> <li>If you are working with large 3D images, consider resampling to a coarser spacing for exploratory work.</li> </ul>"},{"location":"user_guide/quickstart/#method-2-step-by-step-manual-extraction","title":"Method 2: Step-by-Step Manual Extraction","text":"<p>If you want to understand the underlying process or need granular control over a specific function, you can call the feature extraction functions directly.</p> <p>Create a new Python file (e.g., <code>extract.py</code>) and add the following code:</p> <pre><code>import numpy as np\nfrom pictologics import load_image\nfrom pictologics.preprocessing import discretise_image, resample_image\nfrom pictologics.features.intensity import calculate_intensity_features\nfrom pictologics.features.morphology import calculate_morphology_features\nfrom pictologics.features.texture import calculate_all_texture_features\n\n# 1. Load Data\n# ---------------------------------------------------------\nprint(\"Loading data...\")\nimage = load_image(\"path/to/image.nii.gz\")\nmask = load_image(\"path/to/mask.nii.gz\")\n\n# 2. Preprocessing (Optional but Recommended)\n# ---------------------------------------------------------\n# Resample to isotropic 1x1x1 mm spacing for standardisation\nprint(\"Resampling...\")\nimage = resample_image(image, new_spacing=(1.0, 1.0, 1.0))\nmask = resample_image(mask, new_spacing=(1.0, 1.0, 1.0), interpolation=\"nearest\")\n\n# 3. Extract Morphology Features\n# ---------------------------------------------------------\nprint(\"Calculating morphology...\")\nmorph_features = calculate_morphology_features(mask)\n\n# 4. Extract Intensity Features\n# ---------------------------------------------------------\nprint(\"Calculating intensity...\")\n# Get voxels inside the mask\nmasked_voxels = image.array[mask.array == 1]\nintensity_features = calculate_intensity_features(masked_voxels)\n\n# 5. Extract Texture Features\n# ---------------------------------------------------------\nprint(\"Calculating texture...\")\n# Texture requires discretisation (binning)\n# Fixed Bin Number (FBN) with 32 bins is a common choice\ndisc_image = discretise_image(image, method=\"FBN\", n_bins=32, roi_mask=mask)\n\ntexture_features = calculate_all_texture_features(\n    disc_array=disc_image.array,\n    mask_array=mask.array,\n    n_bins=32\n)\n\n# 6. Combine and Print Results\n# ---------------------------------------------------------\nall_features = {**morph_features, **intensity_features, **texture_features}\n\nprint(\"\\n--- Extraction Complete ---\")\nprint(f\"Total Features: {len(all_features)}\")\nprint(f\"Volume (RNU0): {all_features.get('volume_RNU0', 'N/A')} mm^3\")\nprint(f\"Mean Intensity (Q4LE): {all_features.get('mean_intensity_Q4LE', 'N/A')}\")\nprint(f\"Contrast (ACUI): {all_features.get('contrast_ACUI', 'N/A')}\")\n</code></pre>"},{"location":"user_guide/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Read the Pipeline guide for configuration patterns and advanced parameter pass-through.</li> <li>See the Benchmarks to understand performance and compliance.</li> </ul>"}]}